<!DOCTYPE html><html lang="en" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Energy-Based Modeling library for PyTorch"><meta name="author" content="Soran Ghaderi"><link href="https://soran-ghaderi.github.io/torchebm/0.5.4.dev3/llm/all/" rel="canonical"><link href="../../api/torchebm/utils/visualization/" rel="prev"><link href="../../examples/" rel="next"><link rel="icon" href="../../assets/images/favicon.svg"><meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>All Symbols - TorchEBM</title><link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css"><link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css"><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M64%20480c-35.3%200-64-28.7-64-64V96c0-35.3%2028.7-64%2064-64h320c35.3%200%2064%2028.7%2064%2064v213.5c0%2017-6.7%2033.3-18.7%2045.3L322.7%20461.3c-12%2012-28.3%2018.7-45.3%2018.7zm325.5-176H296c-13.3%200-24%2010.7-24%2024v93.5z%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M384%20512H96c-53%200-96-43-96-96V96C0%2043%2043%200%2096%200h304c26.5%200%2048%2021.5%2048%2048v288c0%2020.9-13.4%2038.7-32%2045.3V448c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032zM96%20384c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h256v-64zm32-232c0%2013.3%2010.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24H152c-13.3%200-24%2010.7-24%2024m24%2072c-13.3%200-24%2010.7-24%2024s10.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24z%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m-32-352a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200m-8%2064h48c13.3%200%2024%2010.7%2024%2024v88h8c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-80c-13.3%200-24-10.7-24-24s10.7-24%2024-24h24v-64h-24c-13.3%200-24-10.7-24-24s10.7-24%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M461.2%2018.9C472.7%2024%20480%2035.4%20480%2048v416c0%2012.6-7.3%2024-18.8%2029.1s-24.8%203.2-34.3-5.1l-46.6-40.7c-43.6-38.1-98.7-60.3-156.4-63V480c0%2017.7-14.3%2032-32%2032h-32c-17.7%200-32-14.3-32-32v-96C57.3%20384%200%20326.7%200%20256s57.3-128%20128-128h84.5c61.8-.2%20121.4-22.7%20167.9-63.3L427%2024c9.4-8.3%2022.9-10.2%2034.3-5.1zM224%20320v.2c70.3%202.7%20137.8%2028.5%20192%2073.4V118.3c-54.2%2044.9-121.7%2070.7-192%2073.4z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M434.8%2070.1c14.3%2010.4%2017.5%2030.4%207.1%2044.7l-256%20352c-5.5%207.6-14%2012.3-23.4%2013.1s-18.5-2.7-25.1-9.3l-128-128c-12.5-12.5-12.5-32.8%200-45.3s32.8-12.5%2045.3%200l101.5%20101.5%20234-321.7c10.4-14.3%2030.4-17.5%2044.7-7.1z%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m0-336c-17.7%200-32%2014.3-32%2032%200%2013.3-10.7%2024-24%2024s-24-10.7-24-24c0-44.2%2035.8-80%2080-80s80%2035.8%2080%2080c0%2047.2-36%2067.2-56%2074.5v3.8c0%2013.3-10.7%2024-24%2024s-24-10.7-24-24v-8.1c0-20.5%2014.8-35.2%2030.1-40.2%206.4-2.1%2013.2-5.5%2018.2-10.3%204.3-4.2%207.7-10%207.7-19.6%200-17.7-14.3-32-32-32zm-32%20192a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%200c14.7%200%2028.2%208.1%2035.2%2021l216%20400c6.7%2012.4%206.4%2027.4-.8%2039.5S486.1%20480%20472%20480H40c-14.1%200-27.2-7.4-34.4-19.5s-7.5-27.1-.8-39.5l216-400c7-12.9%2020.5-21%2035.2-21m0%20352a32%2032%200%201%200%200%2064%2032%2032%200%201%200%200-64m0-192c-18.2%200-32.7%2015.5-31.4%2033.7l7.4%20104c.9%2012.5%2011.4%2022.3%2023.9%2022.3%2012.6%200%2023-9.7%2023.9-22.3l7.4-104c1.3-18.2-13.1-33.7-31.4-33.7z%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20576%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M480-16c6.9%200%2013%204.4%2015.2%2010.9l13.5%2040.4%2040.4%2013.5C555.6%2051%20560%2057.1%20560%2064s-4.4%2013-10.9%2015.2l-40.4%2013.5-13.5%2040.4c-2.2%206.5-8.3%2010.9-15.2%2010.9s-13-4.4-15.2-10.9l-13.5-40.4-40.4-13.5C404.4%2077%20400%2070.9%20400%2064s4.4-13%2010.9-15.2l40.4-13.5%2013.5-40.4C467-11.6%20473.1-16%20480-16M321.4%2097.4c12.5-12.5%2032.8-12.5%2045.3%200l80%2080c12.5%2012.5%2012.5%2032.8%200%2045.3l-10.9%2010.9c7.9%2022%2012.2%2045.7%2012.2%2070.5%200%20114.9-93.1%20208-208%20208S32%20418.9%2032%20304%20125.1%2096%20240%2096c24.7%200%2048.5%204.3%2070.5%2012.3zM144%20304c0-53%2043-96%2096-96%2013.3%200%2024-10.7%2024-24s-10.7-24-24-24c-79.5%200-144%2064.5-144%20144%200%2013.3%2010.7%2024%2024%2024s24-10.7%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M416%20427.4c58.5-44%2096-111.6%2096-187.4C512%20107.5%20397.4%200%20256%200S0%20107.5%200%20240c0%2075.8%2037.5%20143.4%2096%20187.4V464c0%2026.5%2021.5%2048%2048%2048h32v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h64v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h32c26.5%200%2048-21.5%2048-48zM96%20256a64%2064%200%201%201%20128%200%2064%2064%200%201%201-128%200m256-64a64%2064%200%201%201%200%20128%2064%2064%200%201%201%200-128%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M352%200c0-17.7-14.3-32-32-32s-32%2014.3-32%2032v64h-96c-53%200-96%2043-96%2096v224c0%2053%2043%2096%2096%2096h256c53%200%2096-43%2096-96V160c0-53-43-96-96-96h-96zM160%20368c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24M224%20176a48%2048%200%201%201%200%2096%2048%2048%200%201%201%200-96m144%2048a48%2048%200%201%201%2096%200%2048%2048%200%201%201-96%200m-304%200c0-17.7-14.3-32-32-32S0%20206.3%200%20224v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32zm544-32c-17.7%200-32%2014.3-32%2032v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32v-96c0-17.7-14.3-32-32-32%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M288%200H128c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032v151.5L7.5%20426.3C2.6%20435%200%20444.7%200%20454.7%200%20486.4%2025.6%20512%2057.3%20512h333.4c31.6%200%2057.3-25.6%2057.3-57.3%200-10-2.6-19.8-7.5-28.4L320%20215.5V64c17.7%200%2032-14.3%2032-32S337.7%200%20320%200zm-96%20215.5V64h64v151.5c0%2011.1%202.9%2022.1%208.4%2031.8L306%20320H142l41.6-72.7c5.5-9.7%208.4-20.6%208.4-31.8%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M0%20216C0%20149.7%2053.7%2096%20120%2096h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064H64c-35.3%200-64-28.7-64-64zm256%200c0-66.3%2053.7-120%20120-120h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064h-64c-35.3%200-64-28.7-64-64z%22/%3E%3C/svg%3E');}</style><link rel="stylesheet" href="../../assets/_mkdocstrings.css"><link rel="stylesheet" href="../../stylesheets/extra.css"><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config",""),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="lime"> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" for="__drawer"></label> <div data-md-component="skip"> <a href="#torchebm-api-reference" class="md-skip"> Skip to content </a> </div> <div data-md-component="announce"> </div> <div data-md-color-scheme="default" data-md-component="outdated" hidden> </div> <header class="md-header" data-md-component="header"> <nav class="md-header__inner md-grid" aria-label="Header"> <a href="../.." title="TorchEBM" class="md-header__button md-logo" aria-label="TorchEBM" data-md-component="logo"> <img src="../../assets/images/nabla_icon.svg" alt="logo"> </a> <label class="md-header__button md-icon" for="__drawer"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class="md-header__title" data-md-component="header-title"> <div class="md-header__ellipsis"> <div class="md-header__topic"> <span class="md-ellipsis"> TorchEBM </span> </div> <div class="md-header__topic" data-md-component="header-topic"> <span class="md-ellipsis"> All Symbols </span> </div> </div> </div> <form class="md-header__option" data-md-component="palette"> <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="lime" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0"> <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="lime" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1"> <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for="__search"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class="md-search" data-md-component="search" role="dialog"> <label class="md-search__overlay" for="__search"></label> <div class="md-search__inner" role="search"> <form class="md-search__form" name="search"> <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required> <label class="md-search__icon md-icon" for="__search"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class="md-search__options" aria-label="Search"> <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text data-md-component="search-share" tabindex="-1"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class="md-search__suggest" data-md-component="search-suggest"></div> </form> <div class="md-search__output"> <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix> <div class="md-search-result" data-md-component="search-result"> <div class="md-search-result__meta"> Initializing search </div> <ol class="md-search-result__list" role="presentation"></ol> </div> </div> </div> </div> </div> <div class="md-header__source"> <a href="https://github.com/soran-ghaderi/torchebm" title="Go to repository" class="md-source" data-md-component="source"> <div class="md-source__icon md-icon"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class="md-source__repository"> soran-ghaderi/torchebm </div> </a> </div> </nav> </header> <div class="md-container" data-md-component="container"> <nav class="md-tabs" aria-label="Tabs" data-md-component="tabs"> <div class="md-grid"> <ul class="md-tabs__list"> <li class="md-tabs__item"> <a href="../.." class="md-tabs__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.906.384a1.75 1.75 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019a1.75 1.75 0 0 1-1.75 1.75h-2.5a.75.75 0 0 1-.75-.75V8.72H6v5.25a.75.75 0 0 1-.75.75h-2.5A1.75 1.75 0 0 1 1 12.97V5.95c0-.531.242-1.034.657-1.366z"></path></svg> Home </a> </li> <li class="md-tabs__item"> <a href="../../tutorials/" class="md-tabs__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 3 1 9l11 6 9-4.91V17h2V9M5 13.18v4L12 21l7-3.82v-4L12 17z"></path></svg> Tutorials </a> </li> <li class="md-tabs__item"> <a href="../../api/" class="md-tabs__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 9h5.5L13 3.5zM6 2h8l6 6v12a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V4c0-1.11.89-2 2-2m9 16v-2H6v2zm3-4v-2H6v2z"></path></svg> API Reference </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href="../index.md" class="md-tabs__link"> LLM API Reference </a> </li> <li class="md-tabs__item"> <a href="../../examples/" class="md-tabs__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M64 80c-8.8 0-16 7.2-16 16v320c0 8.8 7.2 16 16 16h320c8.8 0 16-7.2 16-16V96c0-8.8-7.2-16-16-16zM0 96c0-35.3 28.7-64 64-64h320c35.3 0 64 28.7 64 64v320c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64zm128 32a32 32 0 1 1 0 64 32 32 0 1 1 0-64m136 72c8.5 0 16.4 4.5 20.7 11.8l80 136c4.4 7.4 4.4 16.6.1 24.1S352.6 384 344 384H104c-8.9 0-17.2-5-21.3-12.9s-3.5-17.5 1.6-24.8l56-80c4.5-6.4 11.8-10.2 19.7-10.2s15.2 3.8 19.7 10.2l17.2 24.6 46.5-79c4.3-7.3 12.2-11.8 20.7-11.8z"></path></svg> Examples </a> </li> <li class="md-tabs__item"> <a href="../../developer_guide/" class="md-tabs__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5zM6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5"></path></svg> Developer Guide </a> </li> <li class="md-tabs__item"> <a href="../../blog/" class="md-tabs__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 5v14H5V5zm2-2H3v18h18zm-4 14H7v-1h10zm0-2H7v-1h10zm0-3H7V7h10z"></path></svg> Blog </a> </li> <li class="md-tabs__item"> <a href="../../faq/" class="md-tabs__link"> FAQ </a> </li> </ul> </div> </nav> <main class="md-main" data-md-component="main"> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"> <div class="md-sidebar__scrollwrap"> <div class="md-sidebar__inner"> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0"> <label class="md-nav__title" for="__drawer"> <a href="../.." title="TorchEBM" class="md-nav__button md-logo" aria-label="TorchEBM" data-md-component="logo"> <img src="../../assets/images/nabla_icon.svg" alt="logo"> </a> TorchEBM </label> <div class="md-nav__source"> <a href="https://github.com/soran-ghaderi/torchebm" title="Go to repository" class="md-source" data-md-component="source"> <div class="md-source__icon md-icon"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class="md-source__repository"> soran-ghaderi/torchebm </div> </a> </div> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../.." class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.906.384a1.75 1.75 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019a1.75 1.75 0 0 1-1.75 1.75h-2.5a.75.75 0 0 1-.75-.75V8.72H6v5.25a.75.75 0 0 1-.75.75h-2.5A1.75 1.75 0 0 1 1 12.97V5.95c0-.531.242-1.034.657-1.366z"></path></svg> <span class="md-ellipsis"> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2"> <div class="md-nav__link md-nav__container"> <a href="../../tutorials/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 3 1 9l11 6 9-4.91V17h2V9M5 13.18v4L12 21l7-3.82v-4L12 17z"></path></svg> <span class="md-ellipsis"> Tutorials </span> </a> <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_2"> <span class="md-nav__icon md-icon"></span> Tutorials </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../tutorials/getting_started/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m13.13 22.19-1.63-3.83c1.57-.58 3.04-1.36 4.4-2.27zM5.64 12.5l-3.83-1.63 6.1-2.77C7 9.46 6.22 10.93 5.64 12.5M21.61 2.39S16.66.269 11 5.93c-2.19 2.19-3.5 4.6-4.35 6.71-.28.75-.09 1.57.46 2.13l2.13 2.12c.55.56 1.37.74 2.12.46A19.1 19.1 0 0 0 18.07 13c5.66-5.66 3.54-10.61 3.54-10.61m-7.07 7.07c-.78-.78-.78-2.05 0-2.83s2.05-.78 2.83 0c.77.78.78 2.05 0 2.83s-2.05.78-2.83 0m-5.66 7.07-1.41-1.41zM6.24 22l3.64-3.64c-.34-.09-.67-.24-.97-.45L4.83 22zM2 22h1.41l4.77-4.76-1.42-1.41L2 20.59zm0-2.83 4.09-4.08c-.21-.3-.36-.62-.45-.97L2 17.76z"></path></svg> <span class="md-ellipsis"> Getting Started </span> </a> </li> <li class="md-nav__item"> <a href="../../tutorials/samplers/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M2 2h2v18h18v2H2zm7 8a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3m4-8a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3m5 10a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3"></path></svg> <span class="md-ellipsis"> Samplers </span> </a> </li> <li class="md-nav__item"> <a href="../../tutorials/loss_functions/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2m-6 4.1L14.1 6l1.4 1.4L16.9 6 18 7.1l-1.4 1.4L18 9.9 16.9 11l-1.4-1.4-1.4 1.4L13 9.9l1.4-1.4zm-6.8.6h5v1.5h-5zm5.3 8.3h-2v2H8v-2H6v-1.5h2v-2h1.5v2h2zm6.5 1.2h-5v-1.5h5zm0-2.4h-5v-1.5h5z"></path></svg> <span class="md-ellipsis"> Loss Functions </span> </a> </li> <li class="md-nav__item"> <a href="../../tutorials/training/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 3 1 9l11 6 9-4.91V17h2V9M5 13.18v4L12 21l7-3.82v-4L12 17z"></path></svg> <span class="md-ellipsis"> Training EBMs </span> </a> </li> <li class="md-nav__item"> <a href="../../tutorials/visualization/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 21H2V3h2v16h2v-9h4v9h2V6h4v13h2v-5h4z"></path></svg> <span class="md-ellipsis"> Visualization </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_7"> <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="0"> <span class="md-ellipsis"> Advanced </span> <span class="md-nav__icon md-icon"></span> </label> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_2_7"> <span class="md-nav__icon md-icon"></span> Advanced </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../tutorials/custom_neural_networks/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21.33 12.91c.09 1.55-.62 3.04-1.89 3.95l.77 1.49c.23.45.26.98.06 1.45-.19.47-.58.84-1.06 1l-.79.25a1.687 1.687 0 0 1-1.86-.55L14.44 18c-.89-.15-1.73-.53-2.44-1.1-.5.15-1 .23-1.5.23-.88 0-1.76-.27-2.5-.79-.53.16-1.07.23-1.62.22-.79.01-1.57-.15-2.3-.45a4.1 4.1 0 0 1-2.43-3.61c-.08-.72.04-1.45.35-2.11-.29-.75-.32-1.57-.07-2.33C2.3 7.11 3 6.32 3.87 5.82c.58-1.69 2.21-2.82 4-2.7 1.6-1.5 4.05-1.66 5.83-.37.42-.11.86-.17 1.3-.17 1.36-.03 2.65.57 3.5 1.64 2.04.53 3.5 2.35 3.58 4.47.05 1.11-.25 2.2-.86 3.13.07.36.11.72.11 1.09m-5-1.41c.57.07 1.02.5 1.02 1.07a1 1 0 0 1-1 1h-.63c-.32.9-.88 1.69-1.62 2.29.25.09.51.14.77.21 5.13-.07 4.53-3.2 4.53-3.25a2.59 2.59 0 0 0-2.69-2.49 1 1 0 0 1-1-1 1 1 0 0 1 1-1c1.23.03 2.41.49 3.33 1.3.05-.29.08-.59.08-.89-.06-1.24-.62-2.32-2.87-2.53-1.25-2.96-4.4-1.32-4.4-.4-.03.23.21.72.25.75a1 1 0 0 1 1 1c0 .55-.45 1-1 1-.53-.02-1.03-.22-1.43-.56-.48.31-1.03.5-1.6.56-.57.05-1.04-.35-1.07-.9a.97.97 0 0 1 .88-1.1c.16-.02.94-.14.94-.77 0-.66.25-1.29.68-1.79-.92-.25-1.91.08-2.91 1.29C6.75 5 6 5.25 5.45 7.2 4.5 7.67 4 8 3.78 9c1.08-.22 2.19-.13 3.22.25.5.19.78.75.59 1.29-.19.52-.77.78-1.29.59-.73-.32-1.55-.34-2.3-.06-.32.27-.32.83-.32 1.27 0 .74.37 1.43 1 1.83.53.27 1.12.41 1.71.4q-.225-.39-.39-.81a1.038 1.038 0 0 1 1.96-.68c.4 1.14 1.42 1.92 2.62 2.05 1.37-.07 2.59-.88 3.19-2.13.23-1.38 1.34-1.5 2.56-1.5m2 7.47-.62-1.3-.71.16 1 1.25zm-4.65-8.61a1 1 0 0 0-.91-1.03c-.71-.04-1.4.2-1.93.67-.57.58-.87 1.38-.84 2.19a1 1 0 0 0 1 1c.57 0 1-.45 1-1 0-.27.07-.54.23-.76.12-.1.27-.15.43-.15.55.03 1.02-.38 1.02-.92"></path></svg> <span class="md-ellipsis"> Custom Neural Networks </span> </a> </li> <li class="md-nav__item"> <a href="../../tutorials/parallel_sampling/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M2 3v6h2.95l2 6H6v6h6v-4.59L17.41 11H22V5h-6v4.57L10.59 15H9.06l-2-6H8V3zm2 2h2v2H4zm14 2h2v2h-2zm0 8v3h-3v2h3v3h2v-3h3v-2h-3v-3zM8 17h2v2H8z"></path></svg> <span class="md-ellipsis"> Parallel Sampling </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 9h5.5L13 3.5zM6 2h8l6 6v12a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V4c0-1.11.89-2 2-2m9 16v-2H6v2zm3-4v-2H6v2z"></path></svg> <span class="md-ellipsis"> API Reference </span> </a> <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3"> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Core </span> </a> <label class="md-nav__link " for="__nav_3_2" id="__nav_3_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2"> <span class="md-nav__icon md-icon"></span> Core </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/base_integrator/" class="md-nav__link "> <span class="md-ellipsis"> Base_integrator </span> </a> <label class="md-nav__link " for="__nav_3_2_2" id="__nav_3_2_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_2"> <span class="md-nav__icon md-icon"></span> Base_integrator </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_integrator/classes/BaseIntegrator/" class="md-nav__link"> <span class="md-ellipsis"> BaseIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/base_interpolant/" class="md-nav__link "> <span class="md-ellipsis"> Base_interpolant </span> </a> <label class="md-nav__link " for="__nav_3_2_3" id="__nav_3_2_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_3"> <span class="md-nav__icon md-icon"></span> Base_interpolant </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_interpolant/classes/BaseInterpolant/" class="md-nav__link"> <span class="md-ellipsis"> BaseInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_4"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/base_loss/" class="md-nav__link "> <span class="md-ellipsis"> Base_loss </span> </a> <label class="md-nav__link " for="__nav_3_2_4" id="__nav_3_2_4_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_4_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_4"> <span class="md-nav__icon md-icon"></span> Base_loss </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/" class="md-nav__link"> <span class="md-ellipsis"> BaseContrastiveDivergence </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_loss/classes/BaseLoss/" class="md-nav__link"> <span class="md-ellipsis"> BaseLoss </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_loss/classes/BaseScoreMatching/" class="md-nav__link"> <span class="md-ellipsis"> BaseScoreMatching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_5"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/base_model/" class="md-nav__link "> <span class="md-ellipsis"> Base_model </span> </a> <label class="md-nav__link " for="__nav_3_2_5" id="__nav_3_2_5_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_5_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_5"> <span class="md-nav__icon md-icon"></span> Base_model </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/AckleyEnergy/" class="md-nav__link"> <span class="md-ellipsis"> AckleyEnergy </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/AckleyModel/" class="md-nav__link"> <span class="md-ellipsis"> AckleyModel </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/BaseEnergyFunction/" class="md-nav__link"> <span class="md-ellipsis"> BaseEnergyFunction </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/BaseModel/" class="md-nav__link"> <span class="md-ellipsis"> BaseModel </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/DoubleWellEnergy/" class="md-nav__link"> <span class="md-ellipsis"> DoubleWellEnergy </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/DoubleWellModel/" class="md-nav__link"> <span class="md-ellipsis"> DoubleWellModel </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/GaussianEnergy/" class="md-nav__link"> <span class="md-ellipsis"> GaussianEnergy </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/GaussianModel/" class="md-nav__link"> <span class="md-ellipsis"> GaussianModel </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/HarmonicEnergy/" class="md-nav__link"> <span class="md-ellipsis"> HarmonicEnergy </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/HarmonicModel/" class="md-nav__link"> <span class="md-ellipsis"> HarmonicModel </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/RastriginEnergy/" class="md-nav__link"> <span class="md-ellipsis"> RastriginEnergy </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/RastriginModel/" class="md-nav__link"> <span class="md-ellipsis"> RastriginModel </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/RosenbrockEnergy/" class="md-nav__link"> <span class="md-ellipsis"> RosenbrockEnergy </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_model/classes/RosenbrockModel/" class="md-nav__link"> <span class="md-ellipsis"> RosenbrockModel </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_6"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/base_sampler/" class="md-nav__link "> <span class="md-ellipsis"> Base_sampler </span> </a> <label class="md-nav__link " for="__nav_3_2_6" id="__nav_3_2_6_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_6_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_6"> <span class="md-nav__icon md-icon"></span> Base_sampler </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_sampler/classes/BaseSampler/" class="md-nav__link"> <span class="md-ellipsis"> BaseSampler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_7"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/base_scheduler/" class="md-nav__link "> <span class="md-ellipsis"> Base_scheduler </span> </a> <label class="md-nav__link " for="__nav_3_2_7" id="__nav_3_2_7_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_7_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_7"> <span class="md-nav__icon md-icon"></span> Base_scheduler </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_scheduler/classes/BaseScheduler/" class="md-nav__link"> <span class="md-ellipsis"> BaseScheduler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_scheduler/classes/ConstantScheduler/" class="md-nav__link"> <span class="md-ellipsis"> ConstantScheduler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_scheduler/classes/CosineScheduler/" class="md-nav__link"> <span class="md-ellipsis"> CosineScheduler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_scheduler/classes/ExponentialDecayScheduler/" class="md-nav__link"> <span class="md-ellipsis"> ExponentialDecayScheduler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_scheduler/classes/LinearScheduler/" class="md-nav__link"> <span class="md-ellipsis"> LinearScheduler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_scheduler/classes/MultiStepScheduler/" class="md-nav__link"> <span class="md-ellipsis"> MultiStepScheduler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_scheduler/classes/WarmupScheduler/" class="md-nav__link"> <span class="md-ellipsis"> WarmupScheduler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_8"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/base_trainer/" class="md-nav__link "> <span class="md-ellipsis"> Base_trainer </span> </a> <label class="md-nav__link " for="__nav_3_2_8" id="__nav_3_2_8_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_8_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_8"> <span class="md-nav__icon md-icon"></span> Base_trainer </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_trainer/classes/BaseTrainer/" class="md-nav__link"> <span class="md-ellipsis"> BaseTrainer </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/core/base_trainer/classes/ContrastiveDivergenceTrainer/" class="md-nav__link"> <span class="md-ellipsis"> ContrastiveDivergenceTrainer </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2_9"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/core/device_mixin/" class="md-nav__link "> <span class="md-ellipsis"> Device_mixin </span> </a> <label class="md-nav__link " for="__nav_3_2_9" id="__nav_3_2_9_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_9_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_2_9"> <span class="md-nav__icon md-icon"></span> Device_mixin </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/core/device_mixin/classes/DeviceMixin/" class="md-nav__link"> <span class="md-ellipsis"> DeviceMixin </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="../../api/torchebm/cuda/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Cuda </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_4"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/datasets/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Datasets </span> </a> <label class="md-nav__link " for="__nav_3_4" id="__nav_3_4_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_4"> <span class="md-nav__icon md-icon"></span> Datasets </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_4_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/datasets/generators/" class="md-nav__link "> <span class="md-ellipsis"> Generators </span> </a> <label class="md-nav__link " for="__nav_3_4_2" id="__nav_3_4_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_4_2"> <span class="md-nav__icon md-icon"></span> Generators </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/BaseSyntheticDataset/" class="md-nav__link"> <span class="md-ellipsis"> BaseSyntheticDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/CheckerboardDataset/" class="md-nav__link"> <span class="md-ellipsis"> CheckerboardDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/CircleDataset/" class="md-nav__link"> <span class="md-ellipsis"> CircleDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/EightGaussiansDataset/" class="md-nav__link"> <span class="md-ellipsis"> EightGaussiansDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/GaussianMixtureDataset/" class="md-nav__link"> <span class="md-ellipsis"> GaussianMixtureDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/GridDataset/" class="md-nav__link"> <span class="md-ellipsis"> GridDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/PinwheelDataset/" class="md-nav__link"> <span class="md-ellipsis"> PinwheelDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/SwissRollDataset/" class="md-nav__link"> <span class="md-ellipsis"> SwissRollDataset </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/datasets/generators/classes/TwoMoonsDataset/" class="md-nav__link"> <span class="md-ellipsis"> TwoMoonsDataset </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/integrators/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Integrators </span> </a> <label class="md-nav__link " for="__nav_3_5" id="__nav_3_5_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_5"> <span class="md-nav__icon md-icon"></span> Integrators </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/integrators/euler_maruyama/" class="md-nav__link "> <span class="md-ellipsis"> Euler_maruyama </span> </a> <label class="md-nav__link " for="__nav_3_5_2" id="__nav_3_5_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_5_2"> <span class="md-nav__icon md-icon"></span> Euler_maruyama </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/integrators/euler_maruyama/classes/EulerMaruyamaIntegrator/" class="md-nav__link"> <span class="md-ellipsis"> EulerMaruyamaIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/integrators/heun/" class="md-nav__link "> <span class="md-ellipsis"> Heun </span> </a> <label class="md-nav__link " for="__nav_3_5_3" id="__nav_3_5_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_5_3"> <span class="md-nav__icon md-icon"></span> Heun </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/integrators/heun/classes/HeunIntegrator/" class="md-nav__link"> <span class="md-ellipsis"> HeunIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="../../api/torchebm/integrators/integrator_utils/" class="md-nav__link"> <span class="md-ellipsis"> Integrator_utils </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5_5"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/integrators/leapfrog/" class="md-nav__link "> <span class="md-ellipsis"> Leapfrog </span> </a> <label class="md-nav__link " for="__nav_3_5_5" id="__nav_3_5_5_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_5_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_5_5"> <span class="md-nav__icon md-icon"></span> Leapfrog </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/integrators/leapfrog/classes/LeapfrogIntegrator/" class="md-nav__link"> <span class="md-ellipsis"> LeapfrogIntegrator </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_6"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/interpolants/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Interpolants </span> </a> <label class="md-nav__link " for="__nav_3_6" id="__nav_3_6_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_6"> <span class="md-nav__icon md-icon"></span> Interpolants </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_6_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/interpolants/cosine/" class="md-nav__link "> <span class="md-ellipsis"> Cosine </span> </a> <label class="md-nav__link " for="__nav_3_6_2" id="__nav_3_6_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_6_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_6_2"> <span class="md-nav__icon md-icon"></span> Cosine </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/interpolants/cosine/classes/CosineInterpolant/" class="md-nav__link"> <span class="md-ellipsis"> CosineInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_6_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/interpolants/linear/" class="md-nav__link "> <span class="md-ellipsis"> Linear </span> </a> <label class="md-nav__link " for="__nav_3_6_3" id="__nav_3_6_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_6_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_6_3"> <span class="md-nav__icon md-icon"></span> Linear </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/interpolants/linear/classes/LinearInterpolant/" class="md-nav__link"> <span class="md-ellipsis"> LinearInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_6_4"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/interpolants/variance_preserving/" class="md-nav__link "> <span class="md-ellipsis"> Variance_preserving </span> </a> <label class="md-nav__link " for="__nav_3_6_4" id="__nav_3_6_4_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_6_4_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_6_4"> <span class="md-nav__icon md-icon"></span> Variance_preserving </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/interpolants/variance_preserving/classes/VariancePreservingInterpolant/" class="md-nav__link"> <span class="md-ellipsis"> VariancePreservingInterpolant </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_7"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/losses/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Losses </span> </a> <label class="md-nav__link " for="__nav_3_7" id="__nav_3_7_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_7"> <span class="md-nav__icon md-icon"></span> Losses </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_7_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/losses/contrastive_divergence/" class="md-nav__link "> <span class="md-ellipsis"> Contrastive_divergence </span> </a> <label class="md-nav__link " for="__nav_3_7_2" id="__nav_3_7_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_7_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_7_2"> <span class="md-nav__icon md-icon"></span> Contrastive_divergence </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/" class="md-nav__link"> <span class="md-ellipsis"> ContrastiveDivergence </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/" class="md-nav__link"> <span class="md-ellipsis"> ParallelTemperingCD </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/" class="md-nav__link"> <span class="md-ellipsis"> PersistentContrastiveDivergence </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_7_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/losses/equilibrium_matching/" class="md-nav__link "> <span class="md-ellipsis"> Equilibrium_matching </span> </a> <label class="md-nav__link " for="__nav_3_7_3" id="__nav_3_7_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_7_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_7_3"> <span class="md-nav__icon md-icon"></span> Equilibrium_matching </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/losses/equilibrium_matching/classes/EquilibriumMatchingLoss/" class="md-nav__link"> <span class="md-ellipsis"> EquilibriumMatchingLoss </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="../../api/torchebm/losses/loss_utils/" class="md-nav__link"> <span class="md-ellipsis"> Loss_utils </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_7_5"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/losses/score_matching/" class="md-nav__link "> <span class="md-ellipsis"> Score_matching </span> </a> <label class="md-nav__link " for="__nav_3_7_5" id="__nav_3_7_5_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_7_5_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_7_5"> <span class="md-nav__icon md-icon"></span> Score_matching </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/losses/score_matching/classes/DenoisingScoreMatching/" class="md-nav__link"> <span class="md-ellipsis"> DenoisingScoreMatching </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/losses/score_matching/classes/ScoreMatching/" class="md-nav__link"> <span class="md-ellipsis"> ScoreMatching </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/losses/score_matching/classes/SlicedScoreMatching/" class="md-nav__link"> <span class="md-ellipsis"> SlicedScoreMatching </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Models </span> </a> <label class="md-nav__link " for="__nav_3_8" id="__nav_3_8_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8"> <span class="md-nav__icon md-icon"></span> Models </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/components/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Components </span> </a> <label class="md-nav__link " for="__nav_3_8_2" id="__nav_3_8_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_8_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8_2"> <span class="md-nav__icon md-icon"></span> Components </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8_2_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/components/embeddings/" class="md-nav__link "> <span class="md-ellipsis"> Embeddings </span> </a> <label class="md-nav__link " for="__nav_3_8_2_2" id="__nav_3_8_2_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_8_2_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8_2_2"> <span class="md-nav__icon md-icon"></span> Embeddings </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/embeddings/classes/LabelEmbedder/" class="md-nav__link"> <span class="md-ellipsis"> LabelEmbedder </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/embeddings/classes/MLPTimestepEmbedder/" class="md-nav__link"> <span class="md-ellipsis"> MLPTimestepEmbedder </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8_2_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/components/heads/" class="md-nav__link "> <span class="md-ellipsis"> Heads </span> </a> <label class="md-nav__link " for="__nav_3_8_2_3" id="__nav_3_8_2_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_8_2_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8_2_3"> <span class="md-nav__icon md-icon"></span> Heads </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/heads/classes/AdaLNZeroPatchHead/" class="md-nav__link"> <span class="md-ellipsis"> AdaLNZeroPatchHead </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8_2_4"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/components/patch/" class="md-nav__link "> <span class="md-ellipsis"> Patch </span> </a> <label class="md-nav__link " for="__nav_3_8_2_4" id="__nav_3_8_2_4_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_8_2_4_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8_2_4"> <span class="md-nav__icon md-icon"></span> Patch </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/patch/classes/ConvPatchEmbed2d/" class="md-nav__link"> <span class="md-ellipsis"> ConvPatchEmbed2d </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/positional/" class="md-nav__link"> <span class="md-ellipsis"> Positional </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8_2_6"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/components/transformer/" class="md-nav__link "> <span class="md-ellipsis"> Transformer </span> </a> <label class="md-nav__link " for="__nav_3_8_2_6" id="__nav_3_8_2_6_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_8_2_6_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8_2_6"> <span class="md-nav__icon md-icon"></span> Transformer </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/transformer/classes/AdaLNZeroBlock/" class="md-nav__link"> <span class="md-ellipsis"> AdaLNZeroBlock </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/transformer/classes/FeedForward/" class="md-nav__link"> <span class="md-ellipsis"> FeedForward </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/models/components/transformer/classes/MultiheadSelfAttention/" class="md-nav__link"> <span class="md-ellipsis"> MultiheadSelfAttention </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/conditional_transformer_2d/" class="md-nav__link "> <span class="md-ellipsis"> Conditional_transformer_2d </span> </a> <label class="md-nav__link " for="__nav_3_8_3" id="__nav_3_8_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_8_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8_3"> <span class="md-nav__icon md-icon"></span> Conditional_transformer_2d </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/models/conditional_transformer_2d/classes/ConditionalTransformer2D/" class="md-nav__link"> <span class="md-ellipsis"> ConditionalTransformer2D </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_8_4"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/models/wrappers/" class="md-nav__link "> <span class="md-ellipsis"> Wrappers </span> </a> <label class="md-nav__link " for="__nav_3_8_4" id="__nav_3_8_4_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_8_4_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_8_4"> <span class="md-nav__icon md-icon"></span> Wrappers </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/models/wrappers/classes/LabelClassifierFreeGuidance/" class="md-nav__link"> <span class="md-ellipsis"> LabelClassifierFreeGuidance </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_9"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/samplers/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Samplers </span> </a> <label class="md-nav__link " for="__nav_3_9" id="__nav_3_9_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_9"> <span class="md-nav__icon md-icon"></span> Samplers </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_9_2"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/samplers/flow/" class="md-nav__link "> <span class="md-ellipsis"> Flow </span> </a> <label class="md-nav__link " for="__nav_3_9_2" id="__nav_3_9_2_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_9_2_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_9_2"> <span class="md-nav__icon md-icon"></span> Flow </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/samplers/flow/classes/FlowSampler/" class="md-nav__link"> <span class="md-ellipsis"> FlowSampler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/samplers/flow/classes/PredictionType/" class="md-nav__link"> <span class="md-ellipsis"> PredictionType </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_9_3"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/samplers/gradient_descent/" class="md-nav__link "> <span class="md-ellipsis"> Gradient_descent </span> </a> <label class="md-nav__link " for="__nav_3_9_3" id="__nav_3_9_3_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_9_3_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_9_3"> <span class="md-nav__icon md-icon"></span> Gradient_descent </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/samplers/gradient_descent/classes/GradientDescentSampler/" class="md-nav__link"> <span class="md-ellipsis"> GradientDescentSampler </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/samplers/gradient_descent/classes/NesterovSampler/" class="md-nav__link"> <span class="md-ellipsis"> NesterovSampler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_9_4"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/samplers/hmc/" class="md-nav__link "> <span class="md-ellipsis"> Hmc </span> </a> <label class="md-nav__link " for="__nav_3_9_4" id="__nav_3_9_4_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_9_4_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_9_4"> <span class="md-nav__icon md-icon"></span> Hmc </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/" class="md-nav__link"> <span class="md-ellipsis"> HamiltonianMonteCarlo </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_9_5"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/samplers/langevin_dynamics/" class="md-nav__link "> <span class="md-ellipsis"> Langevin_dynamics </span> </a> <label class="md-nav__link " for="__nav_3_9_5" id="__nav_3_9_5_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_9_5_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_9_5"> <span class="md-nav__icon md-icon"></span> Langevin_dynamics </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/" class="md-nav__link"> <span class="md-ellipsis"> LangevinDynamics </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_10"> <div class="md-nav__link md-nav__container"> <a href="../../api/torchebm/utils/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.75 1.75 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.75 1.75 0 0 1 1.75 0m-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.25.25 0 0 0-.25 0m.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z"></path></svg> <span class="md-ellipsis"> Utils </span> </a> <label class="md-nav__link " for="__nav_3_10" id="__nav_3_10_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_10_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_3_10"> <span class="md-nav__icon md-icon"></span> Utils </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../api/torchebm/utils/image/" class="md-nav__link"> <span class="md-ellipsis"> Image </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/utils/training/" class="md-nav__link"> <span class="md-ellipsis"> Training </span> </a> </li> <li class="md-nav__item"> <a href="../../api/torchebm/utils/visualization/" class="md-nav__link"> <span class="md-ellipsis"> Visualization </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked> <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex> <span class="md-ellipsis"> LLM API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true"> <label class="md-nav__title" for="__nav_4"> <span class="md-nav__icon md-icon"></span> LLM API Reference </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../index.md" class="md-nav__link"> <span class="md-ellipsis"> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc"> <label class="md-nav__link md-nav__link--active" for="__toc"> <span class="md-ellipsis"> All Symbols </span> <span class="md-nav__icon md-icon"></span> </label> <a href="./" class="md-nav__link md-nav__link--active"> <span class="md-ellipsis"> All Symbols </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class="md-nav__title" for="__toc"> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix> <li class="md-nav__item"> <a href="#corebase_integrator" class="md-nav__link"> <span class="md-ellipsis"> core.base_integrator </span> </a> <nav class="md-nav" aria-label="core.base_integrator"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#baseintegrator" class="md-nav__link"> <span class="md-ellipsis"> BaseIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_interpolant" class="md-nav__link"> <span class="md-ellipsis"> core.base_interpolant </span> </a> <nav class="md-nav" aria-label="core.base_interpolant"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#baseinterpolant" class="md-nav__link"> <span class="md-ellipsis"> BaseInterpolant </span> </a> </li> <li class="md-nav__item"> <a href="#expand_t_like_xt-torchtensor-x-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> expand_t_like_x(t: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_loss" class="md-nav__link"> <span class="md-ellipsis"> core.base_loss </span> </a> <nav class="md-nav" aria-label="core.base_loss"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basecontrastivedivergence" class="md-nav__link"> <span class="md-ellipsis"> BaseContrastiveDivergence </span> </a> </li> <li class="md-nav__item"> <a href="#baseloss" class="md-nav__link"> <span class="md-ellipsis"> BaseLoss </span> </a> </li> <li class="md-nav__item"> <a href="#basescorematching" class="md-nav__link"> <span class="md-ellipsis"> BaseScoreMatching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_model" class="md-nav__link"> <span class="md-ellipsis"> core.base_model </span> </a> <nav class="md-nav" aria-label="core.base_model"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#ackleyenergy" class="md-nav__link"> <span class="md-ellipsis"> AckleyEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#ackleymodel" class="md-nav__link"> <span class="md-ellipsis"> AckleyModel </span> </a> </li> <li class="md-nav__item"> <a href="#baseenergyfunction" class="md-nav__link"> <span class="md-ellipsis"> BaseEnergyFunction </span> </a> </li> <li class="md-nav__item"> <a href="#basemodel" class="md-nav__link"> <span class="md-ellipsis"> BaseModel </span> </a> </li> <li class="md-nav__item"> <a href="#doublewellenergy" class="md-nav__link"> <span class="md-ellipsis"> DoubleWellEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#doublewellmodel" class="md-nav__link"> <span class="md-ellipsis"> DoubleWellModel </span> </a> </li> <li class="md-nav__item"> <a href="#gaussianenergy" class="md-nav__link"> <span class="md-ellipsis"> GaussianEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#gaussianmodel" class="md-nav__link"> <span class="md-ellipsis"> GaussianModel </span> </a> </li> <li class="md-nav__item"> <a href="#harmonicenergy" class="md-nav__link"> <span class="md-ellipsis"> HarmonicEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#harmonicmodel" class="md-nav__link"> <span class="md-ellipsis"> HarmonicModel </span> </a> </li> <li class="md-nav__item"> <a href="#rastriginenergy" class="md-nav__link"> <span class="md-ellipsis"> RastriginEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#rastriginmodel" class="md-nav__link"> <span class="md-ellipsis"> RastriginModel </span> </a> </li> <li class="md-nav__item"> <a href="#rosenbrockenergy" class="md-nav__link"> <span class="md-ellipsis"> RosenbrockEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#rosenbrockmodel" class="md-nav__link"> <span class="md-ellipsis"> RosenbrockModel </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_sampler" class="md-nav__link"> <span class="md-ellipsis"> core.base_sampler </span> </a> <nav class="md-nav" aria-label="core.base_sampler"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basesampler" class="md-nav__link"> <span class="md-ellipsis"> BaseSampler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_scheduler" class="md-nav__link"> <span class="md-ellipsis"> core.base_scheduler </span> </a> <nav class="md-nav" aria-label="core.base_scheduler"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basescheduler" class="md-nav__link"> <span class="md-ellipsis"> BaseScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#constantscheduler" class="md-nav__link"> <span class="md-ellipsis"> ConstantScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#cosinescheduler" class="md-nav__link"> <span class="md-ellipsis"> CosineScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#exponentialdecayscheduler" class="md-nav__link"> <span class="md-ellipsis"> ExponentialDecayScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#linearscheduler" class="md-nav__link"> <span class="md-ellipsis"> LinearScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#multistepscheduler" class="md-nav__link"> <span class="md-ellipsis"> MultiStepScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#warmupscheduler" class="md-nav__link"> <span class="md-ellipsis"> WarmupScheduler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_trainer" class="md-nav__link"> <span class="md-ellipsis"> core.base_trainer </span> </a> <nav class="md-nav" aria-label="core.base_trainer"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basetrainer" class="md-nav__link"> <span class="md-ellipsis"> BaseTrainer </span> </a> </li> <li class="md-nav__item"> <a href="#contrastivedivergencetrainer" class="md-nav__link"> <span class="md-ellipsis"> ContrastiveDivergenceTrainer </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#coredevice_mixin" class="md-nav__link"> <span class="md-ellipsis"> core.device_mixin </span> </a> <nav class="md-nav" aria-label="core.device_mixin"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#devicemixin" class="md-nav__link"> <span class="md-ellipsis"> DeviceMixin </span> </a> </li> <li class="md-nav__item"> <a href="#normalize_devicedevice" class="md-nav__link"> <span class="md-ellipsis"> normalize_device(device) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#datasetsgenerators" class="md-nav__link"> <span class="md-ellipsis"> datasets.generators </span> </a> <nav class="md-nav" aria-label="datasets.generators"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basesyntheticdataset" class="md-nav__link"> <span class="md-ellipsis"> BaseSyntheticDataset </span> </a> </li> <li class="md-nav__item"> <a href="#checkerboarddataset" class="md-nav__link"> <span class="md-ellipsis"> CheckerboardDataset </span> </a> </li> <li class="md-nav__item"> <a href="#circledataset" class="md-nav__link"> <span class="md-ellipsis"> CircleDataset </span> </a> </li> <li class="md-nav__item"> <a href="#eightgaussiansdataset" class="md-nav__link"> <span class="md-ellipsis"> EightGaussiansDataset </span> </a> </li> <li class="md-nav__item"> <a href="#gaussianmixturedataset" class="md-nav__link"> <span class="md-ellipsis"> GaussianMixtureDataset </span> </a> </li> <li class="md-nav__item"> <a href="#griddataset" class="md-nav__link"> <span class="md-ellipsis"> GridDataset </span> </a> </li> <li class="md-nav__item"> <a href="#pinwheeldataset" class="md-nav__link"> <span class="md-ellipsis"> PinwheelDataset </span> </a> </li> <li class="md-nav__item"> <a href="#swissrolldataset" class="md-nav__link"> <span class="md-ellipsis"> SwissRollDataset </span> </a> </li> <li class="md-nav__item"> <a href="#twomoonsdataset" class="md-nav__link"> <span class="md-ellipsis"> TwoMoonsDataset </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#integratorseuler_maruyama" class="md-nav__link"> <span class="md-ellipsis"> integrators.euler_maruyama </span> </a> <nav class="md-nav" aria-label="integrators.euler_maruyama"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#eulermaruyamaintegrator" class="md-nav__link"> <span class="md-ellipsis"> EulerMaruyamaIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#integratorsheun" class="md-nav__link"> <span class="md-ellipsis"> integrators.heun </span> </a> <nav class="md-nav" aria-label="integrators.heun"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#heunintegrator" class="md-nav__link"> <span class="md-ellipsis"> HeunIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#integratorsleapfrog" class="md-nav__link"> <span class="md-ellipsis"> integrators.leapfrog </span> </a> <nav class="md-nav" aria-label="integrators.leapfrog"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#leapfrogintegrator" class="md-nav__link"> <span class="md-ellipsis"> LeapfrogIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#interpolantscosine" class="md-nav__link"> <span class="md-ellipsis"> interpolants.cosine </span> </a> <nav class="md-nav" aria-label="interpolants.cosine"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#cosineinterpolant" class="md-nav__link"> <span class="md-ellipsis"> CosineInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#interpolantslinear" class="md-nav__link"> <span class="md-ellipsis"> interpolants.linear </span> </a> <nav class="md-nav" aria-label="interpolants.linear"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#linearinterpolant" class="md-nav__link"> <span class="md-ellipsis"> LinearInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#interpolantsvariance_preserving" class="md-nav__link"> <span class="md-ellipsis"> interpolants.variance_preserving </span> </a> <nav class="md-nav" aria-label="interpolants.variance_preserving"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#variancepreservinginterpolant" class="md-nav__link"> <span class="md-ellipsis"> VariancePreservingInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossescontrastive_divergence" class="md-nav__link"> <span class="md-ellipsis"> losses.contrastive_divergence </span> </a> <nav class="md-nav" aria-label="losses.contrastive_divergence"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#contrastivedivergence" class="md-nav__link"> <span class="md-ellipsis"> ContrastiveDivergence </span> </a> </li> <li class="md-nav__item"> <a href="#paralleltemperingcd" class="md-nav__link"> <span class="md-ellipsis"> ParallelTemperingCD </span> </a> </li> <li class="md-nav__item"> <a href="#persistentcontrastivedivergence" class="md-nav__link"> <span class="md-ellipsis"> PersistentContrastiveDivergence </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossesequilibrium_matching" class="md-nav__link"> <span class="md-ellipsis"> losses.equilibrium_matching </span> </a> <nav class="md-nav" aria-label="losses.equilibrium_matching"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#equilibriummatchingloss" class="md-nav__link"> <span class="md-ellipsis"> EquilibriumMatchingLoss </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossesloss_utils" class="md-nav__link"> <span class="md-ellipsis"> losses.loss_utils </span> </a> <nav class="md-nav" aria-label="losses.loss_utils"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#compute_eqm_ctt-torchtensor-threshold-float-08-multiplier-float-40-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> compute_eqm_ct(t: torch.Tensor, threshold: float = 0.8, multiplier: float = 4.0) -&gt; torch.Tensor </span> </a> </li> <li class="md-nav__item"> <a href="#dispersive_lossz-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> dispersive_loss(z: torch.Tensor) -&gt; torch.Tensor </span> </a> </li> <li class="md-nav__item"> <a href="#get_interpolantinterpolant_type-str-torchebmcorebase_interpolantbaseinterpolant" class="md-nav__link"> <span class="md-ellipsis"> get_interpolant(interpolant_type: str) -&gt; torchebm.core.base_interpolant.BaseInterpolant </span> </a> </li> <li class="md-nav__item"> <a href="#mean_flattensor-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> mean_flat(tensor: torch.Tensor) -&gt; torch.Tensor </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossesscore_matching" class="md-nav__link"> <span class="md-ellipsis"> losses.score_matching </span> </a> <nav class="md-nav" aria-label="losses.score_matching"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#denoisingscorematching" class="md-nav__link"> <span class="md-ellipsis"> DenoisingScoreMatching </span> </a> </li> <li class="md-nav__item"> <a href="#scorematching" class="md-nav__link"> <span class="md-ellipsis"> ScoreMatching </span> </a> </li> <li class="md-nav__item"> <a href="#slicedscorematching" class="md-nav__link"> <span class="md-ellipsis"> SlicedScoreMatching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentsembeddings" class="md-nav__link"> <span class="md-ellipsis"> models.components.embeddings </span> </a> <nav class="md-nav" aria-label="models.components.embeddings"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#labelembedder" class="md-nav__link"> <span class="md-ellipsis"> LabelEmbedder </span> </a> </li> <li class="md-nav__item"> <a href="#mlptimestepembedder" class="md-nav__link"> <span class="md-ellipsis"> MLPTimestepEmbedder </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentsheads" class="md-nav__link"> <span class="md-ellipsis"> models.components.heads </span> </a> <nav class="md-nav" aria-label="models.components.heads"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#adalnzeropatchhead" class="md-nav__link"> <span class="md-ellipsis"> AdaLNZeroPatchHead </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentspatch" class="md-nav__link"> <span class="md-ellipsis"> models.components.patch </span> </a> <nav class="md-nav" aria-label="models.components.patch"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#convpatchembed2d" class="md-nav__link"> <span class="md-ellipsis"> ConvPatchEmbed2d </span> </a> </li> <li class="md-nav__item"> <a href="#patchify2dx-torchtensor-patch_size-int-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> patchify2d(x: 'torch.Tensor', patch_size: 'int') -&gt; 'torch.Tensor' </span> </a> </li> <li class="md-nav__item"> <a href="#unpatchify2dtokens-torchtensor-patch_size-int-out_channels-int-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> unpatchify2d(tokens: 'torch.Tensor', patch_size: 'int', *, out_channels: 'int') -&gt; 'torch.Tensor' </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentspositional" class="md-nav__link"> <span class="md-ellipsis"> models.components.positional </span> </a> <nav class="md-nav" aria-label="models.components.positional"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#build_2d_sincos_pos_embedembed_dim-int-grid_size-int-device-torchdevice-none-none-dtype-torchdtype-torchfloat32-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> build_2d_sincos_pos_embed(embed_dim: 'int', grid_size: 'int', *, device: 'torch.device | None' = None, dtype: 'torch.dtype' = torch.float32) -&gt; 'torch.Tensor' </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentstransformer" class="md-nav__link"> <span class="md-ellipsis"> models.components.transformer </span> </a> <nav class="md-nav" aria-label="models.components.transformer"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#adalnzeroblock" class="md-nav__link"> <span class="md-ellipsis"> AdaLNZeroBlock </span> </a> </li> <li class="md-nav__item"> <a href="#feedforward" class="md-nav__link"> <span class="md-ellipsis"> FeedForward </span> </a> </li> <li class="md-nav__item"> <a href="#multiheadselfattention" class="md-nav__link"> <span class="md-ellipsis"> MultiheadSelfAttention </span> </a> </li> <li class="md-nav__item"> <a href="#modulatex-torchtensor-shift-torchtensor-scale-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> modulate(x: 'torch.Tensor', shift: 'torch.Tensor', scale: 'torch.Tensor') -&gt; 'torch.Tensor' </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelsconditional_transformer_2d" class="md-nav__link"> <span class="md-ellipsis"> models.conditional_transformer_2d </span> </a> <nav class="md-nav" aria-label="models.conditional_transformer_2d"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#conditionaltransformer2d" class="md-nav__link"> <span class="md-ellipsis"> ConditionalTransformer2D </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelswrappers" class="md-nav__link"> <span class="md-ellipsis"> models.wrappers </span> </a> <nav class="md-nav" aria-label="models.wrappers"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#labelclassifierfreeguidance" class="md-nav__link"> <span class="md-ellipsis"> LabelClassifierFreeGuidance </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplersflow" class="md-nav__link"> <span class="md-ellipsis"> samplers.flow </span> </a> <nav class="md-nav" aria-label="samplers.flow"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#flowsampler" class="md-nav__link"> <span class="md-ellipsis"> FlowSampler </span> </a> </li> <li class="md-nav__item"> <a href="#predictiontype" class="md-nav__link"> <span class="md-ellipsis"> PredictionType </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplersgradient_descent" class="md-nav__link"> <span class="md-ellipsis"> samplers.gradient_descent </span> </a> <nav class="md-nav" aria-label="samplers.gradient_descent"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#gradientdescentsampler" class="md-nav__link"> <span class="md-ellipsis"> GradientDescentSampler </span> </a> </li> <li class="md-nav__item"> <a href="#nesterovsampler" class="md-nav__link"> <span class="md-ellipsis"> NesterovSampler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplershmc" class="md-nav__link"> <span class="md-ellipsis"> samplers.hmc </span> </a> <nav class="md-nav" aria-label="samplers.hmc"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#hamiltonianmontecarlo" class="md-nav__link"> <span class="md-ellipsis"> HamiltonianMonteCarlo </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplerslangevin_dynamics" class="md-nav__link"> <span class="md-ellipsis"> samplers.langevin_dynamics </span> </a> <nav class="md-nav" aria-label="samplers.langevin_dynamics"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#langevindynamics" class="md-nav__link"> <span class="md-ellipsis"> LangevinDynamics </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#utilsimage" class="md-nav__link"> <span class="md-ellipsis"> utils.image </span> </a> <nav class="md-nav" aria-label="utils.image"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#center_crop_arrpil_image-pilimageimage-image_size-int-pilimageimage" class="md-nav__link"> <span class="md-ellipsis"> center_crop_arr(pil_image: PIL.Image.Image, image_size: int) -&gt; PIL.Image.Image </span> </a> </li> <li class="md-nav__item"> <a href="#create_npz_from_sample_foldersample_dir-str-num-int-50000-str" class="md-nav__link"> <span class="md-ellipsis"> create_npz_from_sample_folder(sample_dir: str, num: int = 50000) -&gt; str </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#utilstraining" class="md-nav__link"> <span class="md-ellipsis"> utils.training </span> </a> <nav class="md-nav" aria-label="utils.training"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#load_checkpointcheckpoint_path-str-model-torchnnmodulesmodulemodule-ema_model-torchnnmodulesmodulemodule-none-none-optimizer-torchoptimoptimizeroptimizer-none-none-device-torchdevice-none-none-dictstr-any" class="md-nav__link"> <span class="md-ellipsis"> load_checkpoint(checkpoint_path: str, model: torch.nn.modules.module.Module, ema_model: torch.nn.modules.module.Module | None = None, optimizer: torch.optim.optimizer.Optimizer | None = None, device: torch.device | None = None) -&gt; Dict[str, Any] </span> </a> </li> <li class="md-nav__item"> <a href="#requires_gradmodel-torchnnmodulesmodulemodule-flag-bool-true-none" class="md-nav__link"> <span class="md-ellipsis"> requires_grad(model: torch.nn.modules.module.Module, flag: bool = True) -&gt; None </span> </a> </li> <li class="md-nav__item"> <a href="#save_checkpointmodel-torchnnmodulesmodulemodule-optimizer-torchoptimoptimizeroptimizer-step-int-checkpoint_dir-str-ema_model-torchnnmodulesmodulemodule-none-none-args-dictstr-any-none-none-str" class="md-nav__link"> <span class="md-ellipsis"> save_checkpoint(model: torch.nn.modules.module.Module, optimizer: torch.optim.optimizer.Optimizer, step: int, checkpoint_dir: str, ema_model: torch.nn.modules.module.Module | None = None, args: Dict[str, Any] | None = None) -&gt; str </span> </a> </li> <li class="md-nav__item"> <a href="#update_emaema_model-torchnnmodulesmodulemodule-model-torchnnmodulesmodulemodule-decay-float-09999-none" class="md-nav__link"> <span class="md-ellipsis"> update_ema(ema_model: torch.nn.modules.module.Module, model: torch.nn.modules.module.Module, decay: float = 0.9999) -&gt; None </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#utilsvisualization" class="md-nav__link"> <span class="md-ellipsis"> utils.visualization </span> </a> <nav class="md-nav" aria-label="utils.visualization"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#plot_2d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_2d_energy_landscape(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> <li class="md-nav__item"> <a href="#plot_3d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-50-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-10-8-alpha-float-09-elev-float-30-azim-float-45-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_3d_energy_landscape(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 50, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (10, 8), alpha: float = 0.9, elev: float = 30, azim: float = -45, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> <li class="md-nav__item"> <a href="#plot_sample_trajectoriestrajectories-torchtensor-model-torchebmcorebase_modelbasemodel-none-none-x_range-tuplefloat-float-none-y_range-tuplefloat-float-none-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-trajectory_colors-liststr-none-none-trajectory_alpha-float-07-line_width-float-10-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_sample_trajectories(trajectories: torch.Tensor, model: torchebm.core.base_model.BaseModel | None = None, x_range: Tuple[float, float] = None, y_range: Tuple[float, float] = None, resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), trajectory_colors: List[str] | None = None, trajectory_alpha: float = 0.7, line_width: float = 1.0, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> <li class="md-nav__item"> <a href="#plot_samples_on_energymodel-torchebmcorebase_modelbasemodel-samples-torchtensor-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-sample_color-str-red-sample_alpha-float-05-sample_size-float-5-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_samples_on_energy(model: torchebm.core.base_model.BaseModel, samples: torch.Tensor, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, sample_color: str = 'red', sample_alpha: float = 0.5, sample_size: float = 5, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5"> <div class="md-nav__link md-nav__container"> <a href="../../examples/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M64 80c-8.8 0-16 7.2-16 16v320c0 8.8 7.2 16 16 16h320c8.8 0 16-7.2 16-16V96c0-8.8-7.2-16-16-16zM0 96c0-35.3 28.7-64 64-64h320c35.3 0 64 28.7 64 64v320c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64zm128 32a32 32 0 1 1 0 64 32 32 0 1 1 0-64m136 72c8.5 0 16.4 4.5 20.7 11.8l80 136c4.4 7.4 4.4 16.6.1 24.1S352.6 384 344 384H104c-8.9 0-17.2-5-21.3-12.9s-3.5-17.5 1.6-24.8l56-80c4.5-6.4 11.8-10.2 19.7-10.2s15.2 3.8 19.7 10.2l17.2 24.6 46.5-79c4.3-7.3 12.2-11.8 20.7-11.8z"></path></svg> <span class="md-ellipsis"> Examples </span> </a> <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_5"> <span class="md-nav__icon md-icon"></span> Examples </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../examples/datasets/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 3C7.58 3 4 4.79 4 7s3.58 4 8 4 8-1.79 8-4-3.58-4-8-4M4 9v3c0 2.21 3.58 4 8 4s8-1.79 8-4V9c0 2.21-3.58 4-8 4s-8-1.79-8-4m0 5v3c0 2.21 3.58 4 8 4s8-1.79 8-4v-3c0 2.21-3.58 4-8 4s-8-1.79-8-4"></path></svg> <span class="md-ellipsis"> Datasets </span> </a> </li> <li class="md-nav__item"> <a href="../../examples/samplers/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M2 2h2v18h18v2H2zm7 8a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3m4-8a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3m5 10a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3"></path></svg> <span class="md-ellipsis"> Samplers </span> </a> </li> <li class="md-nav__item"> <a href="../../examples/training/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 3 1 9l11 6 9-4.91V17h2V9M5 13.18v4L12 21l7-3.82v-4L12 17z"></path></svg> <span class="md-ellipsis"> Training EBMs </span> </a> </li> <li class="md-nav__item"> <a href="../../examples/visualization/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 21H2V3h2v16h2v-9h4v9h2V6h4v13h2v-5h4z"></path></svg> <span class="md-ellipsis"> Visualization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6"> <div class="md-nav__link md-nav__container"> <a href="../../developer_guide/" class="md-nav__link "> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5zM6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5"></path></svg> <span class="md-ellipsis"> Developer Guide </span> </a> <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0"> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false"> <label class="md-nav__title" for="__nav_6"> <span class="md-nav__icon md-icon"></span> Developer Guide </label> <ul class="md-nav__list" data-md-scrollfix> <li class="md-nav__item"> <a href="../../developer_guide/getting_started/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m13.13 22.19-1.63-3.83c1.57-.58 3.04-1.36 4.4-2.27zM5.64 12.5l-3.83-1.63 6.1-2.77C7 9.46 6.22 10.93 5.64 12.5M21.61 2.39S16.66.269 11 5.93c-2.19 2.19-3.5 4.6-4.35 6.71-.28.75-.09 1.57.46 2.13l2.13 2.12c.55.56 1.37.74 2.12.46A19.1 19.1 0 0 0 18.07 13c5.66-5.66 3.54-10.61 3.54-10.61m-7.07 7.07c-.78-.78-.78-2.05 0-2.83s2.05-.78 2.83 0c.77.78.78 2.05 0 2.83s-2.05.78-2.83 0m-5.66 7.07-1.41-1.41zM6.24 22l3.64-3.64c-.34-.09-.67-.24-.97-.45L4.83 22zM2 22h1.41l4.77-4.76-1.42-1.41L2 20.59zm0-2.83 4.09-4.08c-.21-.3-.36-.62-.45-.97L2 17.76z"></path></svg> <span class="md-ellipsis"> Getting Started </span> </a> </li> <li class="md-nav__item"> <a href="../../developer_guide/code_guidelines/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8 3a2 2 0 0 0-2 2v4a2 2 0 0 1-2 2H3v2h1a2 2 0 0 1 2 2v4a2 2 0 0 0 2 2h2v-2H8v-5a2 2 0 0 0-2-2 2 2 0 0 0 2-2V5h2V3m6 0a2 2 0 0 1 2 2v4a2 2 0 0 0 2 2h1v2h-1a2 2 0 0 0-2 2v4a2 2 0 0 1-2 2h-2v-2h2v-5a2 2 0 0 1 2-2 2 2 0 0 1-2-2V5h-2V3z"></path></svg> <span class="md-ellipsis"> Code Guidelines </span> </a> </li> <li class="md-nav__item"> <a href="../../developer_guide/architecture/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 18H4V8h16m0-2h-8l-2-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V8a2 2 0 0 0-2-2"></path></svg> <span class="md-ellipsis"> Architecture </span> </a> </li> <li class="md-nav__item"> <a href="../../developer_guide/performance/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 16a3 3 0 0 1-3-3c0-1.12.61-2.1 1.5-2.61l9.71-5.62-5.53 9.58c-.5.98-1.51 1.65-2.68 1.65m0-13c1.81 0 3.5.5 4.97 1.32l-2.1 1.21C14 5.19 13 5 12 5a8 8 0 0 0-8 8c0 2.21.89 4.21 2.34 5.65h.01c.39.39.39 1.02 0 1.41s-1.03.39-1.42.01A9.97 9.97 0 0 1 2 13 10 10 0 0 1 12 3m10 10c0 2.76-1.12 5.26-2.93 7.07-.39.38-1.02.38-1.41-.01a.996.996 0 0 1 0-1.41A7.95 7.95 0 0 0 20 13c0-1-.19-2-.54-2.9L20.67 8C21.5 9.5 22 11.18 22 13"></path></svg> <span class="md-ellipsis"> Performance </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="../../blog/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 5v14H5V5zm2-2H3v18h18zm-4 14H7v-1h10zm0-2H7v-1h10zm0-3H7V7h10z"></path></svg> <span class="md-ellipsis"> Blog </span> </a> </li> <li class="md-nav__item"> <a href="../../faq/" class="md-nav__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M11 18h2v-2h-2zm1-16A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-14a4 4 0 0 0-4 4h2a2 2 0 0 1 2-2 2 2 0 0 1 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5a4 4 0 0 0-4-4"></path></svg> <span class="md-ellipsis"> FAQ </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc"> <div class="md-sidebar__scrollwrap"> <div class="md-sidebar__inner"> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class="md-nav__title" for="__toc"> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix> <li class="md-nav__item"> <a href="#corebase_integrator" class="md-nav__link"> <span class="md-ellipsis"> core.base_integrator </span> </a> <nav class="md-nav" aria-label="core.base_integrator"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#baseintegrator" class="md-nav__link"> <span class="md-ellipsis"> BaseIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_interpolant" class="md-nav__link"> <span class="md-ellipsis"> core.base_interpolant </span> </a> <nav class="md-nav" aria-label="core.base_interpolant"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#baseinterpolant" class="md-nav__link"> <span class="md-ellipsis"> BaseInterpolant </span> </a> </li> <li class="md-nav__item"> <a href="#expand_t_like_xt-torchtensor-x-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> expand_t_like_x(t: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_loss" class="md-nav__link"> <span class="md-ellipsis"> core.base_loss </span> </a> <nav class="md-nav" aria-label="core.base_loss"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basecontrastivedivergence" class="md-nav__link"> <span class="md-ellipsis"> BaseContrastiveDivergence </span> </a> </li> <li class="md-nav__item"> <a href="#baseloss" class="md-nav__link"> <span class="md-ellipsis"> BaseLoss </span> </a> </li> <li class="md-nav__item"> <a href="#basescorematching" class="md-nav__link"> <span class="md-ellipsis"> BaseScoreMatching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_model" class="md-nav__link"> <span class="md-ellipsis"> core.base_model </span> </a> <nav class="md-nav" aria-label="core.base_model"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#ackleyenergy" class="md-nav__link"> <span class="md-ellipsis"> AckleyEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#ackleymodel" class="md-nav__link"> <span class="md-ellipsis"> AckleyModel </span> </a> </li> <li class="md-nav__item"> <a href="#baseenergyfunction" class="md-nav__link"> <span class="md-ellipsis"> BaseEnergyFunction </span> </a> </li> <li class="md-nav__item"> <a href="#basemodel" class="md-nav__link"> <span class="md-ellipsis"> BaseModel </span> </a> </li> <li class="md-nav__item"> <a href="#doublewellenergy" class="md-nav__link"> <span class="md-ellipsis"> DoubleWellEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#doublewellmodel" class="md-nav__link"> <span class="md-ellipsis"> DoubleWellModel </span> </a> </li> <li class="md-nav__item"> <a href="#gaussianenergy" class="md-nav__link"> <span class="md-ellipsis"> GaussianEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#gaussianmodel" class="md-nav__link"> <span class="md-ellipsis"> GaussianModel </span> </a> </li> <li class="md-nav__item"> <a href="#harmonicenergy" class="md-nav__link"> <span class="md-ellipsis"> HarmonicEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#harmonicmodel" class="md-nav__link"> <span class="md-ellipsis"> HarmonicModel </span> </a> </li> <li class="md-nav__item"> <a href="#rastriginenergy" class="md-nav__link"> <span class="md-ellipsis"> RastriginEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#rastriginmodel" class="md-nav__link"> <span class="md-ellipsis"> RastriginModel </span> </a> </li> <li class="md-nav__item"> <a href="#rosenbrockenergy" class="md-nav__link"> <span class="md-ellipsis"> RosenbrockEnergy </span> </a> </li> <li class="md-nav__item"> <a href="#rosenbrockmodel" class="md-nav__link"> <span class="md-ellipsis"> RosenbrockModel </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_sampler" class="md-nav__link"> <span class="md-ellipsis"> core.base_sampler </span> </a> <nav class="md-nav" aria-label="core.base_sampler"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basesampler" class="md-nav__link"> <span class="md-ellipsis"> BaseSampler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_scheduler" class="md-nav__link"> <span class="md-ellipsis"> core.base_scheduler </span> </a> <nav class="md-nav" aria-label="core.base_scheduler"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basescheduler" class="md-nav__link"> <span class="md-ellipsis"> BaseScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#constantscheduler" class="md-nav__link"> <span class="md-ellipsis"> ConstantScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#cosinescheduler" class="md-nav__link"> <span class="md-ellipsis"> CosineScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#exponentialdecayscheduler" class="md-nav__link"> <span class="md-ellipsis"> ExponentialDecayScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#linearscheduler" class="md-nav__link"> <span class="md-ellipsis"> LinearScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#multistepscheduler" class="md-nav__link"> <span class="md-ellipsis"> MultiStepScheduler </span> </a> </li> <li class="md-nav__item"> <a href="#warmupscheduler" class="md-nav__link"> <span class="md-ellipsis"> WarmupScheduler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#corebase_trainer" class="md-nav__link"> <span class="md-ellipsis"> core.base_trainer </span> </a> <nav class="md-nav" aria-label="core.base_trainer"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basetrainer" class="md-nav__link"> <span class="md-ellipsis"> BaseTrainer </span> </a> </li> <li class="md-nav__item"> <a href="#contrastivedivergencetrainer" class="md-nav__link"> <span class="md-ellipsis"> ContrastiveDivergenceTrainer </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#coredevice_mixin" class="md-nav__link"> <span class="md-ellipsis"> core.device_mixin </span> </a> <nav class="md-nav" aria-label="core.device_mixin"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#devicemixin" class="md-nav__link"> <span class="md-ellipsis"> DeviceMixin </span> </a> </li> <li class="md-nav__item"> <a href="#normalize_devicedevice" class="md-nav__link"> <span class="md-ellipsis"> normalize_device(device) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#datasetsgenerators" class="md-nav__link"> <span class="md-ellipsis"> datasets.generators </span> </a> <nav class="md-nav" aria-label="datasets.generators"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#basesyntheticdataset" class="md-nav__link"> <span class="md-ellipsis"> BaseSyntheticDataset </span> </a> </li> <li class="md-nav__item"> <a href="#checkerboarddataset" class="md-nav__link"> <span class="md-ellipsis"> CheckerboardDataset </span> </a> </li> <li class="md-nav__item"> <a href="#circledataset" class="md-nav__link"> <span class="md-ellipsis"> CircleDataset </span> </a> </li> <li class="md-nav__item"> <a href="#eightgaussiansdataset" class="md-nav__link"> <span class="md-ellipsis"> EightGaussiansDataset </span> </a> </li> <li class="md-nav__item"> <a href="#gaussianmixturedataset" class="md-nav__link"> <span class="md-ellipsis"> GaussianMixtureDataset </span> </a> </li> <li class="md-nav__item"> <a href="#griddataset" class="md-nav__link"> <span class="md-ellipsis"> GridDataset </span> </a> </li> <li class="md-nav__item"> <a href="#pinwheeldataset" class="md-nav__link"> <span class="md-ellipsis"> PinwheelDataset </span> </a> </li> <li class="md-nav__item"> <a href="#swissrolldataset" class="md-nav__link"> <span class="md-ellipsis"> SwissRollDataset </span> </a> </li> <li class="md-nav__item"> <a href="#twomoonsdataset" class="md-nav__link"> <span class="md-ellipsis"> TwoMoonsDataset </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#integratorseuler_maruyama" class="md-nav__link"> <span class="md-ellipsis"> integrators.euler_maruyama </span> </a> <nav class="md-nav" aria-label="integrators.euler_maruyama"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#eulermaruyamaintegrator" class="md-nav__link"> <span class="md-ellipsis"> EulerMaruyamaIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#integratorsheun" class="md-nav__link"> <span class="md-ellipsis"> integrators.heun </span> </a> <nav class="md-nav" aria-label="integrators.heun"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#heunintegrator" class="md-nav__link"> <span class="md-ellipsis"> HeunIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#integratorsleapfrog" class="md-nav__link"> <span class="md-ellipsis"> integrators.leapfrog </span> </a> <nav class="md-nav" aria-label="integrators.leapfrog"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#leapfrogintegrator" class="md-nav__link"> <span class="md-ellipsis"> LeapfrogIntegrator </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#interpolantscosine" class="md-nav__link"> <span class="md-ellipsis"> interpolants.cosine </span> </a> <nav class="md-nav" aria-label="interpolants.cosine"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#cosineinterpolant" class="md-nav__link"> <span class="md-ellipsis"> CosineInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#interpolantslinear" class="md-nav__link"> <span class="md-ellipsis"> interpolants.linear </span> </a> <nav class="md-nav" aria-label="interpolants.linear"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#linearinterpolant" class="md-nav__link"> <span class="md-ellipsis"> LinearInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#interpolantsvariance_preserving" class="md-nav__link"> <span class="md-ellipsis"> interpolants.variance_preserving </span> </a> <nav class="md-nav" aria-label="interpolants.variance_preserving"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#variancepreservinginterpolant" class="md-nav__link"> <span class="md-ellipsis"> VariancePreservingInterpolant </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossescontrastive_divergence" class="md-nav__link"> <span class="md-ellipsis"> losses.contrastive_divergence </span> </a> <nav class="md-nav" aria-label="losses.contrastive_divergence"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#contrastivedivergence" class="md-nav__link"> <span class="md-ellipsis"> ContrastiveDivergence </span> </a> </li> <li class="md-nav__item"> <a href="#paralleltemperingcd" class="md-nav__link"> <span class="md-ellipsis"> ParallelTemperingCD </span> </a> </li> <li class="md-nav__item"> <a href="#persistentcontrastivedivergence" class="md-nav__link"> <span class="md-ellipsis"> PersistentContrastiveDivergence </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossesequilibrium_matching" class="md-nav__link"> <span class="md-ellipsis"> losses.equilibrium_matching </span> </a> <nav class="md-nav" aria-label="losses.equilibrium_matching"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#equilibriummatchingloss" class="md-nav__link"> <span class="md-ellipsis"> EquilibriumMatchingLoss </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossesloss_utils" class="md-nav__link"> <span class="md-ellipsis"> losses.loss_utils </span> </a> <nav class="md-nav" aria-label="losses.loss_utils"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#compute_eqm_ctt-torchtensor-threshold-float-08-multiplier-float-40-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> compute_eqm_ct(t: torch.Tensor, threshold: float = 0.8, multiplier: float = 4.0) -&gt; torch.Tensor </span> </a> </li> <li class="md-nav__item"> <a href="#dispersive_lossz-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> dispersive_loss(z: torch.Tensor) -&gt; torch.Tensor </span> </a> </li> <li class="md-nav__item"> <a href="#get_interpolantinterpolant_type-str-torchebmcorebase_interpolantbaseinterpolant" class="md-nav__link"> <span class="md-ellipsis"> get_interpolant(interpolant_type: str) -&gt; torchebm.core.base_interpolant.BaseInterpolant </span> </a> </li> <li class="md-nav__item"> <a href="#mean_flattensor-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> mean_flat(tensor: torch.Tensor) -&gt; torch.Tensor </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#lossesscore_matching" class="md-nav__link"> <span class="md-ellipsis"> losses.score_matching </span> </a> <nav class="md-nav" aria-label="losses.score_matching"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#denoisingscorematching" class="md-nav__link"> <span class="md-ellipsis"> DenoisingScoreMatching </span> </a> </li> <li class="md-nav__item"> <a href="#scorematching" class="md-nav__link"> <span class="md-ellipsis"> ScoreMatching </span> </a> </li> <li class="md-nav__item"> <a href="#slicedscorematching" class="md-nav__link"> <span class="md-ellipsis"> SlicedScoreMatching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentsembeddings" class="md-nav__link"> <span class="md-ellipsis"> models.components.embeddings </span> </a> <nav class="md-nav" aria-label="models.components.embeddings"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#labelembedder" class="md-nav__link"> <span class="md-ellipsis"> LabelEmbedder </span> </a> </li> <li class="md-nav__item"> <a href="#mlptimestepembedder" class="md-nav__link"> <span class="md-ellipsis"> MLPTimestepEmbedder </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentsheads" class="md-nav__link"> <span class="md-ellipsis"> models.components.heads </span> </a> <nav class="md-nav" aria-label="models.components.heads"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#adalnzeropatchhead" class="md-nav__link"> <span class="md-ellipsis"> AdaLNZeroPatchHead </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentspatch" class="md-nav__link"> <span class="md-ellipsis"> models.components.patch </span> </a> <nav class="md-nav" aria-label="models.components.patch"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#convpatchembed2d" class="md-nav__link"> <span class="md-ellipsis"> ConvPatchEmbed2d </span> </a> </li> <li class="md-nav__item"> <a href="#patchify2dx-torchtensor-patch_size-int-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> patchify2d(x: 'torch.Tensor', patch_size: 'int') -&gt; 'torch.Tensor' </span> </a> </li> <li class="md-nav__item"> <a href="#unpatchify2dtokens-torchtensor-patch_size-int-out_channels-int-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> unpatchify2d(tokens: 'torch.Tensor', patch_size: 'int', *, out_channels: 'int') -&gt; 'torch.Tensor' </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentspositional" class="md-nav__link"> <span class="md-ellipsis"> models.components.positional </span> </a> <nav class="md-nav" aria-label="models.components.positional"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#build_2d_sincos_pos_embedembed_dim-int-grid_size-int-device-torchdevice-none-none-dtype-torchdtype-torchfloat32-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> build_2d_sincos_pos_embed(embed_dim: 'int', grid_size: 'int', *, device: 'torch.device | None' = None, dtype: 'torch.dtype' = torch.float32) -&gt; 'torch.Tensor' </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelscomponentstransformer" class="md-nav__link"> <span class="md-ellipsis"> models.components.transformer </span> </a> <nav class="md-nav" aria-label="models.components.transformer"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#adalnzeroblock" class="md-nav__link"> <span class="md-ellipsis"> AdaLNZeroBlock </span> </a> </li> <li class="md-nav__item"> <a href="#feedforward" class="md-nav__link"> <span class="md-ellipsis"> FeedForward </span> </a> </li> <li class="md-nav__item"> <a href="#multiheadselfattention" class="md-nav__link"> <span class="md-ellipsis"> MultiheadSelfAttention </span> </a> </li> <li class="md-nav__item"> <a href="#modulatex-torchtensor-shift-torchtensor-scale-torchtensor-torchtensor" class="md-nav__link"> <span class="md-ellipsis"> modulate(x: 'torch.Tensor', shift: 'torch.Tensor', scale: 'torch.Tensor') -&gt; 'torch.Tensor' </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelsconditional_transformer_2d" class="md-nav__link"> <span class="md-ellipsis"> models.conditional_transformer_2d </span> </a> <nav class="md-nav" aria-label="models.conditional_transformer_2d"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#conditionaltransformer2d" class="md-nav__link"> <span class="md-ellipsis"> ConditionalTransformer2D </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#modelswrappers" class="md-nav__link"> <span class="md-ellipsis"> models.wrappers </span> </a> <nav class="md-nav" aria-label="models.wrappers"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#labelclassifierfreeguidance" class="md-nav__link"> <span class="md-ellipsis"> LabelClassifierFreeGuidance </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplersflow" class="md-nav__link"> <span class="md-ellipsis"> samplers.flow </span> </a> <nav class="md-nav" aria-label="samplers.flow"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#flowsampler" class="md-nav__link"> <span class="md-ellipsis"> FlowSampler </span> </a> </li> <li class="md-nav__item"> <a href="#predictiontype" class="md-nav__link"> <span class="md-ellipsis"> PredictionType </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplersgradient_descent" class="md-nav__link"> <span class="md-ellipsis"> samplers.gradient_descent </span> </a> <nav class="md-nav" aria-label="samplers.gradient_descent"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#gradientdescentsampler" class="md-nav__link"> <span class="md-ellipsis"> GradientDescentSampler </span> </a> </li> <li class="md-nav__item"> <a href="#nesterovsampler" class="md-nav__link"> <span class="md-ellipsis"> NesterovSampler </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplershmc" class="md-nav__link"> <span class="md-ellipsis"> samplers.hmc </span> </a> <nav class="md-nav" aria-label="samplers.hmc"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#hamiltonianmontecarlo" class="md-nav__link"> <span class="md-ellipsis"> HamiltonianMonteCarlo </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#samplerslangevin_dynamics" class="md-nav__link"> <span class="md-ellipsis"> samplers.langevin_dynamics </span> </a> <nav class="md-nav" aria-label="samplers.langevin_dynamics"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#langevindynamics" class="md-nav__link"> <span class="md-ellipsis"> LangevinDynamics </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#utilsimage" class="md-nav__link"> <span class="md-ellipsis"> utils.image </span> </a> <nav class="md-nav" aria-label="utils.image"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#center_crop_arrpil_image-pilimageimage-image_size-int-pilimageimage" class="md-nav__link"> <span class="md-ellipsis"> center_crop_arr(pil_image: PIL.Image.Image, image_size: int) -&gt; PIL.Image.Image </span> </a> </li> <li class="md-nav__item"> <a href="#create_npz_from_sample_foldersample_dir-str-num-int-50000-str" class="md-nav__link"> <span class="md-ellipsis"> create_npz_from_sample_folder(sample_dir: str, num: int = 50000) -&gt; str </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#utilstraining" class="md-nav__link"> <span class="md-ellipsis"> utils.training </span> </a> <nav class="md-nav" aria-label="utils.training"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#load_checkpointcheckpoint_path-str-model-torchnnmodulesmodulemodule-ema_model-torchnnmodulesmodulemodule-none-none-optimizer-torchoptimoptimizeroptimizer-none-none-device-torchdevice-none-none-dictstr-any" class="md-nav__link"> <span class="md-ellipsis"> load_checkpoint(checkpoint_path: str, model: torch.nn.modules.module.Module, ema_model: torch.nn.modules.module.Module | None = None, optimizer: torch.optim.optimizer.Optimizer | None = None, device: torch.device | None = None) -&gt; Dict[str, Any] </span> </a> </li> <li class="md-nav__item"> <a href="#requires_gradmodel-torchnnmodulesmodulemodule-flag-bool-true-none" class="md-nav__link"> <span class="md-ellipsis"> requires_grad(model: torch.nn.modules.module.Module, flag: bool = True) -&gt; None </span> </a> </li> <li class="md-nav__item"> <a href="#save_checkpointmodel-torchnnmodulesmodulemodule-optimizer-torchoptimoptimizeroptimizer-step-int-checkpoint_dir-str-ema_model-torchnnmodulesmodulemodule-none-none-args-dictstr-any-none-none-str" class="md-nav__link"> <span class="md-ellipsis"> save_checkpoint(model: torch.nn.modules.module.Module, optimizer: torch.optim.optimizer.Optimizer, step: int, checkpoint_dir: str, ema_model: torch.nn.modules.module.Module | None = None, args: Dict[str, Any] | None = None) -&gt; str </span> </a> </li> <li class="md-nav__item"> <a href="#update_emaema_model-torchnnmodulesmodulemodule-model-torchnnmodulesmodulemodule-decay-float-09999-none" class="md-nav__link"> <span class="md-ellipsis"> update_ema(ema_model: torch.nn.modules.module.Module, model: torch.nn.modules.module.Module, decay: float = 0.9999) -&gt; None </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a href="#utilsvisualization" class="md-nav__link"> <span class="md-ellipsis"> utils.visualization </span> </a> <nav class="md-nav" aria-label="utils.visualization"> <ul class="md-nav__list"> <li class="md-nav__item"> <a href="#plot_2d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_2d_energy_landscape(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> <li class="md-nav__item"> <a href="#plot_3d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-50-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-10-8-alpha-float-09-elev-float-30-azim-float-45-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_3d_energy_landscape(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 50, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (10, 8), alpha: float = 0.9, elev: float = 30, azim: float = -45, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> <li class="md-nav__item"> <a href="#plot_sample_trajectoriestrajectories-torchtensor-model-torchebmcorebase_modelbasemodel-none-none-x_range-tuplefloat-float-none-y_range-tuplefloat-float-none-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-trajectory_colors-liststr-none-none-trajectory_alpha-float-07-line_width-float-10-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_sample_trajectories(trajectories: torch.Tensor, model: torchebm.core.base_model.BaseModel | None = None, x_range: Tuple[float, float] = None, y_range: Tuple[float, float] = None, resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), trajectory_colors: List[str] | None = None, trajectory_alpha: float = 0.7, line_width: float = 1.0, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> <li class="md-nav__item"> <a href="#plot_samples_on_energymodel-torchebmcorebase_modelbasemodel-samples-torchtensor-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-sample_color-str-red-sample_alpha-float-05-sample_size-float-5-device-str-none-none-matplotlibfigurefigure" class="md-nav__link"> <span class="md-ellipsis"> plot_samples_on_energy(model: torchebm.core.base_model.BaseModel, samples: torch.Tensor, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, sample_color: str = 'red', sample_alpha: float = 0.5, sample_size: float = 5, device: str | None = None) -&gt; matplotlib.figure.Figure </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-content" data-md-component="content"> <nav class="md-path" aria-label="Navigation"> <ol class="md-path__list"> <li class="md-path__item"> <a href="../.." class="md-path__link"> <span class="md-ellipsis"> Home </span> </a> </li> <li class="md-path__item"> <a href="../index.md" class="md-path__link"> <span class="md-ellipsis"> LLM API Reference </span> </a> </li> </ol> </nav> <article class="md-content__inner md-typeset"> <h1 id="torchebm-api-reference">TorchEBM API Reference<a class="headerlink" href="#torchebm-api-reference" title="Permanent link"></a></h1> <p>Complete API reference for LLM consumption.</p> <h2 id="corebase_integrator">core.base_integrator<a class="headerlink" href="#corebase_integrator" title="Permanent link"></a></h2> <h3 id="baseintegrator"><code>BaseIntegrator</code><a class="headerlink" href="#baseintegrator" title="Permanent link"></a></h3> <p>Abstract integrator that advances a sampler state according to dynamics.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="n">BaseIntegrator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>integrate(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, *args, **kwargs) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by <code>n_steps</code> integrator applications. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, *args, **kwargs) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by one integrator application. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="corebase_interpolant">core.base_interpolant<a class="headerlink" href="#corebase_interpolant" title="Permanent link"></a></h2> <p><em>Base class for interpolant schedules.</em></p> <h3 id="baseinterpolant"><code>BaseInterpolant</code><a class="headerlink" href="#baseinterpolant" title="Permanent link"></a></h3> <p>Abstract base class for stochastic interpolants.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-1-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1"></a><span class="n">BaseInterpolant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">/</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>compute_alpha_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute the data coefficient <span class="arithmatex">\(\alpha(t)\)</span> and its time derivative. - <code>compute_d_alpha_alpha_ratio_t(self, t: torch.Tensor) -&gt; torch.Tensor</code> - Compute the ratio <span class="arithmatex">\(\dot{\alpha}(t) / \alpha(t)\)</span> for numerical stability. - <code>compute_diffusion(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -&gt; torch.Tensor</code> - Compute diffusion coefficient for SDE sampling. - <code>compute_drift(self, x: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute drift coefficients for score-based parameterization. - <code>compute_sigma_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute the noise coefficient <span class="arithmatex">\(\sigma(t)\)</span> and its time derivative. - <code>interpolate(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute the interpolated sample <span class="arithmatex">\(x_t\)</span> and conditional velocity <span class="arithmatex">\(u_t\)</span>. - <code>score_to_velocity(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert score prediction to velocity. - <code>velocity_to_noise(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to noise prediction. - <code>velocity_to_score(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to score.</p> <h3 id="expand_t_like_xt-torchtensor-x-torchtensor-torchtensor"><code>expand_t_like_x(t: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor</code><a class="headerlink" href="#expand_t_like_xt-torchtensor-x-torchtensor-torchtensor" title="Permanent link"></a></h3> <p>Expand time tensor to match spatial dimensions of x.</p> <h2 id="corebase_loss">core.base_loss<a class="headerlink" href="#corebase_loss" title="Permanent link"></a></h2> <p><em>Base Loss Classes for Energy-Based Models</em></p> <h3 id="basecontrastivedivergence"><code>BaseContrastiveDivergence</code><a class="headerlink" href="#basecontrastivedivergence" title="Permanent link"></a></h3> <p>Abstract base class for Contrastive Divergence (CD) based loss functions.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-2-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="n">BaseContrastiveDivergence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">sampler</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_sampler</span><span class="o">.</span><span class="n">BaseSampler</span><span class="p">,</span> <span class="n">k_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">new_sample_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">init_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the contrastive divergence loss from positive and negative samples. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Computes the CD loss given real data samples. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_negative_samples(self, x, batch_size, data_shape) -&gt; torch.Tensor</code> - Gets negative samples using the replay buffer strategy. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_start_points(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Gets the starting points for the MCMC sampler. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>initialize_buffer(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -&gt; torch.Tensor</code> - Initializes the replay buffer with random noise for PCD. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>update_buffer(self, samples: torch.Tensor) -&gt; None</code> - Updates the replay buffer with new samples using a FIFO strategy. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="baseloss"><code>BaseLoss</code><a class="headerlink" href="#baseloss" title="Permanent link"></a></h3> <p>Abstract base class for loss functions used in energy-based models.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-3-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1"></a><span class="n">BaseLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the loss value. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="basescorematching"><code>BaseScoreMatching</code><a class="headerlink" href="#basescorematching" title="Permanent link"></a></h3> <p>Abstract base class for Score Matching based loss functions.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-4-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1"></a><span class="n">BaseScoreMatching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">use_autograd</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">hutchinson_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">custom_regularization</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>add_regularization(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -&gt; torch.Tensor</code> - Adds regularization terms to the loss. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the specific score matching loss variant. - <code>compute_score(self, x: torch.Tensor, noise: torch.Tensor | None = None) -&gt; torch.Tensor</code> - Computes the score function, ( - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the score matching loss given input data. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>perturb_data(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Perturbs the input data with Gaussian noise for denoising variants. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="corebase_model">core.base_model<a class="headerlink" href="#corebase_model" title="Permanent link"></a></h2> <h3 id="ackleyenergy"><code>AckleyEnergy</code><a class="headerlink" href="#ackleyenergy" title="Permanent link"></a></h3> <p>Energy-based model for the Ackley function.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-5-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1"></a><span class="n">AckleyEnergy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Ackley energy. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="ackleymodel"><code>AckleyModel</code><a class="headerlink" href="#ackleymodel" title="Permanent link"></a></h3> <p>Energy-based model for the Ackley function.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-6-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1"></a><span class="n">AckleyModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">6.283185307179586</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Ackley energy. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="baseenergyfunction"><code>BaseEnergyFunction</code><a class="headerlink" href="#baseenergyfunction" title="Permanent link"></a></h3> <p>Abstract base class for energy-based models (EBMs).</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-7-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1"></a><span class="n">BaseEnergyFunction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the scalar energy value for each input sample. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="basemodel"><code>BaseModel</code><a class="headerlink" href="#basemodel" title="Permanent link"></a></h3> <p>Abstract base class for energy-based models (EBMs).</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-8-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1"></a><span class="n">BaseModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the scalar energy value for each input sample. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="doublewellenergy"><code>DoubleWellEnergy</code><a class="headerlink" href="#doublewellenergy" title="Permanent link"></a></h3> <p>Energy-based model for a double-well potential.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-9-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1"></a><span class="n">DoubleWellEnergy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the double well energy: <span class="arithmatex">\(h \sum_{i=1}^{n} (x_i^2 - b^2)^2\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="doublewellmodel"><code>DoubleWellModel</code><a class="headerlink" href="#doublewellmodel" title="Permanent link"></a></h3> <p>Energy-based model for a double-well potential.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-10-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1"></a><span class="n">DoubleWellModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">barrier_height</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the double well energy: <span class="arithmatex">\(h \sum_{i=1}^{n} (x_i^2 - b^2)^2\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="gaussianenergy"><code>GaussianEnergy</code><a class="headerlink" href="#gaussianenergy" title="Permanent link"></a></h3> <p>Energy-based model for a Gaussian distribution.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-11-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1"></a><span class="n">GaussianEnergy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Gaussian energy: <span class="arithmatex">\(E(x) = \frac{1}{2} (x - \mu)^{\top} \Sigma^{-1} (x - \mu)\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="gaussianmodel"><code>GaussianModel</code><a class="headerlink" href="#gaussianmodel" title="Permanent link"></a></h3> <p>Energy-based model for a Gaussian distribution.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-12-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1"></a><span class="n">GaussianModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">cov</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Gaussian energy: <span class="arithmatex">\(E(x) = \frac{1}{2} (x - \mu)^{\top} \Sigma^{-1} (x - \mu)\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="harmonicenergy"><code>HarmonicEnergy</code><a class="headerlink" href="#harmonicenergy" title="Permanent link"></a></h3> <p>Energy-based model for a harmonic oscillator.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-13-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1"></a><span class="n">HarmonicEnergy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the harmonic oscillator energy: <span class="arithmatex">\(\frac{1}{2} k \sum_{i=1}^{n} x_i^{2}\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="harmonicmodel"><code>HarmonicModel</code><a class="headerlink" href="#harmonicmodel" title="Permanent link"></a></h3> <p>Energy-based model for a harmonic oscillator.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-14-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1"></a><span class="n">HarmonicModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the harmonic oscillator energy: <span class="arithmatex">\(\frac{1}{2} k \sum_{i=1}^{n} x_i^{2}\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="rastriginenergy"><code>RastriginEnergy</code><a class="headerlink" href="#rastriginenergy" title="Permanent link"></a></h3> <p>Energy-based model for the Rastrigin function.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-15-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1"></a><span class="n">RastriginEnergy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Rastrigin energy. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="rastriginmodel"><code>RastriginModel</code><a class="headerlink" href="#rastriginmodel" title="Permanent link"></a></h3> <p>Energy-based model for the Rastrigin function.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-16-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1"></a><span class="n">RastriginModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Rastrigin energy. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="rosenbrockenergy"><code>RosenbrockEnergy</code><a class="headerlink" href="#rosenbrockenergy" title="Permanent link"></a></h3> <p>Energy-based model for the Rosenbrock function.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-17-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1"></a><span class="n">RosenbrockEnergy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Rosenbrock energy: <span class="arithmatex">\(\sum_{i=1}^{n-1} \left[ b(x_{i+1} - x_i^2)^2 + (a - x_i)^2 \right]\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="rosenbrockmodel"><code>RosenbrockModel</code><a class="headerlink" href="#rosenbrockmodel" title="Permanent link"></a></h3> <p>Energy-based model for the Rosenbrock function.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-18-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1"></a><span class="n">RosenbrockModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">100.0</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the Rosenbrock energy: <span class="arithmatex">\(\sum_{i=1}^{n-1} \left[ b(x_{i+1} - x_i^2)^2 + (a - x_i)^2 \right]\)</span>. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>gradient(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Computes the gradient of the energy function with respect to the input, <span class="arithmatex">\(\nabla_x E(x)\)</span>. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="corebase_sampler">core.base_sampler<a class="headerlink" href="#corebase_sampler" title="Permanent link"></a></h2> <h3 id="basesampler"><code>BaseSampler</code><a class="headerlink" href="#basesampler" title="Permanent link"></a></h3> <p>Abstract base class for samplers.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-19-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1"></a><span class="n">BaseSampler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>apply_mixed_precision(self, func)</code> - A decorator to apply the mixed precision context to a method. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_scheduled_value(self, name: str) -&gt; float</code> - Gets the current value for a scheduled parameter. - <code>get_schedulers(self) -&gt; Dict[str, torchebm.core.base_scheduler.BaseScheduler]</code> - Gets all registered schedulers. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_scheduler(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -&gt; None</code> - Registers a parameter scheduler. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>reset_schedulers(self) -&gt; None</code> - Resets all schedulers to their initial state. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>sample(self, x: torch.Tensor | None = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -&gt; torch.Tensor | Tuple[torch.Tensor, List[dict]]</code> - Runs the sampling process. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step_schedulers(self) -&gt; Dict[str, float]</code> - Advances all schedulers by one step. - <code>to(self, *args, **kwargs)</code> - Moves the sampler and its components to the specified device and/or dtype. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="corebase_scheduler">core.base_scheduler<a class="headerlink" href="#corebase_scheduler" title="Permanent link"></a></h2> <p><em>Parameter schedulers for MCMC samplers and optimization algorithms.</em></p> <h3 id="basescheduler"><code>BaseScheduler</code><a class="headerlink" href="#basescheduler" title="Permanent link"></a></h3> <p>Abstract base class for parameter schedulers.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-20-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1"></a><span class="n">BaseScheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_value(self) -&gt; float</code> - Get the current parameter value without advancing the scheduler. - <code>load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None</code> - Load the scheduler's state from a dictionary. - <code>reset(self) -&gt; None</code> - Reset the scheduler to its initial state. - <code>state_dict(self) -&gt; Dict[str, Any]</code> - Return the state of the scheduler as a dictionary. - <code>step(self) -&gt; float</code> - Advance the scheduler by one step and return the new parameter value.</p> <h3 id="constantscheduler"><code>ConstantScheduler</code><a class="headerlink" href="#constantscheduler" title="Permanent link"></a></h3> <p>Scheduler that maintains a constant parameter value.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-21-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1"></a><span class="n">ConstantScheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_value(self) -&gt; float</code> - Get the current parameter value without advancing the scheduler. - <code>load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None</code> - Load the scheduler's state from a dictionary. - <code>reset(self) -&gt; None</code> - Reset the scheduler to its initial state. - <code>state_dict(self) -&gt; Dict[str, Any]</code> - Return the state of the scheduler as a dictionary. - <code>step(self) -&gt; float</code> - Advance the scheduler by one step and return the new parameter value.</p> <h3 id="cosinescheduler"><code>CosineScheduler</code><a class="headerlink" href="#cosinescheduler" title="Permanent link"></a></h3> <p>Scheduler with cosine annealing.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-22-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1"></a><span class="n">CosineScheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">end_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_value(self) -&gt; float</code> - Get the current parameter value without advancing the scheduler. - <code>load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None</code> - Load the scheduler's state from a dictionary. - <code>reset(self) -&gt; None</code> - Reset the scheduler to its initial state. - <code>state_dict(self) -&gt; Dict[str, Any]</code> - Return the state of the scheduler as a dictionary. - <code>step(self) -&gt; float</code> - Advance the scheduler by one step and return the new parameter value.</p> <h3 id="exponentialdecayscheduler"><code>ExponentialDecayScheduler</code><a class="headerlink" href="#exponentialdecayscheduler" title="Permanent link"></a></h3> <p>Scheduler with exponential decay.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-23-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1"></a><span class="n">ExponentialDecayScheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_value(self) -&gt; float</code> - Get the current parameter value without advancing the scheduler. - <code>load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None</code> - Load the scheduler's state from a dictionary. - <code>reset(self) -&gt; None</code> - Reset the scheduler to its initial state. - <code>state_dict(self) -&gt; Dict[str, Any]</code> - Return the state of the scheduler as a dictionary. - <code>step(self) -&gt; float</code> - Advance the scheduler by one step and return the new parameter value.</p> <h3 id="linearscheduler"><code>LinearScheduler</code><a class="headerlink" href="#linearscheduler" title="Permanent link"></a></h3> <p>Scheduler with linear interpolation between start and end values.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-24-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1"></a><span class="n">LinearScheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">end_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_value(self) -&gt; float</code> - Get the current parameter value without advancing the scheduler. - <code>load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None</code> - Load the scheduler's state from a dictionary. - <code>reset(self) -&gt; None</code> - Reset the scheduler to its initial state. - <code>state_dict(self) -&gt; Dict[str, Any]</code> - Return the state of the scheduler as a dictionary. - <code>step(self) -&gt; float</code> - Advance the scheduler by one step and return the new parameter value.</p> <h3 id="multistepscheduler"><code>MultiStepScheduler</code><a class="headerlink" href="#multistepscheduler" title="Permanent link"></a></h3> <p>Scheduler that reduces the parameter value at specific milestone steps.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-25-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1"></a><span class="n">MultiStepScheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">milestones</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_value(self) -&gt; float</code> - Get the current parameter value without advancing the scheduler. - <code>load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None</code> - Load the scheduler's state from a dictionary. - <code>reset(self) -&gt; None</code> - Reset the scheduler to its initial state. - <code>state_dict(self) -&gt; Dict[str, Any]</code> - Return the state of the scheduler as a dictionary. - <code>step(self) -&gt; float</code> - Advance the scheduler by one step and return the new parameter value.</p> <h3 id="warmupscheduler"><code>WarmupScheduler</code><a class="headerlink" href="#warmupscheduler" title="Permanent link"></a></h3> <p>Scheduler that combines linear warmup with another scheduler.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-26-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1"></a><span class="n">WarmupScheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">main_scheduler</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_scheduler</span><span class="o">.</span><span class="n">BaseScheduler</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">warmup_init_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_value(self) -&gt; float</code> - Get the current parameter value without advancing the scheduler. - <code>load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None</code> - Load the scheduler's state from a dictionary. - <code>reset(self) -&gt; None</code> - Reset the scheduler to its initial state. - <code>state_dict(self) -&gt; Dict[str, Any]</code> - Return the state of the scheduler as a dictionary. - <code>step(self) -&gt; float</code> - Advance the scheduler by one step and return the new parameter value.</p> <h2 id="corebase_trainer">core.base_trainer<a class="headerlink" href="#corebase_trainer" title="Permanent link"></a></h2> <h3 id="basetrainer"><code>BaseTrainer</code><a class="headerlink" href="#basetrainer" title="Permanent link"></a></h3> <p>Base class for training energy-based models.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-27-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1"></a><span class="n">BaseTrainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_loss</span><span class="o">.</span><span class="n">BaseLoss</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>autocast_context(self)</code> - Return autocast context if enabled, else no-op. - <code>load_checkpoint(self, path: str) -&gt; None</code> - Load a checkpoint to resume training. - <code>save_checkpoint(self, path: str) -&gt; None</code> - Save a checkpoint of the current training state. - <code>train(self, dataloader: torch.utils.data.dataloader.DataLoader, num_epochs: int, validate_fn: Callable | None = None) -&gt; Dict[str, List[float]]</code> - Train the model for multiple epochs. - <code>train_epoch(self, dataloader: torch.utils.data.dataloader.DataLoader) -&gt; Dict[str, float]</code> - Train for one epoch. - <code>train_step(self, batch: torch.Tensor) -&gt; Dict[str, Any]</code> - Perform a single training step.</p> <h3 id="contrastivedivergencetrainer"><code>ContrastiveDivergenceTrainer</code><a class="headerlink" href="#contrastivedivergencetrainer" title="Permanent link"></a></h3> <p>Specialized trainer for contrastive divergence training of EBMs.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-28-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1"></a><span class="n">ContrastiveDivergenceTrainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">sampler</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_sampler</span><span class="o">.</span><span class="n">BaseSampler</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">k_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>autocast_context(self)</code> - Return autocast context if enabled, else no-op. - <code>load_checkpoint(self, path: str) -&gt; None</code> - Load a checkpoint to resume training. - <code>save_checkpoint(self, path: str) -&gt; None</code> - Save a checkpoint of the current training state. - <code>train(self, dataloader: torch.utils.data.dataloader.DataLoader, num_epochs: int, validate_fn: Callable | None = None) -&gt; Dict[str, List[float]]</code> - Train the model for multiple epochs. - <code>train_epoch(self, dataloader: torch.utils.data.dataloader.DataLoader) -&gt; Dict[str, float]</code> - Train for one epoch. - <code>train_step(self, batch: torch.Tensor) -&gt; Dict[str, Any]</code> - Perform a single contrastive divergence training step.</p> <h2 id="coredevice_mixin">core.device_mixin<a class="headerlink" href="#coredevice_mixin" title="Permanent link"></a></h2> <p><em>This module handles the device management or TorchEBM modules</em></p> <h3 id="devicemixin"><code>DeviceMixin</code><a class="headerlink" href="#devicemixin" title="Permanent link"></a></h3> <p>A mixin for consistent device and dtype management across all modules.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-29-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1"></a><span class="n">DeviceMixin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking.</p> <h3 id="normalize_devicedevice"><code>normalize_device(device)</code><a class="headerlink" href="#normalize_devicedevice" title="Permanent link"></a></h3> <p>Normalizes the device identifier for consistent usage.</p> <h2 id="datasetsgenerators">datasets.generators<a class="headerlink" href="#datasetsgenerators" title="Permanent link"></a></h2> <p><em>Dataset Generators Module.</em></p> <h3 id="basesyntheticdataset"><code>BaseSyntheticDataset</code><a class="headerlink" href="#basesyntheticdataset" title="Permanent link"></a></h3> <p>Abstract base class for generating 2D synthetic datasets.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-30-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-30-1"><a id="__codelineno-30-1" name="__codelineno-30-1"></a><span class="n">BaseSyntheticDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="checkerboarddataset"><code>CheckerboardDataset</code><a class="headerlink" href="#checkerboarddataset" title="Permanent link"></a></h3> <p>Generates points in a 2D checkerboard pattern using rejection sampling.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-31-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-31-1"><a id="__codelineno-31-1" name="__codelineno-31-1"></a><span class="n">CheckerboardDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">range_limit</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="circledataset"><code>CircleDataset</code><a class="headerlink" href="#circledataset" title="Permanent link"></a></h3> <p>Generates points sampled uniformly on a circle with noise.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-32-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-32-1"><a id="__codelineno-32-1" name="__codelineno-32-1"></a><span class="n">CircleDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="eightgaussiansdataset"><code>EightGaussiansDataset</code><a class="headerlink" href="#eightgaussiansdataset" title="Permanent link"></a></h3> <p>Generates samples from the '8 Gaussians' mixture distribution.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-33-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-33-1"><a id="__codelineno-33-1" name="__codelineno-33-1"></a><span class="n">EightGaussiansDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="gaussianmixturedataset"><code>GaussianMixtureDataset</code><a class="headerlink" href="#gaussianmixturedataset" title="Permanent link"></a></h3> <p>Generates a 2D Gaussian mixture dataset with components arranged in a circle.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-34-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-34-1"><a id="__codelineno-34-1" name="__codelineno-34-1"></a><span class="n">GaussianMixtureDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">n_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="griddataset"><code>GridDataset</code><a class="headerlink" href="#griddataset" title="Permanent link"></a></h3> <p>Generates points on a 2D grid.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-35-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-35-1"><a id="__codelineno-35-1" name="__codelineno-35-1"></a><span class="n">GridDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">range_limit</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="pinwheeldataset"><code>PinwheelDataset</code><a class="headerlink" href="#pinwheeldataset" title="Permanent link"></a></h3> <p>Generates the pinwheel dataset with curved blades.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-36-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-36-1"><a id="__codelineno-36-1" name="__codelineno-36-1"></a><span class="n">PinwheelDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">radial_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">angular_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">spiral_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="swissrolldataset"><code>SwissRollDataset</code><a class="headerlink" href="#swissrolldataset" title="Permanent link"></a></h3> <p>Generates a 2D Swiss roll dataset.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-37-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-37-1"><a id="__codelineno-37-1" name="__codelineno-37-1"></a><span class="n">SwissRollDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">arclength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h3 id="twomoonsdataset"><code>TwoMoonsDataset</code><a class="headerlink" href="#twomoonsdataset" title="Permanent link"></a></h3> <p>Generates the 'two moons' dataset.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-38-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-38-1"><a id="__codelineno-38-1" name="__codelineno-38-1"></a><span class="n">TwoMoonsDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>get_data(self) -&gt; torch.Tensor</code> - Returns the entire generated dataset as a single tensor. - <code>regenerate(self, seed: int | None = None)</code> - Re-generates the dataset, optionally with a new seed.</p> <h2 id="integratorseuler_maruyama">integrators.euler_maruyama<a class="headerlink" href="#integratorseuler_maruyama" title="Permanent link"></a></h2> <p><em>Euler-Maruyama integrator.</em></p> <h3 id="eulermaruyamaintegrator"><code>EulerMaruyamaIntegrator</code><a class="headerlink" href="#eulermaruyamaintegrator" title="Permanent link"></a></h3> <p>Euler-Maruyama integrator for It SDEs and ODEs.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-39-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-39-1"><a id="__codelineno-39-1" name="__codelineno-39-1"></a><span class="n">EulerMaruyamaIntegrator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>integrate(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, noise_scale: torch.Tensor | None = None, t: torch.Tensor | None = None) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by <code>n_steps</code> integrator applications. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: torch.Tensor | None = None, noise: torch.Tensor | None = None, noise_scale: torch.Tensor | None = None, t: torch.Tensor | None = None) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by one integrator application. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="integratorsheun">integrators.heun<a class="headerlink" href="#integratorsheun" title="Permanent link"></a></h2> <h3 id="heunintegrator"><code>HeunIntegrator</code><a class="headerlink" href="#heunintegrator" title="Permanent link"></a></h3> <p>Heun integrator (predictor-corrector) for It SDEs and ODEs.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-40-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-40-1"><a id="__codelineno-40-1" name="__codelineno-40-1"></a><span class="n">HeunIntegrator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>integrate(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, noise_scale: torch.Tensor | None = None, t: torch.Tensor) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by <code>n_steps</code> integrator applications. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: torch.Tensor | None = None, t: torch.Tensor, noise: torch.Tensor | None = None, noise_scale: torch.Tensor | None = None) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by one integrator application. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="integratorsleapfrog">integrators.leapfrog<a class="headerlink" href="#integratorsleapfrog" title="Permanent link"></a></h2> <h3 id="leapfrogintegrator"><code>LeapfrogIntegrator</code><a class="headerlink" href="#leapfrogintegrator" title="Permanent link"></a></h3> <p>Symplectic leapfrog (StrmerVerlet) integrator for Hamiltonian dynamics.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-41-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-41-1"><a id="__codelineno-41-1" name="__codelineno-41-1"></a><span class="n">LeapfrogIntegrator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>integrate(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, mass: float | torch.Tensor | None = None, *, potential_grad: Callable[[torch.Tensor], torch.Tensor] | None = None) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by <code>n_steps</code> integrator applications. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, mass: float | torch.Tensor | None = None, *, potential_grad: Callable[[torch.Tensor], torch.Tensor] | None = None) -&gt; Dict[str, torch.Tensor]</code> - Advance the dynamical state by one integrator application. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="interpolantscosine">interpolants.cosine<a class="headerlink" href="#interpolantscosine" title="Permanent link"></a></h2> <p><em>Cosine interpolant (geodesic variance preserving).</em></p> <h3 id="cosineinterpolant"><code>CosineInterpolant</code><a class="headerlink" href="#cosineinterpolant" title="Permanent link"></a></h3> <p>Cosine (geodesic variance preserving) interpolant.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-42-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-42-1"><a id="__codelineno-42-1" name="__codelineno-42-1"></a><span class="n">CosineInterpolant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">/</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>compute_alpha_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute <span class="arithmatex">\(\alpha(t) = \sin(\pi t / 2)\)</span> and its derivative. - <code>compute_d_alpha_alpha_ratio_t(self, t: torch.Tensor) -&gt; torch.Tensor</code> - Compute <span class="arithmatex">\(\dot{\alpha}(t) / \alpha(t) = (\pi/2) \cot(\pi t / 2)\)</span>. - <code>compute_diffusion(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -&gt; torch.Tensor</code> - Compute diffusion coefficient for SDE sampling. - <code>compute_drift(self, x: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute drift coefficients for score-based parameterization. - <code>compute_sigma_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute <span class="arithmatex">\(\sigma(t) = \cos(\pi t / 2)\)</span> and its derivative. - <code>interpolate(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute the interpolated sample <span class="arithmatex">\(x_t\)</span> and conditional velocity <span class="arithmatex">\(u_t\)</span>. - <code>score_to_velocity(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert score prediction to velocity. - <code>velocity_to_noise(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to noise prediction. - <code>velocity_to_score(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to score.</p> <h2 id="interpolantslinear">interpolants.linear<a class="headerlink" href="#interpolantslinear" title="Permanent link"></a></h2> <p><em>Linear interpolant (optimal transport interpolant).</em></p> <h3 id="linearinterpolant"><code>LinearInterpolant</code><a class="headerlink" href="#linearinterpolant" title="Permanent link"></a></h3> <p>Linear interpolant between noise and data distributions.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-43-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-43-1"><a id="__codelineno-43-1" name="__codelineno-43-1"></a><span class="n">LinearInterpolant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">/</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>compute_alpha_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute <span class="arithmatex">\(\alpha(t) = t\)</span> and <span class="arithmatex">\(\dot{\alpha}(t) = 1\)</span>. - <code>compute_d_alpha_alpha_ratio_t(self, t: torch.Tensor) -&gt; torch.Tensor</code> - Compute <span class="arithmatex">\(\dot{\alpha}(t) / \alpha(t) = 1/t\)</span>. - <code>compute_diffusion(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -&gt; torch.Tensor</code> - Compute diffusion coefficient for SDE sampling. - <code>compute_drift(self, x: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute drift coefficients for score-based parameterization. - <code>compute_sigma_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute <span class="arithmatex">\(\sigma(t) = 1 - t\)</span> and <span class="arithmatex">\(\dot{\sigma}(t) = -1\)</span>. - <code>interpolate(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute the interpolated sample <span class="arithmatex">\(x_t\)</span> and conditional velocity <span class="arithmatex">\(u_t\)</span>. - <code>score_to_velocity(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert score prediction to velocity. - <code>velocity_to_noise(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to noise prediction. - <code>velocity_to_score(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to score.</p> <h2 id="interpolantsvariance_preserving">interpolants.variance_preserving<a class="headerlink" href="#interpolantsvariance_preserving" title="Permanent link"></a></h2> <p><em>Variance preserving interpolant (DDPM-style schedule).</em></p> <h3 id="variancepreservinginterpolant"><code>VariancePreservingInterpolant</code><a class="headerlink" href="#variancepreservinginterpolant" title="Permanent link"></a></h3> <p>Variance preserving (VP) interpolant with linear beta schedule.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-44-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-44-1"><a id="__codelineno-44-1" name="__codelineno-44-1"></a><span class="n">VariancePreservingInterpolant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sigma_min</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">sigma_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>compute_alpha_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute <span class="arithmatex">\(\alpha(t)\)</span> and its derivative for VP schedule. - <code>compute_d_alpha_alpha_ratio_t(self, t: torch.Tensor) -&gt; torch.Tensor</code> - Compute <span class="arithmatex">\(\dot{\alpha}(t) / \alpha(t)\)</span> directly from log mean coefficient. - <code>compute_diffusion(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -&gt; torch.Tensor</code> - Compute diffusion coefficient for SDE sampling. - <code>compute_drift(self, x: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute drift for VP schedule using the beta parameterization. - <code>compute_sigma_t(self, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute <span class="arithmatex">\(\sigma(t)\)</span> and its derivative for VP schedule. - <code>interpolate(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Compute the interpolated sample <span class="arithmatex">\(x_t\)</span> and conditional velocity <span class="arithmatex">\(u_t\)</span>. - <code>score_to_velocity(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert score prediction to velocity. - <code>velocity_to_noise(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to noise prediction. - <code>velocity_to_score(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor</code> - Convert velocity prediction to score.</p> <h2 id="lossescontrastive_divergence">losses.contrastive_divergence<a class="headerlink" href="#lossescontrastive_divergence" title="Permanent link"></a></h2> <p><em>Contrastive Divergence Loss Module.</em></p> <h3 id="contrastivedivergence"><code>ContrastiveDivergence</code><a class="headerlink" href="#contrastivedivergence" title="Permanent link"></a></h3> <p>Standard Contrastive Divergence (CD-k) loss.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-45-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-45-1"><a id="__codelineno-45-1" name="__codelineno-45-1"></a><span class="n">ContrastiveDivergence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">k_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">init_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">new_sample_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">energy_reg_weight</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">use_temperature_annealing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">min_temp</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_temp</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">temp_decay</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the Contrastive Divergence loss from positive and negative samples. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Computes the Contrastive Divergence loss and generates negative samples. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_negative_samples(self, x, batch_size, data_shape) -&gt; torch.Tensor</code> - Gets negative samples using the replay buffer strategy. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_start_points(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Gets the starting points for the MCMC sampler. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>initialize_buffer(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -&gt; torch.Tensor</code> - Initializes the replay buffer with random noise for PCD. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>update_buffer(self, samples: torch.Tensor) -&gt; None</code> - Updates the replay buffer with new samples using a FIFO strategy. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="paralleltemperingcd"><code>ParallelTemperingCD</code><a class="headerlink" href="#paralleltemperingcd" title="Permanent link"></a></h3> <p>Abstract base class for Contrastive Divergence (CD) based loss functions.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-46-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-46-1"><a id="__codelineno-46-1" name="__codelineno-46-1"></a><span class="n">ParallelTemperingCD</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">temps</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the contrastive divergence loss from positive and negative samples. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Computes the CD loss given real data samples. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_negative_samples(self, x, batch_size, data_shape) -&gt; torch.Tensor</code> - Gets negative samples using the replay buffer strategy. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_start_points(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Gets the starting points for the MCMC sampler. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>initialize_buffer(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -&gt; torch.Tensor</code> - Initializes the replay buffer with random noise for PCD. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>update_buffer(self, samples: torch.Tensor) -&gt; None</code> - Updates the replay buffer with new samples using a FIFO strategy. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="persistentcontrastivedivergence"><code>PersistentContrastiveDivergence</code><a class="headerlink" href="#persistentcontrastivedivergence" title="Permanent link"></a></h3> <p>Abstract base class for Contrastive Divergence (CD) based loss functions.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-47-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-47-1"><a id="__codelineno-47-1" name="__codelineno-47-1"></a><span class="n">PersistentContrastiveDivergence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the contrastive divergence loss from positive and negative samples. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Computes the CD loss given real data samples. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_negative_samples(self, x, batch_size, data_shape) -&gt; torch.Tensor</code> - Gets negative samples using the replay buffer strategy. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_start_points(self, x: torch.Tensor) -&gt; torch.Tensor</code> - Gets the starting points for the MCMC sampler. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>initialize_buffer(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -&gt; torch.Tensor</code> - Initializes the replay buffer with random noise for PCD. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>update_buffer(self, samples: torch.Tensor) -&gt; None</code> - Updates the replay buffer with new samples using a FIFO strategy. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="lossesequilibrium_matching">losses.equilibrium_matching<a class="headerlink" href="#lossesequilibrium_matching" title="Permanent link"></a></h2> <p><em>Equilibrium Matching (EqM) loss.</em></p> <h3 id="equilibriummatchingloss"><code>EquilibriumMatchingLoss</code><a class="headerlink" href="#equilibriummatchingloss" title="Permanent link"></a></h3> <p>Equilibrium Matching (EqM) training loss.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-48-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-48-1"><a id="__codelineno-48-1" name="__codelineno-48-1"></a><span class="n">EquilibriumMatchingLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="s1">'nn.Module'</span><span class="p">,</span> <span class="n">prediction</span><span class="p">:</span> <span class="s2">"Literal['velocity', 'score', 'noise']"</span> <span class="o">=</span> <span class="s1">'velocity'</span><span class="p">,</span> <span class="n">energy_type</span><span class="p">:</span> <span class="s2">"Literal['none', 'dot', 'l2', 'mean']"</span> <span class="o">=</span> <span class="s1">'none'</span><span class="p">,</span> <span class="n">interpolant</span><span class="p">:</span> <span class="s1">'Union[str, BaseInterpolant]'</span> <span class="o">=</span> <span class="s1">'linear'</span><span class="p">,</span> <span class="n">loss_weight</span><span class="p">:</span> <span class="s2">"Optional[Literal['velocity', 'likelihood']]"</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">train_eps</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">ct_threshold</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">ct_multiplier</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">apply_dispersion</span><span class="p">:</span> <span class="s1">'bool'</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">dispersion_weight</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">time_invariant</span><span class="p">:</span> <span class="s1">'bool'</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="s1">'torch.dtype'</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="s1">'Optional[Union[str, torch.device]]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="s1">'bool'</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">:</span> <span class="s1">'Optional[float]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: 'torch.Tensor', *args, **kwargs) -&gt; 'torch.Tensor'</code> - Compute the equilibrium matching loss. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: 'torch.Tensor', *args, **kwargs) -&gt; 'torch.Tensor'</code> - Compute EqM loss (nn.Module interface). - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>training_losses(self, x1: 'torch.Tensor', model_kwargs: 'Optional[Dict[str, Any]]' = None) -&gt; 'Dict[str, torch.Tensor]'</code> - Compute training losses with detailed outputs. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="lossesloss_utils">losses.loss_utils<a class="headerlink" href="#lossesloss_utils" title="Permanent link"></a></h2> <p><em>Utility functions for loss computations.</em></p> <h3 id="compute_eqm_ctt-torchtensor-threshold-float-08-multiplier-float-40-torchtensor"><code>compute_eqm_ct(t: torch.Tensor, threshold: float = 0.8, multiplier: float = 4.0) -&gt; torch.Tensor</code><a class="headerlink" href="#compute_eqm_ctt-torchtensor-threshold-float-08-multiplier-float-40-torchtensor" title="Permanent link"></a></h3> <p>Energy-compatible target scaling c(t) used in EqM.</p> <h3 id="dispersive_lossz-torchtensor-torchtensor"><code>dispersive_loss(z: torch.Tensor) -&gt; torch.Tensor</code><a class="headerlink" href="#dispersive_lossz-torchtensor-torchtensor" title="Permanent link"></a></h3> <p>Dispersive loss (InfoNCE-L2 variant) for regularization.</p> <h3 id="get_interpolantinterpolant_type-str-torchebmcorebase_interpolantbaseinterpolant"><code>get_interpolant(interpolant_type: str) -&gt; torchebm.core.base_interpolant.BaseInterpolant</code><a class="headerlink" href="#get_interpolantinterpolant_type-str-torchebmcorebase_interpolantbaseinterpolant" title="Permanent link"></a></h3> <p>Get interpolant instance by name.</p> <h3 id="mean_flattensor-torchtensor-torchtensor"><code>mean_flat(tensor: torch.Tensor) -&gt; torch.Tensor</code><a class="headerlink" href="#mean_flattensor-torchtensor-torchtensor" title="Permanent link"></a></h3> <p>Take mean over all non-batch dimensions.</p> <h2 id="lossesscore_matching">losses.score_matching<a class="headerlink" href="#lossesscore_matching" title="Permanent link"></a></h2> <p><em>Score Matching Loss Module</em></p> <h3 id="denoisingscorematching"><code>DenoisingScoreMatching</code><a class="headerlink" href="#denoisingscorematching" title="Permanent link"></a></h3> <p>Denoising Score Matching (DSM) from Vincent (2011).</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-49-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-49-1"><a id="__codelineno-49-1" name="__codelineno-49-1"></a><span class="n">DenoisingScoreMatching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">custom_regularization</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>add_regularization(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -&gt; torch.Tensor</code> - Adds regularization terms to the loss. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the denoising score matching loss. - <code>compute_score(self, x: torch.Tensor, noise: torch.Tensor | None = None) -&gt; torch.Tensor</code> - Computes the score function, ( - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the denoising score matching loss for a batch of data. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>perturb_data(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Perturbs the input data with Gaussian noise for denoising variants. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="scorematching"><code>ScoreMatching</code><a class="headerlink" href="#scorematching" title="Permanent link"></a></h3> <p>Original Score Matching loss from Hyvrinen (2005).</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-50-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-50-1"><a id="__codelineno-50-1" name="__codelineno-50-1"></a><span class="n">ScoreMatching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">hessian_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">'exact'</span><span class="p">,</span> <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">custom_regularization</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>add_regularization(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -&gt; torch.Tensor</code> - Adds regularization terms to the loss. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the score matching loss using the specified Hessian computation method. - <code>compute_score(self, x: torch.Tensor, noise: torch.Tensor | None = None) -&gt; torch.Tensor</code> - Computes the score function, ( - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the score matching loss for a batch of data. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>perturb_data(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Perturbs the input data with Gaussian noise for denoising variants. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="slicedscorematching"><code>SlicedScoreMatching</code><a class="headerlink" href="#slicedscorematching" title="Permanent link"></a></h3> <p>Sliced Score Matching (SSM) from Song et al. (2019).</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-51-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-51-1"><a id="__codelineno-51-1" name="__codelineno-51-1"></a><span class="n">SlicedScoreMatching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">n_projections</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">projection_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">'rademacher'</span><span class="p">,</span> <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">custom_regularization</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>add_regularization(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -&gt; torch.Tensor</code> - Adds regularization terms to the loss. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the sliced score matching loss using random projections. - <code>compute_score(self, x: torch.Tensor, noise: torch.Tensor | None = None) -&gt; torch.Tensor</code> - Computes the score function, ( - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor</code> - Computes the sliced score matching loss for a batch of data. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>perturb_data(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Perturbs the input data with Gaussian noise for denoising variants. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Override to() to update internal device tracking. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="modelscomponentsembeddings">models.components.embeddings<a class="headerlink" href="#modelscomponentsembeddings" title="Permanent link"></a></h2> <h3 id="labelembedder"><code>LabelEmbedder</code><a class="headerlink" href="#labelembedder" title="Permanent link"></a></h3> <p>Label embedding with optional classifier-free guidance token dropping.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-52-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-52-1"><a id="__codelineno-52-1" name="__codelineno-52-1"></a><span class="n">LabelEmbedder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">dropout_prob</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, labels: 'torch.Tensor', *, training: 'bool', force_drop_mask: 'Optional[torch.Tensor]' = None) -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>maybe_drop_labels(self, labels: 'torch.Tensor', *, force_drop_mask: 'Optional[torch.Tensor]' = None) -&gt; 'torch.Tensor'</code> - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="mlptimestepembedder"><code>MLPTimestepEmbedder</code><a class="headerlink" href="#mlptimestepembedder" title="Permanent link"></a></h3> <p>Embed a scalar timestep into a vector via sinusoid + MLP.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-53-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-53-1"><a id="__codelineno-53-1" name="__codelineno-53-1"></a><span class="n">MLPTimestepEmbedder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">frequency_embedding_size</span><span class="p">:</span> <span class="s1">'int'</span> <span class="o">=</span> <span class="mi">256</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, t: 'torch.Tensor') -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>sinusoidal_embedding(t: 'torch.Tensor', dim: 'int', max_period: 'int' = 10000) -&gt; 'torch.Tensor'</code> - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="modelscomponentsheads">models.components.heads<a class="headerlink" href="#modelscomponentsheads" title="Permanent link"></a></h2> <h3 id="adalnzeropatchhead"><code>AdaLNZeroPatchHead</code><a class="headerlink" href="#adalnzeropatchhead" title="Permanent link"></a></h3> <p>Final layer that maps token features to patch pixels with adaLN-Zero.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-54-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-54-1"><a id="__codelineno-54-1" name="__codelineno-54-1"></a><span class="n">AdaLNZeroPatchHead</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">cond_dim</span><span class="p">:</span> <span class="s1">'Optional[int]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">1e-06</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, tokens: 'torch.Tensor', cond: 'torch.Tensor') -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="modelscomponentspatch">models.components.patch<a class="headerlink" href="#modelscomponentspatch" title="Permanent link"></a></h2> <h3 id="convpatchembed2d"><code>ConvPatchEmbed2d</code><a class="headerlink" href="#convpatchembed2d" title="Permanent link"></a></h3> <p>Patch embedding via strided conv.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-55-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-55-1"><a id="__codelineno-55-1" name="__codelineno-55-1"></a><span class="n">ConvPatchEmbed2d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: 'torch.Tensor') -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="patchify2dx-torchtensor-patch_size-int-torchtensor"><code>patchify2d(x: 'torch.Tensor', patch_size: 'int') -&gt; 'torch.Tensor'</code><a class="headerlink" href="#patchify2dx-torchtensor-patch_size-int-torchtensor" title="Permanent link"></a></h3> <p>Convert (B,C,H,W) into patch tokens (B, N, C*P*P).</p> <h3 id="unpatchify2dtokens-torchtensor-patch_size-int-out_channels-int-torchtensor"><code>unpatchify2d(tokens: 'torch.Tensor', patch_size: 'int', *, out_channels: 'int') -&gt; 'torch.Tensor'</code><a class="headerlink" href="#unpatchify2dtokens-torchtensor-patch_size-int-out_channels-int-torchtensor" title="Permanent link"></a></h3> <p>Convert patch tokens (B,N,P*P*C) back to (B,C,H,W).</p> <h2 id="modelscomponentspositional">models.components.positional<a class="headerlink" href="#modelscomponentspositional" title="Permanent link"></a></h2> <h3 id="build_2d_sincos_pos_embedembed_dim-int-grid_size-int-device-torchdevice-none-none-dtype-torchdtype-torchfloat32-torchtensor"><code>build_2d_sincos_pos_embed(embed_dim: 'int', grid_size: 'int', *, device: 'torch.device | None' = None, dtype: 'torch.dtype' = torch.float32) -&gt; 'torch.Tensor'</code><a class="headerlink" href="#build_2d_sincos_pos_embedembed_dim-int-grid_size-int-device-torchdevice-none-none-dtype-torchdtype-torchfloat32-torchtensor" title="Permanent link"></a></h3> <p>Create 2D sin/cos positional embeddings.</p> <h2 id="modelscomponentstransformer">models.components.transformer<a class="headerlink" href="#modelscomponentstransformer" title="Permanent link"></a></h2> <h3 id="adalnzeroblock"><code>AdaLNZeroBlock</code><a class="headerlink" href="#adalnzeroblock" title="Permanent link"></a></h3> <p>Transformer block with adaLN-Zero conditioning.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-56-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-56-1"><a id="__codelineno-56-1" name="__codelineno-56-1"></a><span class="n">AdaLNZeroBlock</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">cond_dim</span><span class="p">:</span> <span class="s1">'Optional[int]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">attn</span><span class="p">:</span> <span class="s1">'Optional[nn.Module]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">mlp</span><span class="p">:</span> <span class="s1">'Optional[nn.Module]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">1e-06</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: 'torch.Tensor', cond: 'torch.Tensor') -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="feedforward"><code>FeedForward</code><a class="headerlink" href="#feedforward" title="Permanent link"></a></h3> <p>Base class for all neural network modules.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-57-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-57-1"><a id="__codelineno-57-1" name="__codelineno-57-1"></a><span class="n">FeedForward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: 'torch.Tensor') -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="multiheadselfattention"><code>MultiheadSelfAttention</code><a class="headerlink" href="#multiheadselfattention" title="Permanent link"></a></h3> <p>Self-attention wrapper with batch-first API.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-58-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-58-1"><a id="__codelineno-58-1" name="__codelineno-58-1"></a><span class="n">MultiheadSelfAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: 'torch.Tensor') -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="modulatex-torchtensor-shift-torchtensor-scale-torchtensor-torchtensor"><code>modulate(x: 'torch.Tensor', shift: 'torch.Tensor', scale: 'torch.Tensor') -&gt; 'torch.Tensor'</code><a class="headerlink" href="#modulatex-torchtensor-shift-torchtensor-scale-torchtensor-torchtensor" title="Permanent link"></a></h3> <h2 id="modelsconditional_transformer_2d">models.conditional_transformer_2d<a class="headerlink" href="#modelsconditional_transformer_2d" title="Permanent link"></a></h2> <h3 id="conditionaltransformer2d"><code>ConditionalTransformer2D</code><a class="headerlink" href="#conditionaltransformer2d" title="Permanent link"></a></h3> <p>Generic conditional 2D Transformer backbone.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-59-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-59-1"><a id="__codelineno-59-1" name="__codelineno-59-1"></a><span class="n">ConditionalTransformer2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">input_size</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">depth</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">cond_dim</span><span class="p">:</span> <span class="s1">'Optional[int]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">use_sincos_pos_embed</span><span class="p">:</span> <span class="s1">'bool'</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: 'torch.Tensor', cond: 'torch.Tensor') -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="modelswrappers">models.wrappers<a class="headerlink" href="#modelswrappers" title="Permanent link"></a></h2> <h3 id="labelclassifierfreeguidance"><code>LabelClassifierFreeGuidance</code><a class="headerlink" href="#labelclassifierfreeguidance" title="Permanent link"></a></h3> <p>Classifier-free guidance wrapper for label-conditioned models.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-60-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-60-1"><a id="__codelineno-60-1" name="__codelineno-60-1"></a><span class="n">LabelClassifierFreeGuidance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="s1">'nn.Module'</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">null_label_id</span><span class="p">:</span> <span class="s1">'int'</span><span class="p">,</span> <span class="n">cfg_scale</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">guide_channels</span><span class="p">:</span> <span class="s1">'int'</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, x: 'torch.Tensor', t: 'torch.Tensor', *, y: 'torch.Tensor', **kwargs) -&gt; 'torch.Tensor'</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>to(self, *args, **kwargs)</code> - Move and/or cast the parameters and buffers. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="samplersflow">samplers.flow<a class="headerlink" href="#samplersflow" title="Permanent link"></a></h2> <p><em>Flow-based sampler for trained generative models.</em></p> <h3 id="flowsampler"><code>FlowSampler</code><a class="headerlink" href="#flowsampler" title="Permanent link"></a></h3> <p>Sampler for flow-based and diffusion generative models.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-61-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-61-1"><a id="__codelineno-61-1" name="__codelineno-61-1"></a><span class="n">FlowSampler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">interpolant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_interpolant</span><span class="o">.</span><span class="n">BaseInterpolant</span> <span class="o">=</span> <span class="s1">'linear'</span><span class="p">,</span> <span class="n">prediction</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">'velocity'</span><span class="p">,</span> <span class="s1">'score'</span><span class="p">,</span> <span class="s1">'noise'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'velocity'</span><span class="p">,</span> <span class="n">train_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">sample_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">negate_velocity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>apply_mixed_precision(self, func)</code> - A decorator to apply the mixed precision context to a method. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_scheduled_value(self, name: str) -&gt; float</code> - Gets the current value for a scheduled parameter. - <code>get_schedulers(self) -&gt; Dict[str, torchebm.core.base_scheduler.BaseScheduler]</code> - Gets all registered schedulers. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>prior_logp(self, z: torch.Tensor) -&gt; torch.Tensor</code> - Compute log probability under standard Gaussian prior. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_scheduler(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -&gt; None</code> - Registers a parameter scheduler. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>reset_schedulers(self) -&gt; None</code> - Resets all schedulers to their initial state. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>sample(self, x: torch.Tensor | None = None, dim: int = 10, n_steps: int = 50, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *, mode: Literal['ode', 'sde'] = 'ode', shape: Tuple[int, ...] | None = None, ode_method: str = 'dopri5', atol: float = 1e-06, rtol: float = 0.001, reverse: bool = False, sde_method: str = 'euler', diffusion_form: str = 'SBDM', diffusion_norm: float = 1.0, last_step: str | None = 'Mean', last_step_size: float = 0.04, **model_kwargs) -&gt; torch.Tensor</code> - Unified sampling entrypoint for flow/diffusion models. - <code>sample_ode(self, z: torch.Tensor, num_steps: int = 50, method: str = 'dopri5', atol: float = 1e-06, rtol: float = 0.001, reverse: bool = False, **model_kwargs) -&gt; torch.Tensor</code> - Sample using probability flow ODE. - <code>sample_sde(self, z: torch.Tensor, num_steps: int = 250, method: str = 'euler', diffusion_form: str = 'SBDM', diffusion_norm: float = 1.0, last_step: str | None = 'Mean', last_step_size: float = 0.04, **model_kwargs) -&gt; torch.Tensor</code> - Sample using reverse-time SDE. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step_schedulers(self) -&gt; Dict[str, float]</code> - Advances all schedulers by one step. - <code>to(self, *args, **kwargs)</code> - Moves the sampler and its components to the specified device and/or dtype. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="predictiontype"><code>PredictionType</code><a class="headerlink" href="#predictiontype" title="Permanent link"></a></h3> <p>Model prediction type for generative models.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-62-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-62-1"><a id="__codelineno-62-1" name="__codelineno-62-1"></a><span class="n">PredictionType</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">/</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <h2 id="samplersgradient_descent">samplers.gradient_descent<a class="headerlink" href="#samplersgradient_descent" title="Permanent link"></a></h2> <p><em>Gradient-based optimization samplers.</em></p> <h3 id="gradientdescentsampler"><code>GradientDescentSampler</code><a class="headerlink" href="#gradientdescentsampler" title="Permanent link"></a></h3> <p>Gradient descent sampler for energy-based models.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-63-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-63-1"><a id="__codelineno-63-1" name="__codelineno-63-1"></a><span class="n">GradientDescentSampler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="s1">'BaseModel'</span><span class="p">,</span> <span class="n">step_size</span><span class="p">:</span> <span class="s1">'Union[float, BaseScheduler]'</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="s1">'torch.dtype'</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="s1">'Optional[Union[str, torch.device]]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="s1">'bool'</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>apply_mixed_precision(self, func)</code> - A decorator to apply the mixed precision context to a method. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_scheduled_value(self, name: str) -&gt; float</code> - Gets the current value for a scheduled parameter. - <code>get_schedulers(self) -&gt; Dict[str, torchebm.core.base_scheduler.BaseScheduler]</code> - Gets all registered schedulers. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_scheduler(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -&gt; None</code> - Registers a parameter scheduler. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>reset_schedulers(self) -&gt; None</code> - Resets all schedulers to their initial state. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>sample(self, x: 'Optional[torch.Tensor]' = None, dim: 'int' = 10, n_steps: 'int' = 100, n_samples: 'int' = 1, thin: 'int' = 1, return_trajectory: 'bool' = False, return_diagnostics: 'bool' = False, *args, **kwargs) -&gt; 'Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]'</code> - Generate samples via gradient descent optimization. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step_schedulers(self) -&gt; Dict[str, float]</code> - Advances all schedulers by one step. - <code>to(self, *args, **kwargs)</code> - Moves the sampler and its components to the specified device and/or dtype. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h3 id="nesterovsampler"><code>NesterovSampler</code><a class="headerlink" href="#nesterovsampler" title="Permanent link"></a></h3> <p>Nesterov accelerated gradient sampler for energy-based models.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-64-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-64-1"><a id="__codelineno-64-1" name="__codelineno-64-1"></a><span class="n">NesterovSampler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="s1">'BaseModel'</span><span class="p">,</span> <span class="n">step_size</span><span class="p">:</span> <span class="s1">'Union[float, BaseScheduler]'</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="p">:</span> <span class="s1">'float'</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="s1">'torch.dtype'</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="s1">'Optional[Union[str, torch.device]]'</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_mixed_precision</span><span class="p">:</span> <span class="s1">'bool'</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>apply_mixed_precision(self, func)</code> - A decorator to apply the mixed precision context to a method. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_scheduled_value(self, name: str) -&gt; float</code> - Gets the current value for a scheduled parameter. - <code>get_schedulers(self) -&gt; Dict[str, torchebm.core.base_scheduler.BaseScheduler]</code> - Gets all registered schedulers. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_scheduler(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -&gt; None</code> - Registers a parameter scheduler. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>reset_schedulers(self) -&gt; None</code> - Resets all schedulers to their initial state. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>sample(self, x: 'Optional[torch.Tensor]' = None, dim: 'int' = 10, n_steps: 'int' = 100, n_samples: 'int' = 1, thin: 'int' = 1, return_trajectory: 'bool' = False, return_diagnostics: 'bool' = False, *args, **kwargs) -&gt; 'Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]'</code> - Generate samples via Nesterov accelerated gradient descent. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step_schedulers(self) -&gt; Dict[str, float]</code> - Advances all schedulers by one step. - <code>to(self, *args, **kwargs)</code> - Moves the sampler and its components to the specified device and/or dtype. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="samplershmc">samplers.hmc<a class="headerlink" href="#samplershmc" title="Permanent link"></a></h2> <p><em>Hamiltonian Monte Carlo Sampler Module.</em></p> <h3 id="hamiltonianmontecarlo"><code>HamiltonianMonteCarlo</code><a class="headerlink" href="#hamiltonianmontecarlo" title="Permanent link"></a></h3> <p>Hamiltonian Monte Carlo sampler.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-65-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-65-1"><a id="__codelineno-65-1" name="__codelineno-65-1"></a><span class="n">HamiltonianMonteCarlo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">step_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_scheduler</span><span class="o">.</span><span class="n">BaseScheduler</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">n_leapfrog_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">mass</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>apply_mixed_precision(self, func)</code> - A decorator to apply the mixed precision context to a method. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_scheduled_value(self, name: str) -&gt; float</code> - Gets the current value for a scheduled parameter. - <code>get_schedulers(self) -&gt; Dict[str, torchebm.core.base_scheduler.BaseScheduler]</code> - Gets all registered schedulers. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_scheduler(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -&gt; None</code> - Registers a parameter scheduler. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>reset_schedulers(self) -&gt; None</code> - Resets all schedulers to their initial state. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>sample(self, x: torch.Tensor | None = None, dim: int | None = None, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -&gt; Tuple[torch.Tensor, torch.Tensor]</code> - Runs the sampling process. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step_schedulers(self) -&gt; Dict[str, float]</code> - Advances all schedulers by one step. - <code>to(self, *args, **kwargs)</code> - Moves the sampler and its components to the specified device and/or dtype. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="samplerslangevin_dynamics">samplers.langevin_dynamics<a class="headerlink" href="#samplerslangevin_dynamics" title="Permanent link"></a></h2> <p><em>Langevin Dynamics Sampler Module.</em></p> <h3 id="langevindynamics"><code>LangevinDynamics</code><a class="headerlink" href="#langevindynamics" title="Permanent link"></a></h3> <p>Langevin Dynamics sampler.</p> <div class="language-python highlight"><table class="highlighttable"><tbody><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-66-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-66-1"><a id="__codelineno-66-1" name="__codelineno-66-1"></a><span class="n">LangevinDynamics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">step_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_scheduler</span><span class="o">.</span><span class="n">BaseScheduler</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torchebm</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">base_scheduler</span><span class="o">.</span><span class="n">BaseScheduler</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></code></pre></div></td></tr></tbody></table></div> <p><strong>Methods:</strong> - <code>add_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Add a child module to the current module. - <code>apply(self, fn: collections.abc.Callable[['Module'], None]) -&gt; Self</code> - Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. - <code>apply_mixed_precision(self, func)</code> - A decorator to apply the mixed precision context to a method. - <code>autocast_context(self)</code> - Returns a <code>torch.cuda.amp.autocast</code> context manager if mixed precision is enabled, - <code>bfloat16(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. - <code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.Tensor]</code> - Return an iterator over module buffers. - <code>children(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over immediate children modules. - <code>compile(self, *args, **kwargs) -&gt; None</code> - Compile this Module's forward using :func:<code>torch.compile</code>. - <code>cpu(self) -&gt; Self</code> - Move all model parameters and buffers to the CPU. - <code>cuda(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the GPU. - <code>double(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>double</code> datatype. - <code>eval(self) -&gt; Self</code> - Set the module in evaluation mode. - <code>extra_repr(self) -&gt; str</code> - Return the extra representation of the module. - <code>float(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>float</code> datatype. - <code>forward(self, *input: Any) -&gt; None</code> - Define the computation performed at every call. - <code>get_buffer(self, target: str) -&gt; 'Tensor'</code> - Return the buffer given by <code>target</code> if it exists, otherwise throw an error. - <code>get_extra_state(self) -&gt; Any</code> - Return any extra state to include in the module's state_dict. - <code>get_parameter(self, target: str) -&gt; 'Parameter'</code> - Return the parameter given by <code>target</code> if it exists, otherwise throw an error. - <code>get_scheduled_value(self, name: str) -&gt; float</code> - Gets the current value for a scheduled parameter. - <code>get_schedulers(self) -&gt; Dict[str, torchebm.core.base_scheduler.BaseScheduler]</code> - Gets all registered schedulers. - <code>get_submodule(self, target: str) -&gt; 'Module'</code> - Return the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>half(self) -&gt; Self</code> - Casts all floating point parameters and buffers to <code>half</code> datatype. - <code>ipu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the IPU. - <code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> - Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. - <code>modules(self) -&gt; collections.abc.Iterator['Module']</code> - Return an iterator over all modules in the network. - <code>mtia(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the MTIA. - <code>named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.Tensor]]</code> - Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. - <code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, 'Module']]</code> - Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. - <code>named_modules(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)</code> - Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. - <code>named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]</code> - Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. - <code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[torch.nn.parameter.Parameter]</code> - Return an iterator over module parameters. - <code>register_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_buffer(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -&gt; None</code> - Add a buffer to the module. - <code>register_forward_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward hook on the module. - <code>register_forward_pre_hook(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a forward pre-hook on the module. - <code>register_full_backward_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward hook on the module. - <code>register_full_backward_pre_hook(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</code> - Register a backward pre-hook on the module. - <code>register_load_state_dict_post_hook(self, hook)</code> - Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_load_state_dict_pre_hook(self, hook)</code> - Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called. - <code>register_module(self, name: str, module: ForwardRef('Module') | None) -&gt; None</code> - Alias for :func:<code>add_module</code>. - <code>register_parameter(self, name: str, param: torch.nn.parameter.Parameter | None) -&gt; None</code> - Add a parameter to the module. - <code>register_scheduler(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -&gt; None</code> - Registers a parameter scheduler. - <code>register_state_dict_post_hook(self, hook)</code> - Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>register_state_dict_pre_hook(self, hook)</code> - Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method. - <code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> - Change if autograd should record operations on parameters in this module. - <code>reset_schedulers(self) -&gt; None</code> - Resets all schedulers to their initial state. - <code>safe_to(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)</code> - Safely moves an object to a device and/or dtype, if it supports the <code>.to()</code> method. - <code>sample(self, x: torch.Tensor | None = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -&gt; torch.Tensor | Tuple[torch.Tensor, List[dict]]</code> - Generates samples using Langevin dynamics. - <code>set_extra_state(self, state: Any) -&gt; None</code> - Set extra state contained in the loaded <code>state_dict</code>. - <code>set_submodule(self, target: str, module: 'Module', strict: bool = False) -&gt; None</code> - Set the submodule given by <code>target</code> if it exists, otherwise throw an error. - <code>setup_mixed_precision(self, use_mixed_precision: bool) -&gt; None</code> - Configures mixed precision settings. - <code>share_memory(self) -&gt; Self</code> - See :meth:<code>torch.Tensor.share_memory_</code>. - <code>state_dict(self, *args, destination=None, prefix='', keep_vars=False)</code> - Return a dictionary containing references to the whole state of the module. - <code>step_schedulers(self) -&gt; Dict[str, float]</code> - Advances all schedulers by one step. - <code>to(self, *args, **kwargs)</code> - Moves the sampler and its components to the specified device and/or dtype. - <code>to_empty(self, *, device: str | torch.device | int | None, recurse: bool = True) -&gt; Self</code> - Move the parameters and buffers to the specified device without copying storage. - <code>train(self, mode: bool = True) -&gt; Self</code> - Set the module in training mode. - <code>type(self, dst_type: torch.dtype | str) -&gt; Self</code> - Casts all parameters and buffers to :attr:<code>dst_type</code>. - <code>xpu(self, device: int | torch.device | None = None) -&gt; Self</code> - Move all model parameters and buffers to the XPU. - <code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> - Reset gradients of all model parameters.</p> <h2 id="utilsimage">utils.image<a class="headerlink" href="#utilsimage" title="Permanent link"></a></h2> <p><em>Image processing utilities for TorchEBM.</em></p> <h3 id="center_crop_arrpil_image-pilimageimage-image_size-int-pilimageimage"><code>center_crop_arr(pil_image: PIL.Image.Image, image_size: int) -&gt; PIL.Image.Image</code><a class="headerlink" href="#center_crop_arrpil_image-pilimageimage-image_size-int-pilimageimage" title="Permanent link"></a></h3> <p>Center crop and resize image to target size.</p> <h3 id="create_npz_from_sample_foldersample_dir-str-num-int-50000-str"><code>create_npz_from_sample_folder(sample_dir: str, num: int = 50000) -&gt; str</code><a class="headerlink" href="#create_npz_from_sample_foldersample_dir-str-num-int-50000-str" title="Permanent link"></a></h3> <p>Build .npz file from folder of PNG samples.</p> <h2 id="utilstraining">utils.training<a class="headerlink" href="#utilstraining" title="Permanent link"></a></h2> <p><em>General training utilities for TorchEBM.</em></p> <h3 id="load_checkpointcheckpoint_path-str-model-torchnnmodulesmodulemodule-ema_model-torchnnmodulesmodulemodule-none-none-optimizer-torchoptimoptimizeroptimizer-none-none-device-torchdevice-none-none-dictstr-any"><code>load_checkpoint(checkpoint_path: str, model: torch.nn.modules.module.Module, ema_model: torch.nn.modules.module.Module | None = None, optimizer: torch.optim.optimizer.Optimizer | None = None, device: torch.device | None = None) -&gt; Dict[str, Any]</code><a class="headerlink" href="#load_checkpointcheckpoint_path-str-model-torchnnmodulesmodulemodule-ema_model-torchnnmodulesmodulemodule-none-none-optimizer-torchoptimoptimizeroptimizer-none-none-device-torchdevice-none-none-dictstr-any" title="Permanent link"></a></h3> <p>Load training checkpoint.</p> <h3 id="requires_gradmodel-torchnnmodulesmodulemodule-flag-bool-true-none"><code>requires_grad(model: torch.nn.modules.module.Module, flag: bool = True) -&gt; None</code><a class="headerlink" href="#requires_gradmodel-torchnnmodulesmodulemodule-flag-bool-true-none" title="Permanent link"></a></h3> <p>Set requires_grad flag for all model parameters.</p> <h3 id="save_checkpointmodel-torchnnmodulesmodulemodule-optimizer-torchoptimoptimizeroptimizer-step-int-checkpoint_dir-str-ema_model-torchnnmodulesmodulemodule-none-none-args-dictstr-any-none-none-str"><code>save_checkpoint(model: torch.nn.modules.module.Module, optimizer: torch.optim.optimizer.Optimizer, step: int, checkpoint_dir: str, ema_model: torch.nn.modules.module.Module | None = None, args: Dict[str, Any] | None = None) -&gt; str</code><a class="headerlink" href="#save_checkpointmodel-torchnnmodulesmodulemodule-optimizer-torchoptimoptimizeroptimizer-step-int-checkpoint_dir-str-ema_model-torchnnmodulesmodulemodule-none-none-args-dictstr-any-none-none-str" title="Permanent link"></a></h3> <p>Save training checkpoint.</p> <h3 id="update_emaema_model-torchnnmodulesmodulemodule-model-torchnnmodulesmodulemodule-decay-float-09999-none"><code>update_ema(ema_model: torch.nn.modules.module.Module, model: torch.nn.modules.module.Module, decay: float = 0.9999) -&gt; None</code><a class="headerlink" href="#update_emaema_model-torchnnmodulesmodulemodule-model-torchnnmodulesmodulemodule-decay-float-09999-none" title="Permanent link"></a></h3> <p>Update EMA model parameters.</p> <h2 id="utilsvisualization">utils.visualization<a class="headerlink" href="#utilsvisualization" title="Permanent link"></a></h2> <h3 id="plot_2d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-device-str-none-none-matplotlibfigurefigure"><code>plot_2d_energy_landscape(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, device: str | None = None) -&gt; matplotlib.figure.Figure</code><a class="headerlink" href="#plot_2d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-device-str-none-none-matplotlibfigurefigure" title="Permanent link"></a></h3> <p>Plots a 2D energy landscape of a model.</p> <h3 id="plot_3d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-50-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-10-8-alpha-float-09-elev-float-30-azim-float-45-device-str-none-none-matplotlibfigurefigure"><code>plot_3d_energy_landscape(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 50, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (10, 8), alpha: float = 0.9, elev: float = 30, azim: float = -45, device: str | None = None) -&gt; matplotlib.figure.Figure</code><a class="headerlink" href="#plot_3d_energy_landscapemodel-torchebmcorebase_modelbasemodel-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-50-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-10-8-alpha-float-09-elev-float-30-azim-float-45-device-str-none-none-matplotlibfigurefigure" title="Permanent link"></a></h3> <p>Plots a 3D surface visualization of a 2D energy landscape.</p> <h3 id="plot_sample_trajectoriestrajectories-torchtensor-model-torchebmcorebase_modelbasemodel-none-none-x_range-tuplefloat-float-none-y_range-tuplefloat-float-none-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-trajectory_colors-liststr-none-none-trajectory_alpha-float-07-line_width-float-10-device-str-none-none-matplotlibfigurefigure"><code>plot_sample_trajectories(trajectories: torch.Tensor, model: torchebm.core.base_model.BaseModel | None = None, x_range: Tuple[float, float] = None, y_range: Tuple[float, float] = None, resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), trajectory_colors: List[str] | None = None, trajectory_alpha: float = 0.7, line_width: float = 1.0, device: str | None = None) -&gt; matplotlib.figure.Figure</code><a class="headerlink" href="#plot_sample_trajectoriestrajectories-torchtensor-model-torchebmcorebase_modelbasemodel-none-none-x_range-tuplefloat-float-none-y_range-tuplefloat-float-none-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-trajectory_colors-liststr-none-none-trajectory_alpha-float-07-line_width-float-10-device-str-none-none-matplotlibfigurefigure" title="Permanent link"></a></h3> <p>Plots sample trajectories, optionally on an energy landscape background.</p> <h3 id="plot_samples_on_energymodel-torchebmcorebase_modelbasemodel-samples-torchtensor-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-sample_color-str-red-sample_alpha-float-05-sample_size-float-5-device-str-none-none-matplotlibfigurefigure"><code>plot_samples_on_energy(model: torchebm.core.base_model.BaseModel, samples: torch.Tensor, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, sample_color: str = 'red', sample_alpha: float = 0.5, sample_size: float = 5, device: str | None = None) -&gt; matplotlib.figure.Figure</code><a class="headerlink" href="#plot_samples_on_energymodel-torchebmcorebase_modelbasemodel-samples-torchtensor-x_range-tuplefloat-float-5-5-y_range-tuplefloat-float-5-5-resolution-int-100-log_scale-bool-false-cmap-str-viridis-title-str-none-none-show_colorbar-bool-true-save_path-str-none-none-fig_size-tupleint-int-8-6-contour-bool-true-contour_levels-int-20-sample_color-str-red-sample_alpha-float-05-sample_size-float-5-device-str-none-none-matplotlibfigurefigure" title="Permanent link"></a></h3> <p>Plots samples on a 2D energy landscape.</p> <aside class="md-source-file"> <span class="md-source-file__fact"> <span class="md-icon" title="Last update"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="February 11, 2026 05:30:35 UTC">February 11, 2026</span> </span> <span class="md-source-file__fact"> <span class="md-icon" title="Created"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="February 11, 2026 05:30:35 UTC">February 11, 2026</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type="button" class="md-top md-icon" data-md-component="top" hidden> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class="md-footer"> <nav class="md-footer__inner md-grid" aria-label="Footer"> <a href="../../api/torchebm/utils/visualization/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Visualization"> <div class="md-footer__button md-icon"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class="md-footer__title"> <span class="md-footer__direction"> Previous </span> <div class="md-ellipsis"> Visualization </div> </div> </a> <a href="../../examples/" class="md-footer__link md-footer__link--next" aria-label="Next: Overview"> <div class="md-footer__title"> <span class="md-footer__direction"> Next </span> <div class="md-ellipsis"> Overview </div> </div> <div class="md-footer__button md-icon"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class="md-copyright"> <div class="md-copyright__highlight"> Copyright  2025 Soran Ghaderi. Licensed under the MIT License. </div> </div> <div class="md-social"> <a href="https://github.com/soran-ghaderi" target="_blank" rel="noopener" title="github.com" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a> <a href="https://pypi.org/project/torchebm/" target="_blank" rel="noopener" title="pypi.org" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8"></path></svg> </a> <a href="https://bsky.app/profile/soranghaderi.bsky.social" target="_blank" rel="noopener" title="bsky.app" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3 3.4.4 6.7.9 10 1.3M288 227.1c-26.1-50.7-97.1-145.2-163.1-191.8C61.6-9.4 37.5-1.7 21.6 5.5 3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7 3.3-.5 6.6-.9 10-1.4-3.3.5-6.6 1-10 1.4-93.9 14-177.3 48.2-67.9 169.9C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4 102.4-103.4 28.1-156-65.8-169.9-3.3-.4-6.7-.8-10-1.3 3.4.4 6.7.9 10 1.3 64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1"></path></svg> </a> <a href="https://twitter.com/soranghadri" target="_blank" rel="noopener" title="twitter.com" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"></path></svg> </a> <a href="https://www.linkedin.com/in/soran-ghaderi/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a> <a href="https://scholar.google.com/citations?user=-2N2iKcAAAAJ&amp;hl=en" target="_blank" rel="noopener" title="scholar.google.com" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7q6.6-11.4 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"></path></svg> </a> <a href="https://www.researchgate.net/profile/Soran-Ghaderi" target="_blank" rel="noopener" title="www.researchgate.net" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M0 32v448h448V32zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9m-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6"></path></svg> </a> <a href="https://orcid.org/0009-0004-6564-4517" target="_blank" rel="noopener" title="orcid.org" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M294.7 188.2h-45.9V342h47.5c67.6 0 83.1-51.3 83.1-76.9 0-41.6-26.5-76.9-84.7-76.9M256 8a248 248 0 1 0 0 496 248 248 0 1 0 0-496m-80.8 360.8h-29.8V161.3h29.8zM160.3 98.5a19.6 19.6 0 1 1 0 39.2 19.6 19.6 0 1 1 0-39.2M300 369h-81V161.3h80.6c76.7 0 110.4 54.8 110.4 103.9 0 53.3-41.7 103.9-110 103.9z"></path></svg> </a> <a href="https://soran-ghaderi.github.io/" target="_blank" rel="noopener" title="soran-ghaderi.github.io" class="md-social__link"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M16.36 14c.08-.66.14-1.32.14-2s-.06-1.34-.14-2h3.38c.16.64.26 1.31.26 2s-.1 1.36-.26 2m-5.15 5.56c.6-1.11 1.06-2.31 1.38-3.56h2.95a8.03 8.03 0 0 1-4.33 3.56M14.34 14H9.66c-.1-.66-.16-1.32-.16-2s.06-1.35.16-2h4.68c.09.65.16 1.32.16 2s-.07 1.34-.16 2M12 19.96c-.83-1.2-1.5-2.53-1.91-3.96h3.82c-.41 1.43-1.08 2.76-1.91 3.96M8 8H5.08A7.92 7.92 0 0 1 9.4 4.44C8.8 5.55 8.35 6.75 8 8m-2.92 8H8c.35 1.25.8 2.45 1.4 3.56A8 8 0 0 1 5.08 16m-.82-2C4.1 13.36 4 12.69 4 12s.1-1.36.26-2h3.38c-.08.66-.14 1.32-.14 2s.06 1.34.14 2M12 4.03c.83 1.2 1.5 2.54 1.91 3.97h-3.82c.41-1.43 1.08-2.77 1.91-3.97M18.92 8h-2.95a15.7 15.7 0 0 0-1.38-3.56c1.84.63 3.37 1.9 4.33 3.56M12 2C6.47 2 2 6.5 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2"></path></svg> </a> </div> </div> </div> </footer> </div> <div class="md-dialog" data-md-component="dialog"> <div class="md-dialog__inner md-typeset"></div> </div> <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.indexes", "navigation.top", "navigation.tracking", "navigation.expand", "navigation.path", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.tabs.link", "content.code.annotation", "content.code.copy", "content.tooltips", "header.autohide", "announce.dismiss", "navigation.footer"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script> <script src="../../javascripts/mathjax.js"></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script src="../../javascripts/cards-clickable.js"></script>  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>