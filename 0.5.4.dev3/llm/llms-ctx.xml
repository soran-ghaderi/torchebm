<torchebm title="TorchEBM API" summary="Energy-Based Modeling library for PyTorch">

<module name="core.base_integrator">
  <class name="BaseIntegrator" init="(self, device: torch.device | None = None, dtype: torch.dtype | None = None, *args, **kwargs)">
    <doc>Abstract integrator that advances a sampler state according to dynamics.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="integrate" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, *args, **kwargs) -> Dict[str, torch.Tensor]"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, *args, **kwargs) -> Dict[str, torch.Tensor]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="core.base_interpolant">
  <class name="BaseInterpolant" init="(self, /, *args, **kwargs)">
    <doc>Abstract base class for stochastic interpolants.</doc>
    <method name="compute_alpha_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_d_alpha_alpha_ratio_t" sig="(self, t: torch.Tensor) -> torch.Tensor"/>
    <method name="compute_diffusion" sig="(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -> torch.Tensor"/>
    <method name="compute_drift" sig="(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_sigma_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="interpolate" sig="(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="score_to_velocity" sig="(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_noise" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_score" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
  </class>
  <function name="expand_t_like_x" sig="(t: torch.Tensor, x: torch.Tensor) -> torch.Tensor"/>
</module>

<module name="core.base_loss">
  <class name="BaseContrastiveDivergence" init="(self, model: torchebm.core.base_model.BaseModel, sampler: torchebm.core.base_sampler.BaseSampler, k_steps: int = 1, persistent: bool = False, buffer_size: int = 100, new_sample_ratio: float = 0.0, init_steps: int = 0, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, use_mixed_precision: bool = False, clip_value: float | None = None, *args, **kwargs)">
    <doc>Abstract base class for Contrastive Divergence (CD) based loss functions.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_negative_samples" sig="(self, x, batch_size, data_shape) -> torch.Tensor"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_start_points" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="initialize_buffer" sig="(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -> torch.Tensor"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="update_buffer" sig="(self, samples: torch.Tensor) -> None"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="BaseLoss" init="(self, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, use_mixed_precision: bool = False, clip_value: float | None = None, *args: Any, **kwargs: Any)">
    <doc>Abstract base class for loss functions used in energy-based models.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="BaseScoreMatching" init="(self, model: torchebm.core.base_model.BaseModel, noise_scale: float = 0.01, regularization_strength: float = 0.0, use_autograd: bool = True, hutchinson_samples: int = 1, custom_regularization: Callable | None = None, use_mixed_precision: bool = False, clip_value: float | None = None, *args, **kwargs)">
    <doc>Abstract base class for Score Matching based loss functions.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="add_regularization" sig="(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -> torch.Tensor"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="compute_score" sig="(self, x: torch.Tensor, noise: torch.Tensor | None = None) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="perturb_data" sig="(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="core.base_model">
  <class name="AckleyEnergy" init="(self, *args, **kwargs)">
    <doc>Energy-based model for the Ackley function.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="AckleyModel" init="(self, a: float = 20.0, b: float = 0.2, c: float = 6.283185307179586, *args, **kwargs)">
    <doc>Energy-based model for the Ackley function.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="BaseEnergyFunction" init="(self, *args, **kwargs)">
    <doc>Abstract base class for energy-based models (EBMs).</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="BaseModel" init="(self, dtype: torch.dtype = torch.float32, use_mixed_precision: bool = False, *args, **kwargs)">
    <doc>Abstract base class for energy-based models (EBMs).</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="DoubleWellEnergy" init="(self, *args, **kwargs)">
    <doc>Energy-based model for a double-well potential.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="DoubleWellModel" init="(self, barrier_height: float = 2.0, b: float = 1.0, *args, **kwargs)">
    <doc>Energy-based model for a double-well potential.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="GaussianEnergy" init="(self, *args, **kwargs)">
    <doc>Energy-based model for a Gaussian distribution.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="GaussianModel" init="(self, mean: torch.Tensor, cov: torch.Tensor, *args, **kwargs)">
    <doc>Energy-based model for a Gaussian distribution.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="HarmonicEnergy" init="(self, *args, **kwargs)">
    <doc>Energy-based model for a harmonic oscillator.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="HarmonicModel" init="(self, k: float = 1.0, *args, **kwargs)">
    <doc>Energy-based model for a harmonic oscillator.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="RastriginEnergy" init="(self, *args, **kwargs)">
    <doc>Energy-based model for the Rastrigin function.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="RastriginModel" init="(self, a: float = 10.0, *args, **kwargs)">
    <doc>Energy-based model for the Rastrigin function.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="RosenbrockEnergy" init="(self, *args, **kwargs)">
    <doc>Energy-based model for the Rosenbrock function.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="RosenbrockModel" init="(self, a: float = 1.0, b: float = 100.0, *args, **kwargs)">
    <doc>Energy-based model for the Rosenbrock function.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="gradient" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="core.base_sampler">
  <class name="BaseSampler" init="(self, model: torch.nn.modules.module.Module, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, use_mixed_precision: bool = False, *args, **kwargs)">
    <doc>Abstract base class for samplers.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="apply_mixed_precision" sig="(self, func)"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_scheduled_value" sig="(self, name: str) -> float"/>
    <method name="get_schedulers" sig="(self) -> Dict[str, torchebm.core.base_scheduler.BaseScheduler]"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_scheduler" sig="(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="reset_schedulers" sig="(self) -> None"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="sample" sig="(self, x: torch.Tensor | None = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -> torch.Tensor | Tuple[torch.Tensor, List[dict]]"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step_schedulers" sig="(self) -> Dict[str, float]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="core.base_scheduler">
  <class name="BaseScheduler" init="(self, start_value: float)">
    <doc>Abstract base class for parameter schedulers.</doc>
    <method name="get_value" sig="(self) -> float"/>
    <method name="load_state_dict" sig="(self, state_dict: Dict[str, Any]) -> None"/>
    <method name="reset" sig="(self) -> None"/>
    <method name="state_dict" sig="(self) -> Dict[str, Any]"/>
    <method name="step" sig="(self) -> float"/>
  </class>
  <class name="ConstantScheduler" init="(self, start_value: float)">
    <doc>Scheduler that maintains a constant parameter value.</doc>
    <method name="get_value" sig="(self) -> float"/>
    <method name="load_state_dict" sig="(self, state_dict: Dict[str, Any]) -> None"/>
    <method name="reset" sig="(self) -> None"/>
    <method name="state_dict" sig="(self) -> Dict[str, Any]"/>
    <method name="step" sig="(self) -> float"/>
  </class>
  <class name="CosineScheduler" init="(self, start_value: float, end_value: float, n_steps: int)">
    <doc>Scheduler with cosine annealing.</doc>
    <method name="get_value" sig="(self) -> float"/>
    <method name="load_state_dict" sig="(self, state_dict: Dict[str, Any]) -> None"/>
    <method name="reset" sig="(self) -> None"/>
    <method name="state_dict" sig="(self) -> Dict[str, Any]"/>
    <method name="step" sig="(self) -> float"/>
  </class>
  <class name="ExponentialDecayScheduler" init="(self, start_value: float, decay_rate: float, min_value: float = 0.0)">
    <doc>Scheduler with exponential decay.</doc>
    <method name="get_value" sig="(self) -> float"/>
    <method name="load_state_dict" sig="(self, state_dict: Dict[str, Any]) -> None"/>
    <method name="reset" sig="(self) -> None"/>
    <method name="state_dict" sig="(self) -> Dict[str, Any]"/>
    <method name="step" sig="(self) -> float"/>
  </class>
  <class name="LinearScheduler" init="(self, start_value: float, end_value: float, n_steps: int)">
    <doc>Scheduler with linear interpolation between start and end values.</doc>
    <method name="get_value" sig="(self) -> float"/>
    <method name="load_state_dict" sig="(self, state_dict: Dict[str, Any]) -> None"/>
    <method name="reset" sig="(self) -> None"/>
    <method name="state_dict" sig="(self) -> Dict[str, Any]"/>
    <method name="step" sig="(self) -> float"/>
  </class>
  <class name="MultiStepScheduler" init="(self, start_value: float, milestones: List[int], gamma: float = 0.1)">
    <doc>Scheduler that reduces the parameter value at specific milestone steps.</doc>
    <method name="get_value" sig="(self) -> float"/>
    <method name="load_state_dict" sig="(self, state_dict: Dict[str, Any]) -> None"/>
    <method name="reset" sig="(self) -> None"/>
    <method name="state_dict" sig="(self) -> Dict[str, Any]"/>
    <method name="step" sig="(self) -> float"/>
  </class>
  <class name="WarmupScheduler" init="(self, main_scheduler: torchebm.core.base_scheduler.BaseScheduler, warmup_steps: int, warmup_init_factor: float = 0.01)">
    <doc>Scheduler that combines linear warmup with another scheduler.</doc>
    <method name="get_value" sig="(self) -> float"/>
    <method name="load_state_dict" sig="(self, state_dict: Dict[str, Any]) -> None"/>
    <method name="reset" sig="(self) -> None"/>
    <method name="state_dict" sig="(self) -> Dict[str, Any]"/>
    <method name="step" sig="(self) -> float"/>
  </class>
</module>

<module name="core.base_trainer">
  <class name="BaseTrainer" init="(self, model: torchebm.core.base_model.BaseModel, optimizer: torch.optim.optimizer.Optimizer, loss_fn: torchebm.core.base_loss.BaseLoss, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, use_mixed_precision: bool = False, callbacks: List[Callable] | None = None)">
    <doc>Base class for training energy-based models.</doc>
    <method name="autocast_context" sig="(self)"/>
    <method name="load_checkpoint" sig="(self, path: str) -> None"/>
    <method name="save_checkpoint" sig="(self, path: str) -> None"/>
    <method name="train" sig="(self, dataloader: torch.utils.data.dataloader.DataLoader, num_epochs: int, validate_fn: Callable | None = None) -> Dict[str, List[float]]"/>
    <method name="train_epoch" sig="(self, dataloader: torch.utils.data.dataloader.DataLoader) -> Dict[str, float]"/>
    <method name="train_step" sig="(self, batch: torch.Tensor) -> Dict[str, Any]"/>
  </class>
  <class name="ContrastiveDivergenceTrainer" init="(self, model: torchebm.core.base_model.BaseModel, sampler: torchebm.core.base_sampler.BaseSampler, optimizer: torch.optim.optimizer.Optimizer | None = None, learning_rate: float = 0.01, k_steps: int = 10, persistent: bool = False, buffer_size: int = 1000, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, use_mixed_precision: bool = False)">
    <doc>Specialized trainer for contrastive divergence training of EBMs.</doc>
    <method name="autocast_context" sig="(self)"/>
    <method name="load_checkpoint" sig="(self, path: str) -> None"/>
    <method name="save_checkpoint" sig="(self, path: str) -> None"/>
    <method name="train" sig="(self, dataloader: torch.utils.data.dataloader.DataLoader, num_epochs: int, validate_fn: Callable | None = None) -> Dict[str, List[float]]"/>
    <method name="train_epoch" sig="(self, dataloader: torch.utils.data.dataloader.DataLoader) -> Dict[str, float]"/>
    <method name="train_step" sig="(self, batch: torch.Tensor) -> Dict[str, Any]"/>
  </class>
</module>

<module name="core.device_mixin">
  <class name="DeviceMixin" init="(self, device: str | torch.device | None = None, dtype: torch.dtype | None = None, *args, **kwargs)">
    <doc>A mixin for consistent device and dtype management across all modules.</doc>
    <method name="autocast_context" sig="(self)"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
  </class>
  <function name="normalize_device" sig="(device)"/>
</module>

<module name="datasets.generators">
  <class name="BaseSyntheticDataset" init="(self, n_samples: int, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Abstract base class for generating 2D synthetic datasets.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="CheckerboardDataset" init="(self, n_samples: int = 2000, range_limit: float = 4.0, noise: float = 0.01, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates points in a 2D checkerboard pattern using rejection sampling.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="CircleDataset" init="(self, n_samples: int = 2000, noise: float = 0.05, radius: float = 1.0, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates points sampled uniformly on a circle with noise.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="EightGaussiansDataset" init="(self, n_samples: int = 2000, std: float = 0.02, scale: float = 2.0, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates samples from the '8 Gaussians' mixture distribution.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="GaussianMixtureDataset" init="(self, n_samples: int = 2000, n_components: int = 8, std: float = 0.05, radius: float = 1.0, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates a 2D Gaussian mixture dataset with components arranged in a circle.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="GridDataset" init="(self, n_samples_per_dim: int = 10, range_limit: float = 1.0, noise: float = 0.01, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates points on a 2D grid.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="PinwheelDataset" init="(self, n_samples: int = 2000, n_classes: int = 5, noise: float = 0.05, radial_scale: float = 2.0, angular_scale: float = 0.1, spiral_scale: float = 5.0, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates the pinwheel dataset with curved blades.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="SwissRollDataset" init="(self, n_samples: int = 2000, noise: float = 0.05, arclength: float = 3.0, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates a 2D Swiss roll dataset.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
  <class name="TwoMoonsDataset" init="(self, n_samples: int = 2000, noise: float = 0.05, device: str | torch.device | None = None, dtype: torch.dtype = torch.float32, seed: int | None = None)">
    <doc>Generates the 'two moons' dataset.</doc>
    <method name="get_data" sig="(self) -> torch.Tensor"/>
    <method name="regenerate" sig="(self, seed: int | None = None)"/>
  </class>
</module>

<module name="integrators.euler_maruyama">
  <class name="EulerMaruyamaIntegrator" init="(self, device: torch.device | None = None, dtype: torch.dtype | None = None, *args, **kwargs)">
    <doc>Euler-Maruyama integrator for It SDEs and ODEs.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="integrate" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, noise_scale: torch.Tensor | None = None, t: torch.Tensor | None = None) -> Dict[str, torch.Tensor]"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: torch.Tensor | None = None, noise: torch.Tensor | None = None, noise_scale: torch.Tensor | None = None, t: torch.Tensor | None = None) -> Dict[str, torch.Tensor]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="integrators.heun">
  <class name="HeunIntegrator" init="(self, device: torch.device | None = None, dtype: torch.dtype | None = None, *args, **kwargs)">
    <doc>Heun integrator (predictor-corrector) for It SDEs and ODEs.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="integrate" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, noise_scale: torch.Tensor | None = None, t: torch.Tensor) -> Dict[str, torch.Tensor]"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, *, drift: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None, diffusion: torch.Tensor | None = None, t: torch.Tensor, noise: torch.Tensor | None = None, noise_scale: torch.Tensor | None = None) -> Dict[str, torch.Tensor]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="integrators.leapfrog">
  <class name="LeapfrogIntegrator" init="(self, device: torch.device | None = None, dtype: torch.dtype | None = None)">
    <doc>Symplectic leapfrog (StrmerVerlet) integrator for Hamiltonian dynamics.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="integrate" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, n_steps: int, mass: float | torch.Tensor | None = None, *, potential_grad: Callable[[torch.Tensor], torch.Tensor] | None = None) -> Dict[str, torch.Tensor]"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step" sig="(self, state: Dict[str, torch.Tensor], model: torchebm.core.base_model.BaseModel | None, step_size: torch.Tensor, mass: float | torch.Tensor | None = None, *, potential_grad: Callable[[torch.Tensor], torch.Tensor] | None = None) -> Dict[str, torch.Tensor]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="interpolants.cosine">
  <class name="CosineInterpolant" init="(self, /, *args, **kwargs)">
    <doc>Cosine (geodesic variance preserving) interpolant.</doc>
    <method name="compute_alpha_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_d_alpha_alpha_ratio_t" sig="(self, t: torch.Tensor) -> torch.Tensor"/>
    <method name="compute_diffusion" sig="(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -> torch.Tensor"/>
    <method name="compute_drift" sig="(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_sigma_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="interpolate" sig="(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="score_to_velocity" sig="(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_noise" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_score" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
  </class>
</module>

<module name="interpolants.linear">
  <class name="LinearInterpolant" init="(self, /, *args, **kwargs)">
    <doc>Linear interpolant between noise and data distributions.</doc>
    <method name="compute_alpha_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_d_alpha_alpha_ratio_t" sig="(self, t: torch.Tensor) -> torch.Tensor"/>
    <method name="compute_diffusion" sig="(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -> torch.Tensor"/>
    <method name="compute_drift" sig="(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_sigma_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="interpolate" sig="(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="score_to_velocity" sig="(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_noise" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_score" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
  </class>
</module>

<module name="interpolants.variance_preserving">
  <class name="VariancePreservingInterpolant" init="(self, sigma_min: float = 0.1, sigma_max: float = 20.0)">
    <doc>Variance preserving (VP) interpolant with linear beta schedule.</doc>
    <method name="compute_alpha_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_d_alpha_alpha_ratio_t" sig="(self, t: torch.Tensor) -> torch.Tensor"/>
    <method name="compute_diffusion" sig="(self, x: torch.Tensor, t: torch.Tensor, form: str = 'SBDM', norm: float = 1.0) -> torch.Tensor"/>
    <method name="compute_drift" sig="(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="compute_sigma_t" sig="(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="interpolate" sig="(self, x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="score_to_velocity" sig="(self, score: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_noise" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
    <method name="velocity_to_score" sig="(self, velocity: torch.Tensor, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor"/>
  </class>
</module>

<module name="losses.contrastive_divergence">
  <class name="ContrastiveDivergence" init="(self, model, sampler, k_steps=10, persistent=False, buffer_size=10000, init_steps=100, new_sample_ratio=0.05, energy_reg_weight=0.001, use_temperature_annealing=False, min_temp=0.01, max_temp=2.0, temp_decay=0.999, dtype=torch.float32, device=device(type='cpu'), *args, **kwargs)">
    <doc>Standard Contrastive Divergence (CD-k) loss.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_negative_samples" sig="(self, x, batch_size, data_shape) -> torch.Tensor"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_start_points" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="initialize_buffer" sig="(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -> torch.Tensor"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="update_buffer" sig="(self, samples: torch.Tensor) -> None"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="ParallelTemperingCD" init="(self, temps=[1.0, 0.5], k=5)">
    <doc>Abstract base class for Contrastive Divergence (CD) based loss functions.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_negative_samples" sig="(self, x, batch_size, data_shape) -> torch.Tensor"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_start_points" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="initialize_buffer" sig="(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -> torch.Tensor"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="update_buffer" sig="(self, samples: torch.Tensor) -> None"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="PersistentContrastiveDivergence" init="(self, buffer_size=100)">
    <doc>Abstract base class for Contrastive Divergence (CD) based loss functions.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_negative_samples" sig="(self, x, batch_size, data_shape) -> torch.Tensor"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_start_points" sig="(self, x: torch.Tensor) -> torch.Tensor"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="initialize_buffer" sig="(self, data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -> torch.Tensor"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="update_buffer" sig="(self, samples: torch.Tensor) -> None"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="losses.equilibrium_matching">
  <class name="EquilibriumMatchingLoss" init="(self, model: 'nn.Module', prediction: "Literal['velocity', 'score', 'noise']" = 'velocity', energy_type: "Literal['none', 'dot', 'l2', 'mean']" = 'none', interpolant: 'Union[str, BaseInterpolant]' = 'linear', loss_weight: "Optional[Literal['velocity', 'likelihood']]" = None, train_eps: 'float' = 0.0, ct_threshold: 'float' = 0.8, ct_multiplier: 'float' = 4.0, apply_dispersion: 'bool' = False, dispersion_weight: 'float' = 0.5, time_invariant: 'bool' = True, dtype: 'torch.dtype' = torch.float32, device: 'Optional[Union[str, torch.device]]' = None, use_mixed_precision: 'bool' = False, clip_value: 'Optional[float]' = None, *args, **kwargs)">
    <doc>Equilibrium Matching (EqM) training loss.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: 'torch.Tensor', *args, **kwargs) -> 'torch.Tensor'"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: 'torch.Tensor', *args, **kwargs) -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="training_losses" sig="(self, x1: 'torch.Tensor', model_kwargs: 'Optional[Dict[str, Any]]' = None) -> 'Dict[str, torch.Tensor]'"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="losses.loss_utils">
  <function name="compute_eqm_ct" sig="(t: torch.Tensor, threshold: float = 0.8, multiplier: float = 4.0) -> torch.Tensor"/>
  <function name="dispersive_loss" sig="(z: torch.Tensor) -> torch.Tensor"/>
  <function name="get_interpolant" sig="(interpolant_type: str) -> torchebm.core.base_interpolant.BaseInterpolant"/>
  <function name="mean_flat" sig="(tensor: torch.Tensor) -> torch.Tensor"/>
</module>

<module name="losses.score_matching">
  <class name="DenoisingScoreMatching" init="(self, model: torchebm.core.base_model.BaseModel, noise_scale: float = 0.01, regularization_strength: float = 0.0, custom_regularization: Callable | None = None, use_mixed_precision: bool = False, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, *args, **kwargs)">
    <doc>Denoising Score Matching (DSM) from Vincent (2011).</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="add_regularization" sig="(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -> torch.Tensor"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="compute_score" sig="(self, x: torch.Tensor, noise: torch.Tensor | None = None) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="perturb_data" sig="(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="ScoreMatching" init="(self, model: torchebm.core.base_model.BaseModel, hessian_method: str = 'exact', regularization_strength: float = 0.0, custom_regularization: Callable | None = None, use_mixed_precision: bool = False, is_training=True, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, *args, **kwargs)">
    <doc>Original Score Matching loss from Hyvrinen (2005).</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="add_regularization" sig="(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -> torch.Tensor"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="compute_score" sig="(self, x: torch.Tensor, noise: torch.Tensor | None = None) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="perturb_data" sig="(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="SlicedScoreMatching" init="(self, model: torchebm.core.base_model.BaseModel, n_projections: int = 5, projection_type: str = 'rademacher', regularization_strength: float = 0.0, custom_regularization: Callable | None = None, use_mixed_precision: bool = False, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, *args, **kwargs)">
    <doc>Sliced Score Matching (SSM) from Song et al. (2019).</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="add_regularization" sig="(self, loss: torch.Tensor, x: torch.Tensor, custom_reg_fn: Callable | None = None, reg_strength: float | None = None) -> torch.Tensor"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="compute_loss" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="compute_score" sig="(self, x: torch.Tensor, noise: torch.Tensor | None = None) -> torch.Tensor"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="perturb_data" sig="(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="models.components.embeddings">
  <class name="LabelEmbedder" init="(self, num_classes: 'int', out_dim: 'int', dropout_prob: 'float' = 0.0)">
    <doc>Label embedding with optional classifier-free guidance token dropping.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, labels: 'torch.Tensor', *, training: 'bool', force_drop_mask: 'Optional[torch.Tensor]' = None) -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="maybe_drop_labels" sig="(self, labels: 'torch.Tensor', *, force_drop_mask: 'Optional[torch.Tensor]' = None) -> 'torch.Tensor'"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="MLPTimestepEmbedder" init="(self, out_dim: 'int', frequency_embedding_size: 'int' = 256)">
    <doc>Embed a scalar timestep into a vector via sinusoid + MLP.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, t: 'torch.Tensor') -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="sinusoidal_embedding" sig="(t: 'torch.Tensor', dim: 'int', max_period: 'int' = 10000) -> 'torch.Tensor'"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="models.components.heads">
  <class name="AdaLNZeroPatchHead" init="(self, *, embed_dim: 'int', cond_dim: 'Optional[int]' = None, patch_size: 'int', out_channels: 'int', eps: 'float' = 1e-06)">
    <doc>Final layer that maps token features to patch pixels with adaLN-Zero.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, tokens: 'torch.Tensor', cond: 'torch.Tensor') -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="models.components.patch">
  <class name="ConvPatchEmbed2d" init="(self, *, in_channels: 'int', embed_dim: 'int', patch_size: 'int')">
    <doc>Patch embedding via strided conv.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: 'torch.Tensor') -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <function name="patchify2d" sig="(x: 'torch.Tensor', patch_size: 'int') -> 'torch.Tensor'"/>
  <function name="unpatchify2d" sig="(tokens: 'torch.Tensor', patch_size: 'int', *, out_channels: 'int') -> 'torch.Tensor'"/>
</module>

<module name="models.components.positional">
  <function name="build_2d_sincos_pos_embed" sig="(embed_dim: 'int', grid_size: 'int', *, device: 'torch.device | None' = None, dtype: 'torch.dtype' = torch.float32) -> 'torch.Tensor'"/>
</module>

<module name="models.components.transformer">
  <class name="AdaLNZeroBlock" init="(self, *, embed_dim: 'int', num_heads: 'int', cond_dim: 'Optional[int]' = None, mlp_ratio: 'float' = 4.0, attn: 'Optional[nn.Module]' = None, mlp: 'Optional[nn.Module]' = None, eps: 'float' = 1e-06)">
    <doc>Transformer block with adaLN-Zero conditioning.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: 'torch.Tensor', cond: 'torch.Tensor') -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="FeedForward" init="(self, embed_dim: 'int', mlp_ratio: 'float' = 4.0, dropout: 'float' = 0.0)">
    <doc>Base class for all neural network modules.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: 'torch.Tensor') -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="MultiheadSelfAttention" init="(self, embed_dim: 'int', num_heads: 'int', dropout: 'float' = 0.0)">
    <doc>Self-attention wrapper with batch-first API.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: 'torch.Tensor') -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <function name="modulate" sig="(x: 'torch.Tensor', shift: 'torch.Tensor', scale: 'torch.Tensor') -> 'torch.Tensor'"/>
</module>

<module name="models.conditional_transformer_2d">
  <class name="ConditionalTransformer2D" init="(self, *, in_channels: 'int', out_channels: 'int', input_size: 'int', patch_size: 'int', embed_dim: 'int', depth: 'int', num_heads: 'int', cond_dim: 'Optional[int]' = None, mlp_ratio: 'float' = 4.0, use_sincos_pos_embed: 'bool' = True)">
    <doc>Generic conditional 2D Transformer backbone.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: 'torch.Tensor', cond: 'torch.Tensor') -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="models.wrappers">
  <class name="LabelClassifierFreeGuidance" init="(self, base: 'nn.Module', *, null_label_id: 'int', cfg_scale: 'float' = 1.0, guide_channels: 'int' = 3)">
    <doc>Classifier-free guidance wrapper for label-conditioned models.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, x: 'torch.Tensor', t: 'torch.Tensor', *, y: 'torch.Tensor', **kwargs) -> 'torch.Tensor'"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="samplers.flow">
  <class name="FlowSampler" init="(self, model: torch.nn.modules.module.Module, interpolant: str | torchebm.core.base_interpolant.BaseInterpolant = 'linear', prediction: Literal['velocity', 'score', 'noise'] = 'velocity', train_eps: float = 0.0, sample_eps: float = 0.0, negate_velocity: bool = False, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, use_mixed_precision: bool = False, *args, **kwargs)">
    <doc>Sampler for flow-based and diffusion generative models.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="apply_mixed_precision" sig="(self, func)"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_scheduled_value" sig="(self, name: str) -> float"/>
    <method name="get_schedulers" sig="(self) -> Dict[str, torchebm.core.base_scheduler.BaseScheduler]"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="prior_logp" sig="(self, z: torch.Tensor) -> torch.Tensor"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_scheduler" sig="(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="reset_schedulers" sig="(self) -> None"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="sample" sig="(self, x: torch.Tensor | None = None, dim: int = 10, n_steps: int = 50, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *, mode: Literal['ode', 'sde'] = 'ode', shape: Tuple[int, ...] | None = None, ode_method: str = 'dopri5', atol: float = 1e-06, rtol: float = 0.001, reverse: bool = False, sde_method: str = 'euler', diffusion_form: str = 'SBDM', diffusion_norm: float = 1.0, last_step: str | None = 'Mean', last_step_size: float = 0.04, **model_kwargs) -> torch.Tensor"/>
    <method name="sample_ode" sig="(self, z: torch.Tensor, num_steps: int = 50, method: str = 'dopri5', atol: float = 1e-06, rtol: float = 0.001, reverse: bool = False, **model_kwargs) -> torch.Tensor"/>
    <method name="sample_sde" sig="(self, z: torch.Tensor, num_steps: int = 250, method: str = 'euler', diffusion_form: str = 'SBDM', diffusion_norm: float = 1.0, last_step: str | None = 'Mean', last_step_size: float = 0.04, **model_kwargs) -> torch.Tensor"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step_schedulers" sig="(self) -> Dict[str, float]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="PredictionType" init="(self, /, *args, **kwargs)">
    <doc>Model prediction type for generative models.</doc>
  </class>
</module>

<module name="samplers.gradient_descent">
  <class name="GradientDescentSampler" init="(self, model: 'BaseModel', step_size: 'Union[float, BaseScheduler]' = 0.001, dtype: 'torch.dtype' = torch.float32, device: 'Optional[Union[str, torch.device]]' = None, use_mixed_precision: 'bool' = False, *args, **kwargs)">
    <doc>Gradient descent sampler for energy-based models.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="apply_mixed_precision" sig="(self, func)"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_scheduled_value" sig="(self, name: str) -> float"/>
    <method name="get_schedulers" sig="(self) -> Dict[str, torchebm.core.base_scheduler.BaseScheduler]"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_scheduler" sig="(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="reset_schedulers" sig="(self) -> None"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="sample" sig="(self, x: 'Optional[torch.Tensor]' = None, dim: 'int' = 10, n_steps: 'int' = 100, n_samples: 'int' = 1, thin: 'int' = 1, return_trajectory: 'bool' = False, return_diagnostics: 'bool' = False, *args, **kwargs) -> 'Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]'"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step_schedulers" sig="(self) -> Dict[str, float]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
  <class name="NesterovSampler" init="(self, model: 'BaseModel', step_size: 'Union[float, BaseScheduler]' = 0.001, momentum: 'float' = 0.9, dtype: 'torch.dtype' = torch.float32, device: 'Optional[Union[str, torch.device]]' = None, use_mixed_precision: 'bool' = False, *args, **kwargs)">
    <doc>Nesterov accelerated gradient sampler for energy-based models.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="apply_mixed_precision" sig="(self, func)"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_scheduled_value" sig="(self, name: str) -> float"/>
    <method name="get_schedulers" sig="(self) -> Dict[str, torchebm.core.base_scheduler.BaseScheduler]"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_scheduler" sig="(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="reset_schedulers" sig="(self) -> None"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="sample" sig="(self, x: 'Optional[torch.Tensor]' = None, dim: 'int' = 10, n_steps: 'int' = 100, n_samples: 'int' = 1, thin: 'int' = 1, return_trajectory: 'bool' = False, return_diagnostics: 'bool' = False, *args, **kwargs) -> 'Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]'"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step_schedulers" sig="(self) -> Dict[str, float]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="samplers.hmc">
  <class name="HamiltonianMonteCarlo" init="(self, model: torchebm.core.base_model.BaseModel, step_size: float | torchebm.core.base_scheduler.BaseScheduler = 0.001, n_leapfrog_steps: int = 10, mass: float | torch.Tensor | None = None, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, *args, **kwargs)">
    <doc>Hamiltonian Monte Carlo sampler.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="apply_mixed_precision" sig="(self, func)"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_scheduled_value" sig="(self, name: str) -> float"/>
    <method name="get_schedulers" sig="(self) -> Dict[str, torchebm.core.base_scheduler.BaseScheduler]"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_scheduler" sig="(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="reset_schedulers" sig="(self) -> None"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="sample" sig="(self, x: torch.Tensor | None = None, dim: int | None = None, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -> Tuple[torch.Tensor, torch.Tensor]"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step_schedulers" sig="(self) -> Dict[str, float]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="samplers.langevin_dynamics">
  <class name="LangevinDynamics" init="(self, model: torchebm.core.base_model.BaseModel, step_size: float | torchebm.core.base_scheduler.BaseScheduler = 0.001, noise_scale: float | torchebm.core.base_scheduler.BaseScheduler = 1.0, decay: float = 0.0, dtype: torch.dtype = torch.float32, device: str | torch.device | None = None, *args, **kwargs)">
    <doc>Langevin Dynamics sampler.</doc>
    <method name="add_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="apply" sig="(self, fn: collections.abc.Callable[['Module'], None]) -> Self"/>
    <method name="apply_mixed_precision" sig="(self, func)"/>
    <method name="autocast_context" sig="(self)"/>
    <method name="bfloat16" sig="(self) -> Self"/>
    <method name="buffers" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]"/>
    <method name="children" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="compile" sig="(self, *args, **kwargs) -> None"/>
    <method name="cpu" sig="(self) -> Self"/>
    <method name="cuda" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="double" sig="(self) -> Self"/>
    <method name="eval" sig="(self) -> Self"/>
    <method name="extra_repr" sig="(self) -> str"/>
    <method name="float" sig="(self) -> Self"/>
    <method name="forward" sig="(self, *input: Any) -> None"/>
    <method name="get_buffer" sig="(self, target: str) -> 'Tensor'"/>
    <method name="get_extra_state" sig="(self) -> Any"/>
    <method name="get_parameter" sig="(self, target: str) -> 'Parameter'"/>
    <method name="get_scheduled_value" sig="(self, name: str) -> float"/>
    <method name="get_schedulers" sig="(self) -> Dict[str, torchebm.core.base_scheduler.BaseScheduler]"/>
    <method name="get_submodule" sig="(self, target: str) -> 'Module'"/>
    <method name="half" sig="(self) -> Self"/>
    <method name="ipu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="load_state_dict" sig="(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)"/>
    <method name="modules" sig="(self) -> collections.abc.Iterator['Module']"/>
    <method name="mtia" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="named_buffers" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]"/>
    <method name="named_children" sig="(self) -> collections.abc.Iterator[tuple[str, 'Module']]"/>
    <method name="named_modules" sig="(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)"/>
    <method name="named_parameters" sig="(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]"/>
    <method name="parameters" sig="(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]"/>
    <method name="register_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor]) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_buffer" sig="(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None"/>
    <method name="register_forward_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...], Any], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_forward_pre_hook" sig="(self, hook: collections.abc.Callable[[~T, tuple[Any, ...]], Any | None] | collections.abc.Callable[[~T, tuple[Any, ...], dict[str, Any]], tuple[Any, dict[str, Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor, tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_full_backward_pre_hook" sig="(self, hook: collections.abc.Callable[['Module', tuple[torch.Tensor, ...] | torch.Tensor], None | tuple[torch.Tensor, ...] | torch.Tensor], prepend: bool = False) -> torch.utils.hooks.RemovableHandle"/>
    <method name="register_load_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_load_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="register_module" sig="(self, name: str, module: ForwardRef('Module') | None) -> None"/>
    <method name="register_parameter" sig="(self, name: str, param: torch.nn.parameter.Parameter | None) -> None"/>
    <method name="register_scheduler" sig="(self, name: str, scheduler: torchebm.core.base_scheduler.BaseScheduler) -> None"/>
    <method name="register_state_dict_post_hook" sig="(self, hook)"/>
    <method name="register_state_dict_pre_hook" sig="(self, hook)"/>
    <method name="requires_grad_" sig="(self, requires_grad: bool = True) -> Self"/>
    <method name="reset_schedulers" sig="(self) -> None"/>
    <method name="safe_to" sig="(obj, device: torch.device | None = None, dtype: torch.dtype | None = None)"/>
    <method name="sample" sig="(self, x: torch.Tensor | None = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -> torch.Tensor | Tuple[torch.Tensor, List[dict]]"/>
    <method name="set_extra_state" sig="(self, state: Any) -> None"/>
    <method name="set_submodule" sig="(self, target: str, module: 'Module', strict: bool = False) -> None"/>
    <method name="setup_mixed_precision" sig="(self, use_mixed_precision: bool) -> None"/>
    <method name="share_memory" sig="(self) -> Self"/>
    <method name="state_dict" sig="(self, *args, destination=None, prefix='', keep_vars=False)"/>
    <method name="step_schedulers" sig="(self) -> Dict[str, float]"/>
    <method name="to" sig="(self, *args, **kwargs)"/>
    <method name="to_empty" sig="(self, *, device: str | torch.device | int | None, recurse: bool = True) -> Self"/>
    <method name="train" sig="(self, mode: bool = True) -> Self"/>
    <method name="type" sig="(self, dst_type: torch.dtype | str) -> Self"/>
    <method name="xpu" sig="(self, device: int | torch.device | None = None) -> Self"/>
    <method name="zero_grad" sig="(self, set_to_none: bool = True) -> None"/>
  </class>
</module>

<module name="utils.image">
  <function name="center_crop_arr" sig="(pil_image: PIL.Image.Image, image_size: int) -> PIL.Image.Image"/>
  <function name="create_npz_from_sample_folder" sig="(sample_dir: str, num: int = 50000) -> str"/>
</module>

<module name="utils.training">
  <function name="load_checkpoint" sig="(checkpoint_path: str, model: torch.nn.modules.module.Module, ema_model: torch.nn.modules.module.Module | None = None, optimizer: torch.optim.optimizer.Optimizer | None = None, device: torch.device | None = None) -> Dict[str, Any]"/>
  <function name="requires_grad" sig="(model: torch.nn.modules.module.Module, flag: bool = True) -> None"/>
  <function name="save_checkpoint" sig="(model: torch.nn.modules.module.Module, optimizer: torch.optim.optimizer.Optimizer, step: int, checkpoint_dir: str, ema_model: torch.nn.modules.module.Module | None = None, args: Dict[str, Any] | None = None) -> str"/>
  <function name="update_ema" sig="(ema_model: torch.nn.modules.module.Module, model: torch.nn.modules.module.Module, decay: float = 0.9999) -> None"/>
</module>

<module name="utils.visualization">
  <function name="plot_2d_energy_landscape" sig="(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, device: str | None = None) -> matplotlib.figure.Figure"/>
  <function name="plot_3d_energy_landscape" sig="(model: torchebm.core.base_model.BaseModel, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 50, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (10, 8), alpha: float = 0.9, elev: float = 30, azim: float = -45, device: str | None = None) -> matplotlib.figure.Figure"/>
  <function name="plot_sample_trajectories" sig="(trajectories: torch.Tensor, model: torchebm.core.base_model.BaseModel | None = None, x_range: Tuple[float, float] = None, y_range: Tuple[float, float] = None, resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), trajectory_colors: List[str] | None = None, trajectory_alpha: float = 0.7, line_width: float = 1.0, device: str | None = None) -> matplotlib.figure.Figure"/>
  <function name="plot_samples_on_energy" sig="(model: torchebm.core.base_model.BaseModel, samples: torch.Tensor, x_range: Tuple[float, float] = (-5, 5), y_range: Tuple[float, float] = (-5, 5), resolution: int = 100, log_scale: bool = False, cmap: str = 'viridis', title: str | None = None, show_colorbar: bool = True, save_path: str | None = None, fig_size: Tuple[int, int] = (8, 6), contour: bool = True, contour_levels: int = 20, sample_color: str = 'red', sample_alpha: float = 0.5, sample_size: float = 5, device: str | None = None) -> matplotlib.figure.Figure"/>
</module>

</torchebm>