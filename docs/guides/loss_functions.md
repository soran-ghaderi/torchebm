---
sidebar_position: 4
title: Loss Functions
description: Understanding and using loss functions for training energy-based models.
---

# Loss Functions

Training an EBM involves shaping the model's energy surface so that the corresponding probability distribution matches the data distribution. In TorchEBM, this is accomplished using various loss functions.

## Contrastive Divergence

Contrastive Divergence (CD) is a standard algorithm for training EBMs. It lowers the energy of real data (positive samples) while increasing the energy of samples generated by the model (negative samples).

### Basic Usage

The `ContrastiveDivergence` loss function requires a `model` and a `sampler` to generate the negative samples.

```python
import torch
import torch.nn as nn
from torchebm.core import BaseModel
from torchebm.losses import ContrastiveDivergence
from torchebm.samplers import LangevinDynamics

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class MLPModel(BaseModel):
    def __init__(self, input_dim=2, hidden_dim=64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, 1)
        )
    def forward(self, x):
        return self.network(x).squeeze(-1)

model = MLPModel().to(device)

sampler = LangevinDynamics(model=model, step_size=0.1)

loss_fn = ContrastiveDivergence(model=model, sampler=sampler, k_steps=10)

# During training:
data_batch = torch.randn(128, 2).to(device)
loss, negative_samples = loss_fn(data_batch)
```

### Persistent Contrastive Divergence (PCD)

PCD is a variant of CD that maintains a persistent chain of samples across training steps, which can improve sample quality. Enable it by setting `persistent=True`.

```python
pcd_loss_fn = ContrastiveDivergence(
    model=model,
    sampler=sampler,
    k_steps=10,
    persistent=True,
    buffer_size=1024
)
```

## Score Matching

Score Matching is an alternative to CD that does not require MCMC sampling. It trains the model by matching the gradient of the log-probability of the model (the "score") to the score of the data.

### Sliced Score Matching

Sliced Score Matching (SSM) is a scalable and efficient version of Score Matching.

```python
from torchebm.losses import SlicedScoreMatching

ssm_loss_fn = SlicedScoreMatching(
    model=model,
    n_projections=5
)

# During training:
loss = ssm_loss_fn(data_batch)
```

### Denoising Score Matching

Denoising Score Matching (DSM) is another efficient variant. It perturbs the data with noise and trains the model to predict the score of the noised data distribution.

```python
from torchebm.losses import DenoisingScoreMatching

dsm_loss_fn = DenoisingScoreMatching(
    model=model,
    noise_scale=0.1
)

# During training:
loss = dsm_loss_fn(data_batch)
```

## Choosing a Loss Function

-   **Contrastive Divergence (CD/PCD)**: A robust, general-purpose choice. PCD is often preferred for its better sample quality.
-   **Sliced Score Matching (SSM)**: A great choice when MCMC sampling is difficult or slow. It can be more stable than CD but is sensitive to the number of projections.
-   **Denoising Score Matching (DSM)**: Another excellent alternative to CD that is often more stable, especially for high-dimensional data.

For a complete guide on how to use these loss functions in a full training loop, see the [Training EBMs](training.md) guide. 