{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Torchebm TorchEBM","text":"<p>\u26a1 Energy-Based Modeling library for PyTorch, offering tools for \ud83d\udd2c sampling, \ud83e\udde0 inference, and \ud83d\udcca learning in complex distributions.</p> Gaussian Function Double well Function Rastrigin Function Rosenbrock Function"},{"location":"#about","title":"About","text":"<p>TorchEBM is a CUDA-accelerated parallel library for Energy-Based Models (EBMs) built on PyTorch. It provides efficient implementations of sampling, inference, and learning algorithms for EBMs, with a focus on scalability and performance. This is an early version and is under development.</p>"},{"location":"#features-so-far","title":"Features (so far)","text":"<p>sign map:</p> <ul> <li> - Work in progress</li> <li> - Completed</li> <li> - Needs improvement</li> <li> - New feature</li> </ul> <p>Current status:</p> <ul> <li> CUDA-accelerated implementations </li> <li> Seamless integration with PyTorch </li> <li> Sampling algorithms <ul> <li> Langevin dynamics sampling </li> <li> Hamiltonian Monte Carlo sampling </li> <li> Metropolis-Hastings sampling </li> </ul> </li> <li> contrastive divergence (CD, PCD, PTCD) </li> <li> Toy energy functions <ul> <li> Gaussian </li> <li> Double well </li> <li> Rastrigin </li> <li> Rosenbrock </li> <li> Ackley </li> </ul> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install torchebm\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>import torch\nfrom torchebm import EnergyFunction, LangevinDynamics\nimport matplotlib.pyplot as plt\n\n# You can define your energy function like the following. However you don't need to implement the gradient and it is already automated, but for the sake of the example, we'll include it.\nclass QuadraticEnergy(EnergyFunction):\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return 0.5 * torch.sum(x**2, dim=-1)\n\n    def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return x\n\n# Instantiate the energy function and the sampler\nenergy_fn = QuadraticEnergy()\nsampler = LangevinDynamics(energy_fn, step_size=0.1, noise_scale=0.1)\n\n# Generate samples\ninitial_state = torch.tensor([2.0, 2.0])\nsamples = sampler.sample_chain(initial_state, n_steps=1000, n_samples=500)\n\n# A Single trajectory\ntrajectory = sampler.sample(initial_state, n_steps, return_trajectory=True)\n\n# Demonstrate parallel sampling\nn_chains = 10\ninitial_states = torch.randn(n_chains, 2) * 2\nparallel_samples = sampler.sample_parallel(initial_states, n_steps=1000)\n</code></pre>"},{"location":"#example-output","title":"Example Output:","text":"<p>For the visualization codes, check out the examples directory</p> Langevin Dynamics Sampling Single Langevin Dynamics Trajectory Parallel Langevin Dynamics Sampling"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please check the issues page for current tasks or create a new issue to discuss proposed changes.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"api/","title":"API Reference","text":"<p>This page provides an overview of the TorchEBM API.</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":""},{"location":"api/#torchebm","title":"Torchebm","text":"<p>Package Documentation</p> <ul> <li>Core</li> <li>Cuda</li> <li>Losses</li> <li>Models</li> <li>Samplers</li> <li>Utils</li> </ul>"},{"location":"api/#torchebm_1","title":"Torchebm","text":"<p>Package Documentation</p>"},{"location":"api/#subpackages","title":"Subpackages","text":""},{"location":"api/#core","title":"Core","text":"<p>Documentation</p> <p>Modules:</p> <ul> <li>Basesampler - No description available.</li> <li>Energy_function - No description available.</li> <li>Losses - No description available.</li> <li>Optimizer - No description available.</li> <li>Score_matching - No description available.</li> <li>Trainer - No description available.</li> </ul>"},{"location":"api/#cuda","title":"Cuda","text":"<p>Documentation</p>"},{"location":"api/#losses","title":"Losses","text":"<p>Documentation</p> <p>Modules:</p> <ul> <li>Contrastive_divergence - No description available.</li> </ul>"},{"location":"api/#models","title":"Models","text":"<p>Documentation</p> <p>Modules:</p> <ul> <li>Base_model - No description available.</li> </ul>"},{"location":"api/#samplers","title":"Samplers","text":"<p>Documentation</p> <p>Modules:</p> <ul> <li>Langevin_dynamics - Langevin Dynamics Sampler Module.</li> <li>Mcmc - Hamiltonian Monte Carlo Sampler Module.</li> </ul>"},{"location":"api/#utils","title":"Utils","text":"<p>Documentation</p>"},{"location":"api/torchebm/","title":"Torchebm","text":""},{"location":"api/torchebm/#contents","title":"Contents","text":""},{"location":"api/torchebm/#subpackages","title":"Subpackages","text":"<ul> <li>Core</li> <li>Cuda</li> <li>Losses</li> <li>Models</li> <li>Samplers</li> <li>Utils</li> </ul>"},{"location":"api/torchebm/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/#torchebm","title":"torchebm","text":""},{"location":"api/torchebm/core/","title":"Core","text":""},{"location":"api/torchebm/core/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/#modules","title":"Modules","text":"<ul> <li>Basesampler</li> <li>Energy_function</li> <li>Losses</li> <li>Optimizer</li> <li>Score_matching</li> <li>Trainer</li> </ul>"},{"location":"api/torchebm/core/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/#torchebm.core","title":"torchebm.core","text":""},{"location":"api/torchebm/cuda/","title":"Cuda","text":""},{"location":"api/torchebm/cuda/#contents","title":"Contents","text":""},{"location":"api/torchebm/cuda/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/cuda/#torchebm.cuda","title":"torchebm.cuda","text":""},{"location":"api/torchebm/losses/","title":"Losses","text":""},{"location":"api/torchebm/losses/#contents","title":"Contents","text":""},{"location":"api/torchebm/losses/#modules","title":"Modules","text":"<ul> <li>Contrastive_divergence</li> </ul>"},{"location":"api/torchebm/losses/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/losses/#torchebm.losses","title":"torchebm.losses","text":""},{"location":"api/torchebm/models/","title":"Models","text":""},{"location":"api/torchebm/models/#contents","title":"Contents","text":""},{"location":"api/torchebm/models/#modules","title":"Modules","text":"<ul> <li>Base_model</li> </ul>"},{"location":"api/torchebm/models/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/models/#torchebm.models","title":"torchebm.models","text":""},{"location":"api/torchebm/samplers/","title":"Samplers","text":""},{"location":"api/torchebm/samplers/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/#modules","title":"Modules","text":"<ul> <li>Langevin_dynamics - Langevin Dynamics Sampler Module.</li> <li>Mcmc - Hamiltonian Monte Carlo Sampler Module.</li> </ul>"},{"location":"api/torchebm/samplers/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/#torchebm.samplers","title":"torchebm.samplers","text":""},{"location":"api/torchebm/utils/","title":"Utils","text":""},{"location":"api/torchebm/utils/#contents","title":"Contents","text":""},{"location":"api/torchebm/utils/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/utils/#torchebm.utils","title":"torchebm.utils","text":""},{"location":"api/torchebm/core/basesampler/","title":"Basesampler","text":""},{"location":"api/torchebm/core/basesampler/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/basesampler/#classes","title":"Classes","text":"<ul> <li><code>BaseSampler</code> - Base class for samplers.</li> </ul>"},{"location":"api/torchebm/core/basesampler/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/basesampler/#torchebm.core.basesampler","title":"torchebm.core.basesampler","text":""},{"location":"api/torchebm/core/energy_function/","title":"Energy_function","text":""},{"location":"api/torchebm/core/energy_function/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/energy_function/#classes","title":"Classes","text":"<ul> <li><code>AckleyEnergy</code> - Energy function for the Ackley function.</li> <li><code>DoubleWellEnergy</code> - Energy function for a double well potential. E(x) = h * \u03a3((x\u00b2-1)\u00b2) where h is the barrier height.</li> <li><code>EnergyFunction</code> - Abstract base class for energy functions (Potential Energy E(x)).</li> <li><code>GaussianEnergy</code> - Energy function for a Gaussian distribution. E(x) = 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).</li> <li><code>HarmonicEnergy</code> - Energy function for a harmonic oscillator. E(x) = 0.5 * k * \u03a3(x\u00b2).</li> <li><code>RastriginEnergy</code> - Energy function for the Rastrigin function.</li> <li><code>RosenbrockEnergy</code> - Energy function for the Rosenbrock function. E(x) = (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.</li> </ul>"},{"location":"api/torchebm/core/energy_function/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/energy_function/#torchebm.core.energy_function","title":"torchebm.core.energy_function","text":""},{"location":"api/torchebm/core/losses/","title":"Losses","text":""},{"location":"api/torchebm/core/losses/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/losses/#classes","title":"Classes","text":"<ul> <li><code>Loss</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/losses/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/losses/#torchebm.core.losses","title":"torchebm.core.losses","text":""},{"location":"api/torchebm/core/optimizer/","title":"Optimizer","text":""},{"location":"api/torchebm/core/optimizer/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/optimizer/#classes","title":"Classes","text":"<ul> <li><code>Optimizer</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/optimizer/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/optimizer/#torchebm.core.optimizer","title":"torchebm.core.optimizer","text":""},{"location":"api/torchebm/core/score_matching/","title":"Score_matching","text":""},{"location":"api/torchebm/core/score_matching/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/score_matching/#classes","title":"Classes","text":"<ul> <li><code>ScoreMatching</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/score_matching/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/score_matching/#torchebm.core.score_matching","title":"torchebm.core.score_matching","text":""},{"location":"api/torchebm/core/trainer/","title":"Trainer","text":""},{"location":"api/torchebm/core/trainer/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/trainer/#classes","title":"Classes","text":"<ul> <li><code>ContrastiveDivergenceTrainer</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/trainer/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/trainer/#torchebm.core.trainer","title":"torchebm.core.trainer","text":""},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/","title":"BaseSampler","text":""},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for samplers.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>dtype</code> <code>dtype</code> <p>Data type to use for the computations.</p> <code>float32</code> <code>device</code> <code>str</code> <p>Device to run the computations on (e.g., \"cpu\" or \"cuda\").</p> <code>None</code> <p>Methods:</p> Name Description <code>sample</code> <p>Run the sampling process.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics dictionary.</p> <code>to</code> <p>Move sampler to specified device.</p> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>class BaseSampler(ABC):\n    \"\"\"\n    Base class for samplers.\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        dtype (torch.dtype): Data type to use for the computations.\n        device (str): Device to run the computations on (e.g., \"cpu\" or \"cuda\").\n\n    Methods:\n        sample(x, dim, n_steps, n_samples, thin, return_trajectory, return_diagnostics): Run the sampling process.\n        sample_chain(dim, n_steps, n_samples, thin, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(): Initialize the diagnostics dictionary.\n        to(device): Move sampler to specified device.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n    ):\n        self.energy_function = energy_function\n        self.dtype = dtype\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def sample(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,  # not supported yet\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        \"\"\"\n        Run the sampling process.\n        Args:\n            x: Initial state to start the sampling from.\n            dim: Dimension of the state space.\n            n_steps: Number of steps to take between samples.\n            n_samples: Number of samples to generate.\n            thin: Thinning factor (not supported yet).\n            return_trajectory: Whether to return the trajectory of the samples.\n            return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n        Returns:\n            torch.Tensor: Samples from the sampler.\n            List[dict]: Diagnostics of the sampling process.\n        \"\"\"\n        return self.sample_chain(\n            x=x,\n            dim=dim,\n            n_steps=n_steps,\n            n_samples=n_samples,\n            return_trajectory=return_trajectory,\n            return_diagnostics=return_diagnostics,\n        )\n\n    @abstractmethod\n    def sample_chain(\n        self,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        raise NotImplementedError\n\n    # @abstractmethod\n    def _setup_diagnostics(self) -&gt; dict:\n        \"\"\"\n        Initialize the diagnostics dictionary.\n\n            .. deprecated:: 1.0\n               This method is deprecated and will be removed in a future version.\n        \"\"\"\n        return {\n            \"energies\": torch.empty(0, device=self.device, dtype=self.dtype),\n            \"acceptance_rate\": torch.tensor(0.0, device=self.device, dtype=self.dtype),\n        }\n        # raise NotImplementedError\n\n    def to(self, device: Union[str, torch.device]) -&gt; \"BaseSampler\":\n        \"\"\"Move sampler to specified device.\"\"\"\n        self.device = device\n        return self\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device or 'cuda' if is_available() else 'cpu'\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.sample","title":"sample","text":"<pre><code>sample(x: Optional[Tensor] = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> <p>Run the sampling process. Args:     x: Initial state to start the sampling from.     dim: Dimension of the state space.     n_steps: Number of steps to take between samples.     n_samples: Number of samples to generate.     thin: Thinning factor (not supported yet).     return_trajectory: Whether to return the trajectory of the samples.     return_diagnostics: Whether to return the diagnostics of the sampling process.</p> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>torch.Tensor: Samples from the sampler.</p> <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>List[dict]: Diagnostics of the sampling process.</p> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>def sample(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    thin: int = 1,  # not supported yet\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    \"\"\"\n    Run the sampling process.\n    Args:\n        x: Initial state to start the sampling from.\n        dim: Dimension of the state space.\n        n_steps: Number of steps to take between samples.\n        n_samples: Number of samples to generate.\n        thin: Thinning factor (not supported yet).\n        return_trajectory: Whether to return the trajectory of the samples.\n        return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n    Returns:\n        torch.Tensor: Samples from the sampler.\n        List[dict]: Diagnostics of the sampling process.\n    \"\"\"\n    return self.sample_chain(\n        x=x,\n        dim=dim,\n        n_steps=n_steps,\n        n_samples=n_samples,\n        return_trajectory=return_trajectory,\n        return_diagnostics=return_diagnostics,\n    )\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.sample_chain","title":"sample_chain  <code>abstractmethod</code>","text":"<pre><code>sample_chain(dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>@abstractmethod\ndef sample_chain(\n    self,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    thin: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.to","title":"to","text":"<pre><code>to(device: Union[str, device]) -&gt; BaseSampler\n</code></pre> <p>Move sampler to specified device.</p> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>def to(self, device: Union[str, torch.device]) -&gt; \"BaseSampler\":\n    \"\"\"Move sampler to specified device.\"\"\"\n    self.device = device\n    return self\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/","title":"AckleyEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for the Ackley function. </p> <p>The Ackley energy is defined as:</p> \\[E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\frac{1}{n}\\sum_{i=1}^{n} x_i^2} ight) - \\exp\\left(\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i) ight) + a + e\\] <p>This function has a global minimum at the origin surrounded by many local minima, creating a challenging optimization landscape that tests an algorithm's ability to escape local optima.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Ackley function.</p> <code>20.0</code> <code>b</code> <code>float</code> <p>Parameter <code>b</code> of the Ackley function.</p> <code>0.2</code> <code>c</code> <code>float</code> <p>Parameter <code>c</code> of the Ackley function.</p> <code>2 * pi</code> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the Ackley energy.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class AckleyEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for the Ackley function. \n\n    The Ackley energy is defined as:\n\n    $$E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i)\\right) + a + e$$\n\n    This function has a global minimum at the origin surrounded by many local minima,\n    creating a challenging optimization landscape that tests an algorithm's ability to\n    escape local optima.\n\n    Args:\n        a (float): Parameter `a` of the Ackley function.\n        b (float): Parameter `b` of the Ackley function.\n        c (float): Parameter `c` of the Ackley function.\n    \"\"\"\n\n    def __init__(self, a: float = 20.0, b: float = 0.2, c: float = 2 * math.pi):\n        super().__init__()\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the Ackley energy.\n\n        $$E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i)\\right) + a + e$$\n        \"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        n = x.shape[-1]\n        sum1 = torch.sum(x**2, dim=-1)\n        sum2 = torch.sum(torch.cos(self.c * x), dim=-1)\n        term1 = -self.a * torch.exp(-self.b * torch.sqrt(sum1 / n))\n        term2 = -torch.exp(sum2 / n)\n        return term1 + term2 + self.a + math.e\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.c","title":"c  <code>instance-attribute</code>","text":"<pre><code>c = c\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Ackley energy.</p> \\[E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\frac{1}{n}\\sum_{i=1}^{n} x_i^2} ight) - \\exp\\left(\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i) ight) + a + e\\] Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the Ackley energy.\n\n    $$E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i)\\right) + a + e$$\n    \"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    n = x.shape[-1]\n    sum1 = torch.sum(x**2, dim=-1)\n    sum2 = torch.sum(torch.cos(self.c * x), dim=-1)\n    term1 = -self.a * torch.exp(-self.b * torch.sqrt(sum1 / n))\n    term2 = -torch.exp(sum2 / n)\n    return term1 + term2 + self.a + math.e\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/","title":"DoubleWellEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for a double well potential. E(x) = h * \u03a3((x\u00b2-1)\u00b2) where h is the barrier height.</p> <p>This energy function creates a bimodal distribution with two modes at x = +1 and x = -1 (in each dimension), separated by a barrier of height h at x = 0.</p> <p>Parameters:</p> Name Type Description Default <code>barrier_height</code> <code>float</code> <p>Height of the barrier between the wells.</p> <code>2.0</code> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the double well energy: h * \u03a3((x\u00b2-1)\u00b2).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class DoubleWellEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for a double well potential. E(x) = h * \u03a3((x\u00b2-1)\u00b2) where h is the barrier height.\n\n    This energy function creates a bimodal distribution with two modes at x = +1 and x = -1\n    (in each dimension), separated by a barrier of height h at x = 0.\n\n    Args:\n        barrier_height (float): Height of the barrier between the wells.\n    \"\"\"\n\n    def __init__(self, barrier_height: float = 2.0):\n        super().__init__()\n        self.barrier_height = barrier_height\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the double well energy: h * \u03a3((x\u00b2-1)\u00b2).\"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        return self.barrier_height * (x.pow(2) - 1).pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/#torchebm.core.energy_function.DoubleWellEnergy.barrier_height","title":"barrier_height  <code>instance-attribute</code>","text":"<pre><code>barrier_height = barrier_height\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/#torchebm.core.energy_function.DoubleWellEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the double well energy: h * \u03a3((x\u00b2-1)\u00b2).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the double well energy: h * \u03a3((x\u00b2-1)\u00b2).\"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    return self.barrier_height * (x.pow(2) - 1).pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/","title":"EnergyFunction","text":""},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for energy functions (Potential Energy E(x)).</p> <p>This class serves as a standard interface for defining energy functions used within the torchebm library. It is compatible with both pre-defined analytical functions (like Gaussian, DoubleWell) and trainable neural network models. It represents the potential energy E(x), often related to a probability distribution p(x) by E(x) = -log p(x) + constant.</p> <p>Core Requirements for Subclasses: 1.  Implement the <code>forward(x)</code> method to compute the scalar energy per sample. 2.  Optionally, override the <code>gradient(x)</code> method if an efficient analytical     gradient is available. Otherwise, the default implementation using     <code>torch.autograd</code> will be used.</p> <p>Inheriting from <code>torch.nn.Module</code> ensures that: - Subclasses can contain trainable parameters (<code>nn.Parameter</code>). - Standard PyTorch methods like <code>.to(device)</code>, <code>.parameters()</code>, <code>.state_dict()</code>,   and integration with <code>torch.optim</code> work as expected.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the scalar energy value for each input sample.</p> <code>gradient</code> <p>Computes the gradient of the energy function with respect to the input x (\u2207_x E(x)).</p> <code>to</code> <p>Moves and/or casts the parameters and buffers.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class EnergyFunction(nn.Module, ABC):\n    \"\"\"\n    Abstract base class for energy functions (Potential Energy E(x)).\n\n    This class serves as a standard interface for defining energy functions used\n    within the torchebm library. It is compatible with both pre-defined analytical\n    functions (like Gaussian, DoubleWell) and trainable neural network models.\n    It represents the potential energy E(x), often related to a probability\n    distribution p(x) by E(x) = -log p(x) + constant.\n\n    Core Requirements for Subclasses:\n    1.  Implement the `forward(x)` method to compute the scalar energy per sample.\n    2.  Optionally, override the `gradient(x)` method if an efficient analytical\n        gradient is available. Otherwise, the default implementation using\n        `torch.autograd` will be used.\n\n    Inheriting from `torch.nn.Module` ensures that:\n    - Subclasses can contain trainable parameters (`nn.Parameter`).\n    - Standard PyTorch methods like `.to(device)`, `.parameters()`, `.state_dict()`,\n      and integration with `torch.optim` work as expected.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the EnergyFunction base class.\"\"\"\n        super().__init__()\n        # Optional: store device, though nn.Module handles parameter/buffer device placement\n        self._device: Optional[torch.device] = None\n\n    @property\n    def device(self) -&gt; Optional[torch.device]:\n        \"\"\"Returns the device associated with the module's parameters/buffers (if any).\"\"\"\n        try:\n            # Attempt to infer device from the first parameter/buffer found\n            return next(self.parameters()).device\n        except StopIteration:\n            try:\n                return next(self.buffers()).device\n            except StopIteration:\n                # If no parameters or buffers, return the explicitly set _device or None\n                return self._device\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the scalar energy value for each input sample.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n                               It's recommended that subclasses handle moving `x`\n                               to the correct device if necessary, although callers\n                               should ideally provide `x` on the correct device.\n\n        Returns:\n            torch.Tensor: Tensor of scalar energy values with shape (batch_size,).\n                          Lower values typically indicate higher probability density.\n        \"\"\"\n        pass\n\n    def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the gradient of the energy function with respect to the input x (\u2207_x E(x)).\n\n        This default implementation uses automatic differentiation based on the\n        `forward` method. Subclasses should override this method if a more\n        efficient or numerically stable analytical gradient is available.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n\n        Returns:\n            torch.Tensor: Gradient tensor of the same shape as x.\n        \"\"\"\n        # Store original dtype and device\n        original_dtype = x.dtype\n        device = x.device\n\n        # Ensure x is on the correct device (if specified by the model)\n        if self.device and device != self.device:\n            x = x.to(self.device)\n            device = self.device  # Update device if x was moved\n\n        with torch.enable_grad():\n            # Detach, convert to float32, and enable gradient tracking\n            x_for_grad = x.detach().to(dtype=torch.float32, device=device).requires_grad_(True)\n\n            # Perform forward pass with float32 input\n            energy = self.forward(x_for_grad)\n\n            # Validate energy shape - should be one scalar per batch item\n            if energy.shape != (x_for_grad.shape[0],):\n                raise ValueError(\n                    f\"EnergyFunction forward() output expected shape ({x_for_grad.shape[0]},), but got {energy.shape}.\"\n                )\n\n            # Check grad_fn on the float32 energy\n            if not energy.grad_fn:\n                raise RuntimeError(\n                    \"Cannot compute gradient: `forward` method did not use the input `x` (as float32) in a differentiable way.\"\n                )\n\n            # Compute gradient using autograd w.r.t. the float32 input\n            gradient_float32 = torch.autograd.grad(\n                outputs=energy,\n                inputs=x_for_grad,  # Compute gradient w.r.t the float32 version\n                grad_outputs=torch.ones_like(energy, device=energy.device),\n                create_graph=False,  # Set to False for standard gradient computation\n                retain_graph=None,  # Usually not needed when create_graph=False, let PyTorch decide\n            )[0]\n\n        if gradient_float32 is None:\n            # This should theoretically not happen if checks above pass, but good to have.\n            raise RuntimeError(\n                \"Gradient computation failed unexpectedly. Check the forward pass implementation.\"\n            )\n\n        # Cast gradient back to the original dtype before returning\n        gradient = gradient_float32.to(original_dtype)\n\n        return gradient\n\n    def __call__(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Alias for the forward method for standard PyTorch module usage.\"\"\"\n        # Note: nn.Module.__call__ has hooks; calling forward directly bypasses them.\n        # It's generally better to call the module instance: energy_fn(x)\n        return super().__call__(x, *args, **kwargs)  # Use nn.Module's __call__\n\n    # Override the base nn.Module `to` method to also store the device hint\n    def to(self, *args, **kwargs):\n        \"\"\"Moves and/or casts the parameters and buffers.\"\"\"\n        new_self = super().to(*args, **kwargs)\n        # Try to update the internal device hint after moving\n        try:\n            # Get device from args/kwargs (handling different ways .to can be called)\n            device = None\n            if args:\n                if isinstance(args[0], torch.device):\n                    device = args[0]\n                elif isinstance(args[0], str):\n                    device = torch.device(args[0])\n            if \"device\" in kwargs:\n                device = kwargs[\"device\"]\n\n            if device:\n                new_self._device = device\n        except Exception:\n            # Ignore potential errors in parsing .to args, rely on parameter/buffer device\n            pass\n        return new_self\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.device","title":"device  <code>property</code>","text":"<pre><code>device: Optional[device]\n</code></pre> <p>Returns the device associated with the module's parameters/buffers (if any).</p>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the scalar energy value for each input sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, *input_dims).                It's recommended that subclasses handle moving <code>x</code>                to the correct device if necessary, although callers                should ideally provide <code>x</code> on the correct device.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Tensor of scalar energy values with shape (batch_size,).           Lower values typically indicate higher probability density.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the scalar energy value for each input sample.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n                           It's recommended that subclasses handle moving `x`\n                           to the correct device if necessary, although callers\n                           should ideally provide `x` on the correct device.\n\n    Returns:\n        torch.Tensor: Tensor of scalar energy values with shape (batch_size,).\n                      Lower values typically indicate higher probability density.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.gradient","title":"gradient","text":"<pre><code>gradient(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the gradient of the energy function with respect to the input x (\u2207_x E(x)).</p> <p>This default implementation uses automatic differentiation based on the <code>forward</code> method. Subclasses should override this method if a more efficient or numerically stable analytical gradient is available.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, *input_dims).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Gradient tensor of the same shape as x.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the gradient of the energy function with respect to the input x (\u2207_x E(x)).\n\n    This default implementation uses automatic differentiation based on the\n    `forward` method. Subclasses should override this method if a more\n    efficient or numerically stable analytical gradient is available.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n\n    Returns:\n        torch.Tensor: Gradient tensor of the same shape as x.\n    \"\"\"\n    # Store original dtype and device\n    original_dtype = x.dtype\n    device = x.device\n\n    # Ensure x is on the correct device (if specified by the model)\n    if self.device and device != self.device:\n        x = x.to(self.device)\n        device = self.device  # Update device if x was moved\n\n    with torch.enable_grad():\n        # Detach, convert to float32, and enable gradient tracking\n        x_for_grad = x.detach().to(dtype=torch.float32, device=device).requires_grad_(True)\n\n        # Perform forward pass with float32 input\n        energy = self.forward(x_for_grad)\n\n        # Validate energy shape - should be one scalar per batch item\n        if energy.shape != (x_for_grad.shape[0],):\n            raise ValueError(\n                f\"EnergyFunction forward() output expected shape ({x_for_grad.shape[0]},), but got {energy.shape}.\"\n            )\n\n        # Check grad_fn on the float32 energy\n        if not energy.grad_fn:\n            raise RuntimeError(\n                \"Cannot compute gradient: `forward` method did not use the input `x` (as float32) in a differentiable way.\"\n            )\n\n        # Compute gradient using autograd w.r.t. the float32 input\n        gradient_float32 = torch.autograd.grad(\n            outputs=energy,\n            inputs=x_for_grad,  # Compute gradient w.r.t the float32 version\n            grad_outputs=torch.ones_like(energy, device=energy.device),\n            create_graph=False,  # Set to False for standard gradient computation\n            retain_graph=None,  # Usually not needed when create_graph=False, let PyTorch decide\n        )[0]\n\n    if gradient_float32 is None:\n        # This should theoretically not happen if checks above pass, but good to have.\n        raise RuntimeError(\n            \"Gradient computation failed unexpectedly. Check the forward pass implementation.\"\n        )\n\n    # Cast gradient back to the original dtype before returning\n    gradient = gradient_float32.to(original_dtype)\n\n    return gradient\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.to","title":"to","text":"<pre><code>to(*args, **kwargs)\n</code></pre> <p>Moves and/or casts the parameters and buffers.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def to(self, *args, **kwargs):\n    \"\"\"Moves and/or casts the parameters and buffers.\"\"\"\n    new_self = super().to(*args, **kwargs)\n    # Try to update the internal device hint after moving\n    try:\n        # Get device from args/kwargs (handling different ways .to can be called)\n        device = None\n        if args:\n            if isinstance(args[0], torch.device):\n                device = args[0]\n            elif isinstance(args[0], str):\n                device = torch.device(args[0])\n        if \"device\" in kwargs:\n            device = kwargs[\"device\"]\n\n        if device:\n            new_self._device = device\n    except Exception:\n        # Ignore potential errors in parsing .to args, rely on parameter/buffer device\n        pass\n    return new_self\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/GaussianEnergy/","title":"GaussianEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/GaussianEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for a Gaussian distribution. E(x) = 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Tensor</code> <p>Mean vector (\u03bc) of the Gaussian distribution.</p> required <code>cov</code> <code>Tensor</code> <p>Covariance matrix (\u03a3) of the Gaussian distribution.</p> required <p>Methods:</p> Name Description <code>forward</code> <p>Computes the Gaussian energy: 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class GaussianEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for a Gaussian distribution. E(x) = 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).\n\n    Args:\n        mean (torch.Tensor): Mean vector (\u03bc) of the Gaussian distribution.\n        cov (torch.Tensor): Covariance matrix (\u03a3) of the Gaussian distribution.\n    \"\"\"\n\n    def __init__(self, mean: torch.Tensor, cov: torch.Tensor):\n        super().__init__()\n        if mean.ndim != 1:\n            raise ValueError(\"Mean must be a 1D tensor.\")\n        if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n            raise ValueError(\"Covariance must be a 2D square matrix.\")\n        if mean.shape[0] != cov.shape[0]:\n            raise ValueError(\n                \"Mean vector dimension must match covariance matrix dimension.\"\n            )\n\n        # Register mean and covariance inverse as buffers.\n        # Buffers are part of the module's state (`state_dict`) and moved by `.to()`,\n        # but are not considered parameters by optimizers.\n        self.register_buffer(\"mean\", mean)\n        try:\n            cov_inv = torch.inverse(cov)\n            self.register_buffer(\"cov_inv\", cov_inv)\n        except RuntimeError as e:\n            raise ValueError(\n                f\"Failed to invert covariance matrix: {e}. Ensure it is invertible.\"\n            ) from e\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the Gaussian energy: 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).\"\"\"\n        # Ensure x is compatible shape (batch_size, dim)\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n        if x.ndim != 2 or x.shape[1] != self.mean.shape[0]:\n            raise ValueError(\n                f\"Input x expected shape (batch_size, {self.mean.shape[0]}), but got {x.shape}\"\n            )\n\n        # Get mean and cov_inv on the same device as x\n        # We don't change the dtype because gradient() already converted x to float32\n        mean = self.mean.to(device=x.device)\n        cov_inv = self.cov_inv.to(device=x.device)\n\n        # Compute centered vectors\n        # Important: use x directly without detaching or converting to maintain grad tracking\n        delta = x - mean\n\n        # Calculate energy\n        # Use batch matrix multiplication for better numerical stability\n        # We use einsum which maintains gradients through operations\n        energy = 0.5 * torch.einsum(\"bi,ij,bj-&gt;b\", delta, cov_inv, delta)\n\n        return energy\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/GaussianEnergy/#torchebm.core.energy_function.GaussianEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Gaussian energy: 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the Gaussian energy: 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).\"\"\"\n    # Ensure x is compatible shape (batch_size, dim)\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n    if x.ndim != 2 or x.shape[1] != self.mean.shape[0]:\n        raise ValueError(\n            f\"Input x expected shape (batch_size, {self.mean.shape[0]}), but got {x.shape}\"\n        )\n\n    # Get mean and cov_inv on the same device as x\n    # We don't change the dtype because gradient() already converted x to float32\n    mean = self.mean.to(device=x.device)\n    cov_inv = self.cov_inv.to(device=x.device)\n\n    # Compute centered vectors\n    # Important: use x directly without detaching or converting to maintain grad tracking\n    delta = x - mean\n\n    # Calculate energy\n    # Use batch matrix multiplication for better numerical stability\n    # We use einsum which maintains gradients through operations\n    energy = 0.5 * torch.einsum(\"bi,ij,bj-&gt;b\", delta, cov_inv, delta)\n\n    return energy\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/","title":"HarmonicEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for a harmonic oscillator. E(x) = 0.5 * k * \u03a3(x\u00b2).</p> <p>This energy function represents a quadratic potential centered at the origin, equivalent to a Gaussian distribution with zero mean and variance proportional to 1/k.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>float</code> <p>Spring constant.</p> <code>1.0</code> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the harmonic oscillator energy: 0.5 * k * \u03a3(x\u00b2).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class HarmonicEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for a harmonic oscillator. E(x) = 0.5 * k * \u03a3(x\u00b2).\n\n    This energy function represents a quadratic potential centered at the origin,\n    equivalent to a Gaussian distribution with zero mean and variance proportional to 1/k.\n\n    Args:\n        k (float): Spring constant.\n    \"\"\"\n\n    def __init__(self, k: float = 1.0):\n        super().__init__()\n        self.k = k\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the harmonic oscillator energy: 0.5 * k * \u03a3(x\u00b2).\"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        return 0.5 * self.k * x.pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/#torchebm.core.energy_function.HarmonicEnergy.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k = k\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/#torchebm.core.energy_function.HarmonicEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the harmonic oscillator energy: 0.5 * k * \u03a3(x\u00b2).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the harmonic oscillator energy: 0.5 * k * \u03a3(x\u00b2).\"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    return 0.5 * self.k * x.pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/","title":"RastriginEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for the Rastrigin function.</p> <p>The Rastrigin energy is defined as:</p> \\[E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]\\] <p>This function is characterized by a large number of local minima arranged in a  regular lattice, with a global minimum at the origin. It's a classic test for optimization algorithms due to its highly multimodal nature.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Rastrigin function.</p> <code>10.0</code> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the Rastrigin energy.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class RastriginEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for the Rastrigin function.\n\n    The Rastrigin energy is defined as:\n\n    $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n\n    This function is characterized by a large number of local minima arranged in a \n    regular lattice, with a global minimum at the origin. It's a classic test for\n    optimization algorithms due to its highly multimodal nature.\n\n    Args:\n        a (float): Parameter `a` of the Rastrigin function.\n    \"\"\"\n\n    def __init__(self, a: float = 10.0):\n        super().__init__()\n        self.a = a\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the Rastrigin energy.\n\n        $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n        \"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        n = x.shape[-1]\n        return self.a * n + torch.sum(\n            x**2 - self.a * torch.cos(2 * math.pi * x), dim=-1\n        )\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/#torchebm.core.energy_function.RastriginEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/#torchebm.core.energy_function.RastriginEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Rastrigin energy.</p> \\[E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]\\] Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the Rastrigin energy.\n\n    $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n    \"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    n = x.shape[-1]\n    return self.a * n + torch.sum(\n        x**2 - self.a * torch.cos(2 * math.pi * x), dim=-1\n    )\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/","title":"RosenbrockEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for the Rosenbrock function. E(x) = (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.</p> <p>This energy function creates a challenging valley-shaped distribution with the  global minimum at (a, a\u00b2). It's commonly used as a benchmark for optimization algorithms due to its curved, narrow valley which is difficult to traverse.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Rosenbrock function.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Parameter <code>b</code> of the Rosenbrock function.</p> <code>100.0</code> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the Rosenbrock energy: (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class RosenbrockEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for the Rosenbrock function. E(x) = (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.\n\n    This energy function creates a challenging valley-shaped distribution with the \n    global minimum at (a, a\u00b2). It's commonly used as a benchmark for optimization algorithms\n    due to its curved, narrow valley which is difficult to traverse.\n\n    Args:\n        a (float): Parameter `a` of the Rosenbrock function.\n        b (float): Parameter `b` of the Rosenbrock function.\n    \"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 100.0):\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the Rosenbrock energy: (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.\"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n        # Validate dimensions - Rosenbrock requires at least 2 dimensions\n        if x.shape[-1] &lt; 2:\n            raise ValueError(\n                f\"Rosenbrock energy function requires at least 2 dimensions, got {x.shape[-1]}\"\n            )\n\n        return (self.a - x[..., 0]) ** 2 + self.b * (x[..., 1] - x[..., 0] ** 2) ** 2\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#torchebm.core.energy_function.RosenbrockEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#torchebm.core.energy_function.RosenbrockEnergy.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#torchebm.core.energy_function.RosenbrockEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Rosenbrock energy: (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the Rosenbrock energy: (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.\"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n    # Validate dimensions - Rosenbrock requires at least 2 dimensions\n    if x.shape[-1] &lt; 2:\n        raise ValueError(\n            f\"Rosenbrock energy function requires at least 2 dimensions, got {x.shape[-1]}\"\n        )\n\n    return (self.a - x[..., 0]) ** 2 + self.b * (x[..., 1] - x[..., 0] ** 2) ** 2\n</code></pre>"},{"location":"api/torchebm/core/losses/classes/Loss/","title":"Loss","text":""},{"location":"api/torchebm/core/losses/classes/Loss/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <code>to</code> Source code in <code>torchebm/core/losses.py</code> <pre><code>class Loss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        pass\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"api/torchebm/core/losses/classes/Loss/#torchebm.core.losses.Loss.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/core/losses.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/core/losses/classes/Loss/#torchebm.core.losses.Loss.to","title":"to","text":"<pre><code>to(device)\n</code></pre> Source code in <code>torchebm/core/losses.py</code> <pre><code>def to(self, device):\n    self.device = device\n    return self\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/","title":"Optimizer","text":""},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>step</code> <code>zero_grad</code> <code>state_dict</code> <code>load_state_dict</code> <code>to</code> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>class Optimizer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def step(self):\n        pass\n\n    @abstractmethod\n    def zero_grad(self):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state_dict):\n        pass\n\n    @abstractmethod\n    def to(self, device):\n        pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.step","title":"step  <code>abstractmethod</code>","text":"<pre><code>step()\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef step(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.zero_grad","title":"zero_grad  <code>abstractmethod</code>","text":"<pre><code>zero_grad()\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef zero_grad(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.state_dict","title":"state_dict  <code>abstractmethod</code>","text":"<pre><code>state_dict()\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef state_dict(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.load_state_dict","title":"load_state_dict  <code>abstractmethod</code>","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.to","title":"to  <code>abstractmethod</code>","text":"<pre><code>to(device)\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef to(self, device):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/","title":"ScoreMatching","text":""},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Methods:</p> Name Description <code>forward</code> <code>training_step</code> <code>configure_optimizers</code> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>class ScoreMatching(ABC):\n    def __init__(\n            self,\n            loss,\n            norm,\n            n_input: int = 2,\n            n_hidden: int = 64,\n            n_hidden_layers: int = 2,\n            batch_norm: bool = False,\n            lr: float = 1e-3,\n            activation: Type[torch.nn.Module] = torch.nn.Softplus,\n    ):\n        super().__init__()\n        layers = [\n            self._make_layer(n_input, n_hidden, bn=batch_norm, dp=0.01, act=activation)\n        ]\n        layers += [\n            self._make_layer(n_hidden, n_hidden, bn=batch_norm, dp=0.01, act=activation)\n            for _ in range(n_hidden_layers - 1)\n        ]\n        layers += [self._make_layer(n_hidden, n_input)]\n        self.layers = torch.nn.Sequential(*layers)\n        # if norm is None:\n        #     norm = normalizers.NoOp()\n        self.norm = norm\n        self.loss_fn = loss\n        self.lr = lr\n        self.save_hyperparameters()\n\n    def forward(self, x):\n        return self.layers(self.norm(x))\n\n    def training_step(self, batch, batch_idx):\n        del batch_idx\n        x = batch\n        loss = self.loss_fn(self, x)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\n    def _make_layer(\n            n_in: int,\n            n_out: int,\n            bn: bool = False,\n            dp: float = 0.0,\n            act: Type[torch.nn.Module] = None,\n    ) -&gt; torch.nn.Sequential:\n        layers = [torch.nn.Linear(n_in, n_out)]\n        if act is not None:\n            layers.append(act())\n        if bn:\n            layers.append(torch.nn.BatchNorm1d(n_out))\n        if dp &gt; 0:\n            layers.append(torch.nn.Dropout(p=dp))\n        return torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = Sequential(*layers)\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = norm\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.loss_fn","title":"loss_fn  <code>instance-attribute</code>","text":"<pre><code>loss_fn = loss\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.lr","title":"lr  <code>instance-attribute</code>","text":"<pre><code>lr = lr\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>def forward(self, x):\n    return self.layers(self.norm(x))\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    del batch_idx\n    x = batch\n    loss = self.loss_fn(self, x)\n    return loss\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>def configure_optimizers(self):\n    return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/","title":"ContrastiveDivergenceTrainer","text":""},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#methods-and-attributes","title":"Methods and Attributes","text":"<p>Methods:</p> Name Description <code>train_step</code> Source code in <code>torchebm/core/trainer.py</code> <pre><code>class ContrastiveDivergenceTrainer:\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        sampler: BaseSampler,\n        learning_rate: float = 0.01,\n    ):\n        self.energy_function = energy_function\n        self.sampler = sampler\n        self.optimizer = torch.optim.Adam(\n            self.energy_function.parameters(), lr=learning_rate\n        )\n\n    def train_step(self, real_data: torch.Tensor) -&gt; dict:\n        self.optimizer.zero_grad()\n\n        # Positive phase\n        positive_energy = self.energy_function(real_data)\n\n        # Negative phase\n        initial_samples = torch.randn_like(real_data)\n        negative_samples = self.sampler.sample(\n            self.energy_function, initial_samples, num_steps=10\n        )\n        negative_energy = self.energy_function(negative_samples)\n\n        # Compute loss\n        loss = positive_energy.mean() - negative_energy.mean()\n\n        # Backpropagation\n        loss.backward()\n        self.optimizer.step()\n\n        return {\n            \"loss\": loss.item(),\n            \"positive_energy\": positive_energy.mean().item(),\n            \"negative_energy\": negative_energy.mean().item(),\n        }\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = sampler\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.train_step","title":"train_step","text":"<pre><code>train_step(real_data: Tensor) -&gt; dict\n</code></pre> Source code in <code>torchebm/core/trainer.py</code> <pre><code>def train_step(self, real_data: torch.Tensor) -&gt; dict:\n    self.optimizer.zero_grad()\n\n    # Positive phase\n    positive_energy = self.energy_function(real_data)\n\n    # Negative phase\n    initial_samples = torch.randn_like(real_data)\n    negative_samples = self.sampler.sample(\n        self.energy_function, initial_samples, num_steps=10\n    )\n    negative_energy = self.energy_function(negative_samples)\n\n    # Compute loss\n    loss = positive_energy.mean() - negative_energy.mean()\n\n    # Backpropagation\n    loss.backward()\n    self.optimizer.step()\n\n    return {\n        \"loss\": loss.item(),\n        \"positive_energy\": positive_energy.mean().item(),\n        \"negative_energy\": negative_energy.mean().item(),\n    }\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/","title":"Contrastive_divergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#contents","title":"Contents","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#classes","title":"Classes","text":"<ul> <li><code>ContrastiveDivergence</code> - No description available.</li> <li><code>ContrastiveDivergenceBase</code> - No description available.</li> <li><code>ParallelTemperingCD</code> - No description available.</li> <li><code>PersistentContrastiveDivergence</code> - No description available.</li> </ul>"},{"location":"api/torchebm/losses/contrastive_divergence/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence","title":"torchebm.losses.contrastive_divergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/","title":"ContrastiveDivergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ContrastiveDivergenceBase</code></p> <p>Methods:</p> Name Description <code>sample</code> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ContrastiveDivergence(ContrastiveDivergenceBase):\n    def __init__(self, k=1):\n        super().__init__(k)\n\n    def sample(self, energy_model, x_pos):\n        x_neg = x_pos.clone().detach()\n        for _ in range(self.k):\n            x_neg = energy_model.gibbs_step(\n                x_neg\n            )  # todo: implement `gibbs_step` in energy_model\n        return x_neg\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.sample","title":"sample","text":"<pre><code>sample(energy_model, x_pos)\n</code></pre> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>def sample(self, energy_model, x_pos):\n    x_neg = x_pos.clone().detach()\n    for _ in range(self.k):\n        x_neg = energy_model.gibbs_step(\n            x_neg\n        )  # todo: implement `gibbs_step` in energy_model\n    return x_neg\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/","title":"ContrastiveDivergenceBase","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Loss</code></p> <p>Methods:</p> Name Description <code>sample</code> <p>Abstract method: Generate negative samples from the energy model.</p> <code>forward</code> <p>Compute the CD loss: E(x_pos) - E(x_neg)</p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ContrastiveDivergenceBase(Loss):\n    def __init__(self, k=1):\n        super().__init__()\n        self.k = k  # Number of sampling steps\n\n    @abstractmethod\n    def sample(self, energy_model, x_pos):\n        \"\"\"Abstract method: Generate negative samples from the energy model.\n        Args:\n            energy_model: Energy-based model (e.g., RBM)\n            x_pos: Positive samples (data)\n        Returns:\n            x_neg: Negative samples (model samples)\n        \"\"\"\n        raise NotImplementedError\n\n    def forward(self, energy_model, x_pos):\n        \"\"\"Compute the CD loss: E(x_pos) - E(x_neg)\"\"\"\n        x_neg = self.sample(energy_model, x_pos)\n        loss = energy_model(x_pos).mean() - energy_model(x_neg).mean()\n        return loss\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#torchebm.losses.contrastive_divergence.ContrastiveDivergenceBase.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k = k\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#torchebm.losses.contrastive_divergence.ContrastiveDivergenceBase.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(energy_model, x_pos)\n</code></pre> <p>Abstract method: Generate negative samples from the energy model. Args:     energy_model: Energy-based model (e.g., RBM)     x_pos: Positive samples (data) Returns:     x_neg: Negative samples (model samples)</p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>@abstractmethod\ndef sample(self, energy_model, x_pos):\n    \"\"\"Abstract method: Generate negative samples from the energy model.\n    Args:\n        energy_model: Energy-based model (e.g., RBM)\n        x_pos: Positive samples (data)\n    Returns:\n        x_neg: Negative samples (model samples)\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#torchebm.losses.contrastive_divergence.ContrastiveDivergenceBase.forward","title":"forward","text":"<pre><code>forward(energy_model, x_pos)\n</code></pre> <p>Compute the CD loss: E(x_pos) - E(x_neg)</p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>def forward(self, energy_model, x_pos):\n    \"\"\"Compute the CD loss: E(x_pos) - E(x_neg)\"\"\"\n    x_neg = self.sample(energy_model, x_pos)\n    loss = energy_model(x_pos).mean() - energy_model(x_neg).mean()\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/","title":"ParallelTemperingCD","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ContrastiveDivergenceBase</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ParallelTemperingCD(ContrastiveDivergenceBase):\n    def __init__(self, temps=[1.0, 0.5], k=5):\n        super().__init__(k)\n        self.temps = temps  # List of temperatures\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/#torchebm.losses.contrastive_divergence.ParallelTemperingCD.temps","title":"temps  <code>instance-attribute</code>","text":"<pre><code>temps = temps\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/","title":"PersistentContrastiveDivergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ContrastiveDivergenceBase</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class PersistentContrastiveDivergence(ContrastiveDivergenceBase):\n    def __init__(self, buffer_size=100):\n        super().__init__(k=1)\n        self.buffer = None  # Persistent chain state\n        self.buffer_size = buffer_size\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#torchebm.losses.contrastive_divergence.PersistentContrastiveDivergence.buffer","title":"buffer  <code>instance-attribute</code>","text":"<pre><code>buffer = None\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#torchebm.losses.contrastive_divergence.PersistentContrastiveDivergence.buffer_size","title":"buffer_size  <code>instance-attribute</code>","text":"<pre><code>buffer_size = buffer_size\n</code></pre>"},{"location":"api/torchebm/models/base_model/","title":"Base_model","text":""},{"location":"api/torchebm/models/base_model/#contents","title":"Contents","text":""},{"location":"api/torchebm/models/base_model/#classes","title":"Classes","text":"<ul> <li><code>BaseModel</code> - Base class for models.</li> </ul>"},{"location":"api/torchebm/models/base_model/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/models/base_model/#torchebm.models.base_model","title":"torchebm.models.base_model","text":""},{"location":"api/torchebm/models/base_model/classes/BaseModel/","title":"BaseModel","text":""},{"location":"api/torchebm/models/base_model/classes/BaseModel/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for models.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>sampler</code> <code>BaseSampler</code> <p>Sampler to use for sampling.</p> required <p>Methods:</p> Name Description <code>energy</code> <p>Compute the energy of the input.</p> <code>sample</code> <p>Sample from the model.</p> <code>train_step</code> <p>Perform a single training step</p> Source code in <code>torchebm/models/base_model.py</code> <pre><code>class BaseModel(ABC):\n    \"\"\"\n    Base class for models.\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        sampler (BaseSampler): Sampler to use for sampling.\n\n    Methods:\n        energy(x): Compute the energy of the input.\n        sample(num_samples): Sample from the model.\n        train_step(real_data): Perform a single training step\n    \"\"\"\n\n    def __init__(self, energy_function: EnergyFunction, sampler: BaseSampler):\n        self.energy_function = energy_function\n        self.sampler = sampler\n\n    @abstractmethod\n    def energy(self, x: torch.Tensor) -&gt; torch.Tensor:\n        pass\n\n    @abstractmethod\n    def sample(self, num_samples: int) -&gt; torch.Tensor:\n        pass\n\n    @abstractmethod\n    def train_step(self, real_data: torch.Tensor) -&gt; dict:\n        pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = sampler\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.energy","title":"energy  <code>abstractmethod</code>","text":"<pre><code>energy(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef energy(self, x: torch.Tensor) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(num_samples: int) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef sample(self, num_samples: int) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.train_step","title":"train_step  <code>abstractmethod</code>","text":"<pre><code>train_step(real_data: Tensor) -&gt; dict\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef train_step(self, real_data: torch.Tensor) -&gt; dict:\n    pass\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/","title":"Langevin_dynamics","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#classes","title":"Classes","text":"<ul> <li><code>LangevinDynamics</code> - Langevin Dynamics sampler implementing discretized gradient-based MCMC.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics","title":"torchebm.samplers.langevin_dynamics","text":"<p>Langevin Dynamics Sampler Module.</p> <p>This module provides an implementation of the Langevin Dynamics algorithm, a gradient-based Markov Chain Monte Carlo (MCMC) method. It leverages stochastic differential equations to sample from complex probability distributions, making it a lightweight yet effective tool for Bayesian inference and generative modeling.</p> <p>Key Features</p> <ul> <li>Gradient-based sampling with stochastic updates.</li> <li>Customizable step sizes and noise scales for flexible tuning.</li> <li>Optional diagnostics and trajectory tracking for analysis.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>LangevinDynamics</code> <p>Core class implementing the Langevin Dynamics sampler.</p>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--usage-example","title":"Usage Example","text":"<p>Sampling from a Custom Energy Function</p> <pre><code>from torchebm.samplers.mcmc.langevin import LangevinDynamics\nfrom torchebm.core.energy_function import GaussianEnergy\nimport torch\n\n# Define a 2D Gaussian energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize Langevin sampler\nsampler = LangevinDynamics(energy_fn, step_size=0.01, noise_scale=0.1)\n\n# Starting points for 5 chains\ninitial_state = torch.randn(5, 2)\n\n# Run sampling\nsamples, diagnostics = sampler.sample_chain(\n    x=initial_state, n_steps=100, n_samples=5, return_diagnostics=True\n)\nprint(f\"Samples shape: {samples.shape}\")\nprint(f\"Diagnostics keys: {diagnostics.shape}\")\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Langevin Dynamics Overview</p> <p>Langevin Dynamics simulates a stochastic process governed by the Langevin equation. For a state \\( x_t \\), the discretized update rule is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla U(x_t) + \\sqrt{2\\eta} \\epsilon_t \\] <ul> <li>\\( U(x) \\): Potential energy, where \\( U(x) = -\\log p(x) \\) and \\( p(x) \\) is the target distribution.</li> <li>\\( \\eta \\): Step size controlling the gradient descent.</li> <li>\\( \\epsilon_t \\sim \\mathcal{N}(0, I) \\): Gaussian noise introducing stochasticity.</li> </ul> <p>Over time, this process converges to samples from the Boltzmann distribution:</p> \\[ p(x) \\propto e^{-U(x)} \\] <p>Why Use Langevin Dynamics?</p> <ul> <li>Simplicity: Requires only first-order gradients, making it computationally lighter than methods like HMC.</li> <li>Exploration: The noise term prevents the sampler from getting stuck in local minima.</li> <li>Flexibility: Applicable to a wide range of energy-based models and score-based generative tasks.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--practical-considerations","title":"Practical Considerations","text":"<p>Parameter Tuning Guide</p> <ul> <li>Step Size (\\(\\eta\\)):<ul> <li>Too large: Instability and divergence</li> <li>Too small: Slow convergence</li> <li>Rule of thumb: Start with \\(\\eta \\approx 10^{-3}\\) to \\(10^{-5}\\)</li> </ul> </li> <li>Noise Scale (\\(\\beta^{-1/2}\\)):<ul> <li>Controls exploration-exploitation tradeoff</li> <li>Higher values help escape local minima</li> </ul> </li> <li>Decay Rate (future implementation):<ul> <li>Momentum-like term for accelerated convergence</li> </ul> </li> </ul> <p>Diagnostics Interpretation</p> <p>Use <code>return_diagnostics=True</code> to monitor: - Mean/Variance: Track distribution stationarity - Energy Gradients: Check for vanishing/exploding gradients - Autocorrelation: Assess mixing efficiency</p> <p>When to Choose Langevin Over HMC?</p> Criterion Langevin HMC Computational Cost Lower Higher Tuning Complexity Simpler More involved High Dimensions Efficient More efficient Multimodal Targets May need annealing Better exploration <p>How to Diagnose Sampling?</p> <p>Check diagnostics for: - Sample mean and variance convergence. - Gradient magnitudes (should stabilize). - Energy trends over iterations.</p> Further Reading <ul> <li>Langevin Dynamics Basics</li> <li>Score-Based Models and Langevin</li> <li>Practical Langevin Tutorial</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/","title":"Mcmc","text":""},{"location":"api/torchebm/samplers/mcmc/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/mcmc/#classes","title":"Classes","text":"<ul> <li><code>HamiltonianMonteCarlo</code> - Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc","title":"torchebm.samplers.mcmc","text":"<p>Hamiltonian Monte Carlo Sampler Module.</p> <p>This module provides a robust implementation of the Hamiltonian Monte Carlo (HMC) algorithm, a powerful Markov Chain Monte Carlo (MCMC) technique. By leveraging Hamiltonian dynamics, HMC efficiently explores complex, high-dimensional probability distributions, making it ideal for Bayesian inference and statistical modeling.</p> <p>Key Features</p> <ul> <li>Efficient sampling using Hamiltonian dynamics.</li> <li>Customizable step sizes and leapfrog steps for fine-tuned performance.</li> <li>Diagnostic tools to monitor convergence and sampling quality.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>HamiltonianMonteCarlo</code> <p>Implements the Hamiltonian Monte Carlo sampler.</p>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--usage-example","title":"Usage Example","text":"<p>Sampling from a Gaussian Distribution</p> <pre><code>from torchebm.samplers.mcmc import HamiltonianMonteCarlo\nfrom torchebm.core.energy_function import GaussianEnergy\nimport torch\n\n# Define a 2D Gaussian energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize HMC sampler\nhmc = HamiltonianMonteCarlo(energy_fn, step_size=0.1, n_leapfrog_steps=10)\n\n# Starting points for 10 chains\ninitial_state = torch.randn(10, 2)\n\n# Run sampling\nsamples, diagnostics = hmc.sample_chain(initial_state, n_steps=100, return_diagnostics=True)\nprint(f\"Samples: {samples.shape}\")\nprint(f\"Diagnostics: {diagnostics.keys()}\")\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Hamiltonian Dynamics in HMC</p> <p>HMC combines statistical sampling with concepts from classical mechanics. It introduces an auxiliary momentum variable \\( p \\) and defines a Hamiltonian:</p> \\[ H(q, p) = U(q) + K(p) \\] <ul> <li>Potential Energy: \\( U(q) = -\\log \\pi(q) \\), where \\( \\pi(q) \\) is the target distribution.</li> <li>Kinetic Energy: \\( K(p) = \\frac{1}{2} p^T M^{-1} p \\), with \\( M \\) as the mass matrix (often set to the identity matrix).</li> </ul> <p>This formulation allows HMC to propose new states by simulating trajectories along the energy landscape.</p> <p>Why Hamiltonian Dynamics?</p> <ul> <li>Efficient Exploration: HMC uses gradient information to propose new states, allowing it to explore the state space more efficiently, especially in high-dimensional and complex distributions.</li> <li>Reduced Correlation: By simulating Hamiltonian dynamics, HMC reduces the correlation between successive samples, leading to faster convergence to the target distribution.</li> <li>High Acceptance Rate: The use of Hamiltonian dynamics and a Metropolis acceptance step ensures that proposed moves are accepted with high probability, provided the numerical integration is accurate.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--leapfrog-integration","title":"Leapfrog Integration","text":"<p>Numerical Simulation of Dynamics</p> <p>HMC approximates Hamiltonian trajectories using the leapfrog integrator, a symplectic method that preserves energy. The steps are:</p> <ol> <li>Momentum Half-Step:     $$     p_{t + \\frac{\\epsilon}{2}} = p_t - \\frac{\\epsilon}{2} \\nabla U(q_t)     $$</li> <li>Position Full-Step:     $$     q_{t + 1} = q_t + \\epsilon M^{-1} p_{t + \\frac{\\epsilon}{2}}     $$</li> <li>Momentum Half-Step:     $$     p_{t + 1} = p_{t + \\frac{\\epsilon}{2}} - \\frac{\\epsilon}{2} \\nabla U(q_{t + 1})     $$</li> </ol> <p>Here, \\( \\epsilon \\) is the step size, and the process is repeated for \\( L \\) leapfrog steps.</p>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--acceptance-step","title":"Acceptance Step","text":"<p>Metropolis-Hastings Correction</p> <p>After proposing a new state \\( (q_{t + 1}, p_{t + 1}) \\), HMC applies an acceptance criterion to ensure detailed balance:</p> \\[ \\alpha = \\min \\left( 1, \\exp \\left( H(q_t, p_t) - H(q_{t + 1}, p_{t + 1}) \\right) \\right) \\] <p>The proposal is accepted with probability \\( \\alpha \\), correcting for numerical errors in the leapfrog integration.</p>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--practical-considerations","title":"Practical Considerations","text":"<p>Tuning Parameters</p> <ul> <li>Step Size (\\( \\epsilon \\)): Too large a step size can lead to unstable trajectories; too small reduces efficiency.</li> <li>Number of Leapfrog Steps (\\( L \\)): Affects the distance traveled per proposal\u2014balance exploration vs. computational cost.</li> <li>Mass Matrix (\\( M \\)): Adjusting \\( M \\) can improve sampling in distributions with varying scales.</li> </ul> <p>How to Diagnose Issues?</p> <p>Use diagnostics to check: - Acceptance rates (ideal: 0.6\u20130.8). - Energy conservation (should be relatively stable). - Autocorrelation of samples (should decrease with lag).</p> <p>Common Pitfalls</p> <ul> <li>Low Acceptance Rate: If the acceptance rate is too low, it may indicate that the step size is too large or the number of leapfrog steps is too high. Try reducing the step size or decreasing the number of leapfrog steps.</li> <li>High Correlation Between Samples: If samples are highly correlated, it may indicate that the step size is too small or the number of leapfrog steps is too few. Increase the step size or the number of leapfrog steps to improve exploration.</li> <li>Divergence or NaN Values: Numerical instability or poor parameter choices can lead to divergent behavior or NaN values. Ensure that the energy function and its gradients are correctly implemented and that parameters are appropriately scaled.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--advanced-insights","title":"Advanced Insights","text":"<p>Why HMC Outperforms Other MCMC Methods</p> <p>HMC's use of gradients and dynamics reduces random-walk behavior, making it particularly effective for: - High-dimensional spaces. - Multimodal distributions (with proper tuning). - Models with strong correlations between variables.</p> Further Reading <ul> <li>Hamiltonian Mechanics Explained</li> <li>Neal, R. M. (2011). \"MCMC using Hamiltonian dynamics.\" Handbook of Markov Chain Monte Carlo.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/","title":"LangevinDynamics","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Langevin Dynamics sampler implementing discretized gradient-based MCMC.</p> <p>This class implements the Langevin Dynamics algorithm, a gradient-based MCMC method that samples from a target distribution defined by an energy function. It uses a stochastic update rule combining gradient descent with Gaussian noise to explore the energy landscape.</p> <p>Each step updates the state \\(x_t\\) according to the discretized Langevin equation:</p> \\[x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t\\] <p>where \\(\\epsilon_t \\sim \\mathcal{N}(0, I)\\) and \\(\\eta\\) is the step size.</p> <p>This process generates samples that asymptotically follow the Boltzmann distribution:</p> \\[p(x) \\propto e^{-U(x)}\\] <p>where \\(U(x)\\) defines the energy landscape.</p> <p>Algorithm Summary</p> <ol> <li>If <code>x</code> is not provided, initialize it with Gaussian noise.</li> <li>Iteratively update <code>x</code> for <code>n_steps</code> using <code>self.langevin_step()</code>.</li> <li>Optionally track trajectory (<code>return_trajectory=True</code>).</li> <li>Optionally collect diagnostics such as mean, variance, and energy gradients.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>step_size</code> <code>float</code> <p>Step size for updates.</p> <code>0.001</code> <code>noise_scale</code> <code>float</code> <p>Scale of the noise.</p> <code>1.0</code> <code>decay</code> <code>float</code> <p>Damping coefficient (not supported yet).</p> <code>0.0</code> <code>dtype</code> <code>dtype</code> <p>Data type to use for the computations.</p> <code>float32</code> <code>device</code> <code>str</code> <p>Device to run the computations on (e.g., \"cpu\" or \"cuda\").</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>For invalid parameter ranges</p> <p>Methods:</p> Name Description <code>langevin_step</code> <p>Perform a Langevin step.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics</p> <p>Basic Usage</p> <pre><code># Define energy function\nenergy_fn = QuadraticEnergy(A=torch.eye(2), b=torch.zeros(2))\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=0.1\n)\n\n# Sample 100 points from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=2,\n    n_steps=50,\n    n_samples=100\n)\n</code></pre> <p>Parameter Relationships</p> <p>The effective temperature is controlled by: \\(\\(\\text{Temperature} = \\frac{\\text{noise_scale}^2}{2 \\cdot \\text{step_size}}\\)\\) Adjust both parameters together to maintain constant temperature.</p> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>class LangevinDynamics(BaseSampler):\n    r\"\"\"\n    Langevin Dynamics sampler implementing discretized gradient-based MCMC.\n\n    This class implements the Langevin Dynamics algorithm, a gradient-based MCMC method that samples from a target\n    distribution defined by an energy function. It uses a stochastic update rule combining gradient descent with Gaussian noise to explore the energy landscape.\n\n    Each step updates the state $x_t$ according to the discretized Langevin equation:\n\n    $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n    where $\\epsilon_t \\sim \\mathcal{N}(0, I)$ and $\\eta$ is the step size.\n\n    This process generates samples that asymptotically follow the Boltzmann distribution:\n\n\n    $$p(x) \\propto e^{-U(x)}$$\n\n    where $U(x)$ defines the energy landscape.\n\n    !!! note \"Algorithm Summary\"\n\n        1. If `x` is not provided, initialize it with Gaussian noise.\n        2. Iteratively update `x` for `n_steps` using `self.langevin_step()`.\n        3. Optionally track trajectory (`return_trajectory=True`).\n        4. Optionally collect diagnostics such as mean, variance, and energy gradients.\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        step_size (float): Step size for updates.\n        noise_scale (float): Scale of the noise.\n        decay (float): Damping coefficient (not supported yet).\n        dtype (torch.dtype): Data type to use for the computations.\n        device (str): Device to run the computations on (e.g., \"cpu\" or \"cuda\").\n\n    Raises:\n        ValueError: For invalid parameter ranges\n\n    Methods:\n        langevin_step(prev_x, noise): Perform a Langevin step.\n        sample_chain(x, dim, n_steps, n_samples, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(dim, n_steps, n_samples): Initialize the diagnostics\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Define energy function\n        energy_fn = QuadraticEnergy(A=torch.eye(2), b=torch.zeros(2))\n\n        # Initialize sampler\n        sampler = LangevinDynamics(\n            energy_function=energy_fn,\n            step_size=0.01,\n            noise_scale=0.1\n        )\n\n        # Sample 100 points from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=2,\n            n_steps=50,\n            n_samples=100\n        )\n        ```\n    !!! warning \"Parameter Relationships\"\n        The effective temperature is controlled by:\n        $$\\text{Temperature} = \\frac{\\text{noise_scale}^2}{2 \\cdot \\text{step_size}}$$\n        Adjust both parameters together to maintain constant temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        step_size: float = 1e-3,\n        noise_scale: float = 1.0,\n        decay: float = 0.0,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n    ):\n        super().__init__(energy_function, dtype, device)\n\n        if step_size &lt;= 0 or noise_scale &lt;= 0:\n            raise ValueError(\"step_size and noise_scale must be positive\")\n        if not 0 &lt;= decay &lt;= 1:\n            raise ValueError(\"decay must be between 0 and 1\")\n\n        if device is not None:\n            self.device = torch.device(device)\n            energy_function = energy_function.to(self.device)\n        else:\n            self.device = torch.device(\"cpu\")\n        self.dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n        self.energy_function = energy_function\n        self.step_size = step_size\n        self.noise_scale = noise_scale\n        self.decay = decay\n\n    def langevin_step(self, prev_x: torch.Tensor, noise: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Perform a single Langevin dynamics update step.\n\n        Implements the discrete Langevin equation:\n\n        $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n        Args:\n            prev_x (torch.Tensor): Current state tensor of shape (batch_size, dim)\n            noise (torch.Tensor): Gaussian noise tensor of shape (batch_size, dim)\n\n        Returns:\n            torch.Tensor: Updated state tensor of same shape as prev_x\n\n        Example:\n            ```python\n            # Single step for 10 particles in 2D space\n            current_state = torch.randn(10, 2)\n            noise = torch.randn_like(current_state)\n            next_state = langevin.langevin_step(current_state, noise)\n            ```\n        \"\"\"\n\n        gradient_fn = partial(self.energy_function.gradient)\n        new_x = (\n            prev_x\n            - self.step_size * gradient_fn(prev_x)\n            + torch.sqrt(torch.tensor(2.0 * self.step_size, device=prev_x.device))\n            * noise\n        )\n        return new_x\n\n    @torch.no_grad()\n    def sample_chain(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        \"\"\"\n        Generate Markov chain samples using Langevin dynamics.\n\n        Args:\n            x: Initial state to start the sampling from.\n            dim: Dimension of the state space.\n            n_steps: Number of steps to take between samples.\n            n_samples: Number of samples to generate.\n            return_trajectory: Whether to return the trajectory of the samples.\n            return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n        Returns:\n            Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n                - If `return_trajectory=False` and `return_diagnostics=False`, returns the final\n                  samples of shape `(n_samples, dim)`.\n                - If `return_trajectory=True`, returns a tensor of shape `(n_samples, n_steps, dim)`,\n                  containing the sampled trajectory.\n                - If `return_diagnostics=True`, returns a tuple `(samples, diagnostics)`, where\n                  `diagnostics` is a list of dictionaries storing per-step statistics.\n\n        Raises:\n            ValueError: If input dimensions mismatch\n\n        Note:\n            - Automatically handles device placement (CPU/GPU)\n            - Uses mixed-precision training when available\n            - Diagnostics include:\n                * Mean and variance across dimensions\n                * Energy gradients\n                * Noise statistics\n\n        Example:\n            ```python\n            # Generate 100 samples from 5 parallel chains\n            samples = sampler.sample_chain(\n                dim=32,\n                n_steps=500,\n                n_samples=100,\n                return_diagnostics=True\n            )\n            ```\n\n        \"\"\"\n        if x is None:\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(self.device)  # Initial batch\n\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        if return_diagnostics:\n            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n\n        with torch.amp.autocast(device_type=\"cuda\" if self.device == \"cuda\" else \"cpu\"):\n            noise = torch.randn_like(x, device=self.device)\n            for i in range(n_steps):\n                x = self.langevin_step(x, noise)\n                if return_trajectory:\n                    trajectory[:, i, :] = x\n\n                if return_diagnostics:\n                    mean_x = x.mean(dim=0)\n                    var_x = x.var(dim=0)\n                    energy = self.energy_function.gradient(x)\n\n                    # Stack the diagnostics along the second dimension (index 1)\n                    diagnostics[i, 0, :, :] = mean_x\n                    diagnostics[i, 1, :, :] = var_x\n                    diagnostics[i, 2, :, :] = energy\n\n        if return_trajectory:\n            if return_diagnostics:\n                return trajectory, diagnostics\n            return trajectory\n        if return_diagnostics:\n            return x, diagnostics\n        return x\n\n    def _setup_diagnostics(\n        self, dim: int, n_steps: int, n_samples: int = None\n    ) -&gt; torch.Tensor:\n        if n_samples is not None:\n            return torch.empty(\n                (n_steps, 3, n_samples, dim), device=self.device, dtype=self.dtype\n            )\n        else:\n            return torch.empty((n_steps, 3, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device(device)\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = float16 if device == 'cuda' else float32\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.step_size","title":"step_size  <code>instance-attribute</code>","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.noise_scale","title":"noise_scale  <code>instance-attribute</code>","text":"<pre><code>noise_scale = noise_scale\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.decay","title":"decay  <code>instance-attribute</code>","text":"<pre><code>decay = decay\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.langevin_step","title":"langevin_step","text":"<pre><code>langevin_step(prev_x: Tensor, noise: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Perform a single Langevin dynamics update step.</p> <p>Implements the discrete Langevin equation:</p> \\[x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t\\] <p>Parameters:</p> Name Type Description Default <code>prev_x</code> <code>Tensor</code> <p>Current state tensor of shape (batch_size, dim)</p> required <code>noise</code> <code>Tensor</code> <p>Gaussian noise tensor of shape (batch_size, dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Updated state tensor of same shape as prev_x</p> Example <pre><code># Single step for 10 particles in 2D space\ncurrent_state = torch.randn(10, 2)\nnoise = torch.randn_like(current_state)\nnext_state = langevin.langevin_step(current_state, noise)\n</code></pre> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>def langevin_step(self, prev_x: torch.Tensor, noise: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Perform a single Langevin dynamics update step.\n\n    Implements the discrete Langevin equation:\n\n    $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n    Args:\n        prev_x (torch.Tensor): Current state tensor of shape (batch_size, dim)\n        noise (torch.Tensor): Gaussian noise tensor of shape (batch_size, dim)\n\n    Returns:\n        torch.Tensor: Updated state tensor of same shape as prev_x\n\n    Example:\n        ```python\n        # Single step for 10 particles in 2D space\n        current_state = torch.randn(10, 2)\n        noise = torch.randn_like(current_state)\n        next_state = langevin.langevin_step(current_state, noise)\n        ```\n    \"\"\"\n\n    gradient_fn = partial(self.energy_function.gradient)\n    new_x = (\n        prev_x\n        - self.step_size * gradient_fn(prev_x)\n        + torch.sqrt(torch.tensor(2.0 * self.step_size, device=prev_x.device))\n        * noise\n    )\n    return new_x\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.sample_chain","title":"sample_chain","text":"<pre><code>sample_chain(x: Optional[Tensor] = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> <p>Generate Markov chain samples using Langevin dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Initial state to start the sampling from.</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of the state space.</p> <code>10</code> <code>n_steps</code> <code>int</code> <p>Number of steps to take between samples.</p> <code>100</code> <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>1</code> <code>return_trajectory</code> <code>bool</code> <p>Whether to return the trajectory of the samples.</p> <code>False</code> <code>return_diagnostics</code> <code>bool</code> <p>Whether to return the diagnostics of the sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]: - If <code>return_trajectory=False</code> and <code>return_diagnostics=False</code>, returns the final   samples of shape <code>(n_samples, dim)</code>. - If <code>return_trajectory=True</code>, returns a tensor of shape <code>(n_samples, n_steps, dim)</code>,   containing the sampled trajectory. - If <code>return_diagnostics=True</code>, returns a tuple <code>(samples, diagnostics)</code>, where   <code>diagnostics</code> is a list of dictionaries storing per-step statistics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input dimensions mismatch</p> Note <ul> <li>Automatically handles device placement (CPU/GPU)</li> <li>Uses mixed-precision training when available</li> <li>Diagnostics include:<ul> <li>Mean and variance across dimensions</li> <li>Energy gradients</li> <li>Noise statistics</li> </ul> </li> </ul> Example <pre><code># Generate 100 samples from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=32,\n    n_steps=500,\n    n_samples=100,\n    return_diagnostics=True\n)\n</code></pre> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>@torch.no_grad()\ndef sample_chain(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    \"\"\"\n    Generate Markov chain samples using Langevin dynamics.\n\n    Args:\n        x: Initial state to start the sampling from.\n        dim: Dimension of the state space.\n        n_steps: Number of steps to take between samples.\n        n_samples: Number of samples to generate.\n        return_trajectory: Whether to return the trajectory of the samples.\n        return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n    Returns:\n        Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n            - If `return_trajectory=False` and `return_diagnostics=False`, returns the final\n              samples of shape `(n_samples, dim)`.\n            - If `return_trajectory=True`, returns a tensor of shape `(n_samples, n_steps, dim)`,\n              containing the sampled trajectory.\n            - If `return_diagnostics=True`, returns a tuple `(samples, diagnostics)`, where\n              `diagnostics` is a list of dictionaries storing per-step statistics.\n\n    Raises:\n        ValueError: If input dimensions mismatch\n\n    Note:\n        - Automatically handles device placement (CPU/GPU)\n        - Uses mixed-precision training when available\n        - Diagnostics include:\n            * Mean and variance across dimensions\n            * Energy gradients\n            * Noise statistics\n\n    Example:\n        ```python\n        # Generate 100 samples from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=32,\n            n_steps=500,\n            n_samples=100,\n            return_diagnostics=True\n        )\n        ```\n\n    \"\"\"\n    if x is None:\n        x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n    else:\n        x = x.to(self.device)  # Initial batch\n\n    if return_trajectory:\n        trajectory = torch.empty(\n            (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n        )\n\n    if return_diagnostics:\n        diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n\n    with torch.amp.autocast(device_type=\"cuda\" if self.device == \"cuda\" else \"cpu\"):\n        noise = torch.randn_like(x, device=self.device)\n        for i in range(n_steps):\n            x = self.langevin_step(x, noise)\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n            if return_diagnostics:\n                mean_x = x.mean(dim=0)\n                var_x = x.var(dim=0)\n                energy = self.energy_function.gradient(x)\n\n                # Stack the diagnostics along the second dimension (index 1)\n                diagnostics[i, 0, :, :] = mean_x\n                diagnostics[i, 1, :, :] = var_x\n                diagnostics[i, 2, :, :] = energy\n\n    if return_trajectory:\n        if return_diagnostics:\n            return trajectory, diagnostics\n        return trajectory\n    if return_diagnostics:\n        return x, diagnostics\n    return x\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/","title":"HamiltonianMonteCarlo","text":""},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.</p> <p>This class implements the Hamiltonian Monte Carlo algorithm, which uses concepts from Hamiltonian mechanics to generate more efficient proposals than traditional random-walk methods. By introducing an auxiliary momentum variable and simulating Hamiltonian dynamics, HMC can make distant proposals with high acceptance probability, particularly in high-dimensional spaces.</p> <p>The method works by: 1. Augmenting the state space with momentum variables 2. Simulating Hamiltonian dynamics using leapfrog integration 3. Accepting or rejecting proposals using a Metropolis-Hastings criterion</p> <p>Algorithm Summary</p> <ol> <li>If <code>x</code> is not provided, initialize it with Gaussian noise.</li> <li>For each step:    a. Sample momentum from Gaussian distribution.    b. Perform leapfrog integration for <code>n_leapfrog_steps</code> steps.    c. Accept or reject the proposal based on Metropolis-Hastings criterion.</li> <li>Optionally track trajectory and diagnostics.</li> </ol> <p>Key Advantages</p> <ul> <li>Efficiency: Performs well in high dimensions by avoiding random walk behavior</li> <li>Exploration: Can efficiently traverse complex probability landscapes</li> <li>Energy Conservation: Uses symplectic integrators that approximately preserve energy</li> <li>Adaptability: Can be adjusted through mass matrices to handle varying scales</li> </ul> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>step_size</code> <code>float</code> <p>Step size for leapfrog updates.</p> <code>0.1</code> <code>n_leapfrog_steps</code> <code>int</code> <p>Number of leapfrog steps per proposal.</p> <code>10</code> <code>mass</code> <code>Optional[Tuple[float, Tensor]]</code> <p>Optional mass matrix or scalar for momentum sampling.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type to use for computations.</p> <code>float32</code> <code>device</code> <code>Optional[Union[Tuple[str, device]]]</code> <p>Device to run computations on.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>For invalid parameter ranges</p> <p>Methods:</p> Name Description <code>_initialize_momentum</code> <p>Generate initial momentum from Gaussian distribution.</p> <code>_compute_kinetic_energy</code> <p>Compute the kinetic energy of the momentum.</p> <code>_leapfrog_step</code> <p>Perform a single leapfrog step.</p> <code>_leapfrog_integration</code> <p>Perform full leapfrog integration.</p> <code>hmc_step</code> <p>Perform one HMC step with Metropolis-Hastings acceptance.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics.</p> <p>Basic Usage</p> <pre><code># Define energy function for a 2D Gaussian\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize HMC sampler\nsampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10\n)\n\n# Sample 100 points from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=2,\n    n_steps=100,\n    n_samples=5\n)\n</code></pre> <p>Parameter Relationships</p> <ul> <li>Decreasing <code>step_size</code> improves stability but may reduce mixing.</li> <li>Increasing <code>n_leapfrog_steps</code> allows exploring more distant regions but increases computation.</li> <li>The <code>mass</code> parameter can be tuned to match the geometry of the target distribution.</li> </ul> Source code in <code>torchebm/samplers/mcmc.py</code> <pre><code>class HamiltonianMonteCarlo(BaseSampler):\n    r\"\"\"\n    Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.\n\n    This class implements the Hamiltonian Monte Carlo algorithm, which uses concepts from\n    Hamiltonian mechanics to generate more efficient proposals than traditional random-walk\n    methods. By introducing an auxiliary momentum variable and simulating Hamiltonian dynamics,\n    HMC can make distant proposals with high acceptance probability, particularly in\n    high-dimensional spaces.\n\n    The method works by:\n    1. Augmenting the state space with momentum variables\n    2. Simulating Hamiltonian dynamics using leapfrog integration\n    3. Accepting or rejecting proposals using a Metropolis-Hastings criterion\n\n    !!! note \"Algorithm Summary\"\n        1. If `x` is not provided, initialize it with Gaussian noise.\n        2. For each step:\n           a. Sample momentum from Gaussian distribution.\n           b. Perform leapfrog integration for `n_leapfrog_steps` steps.\n           c. Accept or reject the proposal based on Metropolis-Hastings criterion.\n        3. Optionally track trajectory and diagnostics.\n\n    !!! tip \"Key Advantages\"\n        - **Efficiency**: Performs well in high dimensions by avoiding random walk behavior\n        - **Exploration**: Can efficiently traverse complex probability landscapes\n        - **Energy Conservation**: Uses symplectic integrators that approximately preserve energy\n        - **Adaptability**: Can be adjusted through mass matrices to handle varying scales\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        step_size (float): Step size for leapfrog updates.\n        n_leapfrog_steps (int): Number of leapfrog steps per proposal.\n        mass (Optional[Tuple[float, torch.Tensor]]): Optional mass matrix or scalar for momentum sampling.\n        dtype (torch.dtype): Data type to use for computations.\n        device (Optional[Union[Tuple[str, torch.device]]]): Device to run computations on.\n\n    Raises:\n        ValueError: For invalid parameter ranges\n\n    Methods:\n        _initialize_momentum(shape): Generate initial momentum from Gaussian distribution.\n        _compute_kinetic_energy(p): Compute the kinetic energy of the momentum.\n        _leapfrog_step(position, momentum, gradient_fn): Perform a single leapfrog step.\n        _leapfrog_integration(position, momentum): Perform full leapfrog integration.\n        hmc_step(current_position): Perform one HMC step with Metropolis-Hastings acceptance.\n        sample_chain(x, dim, n_steps, n_samples, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(dim, n_steps, n_samples): Initialize the diagnostics.\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Define energy function for a 2D Gaussian\n        energy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n        # Initialize HMC sampler\n        sampler = HamiltonianMonteCarlo(\n            energy_function=energy_fn,\n            step_size=0.1,\n            n_leapfrog_steps=10\n        )\n\n        # Sample 100 points from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=2,\n            n_steps=100,\n            n_samples=5\n        )\n        ```\n\n    !!! warning \"Parameter Relationships\"\n        - Decreasing `step_size` improves stability but may reduce mixing.\n        - Increasing `n_leapfrog_steps` allows exploring more distant regions but increases computation.\n        - The `mass` parameter can be tuned to match the geometry of the target distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        step_size: float = 0.1,\n        n_leapfrog_steps: int = 10,\n        mass: Optional[Tuple[float, torch.Tensor]] = None,\n        dtype: torch.Tensor = torch.float32,\n        device: Optional[Union[Tuple[str, torch.device]]] = None,\n    ):\n        \"\"\"Initialize the Hamiltonian Monte Carlo sampler.\n\n        Args:\n            energy_function: Energy function to sample from.\n            step_size: Step size for leapfrog integration (epsilon in equations).\n            n_leapfrog_steps: Number of leapfrog steps per HMC trajectory.\n            mass: Optional mass parameter or matrix for momentum.\n                If float: Uses scalar mass for all dimensions.\n                If Tensor: Uses diagonal mass matrix.\n                If None: Uses identity mass matrix.\n            dtype: Data type for computations.\n            device: Device to run computations on (\"cpu\" or \"cuda\").\n\n        Raises:\n            ValueError: If step_size or n_leapfrog_steps is non-positive.\n        \"\"\"\n        super().__init__(energy_function=energy_function, dtype=dtype, device=device)\n        if step_size &lt;= 0:\n            raise ValueError(\"step_size must be positive\")\n        if n_leapfrog_steps &lt;= 0:\n            raise ValueError(\"n_leapfrog_steps must be positive\")\n\n        # Ensure device consistency: convert device to torch.device and move energy_function\n        if device is not None:\n            self.device = torch.device(device)\n            energy_function = energy_function.to(self.device)\n        else:\n            self.device = torch.device(\"cpu\")\n\n        self.dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n        self.step_size = step_size\n        self.n_leapfrog_steps = n_leapfrog_steps\n        self.energy_function = energy_function\n        if mass is not None and not isinstance(mass, float):\n            self.mass = mass.to(self.device)\n        else:\n            self.mass = mass\n\n    def _initialize_momentum(self, shape: torch.Size) -&gt; torch.Tensor:\n        \"\"\"Initialize momentum variables from Gaussian distribution.\n\n        For HMC, momentum variables are sampled from a multivariate Gaussian distribution\n        determined by the mass matrix. The kinetic energy is then:\n        K(p) = p^T M^(-1) p / 2\n\n        Args:\n            shape: Size of the momentum tensor to generate.\n\n        Returns:\n            Momentum tensor drawn from appropriate Gaussian distribution.\n\n        Note:\n            When using a mass matrix M, we sample from N(0, M) rather than\n            transforming samples from N(0, I).\n        \"\"\"\n        p = torch.randn(shape, dtype=self.dtype, device=self.device)\n\n        if self.mass is not None:\n            # Apply mass matrix (equivalent to sampling from N(0, M))\n            if isinstance(self.mass, float):\n                p = p * torch.sqrt(\n                    torch.tensor(self.mass, dtype=self.dtype, device=self.device)\n                )\n            else:\n                mass_sqrt = torch.sqrt(self.mass)\n                p = p * mass_sqrt.view(*([1] * (len(shape) - 1)), -1).expand_as(p)\n        return p\n\n    def _compute_kinetic_energy(self, p: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the kinetic energy given momentum.\n\n        The kinetic energy is defined as:\n        $$ K(p) = p^T M^(-1) p / 2 $$\n\n        Args:\n            p: Momentum tensor.\n\n        Returns:\n            Kinetic energy for each sample in the batch.\n        \"\"\"\n        if self.mass is None:\n            return 0.5 * torch.sum(p**2, dim=-1)\n        elif isinstance(self.mass, float):\n            return 0.5 * torch.sum(p**2, dim=-1) / self.mass\n        else:\n            return 0.5 * torch.sum(\n                p**2 / self.mass.view(*([1] * (len(p.shape) - 1)), -1), dim=-1\n            )\n\n    def _leapfrog_step(\n        self, position: torch.Tensor, momentum: torch.Tensor, gradient_fn: Callable\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a single leapfrog integration step.\n\n        Implements the symplectic leapfrog integrator for Hamiltonian dynamics:\n        1. Half-step momentum update: p(t+\u03b5/2) = p(t) - (\u03b5/2)\u2207U(q(t))\n        2. Full-step position update: q(t+\u03b5) = q(t) + \u03b5M^(-1)p(t+\u03b5/2)\n        3. Half-step momentum update: p(t+\u03b5) = p(t+\u03b5/2) - (\u03b5/2)\u2207U(q(t+\u03b5))\n\n        Args:\n            position: Current position tensor.\n            momentum: Current momentum tensor.\n            gradient_fn: Function to compute gradient of potential energy.\n\n        Returns:\n            Tuple of (new_position, new_momentum).\n        \"\"\"\n        # Calculate gradient for half-step momentum update with numerical safeguards\n        grad = gradient_fn(position)\n        # Clip extreme gradient values to prevent instability\n        grad = torch.clamp(grad, min=-1e6, max=1e6)\n\n        # Half-step momentum update\n        p_half = momentum - 0.5 * self.step_size * grad\n\n        # Full-step position update with mass matrix adjustment\n        if self.mass is None:\n            x_new = position + self.step_size * p_half\n        else:\n            if isinstance(self.mass, float):\n                # Ensure mass is positive to avoid division issues\n                safe_mass = max(self.mass, 1e-10)\n                x_new = position + self.step_size * p_half / safe_mass\n            else:\n                # Create safe mass tensor avoiding zeros or negative values\n                safe_mass = torch.clamp(self.mass, min=1e-10)\n                x_new = position + self.step_size * p_half / safe_mass.view(\n                    *([1] * (len(position.shape) - 1)), -1\n                )\n\n        # Half-step momentum update with gradient clamping\n        grad_new = gradient_fn(x_new)\n        grad_new = torch.clamp(grad_new, min=-1e6, max=1e6)\n        p_new = p_half - 0.5 * self.step_size * grad_new\n\n        return x_new, p_new\n\n    def _leapfrog_integration(\n        self, position: torch.Tensor, momentum: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a full leapfrog integration for n_leapfrog_steps.\n\n        Applies multiple leapfrog steps to simulate Hamiltonian dynamics\n        for a trajectory of specified length. This is the core of the HMC\n        proposal generation.\n\n        Args:\n            position: Initial position tensor.\n            momentum: Initial momentum tensor.\n\n        Returns:\n            Tuple of (final_position, final_momentum) after integration.\n        \"\"\"\n        gradient_fn = partial(self.energy_function.gradient)\n        x = position\n        p = momentum\n\n        # Add check for NaN values before starting integration\n        if torch.isnan(x).any() or torch.isnan(p).any():\n            # Replace NaN values with zeros\n            x = torch.nan_to_num(x, nan=0.0)\n            p = torch.nan_to_num(p, nan=0.0)\n\n        for _ in range(self.n_leapfrog_steps):\n            x, p = self._leapfrog_step(x, p, gradient_fn)\n\n            # Check for NaN values after each step\n            if torch.isnan(x).any() or torch.isnan(p).any():\n                # If NaN values appear, break the integration\n                # Replace NaN with zeros and return current state\n                x = torch.nan_to_num(x, nan=0.0)\n                p = torch.nan_to_num(p, nan=0.0)\n                break\n\n        return x, p\n\n    def hmc_step(\n        self, current_position: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a single HMC step with Metropolis-Hastings acceptance.\n\n        This implements the core HMC algorithm:\n        1. Sample initial momentum\n        2. Compute initial Hamiltonian\n        3. Perform leapfrog integration to propose new state\n        4. Compute final Hamiltonian\n        5. Accept/reject based on Metropolis-Hastings criterion\n\n        Args:\n            current_position: Current position tensor of shape (batch_size, dim).\n\n        Returns:\n            Tuple containing:\n            - new_position: Updated position tensor\n            - acceptance_prob: Probability of accepting each proposal\n            - accepted: Boolean mask indicating which proposals were accepted\n        \"\"\"\n        batch_size = current_position.shape[0]\n\n        # Sample initial momentum\n        current_momentum = self._initialize_momentum(current_position.shape)\n\n        # Compute current Hamiltonian: H = U(q) + K(p)\n        # Add numerical stability with clamping\n        current_energy = self.energy_function(current_position)\n        current_energy = torch.clamp(current_energy, min=-1e10, max=1e10)  # Prevent extreme energy values\n\n        current_kinetic = self._compute_kinetic_energy(current_momentum)\n        current_kinetic = torch.clamp(current_kinetic, min=0, max=1e10)  # Kinetic energy should be non-negative\n\n        current_hamiltonian = current_energy + current_kinetic\n\n        # Perform leapfrog integration to get proposal\n        proposed_position, proposed_momentum = self._leapfrog_integration(\n            current_position, current_momentum\n        )\n\n        # Compute proposed Hamiltonian with similar numerical stability\n        proposed_energy = self.energy_function(proposed_position)\n        proposed_energy = torch.clamp(proposed_energy, min=-1e10, max=1e10)\n\n        proposed_kinetic = self._compute_kinetic_energy(proposed_momentum)\n        proposed_kinetic = torch.clamp(proposed_kinetic, min=0, max=1e10)\n\n        proposed_hamiltonian = proposed_energy + proposed_kinetic\n\n        # Metropolis-Hastings acceptance criterion\n        # Clamp hamiltonian_diff to avoid overflow in exp()\n        hamiltonian_diff = current_hamiltonian - proposed_hamiltonian\n        hamiltonian_diff = torch.clamp(hamiltonian_diff, max=50, min=-50)\n\n        acceptance_prob = torch.min(\n            torch.ones(batch_size, device=self.device), torch.exp(hamiltonian_diff)\n        )\n\n        # Accept/reject based on acceptance probability\n        random_uniform = torch.rand(batch_size, device=self.device)\n        accepted = random_uniform &lt; acceptance_prob\n        accepted_mask = accepted.float().view(\n            -1, *([1] * (len(current_position.shape) - 1))\n        )\n\n        # Update position based on acceptance\n        new_position = (\n            accepted_mask * proposed_position + (1.0 - accepted_mask) * current_position\n        )\n\n        return new_position, acceptance_prob, accepted\n\n    @torch.no_grad()\n    def sample_chain(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = None,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Generate samples using Hamiltonian Monte Carlo.\n\n        Runs an HMC chain for a specified number of steps, optionally returning\n        the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian\n        dynamics with leapfrog integration to propose samples efficiently, particularly\n        in high-dimensional spaces.\n\n        Args:\n            x: Initial state to start sampling from. If None, random initialization is used.\n            dim: Dimension of the state space when x is None. If None, will attempt to infer from the energy function.\n            n_steps: Number of HMC steps to perform.\n            n_samples: Number of parallel chains to run.\n            return_trajectory: If True, return the entire trajectory of samples.\n            return_diagnostics: If True, return diagnostics about the sampling process.\n\n        Returns:\n            If return_trajectory=False and return_diagnostics=False:\n                Tensor of shape (n_samples, dim) with final samples.\n            If return_trajectory=True and return_diagnostics=False:\n                Tensor of shape (n_samples, n_steps, dim) with the trajectory of all samples.\n            If return_diagnostics=True:\n                Tuple of (samples, diagnostics) where diagnostics contains information about\n                the sampling process, including mean, variance, energy values, and acceptance rates.\n\n        Note:\n            This method uses automatic mixed precision when available on CUDA devices\n            to improve performance while maintaining numerical stability for the\n            Hamiltonian dynamics simulation.\n\n        Example:\n            ```python\n            # Run 10 parallel chains for 1000 steps\n            samples, diagnostics = hmc.sample_chain(\n                dim=10,\n                n_steps=1000,\n                n_samples=10,\n                return_diagnostics=True\n            )\n\n            # Plot acceptance rates\n            import matplotlib.pyplot as plt\n            plt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\n            plt.ylabel('Acceptance Rate')\n            plt.xlabel('Step')\n            plt.show()\n            ```\n        \"\"\"\n        if x is None:\n            # If dim is not provided, try to infer from the energy function\n            if dim is None:\n                # Check if it's GaussianEnergy which has mean attribute\n                if hasattr(self.energy_function, 'mean'):\n                    dim = self.energy_function.mean.shape[0]\n                else:\n                    raise ValueError(\"dim must be provided when x is None and cannot be inferred from the energy function\")\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(self.device)\n\n        # Get dimension from x for later use\n        dim = x.shape[1]\n\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        if return_diagnostics:\n            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n            acceptance_rates = torch.zeros(\n                n_steps, device=self.device, dtype=self.dtype\n            )\n\n        with torch.amp.autocast(device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"):\n            for i in range(n_steps):\n                # Perform single HMC step\n                x, acceptance_prob, accepted = self.hmc_step(x)\n\n                if return_trajectory:\n                    trajectory[:, i, :] = x\n\n                if return_diagnostics:\n                    # Calculate diagnostics with numerical stability safeguards\n                    mean_x = x.mean(dim=0).unsqueeze(0).expand_as(x)\n\n                    # Clamp variance calculations to prevent NaN values\n                    # First compute variance in a numerically stable way\n                    # and then clamp to ensure positive finite values\n                    x_centered = x - mean_x\n                    var_x = torch.mean(x_centered**2, dim=0)\n                    var_x = torch.clamp(var_x, min=1e-10, max=1e10)  # Prevent zero/extreme variances\n                    var_x = var_x.unsqueeze(0).expand_as(x)\n\n                    # Energy values (ensure finite values)\n                    energy = self.energy_function(x)  # assumed to have shape (n_samples,)\n                    energy = torch.clamp(energy, min=-1e10, max=1e10)  # Prevent extreme energy values\n                    energy = energy.unsqueeze(1).expand_as(x)\n\n                    # Acceptance rate is already between 0 and 1\n                    acceptance_rate = accepted.float().mean()\n                    acceptance_rate_expanded = torch.ones_like(x) * acceptance_rate\n\n                    # Stack diagnostics\n                    diagnostics[i, 0, :, :] = mean_x\n                    diagnostics[i, 1, :, :] = var_x\n                    diagnostics[i, 2, :, :] = energy\n                    diagnostics[i, 3, :, :] = acceptance_rate_expanded\n\n        if return_trajectory:\n            if return_diagnostics:\n                return trajectory, diagnostics  # , acceptance_rates\n            return trajectory\n\n        if return_diagnostics:\n            return x, diagnostics  # , acceptance_rates\n\n        return x\n\n    def _setup_diagnostics(\n        self, dim: int, n_steps: int, n_samples: int = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Initialize diagnostics tensor to track HMC sampling metrics.\n\n        Creates a tensor to store diagnostics information during sampling, including:\n        - Mean of samples (dimension 0)\n        - Variance of samples (dimension 1)\n        - Energy values (dimension 2)\n        - Acceptance rates (dimension 3)\n\n        Args:\n            dim: Dimensionality of the state space.\n            n_steps: Number of sampling steps.\n            n_samples: Number of parallel chains (if None, assumed to be 1).\n\n        Returns:\n            Empty tensor of shape (n_steps, 4, n_samples, dim) to store diagnostics.\n        \"\"\"\n        if n_samples is not None:\n            return torch.empty(\n                (n_steps, 4, n_samples, dim), device=self.device, dtype=self.dtype\n            )\n        else:\n            return torch.empty((n_steps, 4, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device(device)\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = float16 if device == 'cuda' else float32\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.step_size","title":"step_size  <code>instance-attribute</code>","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.n_leapfrog_steps","title":"n_leapfrog_steps  <code>instance-attribute</code>","text":"<pre><code>n_leapfrog_steps = n_leapfrog_steps\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.mass","title":"mass  <code>instance-attribute</code>","text":"<pre><code>mass = to(device)\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.hmc_step","title":"hmc_step","text":"<pre><code>hmc_step(current_position: Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Perform a single HMC step with Metropolis-Hastings acceptance.</p> <p>This implements the core HMC algorithm: 1. Sample initial momentum 2. Compute initial Hamiltonian 3. Perform leapfrog integration to propose new state 4. Compute final Hamiltonian 5. Accept/reject based on Metropolis-Hastings criterion</p> <p>Parameters:</p> Name Type Description Default <code>current_position</code> <code>Tensor</code> <p>Current position tensor of shape (batch_size, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple containing:</p> <code>Tensor</code> <ul> <li>new_position: Updated position tensor</li> </ul> <code>Tensor</code> <ul> <li>acceptance_prob: Probability of accepting each proposal</li> </ul> <code>Tuple[Tensor, Tensor, Tensor]</code> <ul> <li>accepted: Boolean mask indicating which proposals were accepted</li> </ul> Source code in <code>torchebm/samplers/mcmc.py</code> <pre><code>def hmc_step(\n    self, current_position: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Perform a single HMC step with Metropolis-Hastings acceptance.\n\n    This implements the core HMC algorithm:\n    1. Sample initial momentum\n    2. Compute initial Hamiltonian\n    3. Perform leapfrog integration to propose new state\n    4. Compute final Hamiltonian\n    5. Accept/reject based on Metropolis-Hastings criterion\n\n    Args:\n        current_position: Current position tensor of shape (batch_size, dim).\n\n    Returns:\n        Tuple containing:\n        - new_position: Updated position tensor\n        - acceptance_prob: Probability of accepting each proposal\n        - accepted: Boolean mask indicating which proposals were accepted\n    \"\"\"\n    batch_size = current_position.shape[0]\n\n    # Sample initial momentum\n    current_momentum = self._initialize_momentum(current_position.shape)\n\n    # Compute current Hamiltonian: H = U(q) + K(p)\n    # Add numerical stability with clamping\n    current_energy = self.energy_function(current_position)\n    current_energy = torch.clamp(current_energy, min=-1e10, max=1e10)  # Prevent extreme energy values\n\n    current_kinetic = self._compute_kinetic_energy(current_momentum)\n    current_kinetic = torch.clamp(current_kinetic, min=0, max=1e10)  # Kinetic energy should be non-negative\n\n    current_hamiltonian = current_energy + current_kinetic\n\n    # Perform leapfrog integration to get proposal\n    proposed_position, proposed_momentum = self._leapfrog_integration(\n        current_position, current_momentum\n    )\n\n    # Compute proposed Hamiltonian with similar numerical stability\n    proposed_energy = self.energy_function(proposed_position)\n    proposed_energy = torch.clamp(proposed_energy, min=-1e10, max=1e10)\n\n    proposed_kinetic = self._compute_kinetic_energy(proposed_momentum)\n    proposed_kinetic = torch.clamp(proposed_kinetic, min=0, max=1e10)\n\n    proposed_hamiltonian = proposed_energy + proposed_kinetic\n\n    # Metropolis-Hastings acceptance criterion\n    # Clamp hamiltonian_diff to avoid overflow in exp()\n    hamiltonian_diff = current_hamiltonian - proposed_hamiltonian\n    hamiltonian_diff = torch.clamp(hamiltonian_diff, max=50, min=-50)\n\n    acceptance_prob = torch.min(\n        torch.ones(batch_size, device=self.device), torch.exp(hamiltonian_diff)\n    )\n\n    # Accept/reject based on acceptance probability\n    random_uniform = torch.rand(batch_size, device=self.device)\n    accepted = random_uniform &lt; acceptance_prob\n    accepted_mask = accepted.float().view(\n        -1, *([1] * (len(current_position.shape) - 1))\n    )\n\n    # Update position based on acceptance\n    new_position = (\n        accepted_mask * proposed_position + (1.0 - accepted_mask) * current_position\n    )\n\n    return new_position, acceptance_prob, accepted\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.sample_chain","title":"sample_chain","text":"<pre><code>sample_chain(x: Optional[Tensor] = None, dim: int = None, n_steps: int = 100, n_samples: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Generate samples using Hamiltonian Monte Carlo.</p> <p>Runs an HMC chain for a specified number of steps, optionally returning the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian dynamics with leapfrog integration to propose samples efficiently, particularly in high-dimensional spaces.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Initial state to start sampling from. If None, random initialization is used.</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of the state space when x is None. If None, will attempt to infer from the energy function.</p> <code>None</code> <code>n_steps</code> <code>int</code> <p>Number of HMC steps to perform.</p> <code>100</code> <code>n_samples</code> <code>int</code> <p>Number of parallel chains to run.</p> <code>1</code> <code>return_trajectory</code> <code>bool</code> <p>If True, return the entire trajectory of samples.</p> <code>False</code> <code>return_diagnostics</code> <code>bool</code> <p>If True, return diagnostics about the sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>If return_trajectory=False and return_diagnostics=False: Tensor of shape (n_samples, dim) with final samples.</p> <code>Tensor</code> <p>If return_trajectory=True and return_diagnostics=False: Tensor of shape (n_samples, n_steps, dim) with the trajectory of all samples.</p> <code>Tuple[Tensor, Tensor]</code> <p>If return_diagnostics=True: Tuple of (samples, diagnostics) where diagnostics contains information about the sampling process, including mean, variance, energy values, and acceptance rates.</p> Note <p>This method uses automatic mixed precision when available on CUDA devices to improve performance while maintaining numerical stability for the Hamiltonian dynamics simulation.</p> Example <pre><code># Run 10 parallel chains for 1000 steps\nsamples, diagnostics = hmc.sample_chain(\n    dim=10,\n    n_steps=1000,\n    n_samples=10,\n    return_diagnostics=True\n)\n\n# Plot acceptance rates\nimport matplotlib.pyplot as plt\nplt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\nplt.ylabel('Acceptance Rate')\nplt.xlabel('Step')\nplt.show()\n</code></pre> Source code in <code>torchebm/samplers/mcmc.py</code> <pre><code>@torch.no_grad()\ndef sample_chain(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = None,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Generate samples using Hamiltonian Monte Carlo.\n\n    Runs an HMC chain for a specified number of steps, optionally returning\n    the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian\n    dynamics with leapfrog integration to propose samples efficiently, particularly\n    in high-dimensional spaces.\n\n    Args:\n        x: Initial state to start sampling from. If None, random initialization is used.\n        dim: Dimension of the state space when x is None. If None, will attempt to infer from the energy function.\n        n_steps: Number of HMC steps to perform.\n        n_samples: Number of parallel chains to run.\n        return_trajectory: If True, return the entire trajectory of samples.\n        return_diagnostics: If True, return diagnostics about the sampling process.\n\n    Returns:\n        If return_trajectory=False and return_diagnostics=False:\n            Tensor of shape (n_samples, dim) with final samples.\n        If return_trajectory=True and return_diagnostics=False:\n            Tensor of shape (n_samples, n_steps, dim) with the trajectory of all samples.\n        If return_diagnostics=True:\n            Tuple of (samples, diagnostics) where diagnostics contains information about\n            the sampling process, including mean, variance, energy values, and acceptance rates.\n\n    Note:\n        This method uses automatic mixed precision when available on CUDA devices\n        to improve performance while maintaining numerical stability for the\n        Hamiltonian dynamics simulation.\n\n    Example:\n        ```python\n        # Run 10 parallel chains for 1000 steps\n        samples, diagnostics = hmc.sample_chain(\n            dim=10,\n            n_steps=1000,\n            n_samples=10,\n            return_diagnostics=True\n        )\n\n        # Plot acceptance rates\n        import matplotlib.pyplot as plt\n        plt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\n        plt.ylabel('Acceptance Rate')\n        plt.xlabel('Step')\n        plt.show()\n        ```\n    \"\"\"\n    if x is None:\n        # If dim is not provided, try to infer from the energy function\n        if dim is None:\n            # Check if it's GaussianEnergy which has mean attribute\n            if hasattr(self.energy_function, 'mean'):\n                dim = self.energy_function.mean.shape[0]\n            else:\n                raise ValueError(\"dim must be provided when x is None and cannot be inferred from the energy function\")\n        x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n    else:\n        x = x.to(self.device)\n\n    # Get dimension from x for later use\n    dim = x.shape[1]\n\n    if return_trajectory:\n        trajectory = torch.empty(\n            (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n        )\n\n    if return_diagnostics:\n        diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n        acceptance_rates = torch.zeros(\n            n_steps, device=self.device, dtype=self.dtype\n        )\n\n    with torch.amp.autocast(device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"):\n        for i in range(n_steps):\n            # Perform single HMC step\n            x, acceptance_prob, accepted = self.hmc_step(x)\n\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n            if return_diagnostics:\n                # Calculate diagnostics with numerical stability safeguards\n                mean_x = x.mean(dim=0).unsqueeze(0).expand_as(x)\n\n                # Clamp variance calculations to prevent NaN values\n                # First compute variance in a numerically stable way\n                # and then clamp to ensure positive finite values\n                x_centered = x - mean_x\n                var_x = torch.mean(x_centered**2, dim=0)\n                var_x = torch.clamp(var_x, min=1e-10, max=1e10)  # Prevent zero/extreme variances\n                var_x = var_x.unsqueeze(0).expand_as(x)\n\n                # Energy values (ensure finite values)\n                energy = self.energy_function(x)  # assumed to have shape (n_samples,)\n                energy = torch.clamp(energy, min=-1e10, max=1e10)  # Prevent extreme energy values\n                energy = energy.unsqueeze(1).expand_as(x)\n\n                # Acceptance rate is already between 0 and 1\n                acceptance_rate = accepted.float().mean()\n                acceptance_rate_expanded = torch.ones_like(x) * acceptance_rate\n\n                # Stack diagnostics\n                diagnostics[i, 0, :, :] = mean_x\n                diagnostics[i, 1, :, :] = var_x\n                diagnostics[i, 2, :, :] = energy\n                diagnostics[i, 3, :, :] = acceptance_rate_expanded\n\n    if return_trajectory:\n        if return_diagnostics:\n            return trajectory, diagnostics  # , acceptance_rates\n        return trajectory\n\n    if return_diagnostics:\n        return x, diagnostics  # , acceptance_rates\n\n    return x\n</code></pre>"},{"location":"blog/","title":"Index","text":"<p>TorchEBM Blog</p>"},{"location":"blog/langevin-dynamics-sampling/","title":"Langevin dynamics sampling","text":"<p>Langevin dynamics sampling</p> Langevin dynamics sampling<pre><code>def langevin_gaussain_sampling():\n\n    energy_fn = GaussianEnergy(mean=torch.zeros(10), cov=torch.eye(10))\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Initialize Langevin dynamics model\n    langevin_sampler = LangevinDynamics(\n        energy_function=energy_fn, step_size=5e-3, device=device\n    ).to(device)\n\n    # Initial state: batch of 100 samples, 10-dimensional space\n    ts = time.time()\n    # Run Langevin sampling for 500 steps\n    final_x = langevin_sampler.sample_chain(\n        dim=10, n_steps=500, n_samples=10000, return_trajectory=False\n    )\n\n    print(final_x.shape)  # Output: (100, 10)  (final state)\n    # print(xs.shape)  # Output: (500, 100, 10)  (history of all states)\n    print(\"Time taken: \", time.time() - ts)\n\n    n_samples = 250\n    n_steps = 500\n    dim = 10\n    final_samples, diagnostics = langevin_sampler.sample_chain(\n        n_samples=n_samples,\n        n_steps=n_steps,\n        dim=dim,\n        return_trajectory=True,\n        return_diagnostics=True,\n    )\n    print(final_samples.shape)  # Output: (100, 10)  (final state)\n    print(diagnostics.shape)  # (500, 3, 100, 10) -&gt; All diagnostics\n\n    x_init = torch.randn(n_samples, dim, dtype=torch.float32, device=\"cuda\")\n    samples = langevin_sampler.sample(x=x_init, n_steps=100)\n    print(samples.shape)  # Output: (100, 10)  (final state)\n</code></pre>","tags":["langevin","sampling"]},{"location":"blog/hamiltonian-mechanics/","title":"Hamiltonian Mechanics","text":"<p> Hamiltonian mechanics is a way to describe how physical systems, like planets or pendulums, move over  time, focusing on energy rather than just forces. By reframing complex dynamics through energy lenses,  this 19th-century physics framework now powers cutting-edge generative AI. It uses generalized coordinates \\( q \\) (like position) and their  conjugate momenta \\( p \\) (related to momentum), forming a phase space that captures the system's state. This approach  is particularly useful for complex systems with many parts, making it easier to find patterns and conservation laws.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#mathematical-reformation-from-second-order-to-phase-flow","title":"Mathematical Reformation: From Second-Order to Phase Flow","text":"<p>Newton's \\( F = m\\ddot{q} \\) requires solving second-order differential equations, which become unwieldy for constrained systems or when identifying conserved quantities. </p> <p>The Core Idea</p> <p>Hamiltonian mechanics splits \\( \\ddot{q} = F(q)/m \\) into two first-order equations by introducing conjugate momentum \\( p \\):</p> \\[\\begin{align*} \\dot{q} = \\frac{\\partial H}{\\partial p} &amp; \\text{(Position)}, \\quad \\dot{p} = -\\frac{\\partial H}{\\partial q} &amp; \\text{(Momentum)} \\end{align*}\\] <p>It decomposes acceleration into complementary momentum/position flows. This phase space perspective reveals hidden geometric structure.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#lagrangian-prelude-action-principles","title":"Lagrangian Prelude: Action Principles","text":"<p>The Lagrangian \\( \\mathcal{L}(q, \\dot{q}) = K - U \\) leads to Euler-Lagrange equations via variational calculus: $$ \\frac{d}{dt}\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}} \\right) - \\frac{\\partial \\mathcal{L}}{\\partial q} = 0 $$</p> <p>Kinetic Energy Symbol</p> <p>Note that the \\( K \\) in the \\( \\mathcal{L}(q, \\dot{q}) = K - U \\) is also represented as \\( T \\).</p> <p>But these remain second-order. The critical leap comes through Legendre Transformation \\( (\\dot{q} \\rightarrow p) \\). The Hamiltonian is derived from the Lagrangian through a Legendre transformation by defining the conjugate momentum  as \\( p_i = \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}_i} \\); then the Hamiltonian can be written as: $$ H(q,p) = \\sum_i p_i \\dot{q}_i - \\mathcal{L}(q, \\dot{q}) $$</p> Annotated \\( H(q,p) \\) \\[ H(q,p) = \\sum_i \\underbrace{p_i}_{\\text{Conjugate Momentum}} \\underbrace{\\dot{q}_i}_{\\text{Generalized Velocity}} - \\underbrace{\\mathcal{L}(q, \\dot{q})}_{\\text{Lagrangian}} \\] <p>We can write \\( H(q,p) \\) more intuitively as: $$ H(q,p) = K(p) + U(q) $$</p> <p>Proof?</p> <p>T is the Kinetic energy; for simplicity, I'll replace it with K.</p> <p>The negative sign arises because we are subtracting the Lagrangian from the sum of the products of momenta and  velocities. This ensures that the Hamiltonian represents the total energy of the system for conservative systems,  where the Lagrangian is \\( K - U \\) and the Hamiltonian becomes \\( K + U \\).</p> <p>For a simple system where \\( K = \\frac{1}{2}m\\dot{q}^2 \\) and \\( U = U(q) \\), the Hamiltonian would be:</p> <ul> <li>Kinetic Energy: \\( K = \\frac{1}{2}m\\dot{q}^2 \\)</li> <li>Potential Energy: \\( U = U(q) \\)</li> <li>Lagrangian: \\( \\mathcal{L} = \\frac{1}{2}m\\dot{q}^2 - U(q) \\)</li> <li>Conjugate Momentum: \\( p = m\\dot{q} \\)</li> <li>Hamiltonian: \\( H = p\\dot{q} - \\mathcal{L} = p\\frac{p}{m} - \\left(\\frac{1}{2}m\\left(\\frac{p}{m}\\right)^2 - U(q)\\right) = \\frac{p^2}{m} - \\frac{p^2}{2m} + U(q) = \\frac{p^2}{2m} + U(q) = K(p) + U(q) \\)</li> </ul> <p>This flips the script: instead of \\( \\dot{q} \\)-centric dynamics, we get symplectic phase flow.</p> <p>Why This Matters</p> <p>The Hamiltonian becomes the system's total energy \\( H = K + U \\) for many physical systems.  It also provides a framework where time evolution is a canonical transformation -  a symmetry preserving the fundamental Poisson bracket structure \\( \\{q_i, p_j\\} = \\delta_{ij} \\).</p> Canonical and Non-Canonical Transformations <p>A canonical transformation is a change of variables that preserves the form of Hamilton's equations. It's like changing the map projection without altering the landscape.</p> <p>Consider a simple translation: $$ Q = q + a, \\quad P = p + b $$ This transformation preserves the Hamiltonian structure and Poisson bracket: \\( \\{Q, P\\} = \\{q + a, p + b\\} = \\{q, p\\} = 1 = \\delta_{ij} \\)</p> <ul> <li>Preserves Hamiltonian structure.</li> <li>Maintains Poisson bracket invariance.</li> <li>Time evolution can be viewed as a canonical transformation.</li> </ul> <p>On the other hand, non-canonical transformation changes the form of Hamilton's equations.</p> <p>For example, consider the transformation:</p> \\[ Q = q^3, \\quad P = p^3 \\] <p>The Poisson bracket is:</p> <p>\\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = 3q^2 \\cdot 3p^2 - 0 = 9q^2p^2 \\neq 1 \\)</p> How to calculate those formula? <p>The Poisson bracket of two functions \\( f \\) and \\( g \\) is defined as: \\( \\{f, g\\} = \\sum_i \\left( \\frac{\\partial f}{\\partial q_i} \\frac{\\partial g}{\\partial p_i} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i} \\right) \\)</p> <p>Transformation 1: \\( Q = q + a, \\quad P = p + b \\)</p> <p>Partial Derivatives: </p> <ul> <li>\\( \\frac{\\partial Q}{\\partial q} = 1 \\)</li> <li>\\( \\frac{\\partial Q}{\\partial p} = 0 \\)</li> <li>\\( \\frac{\\partial P}{\\partial q} = 0 \\)</li> <li>\\( \\frac{\\partial P}{\\partial p} = 1 \\)</li> </ul> <p>These derivatives can be represented in matrix form as: \\( \\frac{\\partial (Q, P)}{\\partial (q, p)} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\)</p> <p>This is a diagonal identity matrix, indicating that the transformation preserves the original structure.</p> <p>Poisson Bracket Calculation \\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = (1)(1) - (0)(0) = 1 \\)</p> <p>Transformation 2: \\( Q = q^3, \\quad P = p^3 \\)</p> <p>Partial Derivatives - \\( \\frac{\\partial Q}{\\partial q} = 3q^2 \\) - \\( \\frac{\\partial Q}{\\partial p} = 0 \\) - \\( \\frac{\\partial P}{\\partial q} = 0 \\) - \\( \\frac{\\partial P}{\\partial p} = 3p^2 \\)</p> <p>These derivatives can be represented as: \\( \\frac{\\partial (Q, P)}{\\partial (q, p)} = \\begin{pmatrix} 3q^2 &amp; 0 \\\\ 0 &amp; 3p^2 \\end{pmatrix} \\)</p> <p>This is a diagonal matrix but not the identity matrix, indicating that the transformation does not preserve the original structure.</p> <p>Poisson Bracket Calculation \\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = (3q^2)(3p^2) - (0)(0) = 9q^2p^2 \\)</p> <ul> <li>Transformation 1 preserves the Poisson bracket structure because it results in a constant value of 1, represented by an identity matrix.</li> <li>Transformation 2 does not preserve the Poisson bracket structure because the result depends on \\( q \\) and \\( p \\), represented by a non-identity diagonal matrix.</li> </ul> <p>This transformation is not canonical because it does not preserve the Poisson bracket structure.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#newton-vs-lagrange-vs-hamilton-a-philosophical-showdown","title":"Newton vs. Lagrange vs. Hamilton: A Philosophical Showdown","text":"Aspect Newtonian Lagrangian Hamiltonian State Variables Position \\( x \\) and velocity \\( \\dot{x} \\) Generalized coordinates \\( q \\) and velocities \\( \\dot{q} \\) Generalized coordinates \\( q \\) and conjugate momenta \\( p \\) Formulation Second-order differential equations \\( (F=ma) \\) Principle of least action (\\( \\delta \\int L \\, dt = 0 \\)) First-order differential equations from Hamiltonian function (Phase flow \\( (dH) \\)) Identifying Symmetries Manual identification or through specific methods Noether's theorem Canonical transformations and Poisson brackets Machine Learning Connection Physics-informed neural networks, simulations Optimal control, reinforcement learning Hamiltonian Monte Carlo (HMC) sampling, energy-based models Energy Conservation Not inherent (must be derived) Built-in through conservation laws Central (Hamiltonian is energy) General Coordinates Possible, but often cumbersome Natural fit Natural fit Time Reversibility Yes Yes Yes, especially in symplectic formulations","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltons-equations-the-geometry-of-phase-space","title":"Hamilton's Equations: The Geometry of Phase Space","text":"<p>The phase space is a mathematical space where we can represent the set of possible states of a physical system. For a system with \\( n \\) degrees of freedom, the phase space is a \\( 2n \\)-dimensional space, often visualized as a map where each point \\( (q, p) \\) represents a unique state. The evolution of the system is described by the motion of a point in this space, governed by Hamilton's equations.</p>      Your browser does not support the video tag.        Phase space portrait of a nonlinear pendulum showing oscillatory motion (closed orbits), rotational motion (wavy trajectories), and separatrices (red curves) connecting unstable equilibrium points. Position (q) and momentum (p) dynamics illustrate energy conservation principles fundamental to Hamiltonian systems.   <p>This formulation offers several advantages. It makes it straightforward to identify conserved quantities and symmetries through canonical transformations and Poisson brackets, which provides deeper insights into the system's behavior. For instance, Liouville's theorem states that the volume in phase space occupied by an ensemble of systems remains constant over time, expressed as:</p> \\[ \\frac{\\partial \\rho}{\\partial t} + \\{\\rho, H\\} = 0 \\] <p>or equivalently:</p> \\[ \\frac{\\partial \\rho}{\\partial t} + \\sum_i \\left(\\frac{\\partial \\rho}{\\partial q_i}\\frac{\\partial H}{\\partial p_i} - \\frac{\\partial \\rho}{\\partial p_i}\\frac{\\partial H}{\\partial q_i}\\right) = 0 \\] <p>where \\( \\rho(q, p, t) \\) is the density function. This helps us to represent the phase space flows and how they  preserve area under symplectic transformations. Its relation to symplectic geometry enables mathematical properties that are directly relevant to many numerical methods. For instance, it enables Hamiltonian Monte Carlo to perform well in high-dimensions by defining MCMC strategies that increases the chances of accepting a sample (particle). </p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplecticity-the-sacred-invariant","title":"Symplecticity: The Sacred Invariant","text":"<p>Hamiltonian flows preserve the symplectic 2-form \\( \\omega = \\sum_i dq_i \\wedge dp_i \\). </p> Symplectic 2-form \\( \\omega \\) <p>The symplectic 2-form, denoted by \\( \\omega = \\sum_i dq_i \\wedge dp_i \\), is a mathematical object used in  symplectic geometry. It measures the area of parallelograms formed by vectors in the tangent space of a phase space.</p> <ul> <li>\\( dq_i \\) and \\( dp_i \\): Infinitesimal changes in position and momentum coordinates.</li> <li>\\( \\wedge \\): The wedge product, which combines differential forms in an antisymmetric way meaning that \\( dq_i \\wedge dp_i = -dp_i \\wedge dq_i \\).</li> <li>\\( \\sum_i \\): Sum over all degrees of freedom.</li> </ul> <p>Imagine a phase space where each point represents a state of a physical system. The symplectic form assigns a  value to each pair of vectors, effectively measuring the area of the parallelogram they span. This area is  preserved under Hamiltonian flows.</p> <p>Key Properties</p> <ol> <li>Closed: \\( d\\omega = 0 \\) which means its exterior derivative is zero \\( d\\omega=0 \\). This property ensures that the form does not change under continuous transformations.</li> <li>Non-degenerate: The form is non-degenerate if \\( d\\omega(X,Y)=0 \\) for all \\( Y \\)s, then \\( X=0 \\). This ensures that every vector has a unique \"partner\" vector such that their pairing under \\( \\omega \\) is non-zero.</li> </ol> <p>Example</p> <p>For a simple harmonic oscillator with one degree of freedom, \\( \\omega = dq \\wedge dp \\). This measures the area of parallelograms in the phase space spanned by vectors representing changes in position and momentum.</p> <p>A Very Simplistic PyTorch Code: While PyTorch doesn't directly handle differential forms, you can conceptually represent the symplectic form using tensors:</p> <pre><code>import torch\n\n# Conceptual representation of dq and dp as tensors\ndq = torch.tensor([1.0])  \ndp = torch.tensor([1.0])  \n\n# \"Wedge product\" conceptually represented using an outer product\nomega = torch.outer(dq, dp) - torch.outer(dp, dq)\n\nprint(omega)\n</code></pre> <p>This code illustrates the antisymmetric nature of the wedge product.</p> <p>Numerically, this means good integrators must respect:</p> \\[ \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} = J \\quad \\text{where } J = \\begin{pmatrix} 0 &amp; I \\\\ -I &amp; 0 \\end{pmatrix} \\] Breaking Down the Formula <ul> <li>Geometric numerical integration solves differential equations while preserving geometric properties of the system.</li> <li> <p>Symplecticity is a geometric property inherent to Hamiltonian systems. It ensures that the area of geometric  structures (e.g., parallelograms) in phase space \\( (q, p) \\) remains constant over time. This is encoded in the  symplectic form \\( \\omega = \\sum_i dq_i \\wedge dp_i \\).</p> </li> <li> <p>A numerical method is symplectic if it preserves \\( \\omega \\). The Jacobian matrix of the transformation  from \\( (q(t), p(t)) \\) to \\( (q(t + \\epsilon), p(t + \\epsilon)) \\) must satisfy the condition above.</p> </li> <li> <p>The Jacobian matrix \\( \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\)  quantifies how small changes in the initial state \\( (q(t), p(t)) \\) propagate to the next state \\( (q(t + \\epsilon), p(t + \\epsilon)) \\).</p> </li> <li> <p>\\( q(t) \\) and \\( p(t) \\) : Position and momentum at time \\( t \\).</p> </li> <li>\\( q(t + \\epsilon) \\) and \\( p(t + \\epsilon) \\) : Updated position and momentum after one time step \\( \\epsilon \\).</li> <li>\\( \\frac{\\partial}{\\partial (q(t), p(t))} \\) : Partial derivatives with respect to the initial state.</li> </ul>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#how-are-we-going-to-solve-it","title":"How are We Going to Solve it?","text":"<p>Numerical solvers for differential equations inevitably introduce errors that affect solution accuracy. These errors manifest as deviations from the true trajectory in phase space, particularly noticeable in energy-conserving systems like the harmonic oscillator. The errors fall into two main categories: local truncation error, arising from the approximation of continuous derivatives with discrete steps (proportional to \\( \\mathcal{O}(\\epsilon^n+1) \\) where \\( \\epsilon \\) is the step size and n depends on the method); and global accumulation error, which compounds over integration time.</p> <p>Forward Euler Method Fails at This!</p> <p>To overcome this, we turn to symplectic integrators\u2014methods that respect the underlying geometry of Hamiltonian  systems, leading us naturally to the Leapfrog Verlet method, a powerful symplectic alternative. \ud83d\ude80</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#key-issue-energy-drift-from-non-symplectic-updates","title":"Key Issue: Energy Drift from Non-Symplectic Updates","text":"<p>The forward Euler method (FEM) violates the geometric structure of Hamiltonian systems,  leading to energy drift in long-term simulations. Let\u2019s dissect why.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#forward-euler-in-hamiltonian-systems","title":"Forward Euler in Hamiltonian Systems","text":"<p>For a Hamiltonian \\( H(q, p) \\), the forward Euler updates position and momentum as: \\( q(t + \\epsilon) = q(t) + \\epsilon \\frac{\\partial H}{\\partial p}(q(t), p(t)),\\quad  p(t + \\epsilon) = p(t) - \\epsilon \\frac{\\partial H}{\\partial q}(q(t), p(t)) \\)</p> <p>Unlike Leapfrog Verlet, these updates do not split the position/momentum dependencies, breaking symplecticity.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#step-by-step-failure-harmonic-oscillator-example","title":"Step-by-Step Failure: Harmonic Oscillator Example","text":"<p>Hamiltonian: \\( H = \\frac{1}{2}(q^2 + p^2) \\quad \\text{(mass-spring system with m = k = 1 )} \\)</p> <p>Forward Euler Updates: \\( q(t + \\epsilon) = q(t) + \\epsilon p(t) \\quad \\text{(position update)} \\quad p(t + \\epsilon) = p(t) - \\epsilon q(t) \\quad \\text{(momentum update)} \\)</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#jacobian-matrix-analysis","title":"Jacobian Matrix Analysis","text":"<p>The Jacobian \\( M = \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\) becomes: \\( M = \\begin{pmatrix} 1 &amp; \\epsilon \\\\ -\\epsilon &amp; 1 \\end{pmatrix} \\)</p> <p>Symplectic Condition Check: Does \\( M^T J M = J \\), where \\( J = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\)?</p> <ol> <li> <p>Transpose \\( M^T \\): \\(    M^T = \\begin{pmatrix}    1 &amp; -\\epsilon \\\\    \\epsilon &amp; 1    \\end{pmatrix}    \\)</p> </li> <li> <p>Compute \\( J M \\): \\(       J M = \\begin{pmatrix}       -\\epsilon &amp; 1 \\\\       -1 &amp; -\\epsilon       \\end{pmatrix}       \\)</p> </li> <li> <p>Final Product \\( M^T J M \\): \\(       M^T J M = \\begin{pmatrix}       0 &amp; 1 + \\epsilon^2 \\\\       -1 - \\epsilon^2 &amp; 0       \\end{pmatrix} \\neq J       \\)</p> </li> </ol> <p>The result violates \\( M^T J M = J \\), proving symplecticity fails unless \\( \\epsilon = 0 \\).</p> \\[ \\boxed{\\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\neq J} \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#practical-consequences-energy-drift","title":"Practical Consequences: Energy Drift","text":"<p>Why This Matters</p> <ul> <li>Energy Drift: FEM artificially injects/dissipates energy over time because \\( H(q,p) \\) is not conserved.  </li> <li>Phase Space Distortion: Volume preservation fails, corrupting long-term trajectories.  </li> <li>Unusable for HMC: Sampling in Hamiltonian Monte Carlo relies on symplectic integrators to maintain detailed balance.  </li> </ul> <p>Example: Simulating a harmonic oscillator with FEM shows spiraling/non-closing orbits in phase space, unlike the stable ellipses from Leapfrog Verlet.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplectic-numerical-integrators","title":"Symplectic Numerical Integrators","text":"","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#leapfrog-verlet","title":"Leapfrog Verlet","text":"<p>For a separable Hamiltonian \\( H(q,p) = K(p) + U(q) \\), where the corresponding probability distribution is given by:</p> \\[ P(q,p) = \\frac{1}{Z} e^{-U(q)} e^{-K(p)}, \\] <p>the Leapfrog Verlet integrator proceeds as follows:</p> \\[ \\begin{aligned} p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) &amp;= p_{i}(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t)) \\\\ q_{i}(t + \\epsilon) &amp;= q_{i}(t) + \\epsilon \\frac{\\partial K}{\\partial p_{i}}\\left(p\\left(t + \\frac{\\epsilon}{2}\\right)\\right) \\\\ p_{i}(t + \\epsilon) &amp;= p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t + \\epsilon)) \\end{aligned} \\] <p>This St\u00f6rmer-Verlet scheme preserves symplecticity exactly, with local error \\( \\mathcal{O}(\\epsilon^3) \\) and global  error \\( \\mathcal{O}(\\epsilon^2) \\). You can read more about   numerical methods and analysis in Python here.</p> <p>How Exactly?</p> <ol> <li> <p>Leapfrog Verlet Update Equations For a separable Hamiltonian \\( H(q, p) = K(p) + U(q) \\), the method splits into three component-wise steps:</p> <ol> <li> <p>Half-step momentum update: \\(    p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) = p_{i}(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t))    \\)</p> </li> <li> <p>Full-step position update: \\(    q_{i}(t + \\epsilon) = q_{i}(t) + \\epsilon \\frac{\\partial K}{\\partial p_{i}}\\left(p\\left(t + \\frac{\\epsilon}{2}\\right)\\right)    \\)</p> </li> <li> <p>Full-step momentum update: \\(    p_{i}(t + \\epsilon) = p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t + \\epsilon))    \\)</p> </li> </ol> </li> <li> <p>Jacobian Matrix Calculation For the harmonic oscillator \\( H(q, p) = \\frac{1}{2}(q^2 + p^2) \\), the updates simplify to:  </p> \\[ q(t + \\epsilon) = q(t) + \\epsilon p(t) - \\frac{\\epsilon^2}{2} q(t), \\quad p(t + \\epsilon) = p(t) - \\epsilon q(t) - \\frac{\\epsilon^2}{2} p(t). \\] <p>The Jacobian matrix \\( M = \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\) becomes: \\( M = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; \\epsilon \\\\ -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix}. \\)</p> </li> <li> <p>Transpose of \\( M \\) The transpose \\( M^T \\) swaps off-diagonal terms: \\( M^T = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; -\\epsilon \\\\ \\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix}. \\)</p> </li> <li> <p>Verify \\( M^T J M = J \\) Let \\( J = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\). Compute \\( M^T J M \\):</p> <ol> <li> <p>Calculate: \\( J M = \\begin{pmatrix} -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\\\ -\\left(1 - \\frac{\\epsilon^2}{2}\\right) &amp; -\\epsilon \\end{pmatrix}. \\)</p> </li> <li> <p>Calculate: \\( M^T J M = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; -\\epsilon \\\\ \\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix} \\begin{pmatrix} -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\\\ -\\left(1 - \\frac{\\epsilon^2}{2}\\right) &amp; -\\epsilon \\end{pmatrix}. \\)</p> </li> </ol> <p>After matrix multiplication: \\( M^T J M = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} = J. \\)</p> </li> </ol> <p>The Leapfrog Verlet method satisfies \\( M^T J M = J \\), proving it preserves the symplectic structure. This matches its theoretical property as a symplectic integrator.</p> \\[ \\boxed{\\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} = J} \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#why-symplectic-matters","title":"Why Symplectic Matters","text":"<p>They're the reversible neural nets of physics simulations!</p> <p>Symplectic integrators like Leapfrog Verlet are critical for long-term stability in Hamiltonian systems.  </p> <ul> <li>Phase space preservation: The volume in \\( (q, p) \\)-space is conserved exactly, avoiding artificial energy drift.  </li> <li>Approximate energy conservation: While energy \\( H(q,p) \\) is not perfectly conserved (due to \\( \\mathcal{O}(\\epsilon^2) \\) error), it oscillates near the true value over exponentially long timescales.  </li> <li>Practical relevance: This makes symplectic integrators indispensable in molecular dynamics and Hamiltonian Monte Carlo (HMC), where accurate sampling relies on stable trajectories.  </li> </ul>  Comparison of numerical integration methods for a simple harmonic oscillator in phase space. Color gradients  indicate error magnitude with brighter colors showing larger divergence from the exact solution (white).  Euler's method (a) exhibits energy growth, Modified Euler's method (b) shows improved stability, while  Leapfrog maintains excellent energy conservation at small stepsize (c) but develops geometric distortion  at larger stepsize (d).  <p>Euler's method (first-order) systematically injects energy into the system, causing the characteristic outward spiral seen in the plots. Modified Euler's method (second-order) significantly reduces this energy drift. Most importantly, symplectic integrators like the Leapfrog method preserve the geometric structure of Hamiltonian systems even with relatively large step sizes by maintaining phase space volume conservation. This structural preservation is why Leapfrog remains the preferred method for long-time simulations in molecular dynamics and astronomy, where energy conservation is critical despite the visible polygon-like discretization artifacts at large step sizes.</p> <p>Non-symplectic methods (e.g., Euler-Maruyama) often fail catastrophically in these settings.</p> Integrator Symplecticity Order Type Local Error Global Error Suitable For Computational Cost Euler Method 1 Explicit O(\u03b5\u00b2) O(\u03b5) Quick prototypes and Short-term simulations of general ODEs Low Symplectically Euler 1 Explicit O(\u03b5\u00b2) O(\u03b5) Simple Hamiltonian systems Low Leapfrog (Verlet) 2 Explicit O(\u03b5\u00b3) O(\u03b5\u00b2) Molecular dynamics, Long-term simulations of Hamiltonian systems Moderate Runge-Kutta 4 4 Explicit O(\u03b5\u2075) O(\u03b5\u2074) Short-term accuracy, General ODEs, but not recommended for long-term Hamiltonian systems High Forest-Ruth Integrator 4 Explicit O(\u03b5\u2075) O(\u03b5\u2074) High-accuracy long-term simulations High Yoshida 6th-order 6 Explicit O(\u03b5\u2077) O(\u03b5\u2076) High-accuracy High Heun\u2019s Method (RK2) 2 Explicit O(\u03b5\u00b3) O(\u03b5\u00b2) General ODEs requiring moderate accuracy Moderate Third-order Runge-Kutta 3 Explicit O(\u03b5\u2074) O(\u03b5\u00b3) When higher accuracy than RK2 is needed without the cost of RK4 High Implicit Midpoint Rule 2 Implicit (solving equations) O(\u03b5\u00b3) O(\u03b5\u00b2) Hamiltonian systems, stiff problems High Fourth-order Adams-Bashforth 4 Multi-step (explicit) O(\u03b5\u2075) O(\u03b5\u2074) Non-stiff problems with smooth solutions, after initial steps Low Backward Euler Method 1 Implicit (solving equations) O(\u03b5\u00b2) O(\u03b5) Stiff problems, where stability is crucial High","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":"<p>Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that leverages Hamiltonian dynamics to  efficiently sample from complex probability distributions, particularly in Bayesian statistics and machine learning.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#from-phase-space-to-probability-space","title":"From Phase Space to Probability Space","text":"<p>HMC interprets target distribution \\( P(z) \\) as a Boltzmann distribution:</p> \\[ P(z) = \\frac{1}{Z} e^{\\frac{-E(z)}{T}} \\] <p>Substituting into this formulation, the Hamiltonian gives us a joint density:</p> \\[  P(q,p) = \\frac{1}{Z} e^{-U(q)} e^{-K(p)} \\quad \\text{where } U(q) = -\\log[p(q), p(q|D)] \\] <p>where \\( p(q|D) \\) is the likelihood of the given data \\( D \\) and T=1 and therefore removed. We estimate our posterior distribution using the potential energy \\( U(q) \\) since \\( P(q,p) \\) consists of two independent probability distributions.</p> <p>Augment with artificial momentum \\( p \\sim \\mathcal{N}(0,M) \\), then simulate Hamiltonian dynamics to propose new \\( q' \\)  based on the distribution of the position variables \\( U(q) \\) which acts as the \"potential energy\" of the target distribution \\( P(q) \\), thereby creating valleys at high-probability regions.</p> <p>For more on HMC, check out this explanation or  this tutorial.</p> <ul> <li>Physical Systems: \\( H(q,p) = U(q) + K(p) \\) represents total energy  </li> <li>Sampling Systems: \\( H(q,p) = -\\log P(q) + \\frac{1}{2}p^T M^{-1} p \\) defines exploration dynamics  </li> </ul> <p>The kinetic energy with the popular form of \\( K(p) = \\frac{1}{2}p^T M^{-1} p \\), often Gaussian,  injects momentum to traverse these landscapes. Crucially, the mass matrix \\( M \\) plays the role of a  preconditioner - diagonal \\( M \\) adapts to parameter scales, while dense \\( M \\) can align with correlation  structure. \\( M \\) is symmetric, positive definite and typically diagonal.</p> <p>What is Positive Definite?</p> <p>Positive Definite: For any non-zero vector \\( x \\), the expression \\( x^T M x \\) is always positive. This ensures stability and efficiency.</p> <p>      Illustration of different quadratic forms in two variables that shows how different covariance matrices      influence the shape of these forms. The plots depict:     a) Positive Definite Form: A bowl-shaped surface where all eigenvalues are positive, indicating a minimum.     b) Negative Definite Form: An inverted bowl where all eigenvalues are negative, indicating a maximum.     c) Indefinite Form: A saddle-shaped surface with both positive and negative eigenvalues, indicating neither a maximum nor a minimum.     Each subplot includes the matrix \\( M \\) and the corresponding quadratic form \\( Q(x) = x^T M x \\).  </p> \\[ x^T M x &gt; 0 \\] Kinetic Energy Choices <ul> <li>Gaussian (Standard HMC): \\( K(p) = \\frac{1}{2}p^T M^{-1} p \\)   Yields Euclidean trajectories, efficient for moderate dimensions.  </li> <li>Relativistic (Riemannian HMC): \\( K(p) = \\sqrt{p^T M^{-1} p + c^2} \\)   Limits maximum velocity, preventing divergences in ill-conditioned spaces.  </li> <li>Adaptive (Surrogate Gradients): Learn \\( K(p) \\) via neural networks to match target geometry.</li> </ul> <p>Key Intuition</p> <p>The Hamiltonian \\( H(q,p) = U(q) + \\frac{1}{2}p^T M^{-1} p \\) creates an energy landscape where momentum carries  the sampler through high-probability regions, avoiding random walk behavior.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#the-hmc-algorithm","title":"The HMC Algorithm","text":"<p>The algorithm involves:</p> <ol> <li> <p>Initialization: Start with an initial position \\( q_0 \\) and sample momentum \\( p_0 \\sim \\mathcal{N}(0,M) \\).</p> </li> <li> <p>Leapfrog Integration: Use the leapfrog method to approximate Hamiltonian dynamics. For a step size \\( \\epsilon \\) and L steps, update:</p> </li> <li> <p>Half-step momentum: \\( p(t + \\frac{\\epsilon}{2}) = p(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t)) \\)</p> </li> <li>Full-step position: \\( q(t + \\epsilon) = q(t) + \\epsilon \\frac{\\partial K}{\\partial p}(p(t + \\frac{\\epsilon}{2})) \\), where \\( K(p) = \\frac{1}{2} p^T M^{-1} p \\), so \\( \\frac{\\partial K}{\\partial p} = M^{-1} p \\)</li> <li>Full-step momentum: \\( p(t + \\epsilon) = p(t + \\frac{\\epsilon}{2}) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t + \\epsilon)) \\)</li> </ol> <p>This is repeated L times to get proposed \\( \\dot{q} \\) and \\( \\dot{p} \\).</p> <ol> <li>Metropolis-Hastings Acceptance: Accept the proposed \\( \\dot{q} \\) with probability \\( \\min(1, e^{H(q_0,p_0) - H(\\dot{q},\\dot{p})}) \\), where \\( H(q,p) = U(q) + K(p) \\).</li> </ol> <p>This process generates a Markov chain with stationary distribution \\( P(q) \\), leveraging Hamiltonian dynamics to take larger, more efficient steps compared to random-walk methods.</p> Why Better Than Random Walk? <p>HMC navigates high-dimensional spaces along energy contours -  like following mountain paths instead of wandering randomly!</p> Recap of the Hamilton's equations? \\[ \\begin{cases} \\dot{q} = \\nabla_p K(p) = M^{-1}p &amp; \\text{(Guided exploration)} \\\\ \\dot{p} = -\\nabla_q U(q) = \\nabla_q \\log P(q) &amp; \\text{(Bayesian updating)} \\end{cases} \\] <p>This coupled system drives \\( (q,p) \\) along iso-probability contours of \\( P(q) \\), with momentum rotating rather  than resetting at each step like in Random Walk Metropolis--think of following mountain paths instead of wandering randomly! The key parameters - integration time \\( \\tau = L\\epsilon \\) and step size \\( \\epsilon \\) - balance exploration vs. computational cost:  </p> <ul> <li>Short \\( \\tau \\): Local exploration, higher acceptance  </li> <li>Long \\( \\tau \\): Global moves, risk of U-turns (periodic orbits)  </li> </ul> <p>Key Parameters and Tuning</p> <p>Tuning \\( M \\) to match the covariance of \\( P(q) \\) (e.g., via warmup adaptation) and setting \\( \\tau \\sim \\mathcal{O}(1/\\lambda_{\\text{max}}) \\), where \\( \\lambda_{\\text{max}} \\) is the largest eigenvalue of \\( \\nabla^2 U \\), often yields optimal mixing.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#connection-with-energy-based-models","title":"Connection with Energy-Based Models","text":"<p>Energy-based models (EBMs) are a class of generative models that define a probability distribution over data points  using an energy function. The probability of a data point is proportional to \\( e^{-E(x)} \\), where \\( E(x) \\) is the  energy function. This formulation is directly analogous to the Boltzmann distribution in statistical physics, where the  probability is related to the energy of a state. In Hamiltonian mechanics, the Hamiltonian function \\( H(q, p) \\)  represents the total energy of the system, and the probability distribution in phase space is given  by \\( e^{-H(q,p)/T} \\), where \\( T \\) is the temperature.</p> <p>In EBMs, Hamiltonian Monte Carlo (HMC) is often used to sample from the model's distribution. HMC leverages  Hamiltonian dynamics to propose new states, which are then accepted or rejected based on the Metropolis-Hastings  criterion. This method is particularly effective for high-dimensional problems, as it reduces the correlation between  samples and allows for more efficient exploration of the state space. For instance, in image generation tasks, HMC  can sample from the distribution defined by the energy function, facilitating the generation of high-quality images.</p> <p>EBMs define probability through Hamiltonians:</p> \\[ p(x) = \\frac{1}{Z}e^{-E(x)} \\quad \\leftrightarrow \\quad H(q,p) = E(q) + K(p) \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#potential-research-directions","title":"Potential Research Directions","text":"","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplecticity-in-machine-learning-models","title":"Symplecticity in Machine Learning Models","text":"Overview of the Hamiltonian Neural Networks architecture. Image from the HNN paper. <p>Incorporate the symplectic structure of Hamiltonian mechanics into machine learning models to preserve properties  like energy conservation, which is crucial for long-term predictions. Generalizing Hamiltonian Neural Networks (HNNs),  as discussed in Hamiltonian Neural Networks, to more complex systems or developing new architectures that preserve symplecticity</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hmc-for-complex-distributions","title":"HMC for Complex Distributions:","text":"<p>HMC for sampling from complex, high-dimensional, and multimodal distributions, such as those encountered in deep learning. Combining HMC with other techniques, like parallel tempering, could handle distributions with multiple modes more  effectively.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#combining-hamiltonian-mechanics-with-other-ml-techniques","title":"Combining Hamiltonian Mechanics with Other ML Techniques:","text":"<p>Integrate Hamiltonian mechanics with reinforcement learning to guide exploration in continuous state and action spaces. Using it to model the environment could improve exploration strategies, as seen in potential applications in robotics.  Additionally, using Hamiltonian mechanics to define approximate posteriors in variational inference could lead to more  flexible and accurate approximations.   </p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltonian-gans","title":"Hamiltonian GANs","text":"<p>Employing Hamiltonian formalism as an inductive bias for the generation of physically plausible videos with neural networks. Imagine generator-discriminator dynamics governed by:</p> <p>The possibilities make phase space feel infinite...</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#want-to-team-up-on-this","title":"Want to Team Up on This \ud83e\udd13","text":"<p>If you are an ML researcher and are interested in collaborating on researching EBMs,  diffusion- and flow-based models, or have other relevant ideas in mind for generalization over out-of-distribution  data (downstream tasks can be anything in from molecular design to robotics motion planning to LLMs), please feel free to reach out!</p> <p>Also follow me on  Twitter /  BlueSky or  GitHub\u2014I\u2019m usually rambling about this stuff there.  Also on LinkedIn and  Medium /  TDS if you\u2019re curious.  To find more about my research interests, check out my  personal website.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#useful-links","title":"Useful Links","text":"<ul> <li>An Introduction to Multistep Methods: Leap-frog</li> <li>The beginners guide to Hamiltonian Monte Carlo</li> <li>Hamiltonian Monte Carlo</li> <li>Hamiltonian Monte Carlo - Stan - Stan explained</li> <li>Hamiltonian Mechanics For Dummies: An Intuitive Introduction.</li> <li>Hamiltonian mechanics Wikipedia page</li> <li>An introduction to Lagrangian and Hamiltonian mechanics Lecture notes</li> <li> <p>Hamiltonian Mechanics - Jeremy Tatum, University of Victoria</p> </li> <li> <p>Hamiltonian Neural Networks - Blog</p> </li> <li>Hamiltonian Neural Networks</li> <li>Other: Natural Intelligence - A blog by Sam Greydanus - Many interesting topics </li> </ul>","tags":["hamiltonian","sampling"]},{"location":"developer_guide/api_generation/","title":"Developer Guide for <code>torchebm</code>","text":""},{"location":"developer_guide/api_generation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Setting Up the Development Environment</li> <li>Prerequisites</li> <li>Cloning the Repository</li> <li>Installing Dependencies</li> <li>Code Style and Quality</li> <li>Code Formatting</li> <li>Linting</li> <li>Type Checking</li> <li>Testing</li> <li>Documentation</li> <li>Docstring Conventions</li> <li>API Documentation Generation</li> <li>Contributing</li> <li>Branching Strategy</li> <li>Commit Messages</li> <li>Pull Requests</li> <li>Additional Resources</li> </ol>"},{"location":"developer_guide/api_generation/#introduction","title":"Introduction","text":"<p>Welcome to the developer guide for <code>torchebm</code>, a Python library focused on components and algorithms for energy-based models. This document provides instructions and best practices for contributing to the project.</p>"},{"location":"developer_guide/api_generation/#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":""},{"location":"developer_guide/api_generation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed:</p> <ul> <li>Python: Version 3.9 or higher.</li> <li>Git: For version control.</li> <li>pip: Python package installer.</li> </ul>"},{"location":"developer_guide/api_generation/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Clone the repository using SSH:</p> <pre><code>git clone git@github.com:soran-ghaderi/torchebm.git\n</code></pre> <p>Or using HTTPS:</p> <pre><code>git clone https://github.com/soran-ghaderi/torchebm.git\n</code></pre> <p>Navigate to the project directory:</p> <pre><code>cd torchebm\n</code></pre>"},{"location":"developer_guide/api_generation/#installing-dependencies","title":"Installing Dependencies","text":"<p>Install the development dependencies:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>This command installs the package in editable mode along with all development dependencies specified in the <code>pyproject.toml</code> file.</p>"},{"location":"developer_guide/api_generation/#code-style-and-quality","title":"Code Style and Quality","text":"<p>Maintaining a consistent code style ensures readability and ease of collaboration.</p> <p>To streamline code formatting and quality checks in your development workflow, integrating tools like Black, isort, and mypy directly into your Integrated Development Environment (IDE) can be highly beneficial. Many modern IDEs, such as PyCharm, offer built-in support or plugins for these tools, enabling automatic code formatting and linting as you write.</p> <p>PyCharm Integration:</p> <ul> <li> <p>Black Integration: Starting from PyCharm 2023.2, Black integration is built-in. To enable it:</p> </li> <li> <p>Navigate to <code>Preferences</code> or <code>Settings</code> &gt; <code>Tools</code> &gt; <code>Black</code>.</p> </li> <li>Configure the settings as desired.</li> <li> <p>Ensure Black is installed in your environment:</p> <pre><code>pip install black\n</code></pre> </li> <li> <p>isort Integration: While PyCharm doesn't have built-in isort support, you can set it up using File Watchers:</p> </li> <li> <p>Install the File Watchers plugin if it's not already installed.</p> </li> <li>Navigate to <code>Preferences</code> or <code>Settings</code> &gt; <code>Tools</code> &gt; <code>File Watchers</code>.</li> <li> <p>Add a new watcher for isort with the following configuration:</p> <ul> <li>File Type: Python</li> <li>Scope: Current File</li> <li>Program: Path to isort executable (e.g., <code>$PyInterpreterDirectory$/isort</code>)</li> <li>Arguments: <code>$FilePath$</code></li> <li>Output Paths to Refresh: <code>$FilePath$</code></li> <li>Working Directory: <code>$ProjectFileDir$</code></li> </ul> </li> <li> <p>mypy Integration: To integrate mypy:</p> </li> <li> <p>Install mypy in your environment:</p> <pre><code>pip install mypy\n</code></pre> </li> <li> <p>Set up a File Watcher in PyCharm similar to isort, replacing the program path with the mypy executable path.</p> </li> </ul> <p>VSCode Integration:</p> <p>For Visual Studio Code users, extensions are available for seamless integration:</p> <ul> <li>Black Formatter: Install the \"Python\" extension by Microsoft, which supports Black formatting.</li> <li>isort: Use the \"Python\" extension's settings to enable isort integration.</li> <li>mypy: Install the \"mypy\" extension for real-time type checking.</li> </ul> <p>By configuring your IDE with these tools, you ensure consistent code quality and adhere to the project's coding standards effortlessly. </p> <p>If you prefer to do these steps manually, please read the following steps, if not, you can ignore this section.</p>"},{"location":"developer_guide/api_generation/#code-formatting","title":"Code Formatting","text":"<p>We use Black for code formatting. To format your code, run:</p> <pre><code>black .\n</code></pre>"},{"location":"developer_guide/api_generation/#linting","title":"Linting","text":"<p>isort is used for sorting imports. To sort imports, execute:</p> <pre><code>isort .\n</code></pre>"},{"location":"developer_guide/api_generation/#type-checking","title":"Type Checking","text":"<p>mypy is employed for static type checking. To check types, run:</p> <pre><code>mypy torchebm/\n</code></pre>"},{"location":"developer_guide/api_generation/#testing","title":"Testing","text":"<p>We utilize pytest for testing. To run tests, execute:</p> <pre><code>pytest\n</code></pre> <p>For test coverage, use:</p> <pre><code>pytest --cov=torchebm\n</code></pre>"},{"location":"developer_guide/api_generation/#documentation","title":"Documentation","text":""},{"location":"developer_guide/api_generation/#docstring-conventions","title":"Docstring Conventions","text":"<p>All docstrings should adhere to the Google style guide. For example:</p> <pre><code>def function_name(param1, param2):\n    \"\"\"Short description of the function.\n\n    Longer description if needed.\n\n    Args:\n        param1 (type): Description of param1.\n        param2 (type): Description of param2.\n\n    Returns:\n        type: Description of return value.\n\n    Raises:\n        ExceptionType: Explanation of when this exception is raised.\n\n    Examples:\n        result = function_name(1, \"test\")\n    \"\"\"\n    # Function implementation\n</code></pre>"},{"location":"developer_guide/api_generation/#api-documentation-generation","title":"API Documentation Generation","text":"<p>The API documentation is automatically generated from docstrings using <code>generate_api_docs.py</code>, MkDocs, and the MkDocstrings plugin.</p> <p>To update the API documentation:</p> <ol> <li> <p>Run the API documentation generator script:</p> <pre><code>python gen_ref_pages.py\n</code></pre> </li> <li> <p>Build the documentation to preview changes:</p> <pre><code>mkdocs serve\n</code></pre> </li> </ol>"},{"location":"developer_guide/api_generation/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please follow the guidelines below.</p>"},{"location":"developer_guide/api_generation/#branching-strategy","title":"Branching Strategy","text":"<ul> <li>main: Contains stable code.</li> <li>feature/branch-name: For developing new features.</li> <li>bugfix/branch-name: For fixing bugs.</li> </ul>"},{"location":"developer_guide/api_generation/#commit-messages","title":"Commit Messages","text":"<p>Use clear and concise commit messages. Follow the format:</p> <pre><code>Subject line (imperative, 50 characters or less)\n\nOptional detailed description, wrapped at 72 characters.\n</code></pre>"},{"location":"developer_guide/api_generation/#pull-requests","title":"Pull Requests","text":"<p>Before submitting a pull request:</p> <ol> <li>Ensure all tests pass.</li> <li>Update documentation if applicable.</li> <li>Adhere to code style guidelines.</li> </ol>"},{"location":"developer_guide/api_generation/#additional-resources","title":"Additional Resources","text":"<ul> <li>Python's PEP 8 Style Guide</li> <li>Google Python Style Guide</li> <li>Black Code Formatter</li> <li>pytest Documentation</li> </ul> <p>By following this guide, you will help maintain the quality and consistency of the <code>torchebm</code> library. Happy coding! </p>"},{"location":"blog/archive/2025/","title":"2025","text":""}]}