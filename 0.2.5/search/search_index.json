{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TorchEBM Docs","text":"<p> \u26a1 Energy-Based Modeling library for PyTorch, offering tools for \ud83d\udd2c sampling, \ud83e\udde0 inference, and \ud83d\udcca learning in complex distributions. </p> <ul> <li> <p> Getting Started</p> <p>Start using TorchEBM in minutes with our quick installation and setup guide.</p> <p> Getting Started</p> </li> <li> <p> Introduction</p> <p>Learn about Energy-Based Models and how TorchEBM can help you work with them.</p> <p> Introduction</p> </li> <li> <p> Guides</p> <p>Explore in-depth guides for energy functions, samplers, and more.</p> <p> Guides</p> </li> <li> <p> Examples</p> <p>Practical examples to help you apply TorchEBM to your projects.</p> <p> Examples</p> </li> <li> <p> Blog</p> <p>Stay updated with the latest news, tutorials, and insights about TorchEBM.</p> <p> Blog</p> </li> </ul>"},{"location":"#quick-installation","title":"Quick Installation","text":"<pre><code>pip install torchebm\n</code></pre>"},{"location":"#example-analytical-energy-landscapes","title":"Example Analytical Energy Landscapes","text":"<p>Toy Examples</p> <p>These are some TorchEBM's built-in toy analytical energy landscapes for functionality and performance testing purposes.</p> Gaussian EnergyDouble Well EnergyRastrigin EnergyRosenbrock Energy <p> <p> Gaussian Energy <p>\\(E(x) = \\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\) </p> <pre><code>from torchebm.core import GaussianEnergy\nimport torch\n\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n</code></pre> <p> <p> Double Well Energy <p>\\(E(x) = h \\sum_{i=1}^n \\left[(x_i^2 - 1)^2\\right]\\) </p> <pre><code>from torchebm.core import DoubleWellEnergy\n\nenergy_fn = DoubleWellEnergy(\n    barrier_height=2.0\n)\n</code></pre> <p> <p> Rastrigin Energy <p>\\(E(x) = an + \\sum_{i=1}^n \\left[ x_i^2 - a\\cos(2\\pi x_i) \\right]\\) </p> <pre><code>from torchebm.core import RastriginEnergy\n\nenergy_fn = RastriginEnergy(\n    a=10.0\n)\n</code></pre> <p> <p> Rosenbrock Energy <p>\\(E(x) = \\sum_{i=1}^{n-1} \\left[ a(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\) </p> <pre><code>from torchebm.core import RosenbrockEnergy\n\nenergy_fn = RosenbrockEnergy(\n    a=1.0, \n    b=100.0\n)\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<ul> <li> <p>Create and Sample from Energy Models</p> <pre><code>import torch\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create an energy function\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n\n# Create a sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Generate samples\nsamples = sampler.sample_chain(\n    dim=2, n_steps=100, n_samples=1000\n)\n</code></pre> </li> </ul> <p>Latest Release</p> <p>TorchEBM is currently in early development. Check our GitHub repository for the latest updates and features.</p>"},{"location":"#features-roadmap","title":"Features &amp; Roadmap","text":"<p>Our goal is to create a comprehensive library for energy-based modeling in PyTorch.</p> <p>Status indicators:</p> <ul> <li> - Completed</li> <li> - Work in progress</li> <li> - Needs improvement</li> <li> - Planned feature</li> </ul>"},{"location":"#core-infrastructure","title":"Core Infrastructure","text":"<ul> <li> CUDA-accelerated implementations </li> <li> Seamless integration with PyTorch </li> <li> Energy function base class </li> <li> Sampler base class </li> </ul> Energy FunctionsImplemented SamplersDiffusion-based SamplersMCMC SamplersLoss FunctionsOther Modules <ul> <li> Gaussian </li> <li> Double well </li> <li> Rastrigin </li> <li> Rosenbrock </li> <li> Ackley </li> </ul> <ul> <li> Langevin Dynamics </li> <li> Hamiltonian Monte Carlo (HMC) </li> <li> Metropolis-Hastings </li> </ul> <ul> <li> Denoising Diffusion Probabilistic Models (DDPM) </li> <li> Denoising Diffusion Implicit Models (DDIM) </li> <li> Generalized Gaussian Diffusion Models (GGDM) </li> <li> Differentiable Diffusion Sampler Search (DDSS) </li> <li> Euler Method </li> <li> Heun's Method </li> <li> PLMS (Pseudo Likelihood Multistep) </li> <li> DPM (Diffusion Probabilistic Models) </li> </ul> <ul> <li> Gibbs Sampling </li> <li> No-U-Turn Sampler (NUTS) </li> <li> Slice Sampling </li> <li> Reversible Jump MCMC </li> <li> Particle Filters (Sequential Monte Carlo) </li> <li> Adaptive Metropolis </li> <li> Parallel Tempering (Replica Exchange) </li> <li> Stochastic Gradient Langevin Dynamics (SGLD) </li> <li> Stein Variational Gradient Descent (SVGD) </li> <li> Metropolis-Adjusted Langevin Algorithm (MALA) </li> <li> Unadjusted Langevin Algorithm (ULA) </li> <li> Bouncy Particle Sampler </li> <li> Zigzag Sampler </li> <li> Annealed Importance Sampling (AIS) </li> <li> Sequential Monte Carlo (SMC) Samplers </li> <li> Elliptical Slice Sampling </li> </ul> <ul> <li> Contrastive Divergence Methods </li> <li> Contrastive Divergence (CD-k) </li> <li> Persistent Contrastive Divergence (PCD) </li> <li> Fast Persistent Contrastive Divergence (FPCD) </li> <li> Parallel Tempering Contrastive Divergence (PTCD) </li> <li> Score Matching Techniques </li> <li> Standard Score Matching </li> <li> Denoising Score Matching </li> <li> Sliced Score Matching </li> <li> Maximum Likelihood Estimation (MLE) </li> <li> Margin Loss </li> <li> Noise Contrastive Estimation (NCE) </li> <li> Ratio Matching </li> <li> Minimum Probability Flow </li> <li> Adversarial Training Loss </li> <li> Kullback-Leibler (KL) Divergence Loss </li> <li> Fisher Divergence </li> <li> Hinge Embedding Loss </li> <li> Cross-Entropy Loss (for discrete outputs) </li> <li> Mean Squared Error (MSE) Loss (for continuous outputs) </li> <li> Improved Contrastive Divergence Loss </li> </ul> <ul> <li> Testing Framework </li> <li> Visualization Tools </li> <li> Performance Benchmarking </li> <li> Neural Network Integration </li> <li> Hyperparameter Optimization </li> <li> Distribution Diagnostics </li> </ul>"},{"location":"#license","title":"License","text":"<p>TorchEBM is released under the MIT License, which is a permissive license that allows for reuse with few restrictions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! If you're interested in improving TorchEBM or adding new features, please check our contributing guidelines.</p> <p>Our project follows specific commit message conventions to maintain a clear project history and generate meaningful changelogs.</p> <p> GitHub</p> <p> API Reference</p> <p> FAQ</p> <p> Development</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-torchebm","title":"What is TorchEBM?","text":"<p>TorchEBM is a PyTorch-based library for Energy-Based Models (EBMs). It provides efficient implementations of sampling, inference, and learning algorithms for EBMs, with a focus on scalability and performance through CUDA acceleration.</p>"},{"location":"faq/#how-does-torchebm-differ-from-other-generative-modeling-libraries","title":"How does TorchEBM differ from other generative modeling libraries?","text":"<p>TorchEBM specifically focuses on energy-based models, which can model complex distributions without assuming a specific functional form. Unlike libraries for GANs or VAEs, TorchEBM emphasizes the energy function formulation and MCMC sampling techniques.</p>"},{"location":"faq/#what-can-i-use-torchebm-for","title":"What can I use TorchEBM for?","text":"<p>TorchEBM can be used for:</p> <ul> <li>Generative modeling</li> <li>Density estimation</li> <li>Unsupervised representation learning</li> <li>Outlier detection</li> <li>Exploration of complex energy landscapes</li> <li>Scientific simulation and statistical physics applications</li> </ul>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#what-are-the-system-requirements-for-torchebm","title":"What are the system requirements for TorchEBM?","text":"<p>TorchEBM requires:</p> <ul> <li>Python 3.8 or newer</li> <li>PyTorch 1.10.0 or newer</li> <li>CUDA (optional, but recommended for performance)</li> </ul>"},{"location":"faq/#does-torchebm-work-on-cpu-only-machines","title":"Does TorchEBM work on CPU-only machines?","text":"<p>Yes, TorchEBM works on CPU-only machines, though many operations will be significantly slower than on GPU.</p>"},{"location":"faq/#how-do-i-install-torchebm-with-cuda-support","title":"How do I install TorchEBM with CUDA support?","text":"<p>Make sure you have PyTorch with CUDA support installed first:</p> <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\npip install torchebm\n</code></pre>"},{"location":"faq/#technical-questions","title":"Technical Questions","text":""},{"location":"faq/#how-do-i-choose-the-right-energy-function-for-my-task","title":"How do I choose the right energy function for my task?","text":"<p>The choice of energy function depends on:</p> <ul> <li>The complexity of the distribution you want to model</li> <li>Domain knowledge about the structure of your data</li> <li>Computational constraints</li> </ul> <p>For complex data like images, neural network-based energy functions are typically used. For simpler problems, analytical energy functions may be sufficient.</p>"},{"location":"faq/#what-sampling-algorithm-should-i-use","title":"What sampling algorithm should I use?","text":"<p>Common considerations:</p> <ul> <li>Langevin Dynamics: Good for general-purpose sampling, especially in high dimensions</li> <li>Hamiltonian Monte Carlo: Better for complex energy landscapes, but more computationally expensive</li> <li>Metropolis-Hastings: Simple to implement, but may mix slowly in high dimensions</li> </ul>"},{"location":"faq/#how-do-i-diagnose-problems-with-sampling","title":"How do I diagnose problems with sampling?","text":"<p>Common issues and solutions:</p> <ul> <li>Poor mixing: Increase step size or try a different sampler</li> <li>Numerical instability: Decrease step size or check energy function implementation</li> <li>Slow convergence: Use more iterations or try a more efficient sampler</li> <li>Mode collapse: Check your energy function or use samplers with better exploration capabilities</li> </ul>"},{"location":"faq/#how-do-i-train-an-energy-based-model-on-my-own-data","title":"How do I train an energy-based model on my own data?","text":"<p>Basic steps:</p> <ol> <li>Define an energy function (e.g., a neural network)</li> <li>Choose a loss function (e.g., contrastive divergence)</li> <li>Set up a sampler for generating negative samples</li> <li>Train using gradient descent</li> <li>Evaluate the learned model</li> </ol> <p>See the training examples for more details.</p>"},{"location":"faq/#performance","title":"Performance","text":""},{"location":"faq/#how-can-i-speed-up-sampling","title":"How can I speed up sampling?","text":"<p>To improve sampling performance:</p> <ul> <li>Use GPU acceleration</li> <li>Reduce the dimensionality of your problem</li> <li>Parallelize sampling across multiple chains</li> <li>Optimize step sizes and other hyperparameters</li> <li>Use more efficient sampling algorithms for your specific energy landscape</li> </ul>"},{"location":"faq/#does-torchebm-support-distributed-training","title":"Does TorchEBM support distributed training?","text":"<p>Currently, TorchEBM focuses on single-machine GPU acceleration. Distributed training across multiple GPUs or machines is on our roadmap.</p>"},{"location":"faq/#how-does-torchebms-performance-compare-to-other-libraries","title":"How does TorchEBM's performance compare to other libraries?","text":"<p>TorchEBM is optimized for performance on GPU hardware, particularly for sampling operations. Our benchmarks show significant speedups compared to non-specialized implementations, especially for large-scale sampling tasks.</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute-to-torchebm","title":"How can I contribute to TorchEBM?","text":"<p>We welcome contributions! Check out:</p> <ul> <li>GitHub Issues for current tasks</li> <li>Contributing Guidelines for code style and contribution workflow</li> </ul>"},{"location":"faq/#i-found-a-bug-how-do-i-report-it","title":"I found a bug, how do I report it?","text":"<p>Please open an issue on our GitHub repository with:</p> <ul> <li>A clear description of the problem</li> <li>Steps to reproduce the issue</li> <li>Expected vs. actual behavior</li> <li>Version information (TorchEBM, PyTorch, Python, CUDA)</li> </ul>"},{"location":"faq/#can-i-add-my-own-sampler-or-energy-function-to-torchebm","title":"Can I add my own sampler or energy function to TorchEBM?","text":"<p>Absolutely! TorchEBM is designed to be extensible. See:</p> <ul> <li>Custom Energy Functions</li> <li>Implementing Custom Samplers</li> </ul>"},{"location":"faq/#future-development","title":"Future Development","text":""},{"location":"faq/#what-features-are-planned-for-future-releases","title":"What features are planned for future releases?","text":"<p>See our Roadmap for planned features, including:</p> <ul> <li>Additional samplers and energy functions</li> <li>More loss functions for training</li> <li>Improved visualization tools</li> <li>Advanced neural network architectures</li> <li>Better integration with the PyTorch ecosystem</li> </ul>"},{"location":"faq/#how-stable-is-the-torchebm-api","title":"How stable is the TorchEBM API?","text":"<p>TorchEBM is currently in early development, so the API may change between versions. We'll do our best to document breaking changes and provide migration guidance. </p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide will help you get started with TorchEBM by walking you through the installation process and demonstrating some basic usage examples.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>TorchEBM can be installed directly from PyPI:</p> <pre><code>pip install torchebm\n</code></pre>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or newer</li> <li>PyTorch 1.10.0 or newer</li> <li>CUDA (optional, but recommended for performance)</li> </ul>"},{"location":"getting_started/#installation-from-source","title":"Installation from Source","text":"<p>If you wish to install the development version:</p> <pre><code>git clone https://github.com/soran-ghaderi/torchebm.git\ncd torchebm\npip install -e .\n</code></pre>"},{"location":"getting_started/#quick-start","title":"Quick Start","text":"<p>Here's a simple example to get you started with TorchEBM:</p> <pre><code>import torch\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Set device for computation\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Define a 2D Gaussian energy function for visualization\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(2, device=device),\n    cov=torch.eye(2, device=device)\n)\n\n# Initialize Langevin dynamics sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    device=device\n).to(device)\n\n# Generate 1000 samples\nsamples = sampler.sample_chain(\n    dim=2,\n    n_steps=100,\n    n_samples=1000,\n    return_trajectory=False\n)\n\nprint(f\"Generated {samples.shape[0]} samples of dimension {samples.shape[1]}\")\n</code></pre>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Energy Functions available in TorchEBM</li> <li>Explore different Sampling Algorithms</li> <li>Try out the Examples for visualizations and advanced usage</li> <li>Check the API Reference for detailed documentation</li> </ul>"},{"location":"getting_started/#common-issues","title":"Common Issues","text":""},{"location":"getting_started/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>If you encounter CUDA out of memory errors, try: - Reducing the number of samples - Reducing the dimension of the problem - Switching to CPU if needed</p>"},{"location":"getting_started/#support","title":"Support","text":"<p>If you encounter any issues or have questions: - Check the FAQ - Open an issue on GitHub</p>"},{"location":"introduction/","title":"Introduction to TorchEBM","text":"<p>TorchEBM is a CUDA-accelerated library for Energy-Based Models (EBMs) built on PyTorch. It provides efficient implementations of sampling, inference, and learning algorithms for EBMs, with a focus on scalability and performance.</p>"},{"location":"introduction/#what-are-energy-based-models","title":"What are Energy-Based Models?","text":"<p>Energy-Based Models (EBMs) are a class of machine learning models that define a probability distribution through an energy function. The energy function assigns a scalar energy value to each configuration of the variables of interest, with lower energy values indicating more probable configurations.</p> <p>The probability density of a configuration x is proportional to the negative exponential of its energy:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>where:</p> <ul> <li>\\(E(x)\\) is the energy function</li> <li>\\(Z = \\int e^{-E(x)} dx\\) is the normalizing constant (partition function)</li> </ul> <p>EBMs are powerful because they can model complex dependencies between variables and capture multimodal distributions. They are applicable to a wide range of tasks including generative modeling, density estimation, and representation learning.</p>"},{"location":"introduction/#why-torchebm","title":"Why TorchEBM?","text":"<p>While Energy-Based Models are powerful, they present several challenges:</p> <ul> <li>The partition function Z is often intractable to compute directly</li> <li>Sampling from EBMs requires advanced Markov Chain Monte Carlo (MCMC) methods</li> <li>Training can be computationally intensive</li> </ul> <p>TorchEBM addresses these challenges by providing:</p> <ul> <li>Efficient samplers: CUDA-accelerated implementations of MCMC samplers like Langevin Dynamics and Hamiltonian Monte Carlo</li> <li>Training methods: Implementations of contrastive divergence and other specialized loss functions</li> <li>Integration with PyTorch: Seamless compatibility with the PyTorch ecosystem</li> </ul>"},{"location":"introduction/#key-concepts","title":"Key Concepts","text":""},{"location":"introduction/#energy-functions","title":"Energy Functions","text":"<p>Energy functions are the core component of EBMs. TorchEBM provides implementations of common energy functions like Gaussian, Double Well, and Rosenbrock, as well as a base class for creating custom energy functions.</p>"},{"location":"introduction/#samplers","title":"Samplers","text":"<p>Sampling from EBMs typically involves MCMC methods. TorchEBM implements several sampling algorithms:</p> <ul> <li>Langevin Dynamics: Updates samples using gradient information plus noise</li> <li>Hamiltonian Monte Carlo: Uses Hamiltonian dynamics for efficient exploration</li> <li>Other samplers: Various specialized samplers for different applications</li> </ul>"},{"location":"introduction/#loss-functions","title":"Loss Functions","text":"<p>Training EBMs requires specialized methods to estimate and minimize the difference between the model distribution and the data distribution. TorchEBM implements several loss functions including contrastive divergence and score matching techniques.</p>"},{"location":"introduction/#applications","title":"Applications","text":"<p>Energy-Based Models and TorchEBM can be applied to various tasks:</p> <ul> <li>Generative modeling</li> <li>Density estimation</li> <li>Unsupervised representation learning</li> <li>Out-of-distribution detection</li> <li>Structured prediction</li> </ul>"},{"location":"introduction/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Getting Started guide to install TorchEBM and run your first examples</li> <li>Check the Guides for more detailed information on specific components</li> <li>Explore the Examples for practical applications </li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:hamiltonian","title":"hamiltonian","text":"<ul> <li>            Hamiltonian Mechanics          </li> </ul>"},{"location":"tags/#tag:langevin","title":"langevin","text":"<ul> <li>            Langevin Dynamics Sampling with TorchEBM          </li> </ul>"},{"location":"tags/#tag:sampling","title":"sampling","text":"<ul> <li>            Hamiltonian Mechanics          </li> <li>            Langevin Dynamics Sampling with TorchEBM          </li> </ul>"},{"location":"tags/#tag:tutorial","title":"tutorial","text":"<ul> <li>            Langevin Dynamics Sampling with TorchEBM          </li> </ul>"},{"location":"api/","title":"TorchEBM API Reference","text":"<p>Welcome to the TorchEBM API reference documentation. This section provides detailed information about the classes and functions available in TorchEBM.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<p>TorchEBM is organized into several modules:</p> <ul> <li> <p> Core</p> <p>Base classes and core functionality for energy functions, samplers, and trainers.</p> <p> Core Module</p> </li> <li> <p> Samplers</p> <p>Sampling algorithms for energy-based models including Langevin Dynamics and MCMC.</p> <p> Samplers</p> </li> <li> <p> Losses</p> <p>Loss functions for training energy-based models.</p> <p> Losses</p> </li> <li> <p> Utils</p> <p>Utility functions for working with energy-based models.</p> <p> Utils</p> </li> <li> <p>:material-gpu:{ .lg .middle } CUDA</p> <p>CUDA-accelerated implementations for faster computation.</p> <p> CUDA</p> </li> </ul>"},{"location":"api/#getting-started-with-the-api","title":"Getting Started with the API","text":"<p>If you're new to TorchEBM, we recommend starting with the following classes:</p> <ul> <li><code>EnergyFunction</code>: Base class for all energy functions</li> <li><code>BaseSampler</code>: Base class for all sampling algorithms</li> <li><code>LangevinDynamics</code>: Implementation of Langevin dynamics sampling</li> </ul>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#energy-functions","title":"Energy Functions","text":"<p>TorchEBM provides various built-in energy functions:</p> Energy Function Description <code>GaussianEnergy</code> Multivariate Gaussian energy function <code>DoubleWellEnergy</code> Double well potential energy function <code>RastriginEnergy</code> Rastrigin function for testing optimization algorithms <code>RosenbrockEnergy</code> Rosenbrock function (banana function) <code>AckleyEnergy</code> Ackley function, a multimodal test function <code>HarmonicEnergy</code> Harmonic oscillator energy function"},{"location":"api/#samplers","title":"Samplers","text":"<p>Available sampling algorithms:</p> Sampler Description <code>LangevinDynamics</code> Langevin dynamics sampling algorithm <code>HamiltonianMonteCarlo</code> Hamiltonian Monte Carlo sampling"},{"location":"api/#loss-functions","title":"Loss Functions","text":"<p>TorchEBM implements several loss functions for training EBMs:</p> Loss Function Description <code>ContrastiveDivergence</code> Standard contrastive divergence (CD-k) <code>PersistentContrastiveDivergence</code> Persistent contrastive divergence <code>ParallelTemperingCD</code> Parallel tempering contrastive divergence"},{"location":"api/#module-details","title":"Module Details","text":"<p>For detailed information about each module, follow the links below:</p> <ul> <li>Core Module</li> <li>Samplers</li> <li>Losses</li> <li>Models</li> <li>Utils</li> <li>CUDA</li> </ul>"},{"location":"api/torchebm/","title":"Torchebm","text":""},{"location":"api/torchebm/#contents","title":"Contents","text":""},{"location":"api/torchebm/#subpackages","title":"Subpackages","text":"<ul> <li>Core</li> <li>Cuda</li> <li>Losses</li> <li>Models</li> <li>Samplers</li> <li>Utils</li> </ul>"},{"location":"api/torchebm/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/#torchebm","title":"torchebm","text":""},{"location":"api/torchebm/core/","title":"Torchebm &gt; Core","text":""},{"location":"api/torchebm/core/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/#modules","title":"Modules","text":"<ul> <li>Basesampler</li> <li>Energy_function</li> <li>Losses</li> <li>Optimizer</li> <li>Score_matching</li> <li>Trainer</li> </ul>"},{"location":"api/torchebm/core/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/#torchebm.core","title":"torchebm.core","text":""},{"location":"api/torchebm/core/basesampler/","title":"Torchebm &gt; Core &gt; Basesampler","text":""},{"location":"api/torchebm/core/basesampler/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/basesampler/#classes","title":"Classes","text":"<ul> <li><code>BaseSampler</code> - Base class for samplers.</li> </ul>"},{"location":"api/torchebm/core/basesampler/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/basesampler/#torchebm.core.basesampler","title":"torchebm.core.basesampler","text":""},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/","title":"BaseSampler","text":""},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for samplers.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>dtype</code> <code>dtype</code> <p>Data type to use for the computations.</p> <code>float32</code> <code>device</code> <code>str</code> <p>Device to run the computations on (e.g., \"cpu\" or \"cuda\").</p> <code>None</code> <p>Methods:</p> Name Description <code>sample</code> <p>Run the sampling process.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics dictionary.</p> <code>to</code> <p>Move sampler to specified device.</p> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>class BaseSampler(ABC):\n    \"\"\"\n    Base class for samplers.\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        dtype (torch.dtype): Data type to use for the computations.\n        device (str): Device to run the computations on (e.g., \"cpu\" or \"cuda\").\n\n    Methods:\n        sample(x, dim, n_steps, n_samples, thin, return_trajectory, return_diagnostics): Run the sampling process.\n        sample_chain(dim, n_steps, n_samples, thin, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(): Initialize the diagnostics dictionary.\n        to(device): Move sampler to specified device.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n    ):\n        self.energy_function = energy_function\n        self.dtype = dtype\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def sample(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,  # not supported yet\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        \"\"\"\n        Run the sampling process.\n        Args:\n            x: Initial state to start the sampling from.\n            dim: Dimension of the state space.\n            n_steps: Number of steps to take between samples.\n            n_samples: Number of samples to generate.\n            thin: Thinning factor (not supported yet).\n            return_trajectory: Whether to return the trajectory of the samples.\n            return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n        Returns:\n            torch.Tensor: Samples from the sampler.\n            List[dict]: Diagnostics of the sampling process.\n        \"\"\"\n        return self.sample_chain(\n            x=x,\n            dim=dim,\n            n_steps=n_steps,\n            n_samples=n_samples,\n            thin=thin,\n            return_trajectory=return_trajectory,\n            return_diagnostics=return_diagnostics,\n            *args,\n            **kwargs,\n        )\n\n    @abstractmethod\n    def sample_chain(\n        self,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        raise NotImplementedError\n\n    # @abstractmethod\n    def _setup_diagnostics(self) -&gt; dict:\n        \"\"\"\n        Initialize the diagnostics dictionary.\n\n            .. deprecated:: 1.0\n               This method is deprecated and will be removed in a future version.\n        \"\"\"\n        return {\n            \"energies\": torch.empty(0, device=self.device, dtype=self.dtype),\n            \"acceptance_rate\": torch.tensor(0.0, device=self.device, dtype=self.dtype),\n        }\n        # raise NotImplementedError\n\n    def to(self, device: Union[str, torch.device]) -&gt; \"BaseSampler\":\n        \"\"\"Move sampler to specified device.\"\"\"\n        self.device = device\n        return self\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device or 'cuda' if is_available() else 'cpu'\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.sample","title":"sample","text":"<pre><code>sample(x: Optional[Tensor] = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> <p>Run the sampling process. Args:     x: Initial state to start the sampling from.     dim: Dimension of the state space.     n_steps: Number of steps to take between samples.     n_samples: Number of samples to generate.     thin: Thinning factor (not supported yet).     return_trajectory: Whether to return the trajectory of the samples.     return_diagnostics: Whether to return the diagnostics of the sampling process.</p> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>torch.Tensor: Samples from the sampler.</p> <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>List[dict]: Diagnostics of the sampling process.</p> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>def sample(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    thin: int = 1,  # not supported yet\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    \"\"\"\n    Run the sampling process.\n    Args:\n        x: Initial state to start the sampling from.\n        dim: Dimension of the state space.\n        n_steps: Number of steps to take between samples.\n        n_samples: Number of samples to generate.\n        thin: Thinning factor (not supported yet).\n        return_trajectory: Whether to return the trajectory of the samples.\n        return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n    Returns:\n        torch.Tensor: Samples from the sampler.\n        List[dict]: Diagnostics of the sampling process.\n    \"\"\"\n    return self.sample_chain(\n        x=x,\n        dim=dim,\n        n_steps=n_steps,\n        n_samples=n_samples,\n        thin=thin,\n        return_trajectory=return_trajectory,\n        return_diagnostics=return_diagnostics,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.sample_chain","title":"sample_chain  <code>abstractmethod</code>","text":"<pre><code>sample_chain(dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>@abstractmethod\ndef sample_chain(\n    self,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    thin: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/torchebm/core/basesampler/classes/BaseSampler/#torchebm.core.basesampler.BaseSampler.to","title":"to","text":"<pre><code>to(device: Union[str, device]) -&gt; BaseSampler\n</code></pre> <p>Move sampler to specified device.</p> Source code in <code>torchebm/core/basesampler.py</code> <pre><code>def to(self, device: Union[str, torch.device]) -&gt; \"BaseSampler\":\n    \"\"\"Move sampler to specified device.\"\"\"\n    self.device = device\n    return self\n</code></pre>"},{"location":"api/torchebm/core/energy_function/","title":"Torchebm &gt; Core &gt; Energy_function","text":""},{"location":"api/torchebm/core/energy_function/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/energy_function/#classes","title":"Classes","text":"<ul> <li><code>AckleyEnergy</code> - Energy function for the Ackley function.</li> <li><code>DoubleWellEnergy</code> - Energy function for a double well potential. E(x) = h * \u03a3((x\u00b2-1)\u00b2) where h is the barrier height.</li> <li><code>EnergyFunction</code> - Abstract base class for energy functions (Potential Energy E(x)).</li> <li><code>GaussianEnergy</code> - Energy function for a Gaussian distribution. E(x) = 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).</li> <li><code>HarmonicEnergy</code> - Energy function for a harmonic oscillator. E(x) = 0.5 * k * \u03a3(x\u00b2).</li> <li><code>RastriginEnergy</code> - Energy function for the Rastrigin function.</li> <li><code>RosenbrockEnergy</code> - Energy function for the Rosenbrock function. E(x) = (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.</li> </ul>"},{"location":"api/torchebm/core/energy_function/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/energy_function/#torchebm.core.energy_function","title":"torchebm.core.energy_function","text":""},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/","title":"AckleyEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for the Ackley function. </p> <p>The Ackley energy is defined as:</p> \\[E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\frac{1}{n}\\sum_{i=1}^{n} x_i^2} ight) - \\exp\\left(\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i) ight) + a + e\\] <p>This function has a global minimum at the origin surrounded by many local minima, creating a challenging optimization landscape that tests an algorithm's ability to escape local optima.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Ackley function.</p> <code>20.0</code> <code>b</code> <code>float</code> <p>Parameter <code>b</code> of the Ackley function.</p> <code>0.2</code> <code>c</code> <code>float</code> <p>Parameter <code>c</code> of the Ackley function.</p> <code>2 * pi</code> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class AckleyEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for the Ackley function. \n\n    The Ackley energy is defined as:\n\n    $$E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i)\\right) + a + e$$\n\n    This function has a global minimum at the origin surrounded by many local minima,\n    creating a challenging optimization landscape that tests an algorithm's ability to\n    escape local optima.\n\n    Args:\n        a (float): Parameter `a` of the Ackley function.\n        b (float): Parameter `b` of the Ackley function.\n        c (float): Parameter `c` of the Ackley function.\n    \"\"\"\n\n    def __init__(self, a: float = 20.0, b: float = 0.2, c: float = 2 * math.pi):\n        super().__init__()\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the Ackley energy.\n\n        $$E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i)\\right) + a + e$$\n        \"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        n = x.shape[-1]\n        sum1 = torch.sum(x**2, dim=-1)\n        sum2 = torch.sum(torch.cos(self.c * x), dim=-1)\n        term1 = -self.a * torch.exp(-self.b * torch.sqrt(sum1 / n))\n        term2 = -torch.exp(sum2 / n)\n        return term1 + term2 + self.a + math.e\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.c","title":"c  <code>instance-attribute</code>","text":"<pre><code>c = c\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/AckleyEnergy/#torchebm.core.energy_function.AckleyEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Ackley energy.</p> \\[E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\frac{1}{n}\\sum_{i=1}^{n} x_i^2} ight) - \\exp\\left(\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i) ight) + a + e\\] Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the Ackley energy.\n\n    $$E(x) = -a \\cdot \\exp\\left(-b \\cdot \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c \\cdot x_i)\\right) + a + e$$\n    \"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    n = x.shape[-1]\n    sum1 = torch.sum(x**2, dim=-1)\n    sum2 = torch.sum(torch.cos(self.c * x), dim=-1)\n    term1 = -self.a * torch.exp(-self.b * torch.sqrt(sum1 / n))\n    term2 = -torch.exp(sum2 / n)\n    return term1 + term2 + self.a + math.e\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/","title":"DoubleWellEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for a double well potential. E(x) = h * \u03a3((x\u00b2-1)\u00b2) where h is the barrier height.</p> <p>This energy function creates a bimodal distribution with two modes at x = +1 and x = -1 (in each dimension), separated by a barrier of height h at x = 0.</p> <p>Parameters:</p> Name Type Description Default <code>barrier_height</code> <code>float</code> <p>Height of the barrier between the wells.</p> <code>2.0</code> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class DoubleWellEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for a double well potential. E(x) = h * \u03a3((x\u00b2-1)\u00b2) where h is the barrier height.\n\n    This energy function creates a bimodal distribution with two modes at x = +1 and x = -1\n    (in each dimension), separated by a barrier of height h at x = 0.\n\n    Args:\n        barrier_height (float): Height of the barrier between the wells.\n    \"\"\"\n\n    def __init__(self, barrier_height: float = 2.0):\n        super().__init__()\n        self.barrier_height = barrier_height\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the double well energy: h * \u03a3((x\u00b2-1)\u00b2).\"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        return self.barrier_height * (x.pow(2) - 1).pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/#torchebm.core.energy_function.DoubleWellEnergy.barrier_height","title":"barrier_height  <code>instance-attribute</code>","text":"<pre><code>barrier_height = barrier_height\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/DoubleWellEnergy/#torchebm.core.energy_function.DoubleWellEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the double well energy: h * \u03a3((x\u00b2-1)\u00b2).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the double well energy: h * \u03a3((x\u00b2-1)\u00b2).\"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    return self.barrier_height * (x.pow(2) - 1).pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/","title":"EnergyFunction","text":""},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for energy functions (Potential Energy E(x)).</p> <p>This class serves as a standard interface for defining energy functions used within the torchebm library. It is compatible with both pre-defined analytical functions (like Gaussian, DoubleWell) and trainable neural network models. It represents the potential energy E(x), often related to a probability distribution p(x) by E(x) = -log p(x) + constant.</p> <p>Core Requirements for Subclasses: 1.  Implement the <code>forward(x)</code> method to compute the scalar energy per sample. 2.  Optionally, override the <code>gradient(x)</code> method if an efficient analytical     gradient is available. Otherwise, the default implementation using     <code>torch.autograd</code> will be used.</p> <p>Inheriting from <code>torch.nn.Module</code> ensures that: - Subclasses can contain trainable parameters (<code>nn.Parameter</code>). - Standard PyTorch methods like <code>.to(device)</code>, <code>.parameters()</code>, <code>.state_dict()</code>,   and integration with <code>torch.optim</code> work as expected.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class EnergyFunction(nn.Module, ABC):\n    \"\"\"\n    Abstract base class for energy functions (Potential Energy E(x)).\n\n    This class serves as a standard interface for defining energy functions used\n    within the torchebm library. It is compatible with both pre-defined analytical\n    functions (like Gaussian, DoubleWell) and trainable neural network models.\n    It represents the potential energy E(x), often related to a probability\n    distribution p(x) by E(x) = -log p(x) + constant.\n\n    Core Requirements for Subclasses:\n    1.  Implement the `forward(x)` method to compute the scalar energy per sample.\n    2.  Optionally, override the `gradient(x)` method if an efficient analytical\n        gradient is available. Otherwise, the default implementation using\n        `torch.autograd` will be used.\n\n    Inheriting from `torch.nn.Module` ensures that:\n    - Subclasses can contain trainable parameters (`nn.Parameter`).\n    - Standard PyTorch methods like `.to(device)`, `.parameters()`, `.state_dict()`,\n      and integration with `torch.optim` work as expected.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the EnergyFunction base class.\"\"\"\n        super().__init__()\n        # Optional: store device, though nn.Module handles parameter/buffer device placement\n        self._device: Optional[torch.device] = None\n\n    @property\n    def device(self) -&gt; Optional[torch.device]:\n        \"\"\"Returns the device associated with the module's parameters/buffers (if any).\"\"\"\n        try:\n            # Attempt to infer device from the first parameter/buffer found\n            return next(self.parameters()).device\n        except StopIteration:\n            try:\n                return next(self.buffers()).device\n            except StopIteration:\n                # If no parameters or buffers, return the explicitly set _device or None\n                return self._device\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the scalar energy value for each input sample.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n                               It's recommended that subclasses handle moving `x`\n                               to the correct device if necessary, although callers\n                               should ideally provide `x` on the correct device.\n\n        Returns:\n            torch.Tensor: Tensor of scalar energy values with shape (batch_size,).\n                          Lower values typically indicate higher probability density.\n        \"\"\"\n        pass\n\n    def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the gradient of the energy function with respect to the input x (\u2207_x E(x)).\n\n        This default implementation uses automatic differentiation based on the\n        `forward` method. Subclasses should override this method if a more\n        efficient or numerically stable analytical gradient is available.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n\n        Returns:\n            torch.Tensor: Gradient tensor of the same shape as x.\n        \"\"\"\n        # Store original dtype and device\n        original_dtype = x.dtype\n        device = x.device\n\n        # Ensure x is on the correct device (if specified by the model)\n        if self.device and device != self.device:\n            x = x.to(self.device)\n            device = self.device  # Update device if x was moved\n\n        with torch.enable_grad():\n            # Detach, convert to float32, and enable gradient tracking\n            x_for_grad = x.detach().to(dtype=torch.float32, device=device).requires_grad_(True)\n\n            # Perform forward pass with float32 input\n            energy = self.forward(x_for_grad)\n\n            # Validate energy shape - should be one scalar per batch item\n            if energy.shape != (x_for_grad.shape[0],):\n                raise ValueError(\n                    f\"EnergyFunction forward() output expected shape ({x_for_grad.shape[0]},), but got {energy.shape}.\"\n                )\n\n            # Check grad_fn on the float32 energy\n            if not energy.grad_fn:\n                raise RuntimeError(\n                    \"Cannot compute gradient: `forward` method did not use the input `x` (as float32) in a differentiable way.\"\n                )\n\n            # Compute gradient using autograd w.r.t. the float32 input\n            gradient_float32 = torch.autograd.grad(\n                outputs=energy,\n                inputs=x_for_grad,  # Compute gradient w.r.t the float32 version\n                grad_outputs=torch.ones_like(energy, device=energy.device),\n                create_graph=False,  # Set to False for standard gradient computation\n                retain_graph=None,  # Usually not needed when create_graph=False, let PyTorch decide\n            )[0]\n\n        if gradient_float32 is None:\n            # This should theoretically not happen if checks above pass, but good to have.\n            raise RuntimeError(\n                \"Gradient computation failed unexpectedly. Check the forward pass implementation.\"\n            )\n\n        # Cast gradient back to the original dtype before returning\n        gradient = gradient_float32.to(original_dtype)\n\n        return gradient\n\n    def __call__(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Alias for the forward method for standard PyTorch module usage.\"\"\"\n        # Note: nn.Module.__call__ has hooks; calling forward directly bypasses them.\n        # It's generally better to call the module instance: energy_fn(x)\n        return super().__call__(x, *args, **kwargs)  # Use nn.Module's __call__\n\n    # Override the base nn.Module `to` method to also store the device hint\n    def to(self, *args, **kwargs):\n        \"\"\"Moves and/or casts the parameters and buffers.\"\"\"\n        new_self = super().to(*args, **kwargs)\n        # Try to update the internal device hint after moving\n        try:\n            # Get device from args/kwargs (handling different ways .to can be called)\n            device = None\n            if args:\n                if isinstance(args[0], torch.device):\n                    device = args[0]\n                elif isinstance(args[0], str):\n                    device = torch.device(args[0])\n            if \"device\" in kwargs:\n                device = kwargs[\"device\"]\n\n            if device:\n                new_self._device = device\n        except Exception:\n            # Ignore potential errors in parsing .to args, rely on parameter/buffer device\n            pass\n        return new_self\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.device","title":"device  <code>property</code>","text":"<pre><code>device: Optional[device]\n</code></pre> <p>Returns the device associated with the module's parameters/buffers (if any).</p>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the scalar energy value for each input sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, *input_dims).                It's recommended that subclasses handle moving <code>x</code>                to the correct device if necessary, although callers                should ideally provide <code>x</code> on the correct device.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Tensor of scalar energy values with shape (batch_size,).           Lower values typically indicate higher probability density.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the scalar energy value for each input sample.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n                           It's recommended that subclasses handle moving `x`\n                           to the correct device if necessary, although callers\n                           should ideally provide `x` on the correct device.\n\n    Returns:\n        torch.Tensor: Tensor of scalar energy values with shape (batch_size,).\n                      Lower values typically indicate higher probability density.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.gradient","title":"gradient","text":"<pre><code>gradient(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the gradient of the energy function with respect to the input x (\u2207_x E(x)).</p> <p>This default implementation uses automatic differentiation based on the <code>forward</code> method. Subclasses should override this method if a more efficient or numerically stable analytical gradient is available.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, *input_dims).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Gradient tensor of the same shape as x.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the gradient of the energy function with respect to the input x (\u2207_x E(x)).\n\n    This default implementation uses automatic differentiation based on the\n    `forward` method. Subclasses should override this method if a more\n    efficient or numerically stable analytical gradient is available.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, *input_dims).\n\n    Returns:\n        torch.Tensor: Gradient tensor of the same shape as x.\n    \"\"\"\n    # Store original dtype and device\n    original_dtype = x.dtype\n    device = x.device\n\n    # Ensure x is on the correct device (if specified by the model)\n    if self.device and device != self.device:\n        x = x.to(self.device)\n        device = self.device  # Update device if x was moved\n\n    with torch.enable_grad():\n        # Detach, convert to float32, and enable gradient tracking\n        x_for_grad = x.detach().to(dtype=torch.float32, device=device).requires_grad_(True)\n\n        # Perform forward pass with float32 input\n        energy = self.forward(x_for_grad)\n\n        # Validate energy shape - should be one scalar per batch item\n        if energy.shape != (x_for_grad.shape[0],):\n            raise ValueError(\n                f\"EnergyFunction forward() output expected shape ({x_for_grad.shape[0]},), but got {energy.shape}.\"\n            )\n\n        # Check grad_fn on the float32 energy\n        if not energy.grad_fn:\n            raise RuntimeError(\n                \"Cannot compute gradient: `forward` method did not use the input `x` (as float32) in a differentiable way.\"\n            )\n\n        # Compute gradient using autograd w.r.t. the float32 input\n        gradient_float32 = torch.autograd.grad(\n            outputs=energy,\n            inputs=x_for_grad,  # Compute gradient w.r.t the float32 version\n            grad_outputs=torch.ones_like(energy, device=energy.device),\n            create_graph=False,  # Set to False for standard gradient computation\n            retain_graph=None,  # Usually not needed when create_graph=False, let PyTorch decide\n        )[0]\n\n    if gradient_float32 is None:\n        # This should theoretically not happen if checks above pass, but good to have.\n        raise RuntimeError(\n            \"Gradient computation failed unexpectedly. Check the forward pass implementation.\"\n        )\n\n    # Cast gradient back to the original dtype before returning\n    gradient = gradient_float32.to(original_dtype)\n\n    return gradient\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/EnergyFunction/#torchebm.core.energy_function.EnergyFunction.to","title":"to","text":"<pre><code>to(*args, **kwargs)\n</code></pre> <p>Moves and/or casts the parameters and buffers.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def to(self, *args, **kwargs):\n    \"\"\"Moves and/or casts the parameters and buffers.\"\"\"\n    new_self = super().to(*args, **kwargs)\n    # Try to update the internal device hint after moving\n    try:\n        # Get device from args/kwargs (handling different ways .to can be called)\n        device = None\n        if args:\n            if isinstance(args[0], torch.device):\n                device = args[0]\n            elif isinstance(args[0], str):\n                device = torch.device(args[0])\n        if \"device\" in kwargs:\n            device = kwargs[\"device\"]\n\n        if device:\n            new_self._device = device\n    except Exception:\n        # Ignore potential errors in parsing .to args, rely on parameter/buffer device\n        pass\n    return new_self\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/GaussianEnergy/","title":"GaussianEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/GaussianEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for a Gaussian distribution. E(x) = 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Tensor</code> <p>Mean vector (\u03bc) of the Gaussian distribution.</p> required <code>cov</code> <code>Tensor</code> <p>Covariance matrix (\u03a3) of the Gaussian distribution.</p> required Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class GaussianEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for a Gaussian distribution. E(x) = 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).\n\n    Args:\n        mean (torch.Tensor): Mean vector (\u03bc) of the Gaussian distribution.\n        cov (torch.Tensor): Covariance matrix (\u03a3) of the Gaussian distribution.\n    \"\"\"\n\n    def __init__(self, mean: torch.Tensor, cov: torch.Tensor):\n        super().__init__()\n        if mean.ndim != 1:\n            raise ValueError(\"Mean must be a 1D tensor.\")\n        if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n            raise ValueError(\"Covariance must be a 2D square matrix.\")\n        if mean.shape[0] != cov.shape[0]:\n            raise ValueError(\n                \"Mean vector dimension must match covariance matrix dimension.\"\n            )\n\n        # Register mean and covariance inverse as buffers.\n        # Buffers are part of the module's state (`state_dict`) and moved by `.to()`,\n        # but are not considered parameters by optimizers.\n        self.register_buffer(\"mean\", mean)\n        try:\n            cov_inv = torch.inverse(cov)\n            self.register_buffer(\"cov_inv\", cov_inv)\n        except RuntimeError as e:\n            raise ValueError(\n                f\"Failed to invert covariance matrix: {e}. Ensure it is invertible.\"\n            ) from e\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the Gaussian energy: 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).\"\"\"\n        # Ensure x is compatible shape (batch_size, dim)\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n        if x.ndim != 2 or x.shape[1] != self.mean.shape[0]:\n            raise ValueError(\n                f\"Input x expected shape (batch_size, {self.mean.shape[0]}), but got {x.shape}\"\n            )\n\n        # Get mean and cov_inv on the same device as x\n        # We don't change the dtype because gradient() already converted x to float32\n        mean = self.mean.to(device=x.device)\n        cov_inv = self.cov_inv.to(device=x.device)\n\n        # Compute centered vectors\n        # Important: use x directly without detaching or converting to maintain grad tracking\n        delta = x - mean\n\n        # Calculate energy\n        # Use batch matrix multiplication for better numerical stability\n        # We use einsum which maintains gradients through operations\n        energy = 0.5 * torch.einsum(\"bi,ij,bj-&gt;b\", delta, cov_inv, delta)\n\n        return energy\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/GaussianEnergy/#torchebm.core.energy_function.GaussianEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Gaussian energy: 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the Gaussian energy: 0.5 * (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc).\"\"\"\n    # Ensure x is compatible shape (batch_size, dim)\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n    if x.ndim != 2 or x.shape[1] != self.mean.shape[0]:\n        raise ValueError(\n            f\"Input x expected shape (batch_size, {self.mean.shape[0]}), but got {x.shape}\"\n        )\n\n    # Get mean and cov_inv on the same device as x\n    # We don't change the dtype because gradient() already converted x to float32\n    mean = self.mean.to(device=x.device)\n    cov_inv = self.cov_inv.to(device=x.device)\n\n    # Compute centered vectors\n    # Important: use x directly without detaching or converting to maintain grad tracking\n    delta = x - mean\n\n    # Calculate energy\n    # Use batch matrix multiplication for better numerical stability\n    # We use einsum which maintains gradients through operations\n    energy = 0.5 * torch.einsum(\"bi,ij,bj-&gt;b\", delta, cov_inv, delta)\n\n    return energy\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/","title":"HarmonicEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for a harmonic oscillator. E(x) = 0.5 * k * \u03a3(x\u00b2).</p> <p>This energy function represents a quadratic potential centered at the origin, equivalent to a Gaussian distribution with zero mean and variance proportional to 1/k.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>float</code> <p>Spring constant.</p> <code>1.0</code> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class HarmonicEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for a harmonic oscillator. E(x) = 0.5 * k * \u03a3(x\u00b2).\n\n    This energy function represents a quadratic potential centered at the origin,\n    equivalent to a Gaussian distribution with zero mean and variance proportional to 1/k.\n\n    Args:\n        k (float): Spring constant.\n    \"\"\"\n\n    def __init__(self, k: float = 1.0):\n        super().__init__()\n        self.k = k\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the harmonic oscillator energy: 0.5 * k * \u03a3(x\u00b2).\"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        return 0.5 * self.k * x.pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/#torchebm.core.energy_function.HarmonicEnergy.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k = k\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/HarmonicEnergy/#torchebm.core.energy_function.HarmonicEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the harmonic oscillator energy: 0.5 * k * \u03a3(x\u00b2).</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the harmonic oscillator energy: 0.5 * k * \u03a3(x\u00b2).\"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    return 0.5 * self.k * x.pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/","title":"RastriginEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for the Rastrigin function.</p> <p>The Rastrigin energy is defined as:</p> \\[E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]\\] <p>This function is characterized by a large number of local minima arranged in a  regular lattice, with a global minimum at the origin. It's a classic test for optimization algorithms due to its highly multimodal nature.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Rastrigin function.</p> <code>10.0</code> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class RastriginEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for the Rastrigin function.\n\n    The Rastrigin energy is defined as:\n\n    $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n\n    This function is characterized by a large number of local minima arranged in a \n    regular lattice, with a global minimum at the origin. It's a classic test for\n    optimization algorithms due to its highly multimodal nature.\n\n    Args:\n        a (float): Parameter `a` of the Rastrigin function.\n    \"\"\"\n\n    def __init__(self, a: float = 10.0):\n        super().__init__()\n        self.a = a\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the Rastrigin energy.\n\n        $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n        \"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n\n        n = x.shape[-1]\n        return self.a * n + torch.sum(\n            x**2 - self.a * torch.cos(2 * math.pi * x), dim=-1\n        )\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/#torchebm.core.energy_function.RastriginEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RastriginEnergy/#torchebm.core.energy_function.RastriginEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Rastrigin energy.</p> \\[E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]\\] Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the Rastrigin energy.\n\n    $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n    \"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n\n    n = x.shape[-1]\n    return self.a * n + torch.sum(\n        x**2 - self.a * torch.cos(2 * math.pi * x), dim=-1\n    )\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/","title":"RosenbrockEnergy","text":""},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>EnergyFunction</code></p> <p>Energy function for the Rosenbrock function. E(x) = (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.</p> <p>This energy function creates a challenging valley-shaped distribution with the  global minimum at (a, a\u00b2). It's commonly used as a benchmark for optimization algorithms due to its curved, narrow valley which is difficult to traverse.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Rosenbrock function.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Parameter <code>b</code> of the Rosenbrock function.</p> <code>100.0</code> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>class RosenbrockEnergy(EnergyFunction):\n    \"\"\"\n    Energy function for the Rosenbrock function. E(x) = (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.\n\n    This energy function creates a challenging valley-shaped distribution with the \n    global minimum at (a, a\u00b2). It's commonly used as a benchmark for optimization algorithms\n    due to its curved, narrow valley which is difficult to traverse.\n\n    Args:\n        a (float): Parameter `a` of the Rosenbrock function.\n        b (float): Parameter `b` of the Rosenbrock function.\n    \"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 100.0):\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes the Rosenbrock energy: (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.\"\"\"\n        # Ensure x is compatible shape\n        if x.ndim == 1:  # Handle single sample case\n            x = x.unsqueeze(0)\n        # Validate dimensions - Rosenbrock requires at least 2 dimensions\n        if x.shape[-1] &lt; 2:\n            raise ValueError(\n                f\"Rosenbrock energy function requires at least 2 dimensions, got {x.shape[-1]}\"\n            )\n\n        return (self.a - x[..., 0]) ** 2 + self.b * (x[..., 1] - x[..., 0] ** 2) ** 2\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#torchebm.core.energy_function.RosenbrockEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#torchebm.core.energy_function.RosenbrockEnergy.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/torchebm/core/energy_function/classes/RosenbrockEnergy/#torchebm.core.energy_function.RosenbrockEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Rosenbrock energy: (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.</p> Source code in <code>torchebm/core/energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the Rosenbrock energy: (a-x\u2081)\u00b2 + b\u00b7(x\u2082-x\u2081\u00b2)\u00b2.\"\"\"\n    # Ensure x is compatible shape\n    if x.ndim == 1:  # Handle single sample case\n        x = x.unsqueeze(0)\n    # Validate dimensions - Rosenbrock requires at least 2 dimensions\n    if x.shape[-1] &lt; 2:\n        raise ValueError(\n            f\"Rosenbrock energy function requires at least 2 dimensions, got {x.shape[-1]}\"\n        )\n\n    return (self.a - x[..., 0]) ** 2 + self.b * (x[..., 1] - x[..., 0] ** 2) ** 2\n</code></pre>"},{"location":"api/torchebm/core/losses/","title":"Torchebm &gt; Core &gt; Losses","text":""},{"location":"api/torchebm/core/losses/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/losses/#classes","title":"Classes","text":"<ul> <li><code>Loss</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/losses/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/losses/#torchebm.core.losses","title":"torchebm.core.losses","text":""},{"location":"api/torchebm/core/losses/classes/Loss/","title":"Loss","text":""},{"location":"api/torchebm/core/losses/classes/Loss/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Module</code></p> Source code in <code>torchebm/core/losses.py</code> <pre><code>class Loss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        pass\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"api/torchebm/core/losses/classes/Loss/#torchebm.core.losses.Loss.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/core/losses.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/core/losses/classes/Loss/#torchebm.core.losses.Loss.to","title":"to","text":"<pre><code>to(device)\n</code></pre> Source code in <code>torchebm/core/losses.py</code> <pre><code>def to(self, device):\n    self.device = device\n    return self\n</code></pre>"},{"location":"api/torchebm/core/optimizer/","title":"Torchebm &gt; Core &gt; Optimizer","text":""},{"location":"api/torchebm/core/optimizer/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/optimizer/#classes","title":"Classes","text":"<ul> <li><code>Optimizer</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/optimizer/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/optimizer/#torchebm.core.optimizer","title":"torchebm.core.optimizer","text":""},{"location":"api/torchebm/core/optimizer/classes/Optimizer/","title":"Optimizer","text":""},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Module</code></p> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>class Optimizer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def step(self):\n        pass\n\n    @abstractmethod\n    def zero_grad(self):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state_dict):\n        pass\n\n    @abstractmethod\n    def to(self, device):\n        pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.step","title":"step  <code>abstractmethod</code>","text":"<pre><code>step()\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef step(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.zero_grad","title":"zero_grad  <code>abstractmethod</code>","text":"<pre><code>zero_grad()\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef zero_grad(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.state_dict","title":"state_dict  <code>abstractmethod</code>","text":"<pre><code>state_dict()\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef state_dict(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.load_state_dict","title":"load_state_dict  <code>abstractmethod</code>","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/optimizer/classes/Optimizer/#torchebm.core.optimizer.Optimizer.to","title":"to  <code>abstractmethod</code>","text":"<pre><code>to(device)\n</code></pre> Source code in <code>torchebm/core/optimizer.py</code> <pre><code>@abstractmethod\ndef to(self, device):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/score_matching/","title":"Torchebm &gt; Core &gt; Score_matching","text":""},{"location":"api/torchebm/core/score_matching/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/score_matching/#classes","title":"Classes","text":"<ul> <li><code>ScoreMatching</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/score_matching/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/score_matching/#torchebm.core.score_matching","title":"torchebm.core.score_matching","text":""},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/","title":"ScoreMatching","text":""},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>class ScoreMatching(ABC):\n    def __init__(\n            self,\n            loss,\n            norm,\n            n_input: int = 2,\n            n_hidden: int = 64,\n            n_hidden_layers: int = 2,\n            batch_norm: bool = False,\n            lr: float = 1e-3,\n            activation: Type[torch.nn.Module] = torch.nn.Softplus,\n    ):\n        super().__init__()\n        layers = [\n            self._make_layer(n_input, n_hidden, bn=batch_norm, dp=0.01, act=activation)\n        ]\n        layers += [\n            self._make_layer(n_hidden, n_hidden, bn=batch_norm, dp=0.01, act=activation)\n            for _ in range(n_hidden_layers - 1)\n        ]\n        layers += [self._make_layer(n_hidden, n_input)]\n        self.layers = torch.nn.Sequential(*layers)\n        # if norm is None:\n        #     norm = normalizers.NoOp()\n        self.norm = norm\n        self.loss_fn = loss\n        self.lr = lr\n        self.save_hyperparameters()\n\n    def forward(self, x):\n        return self.layers(self.norm(x))\n\n    def training_step(self, batch, batch_idx):\n        del batch_idx\n        x = batch\n        loss = self.loss_fn(self, x)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\n    def _make_layer(\n            n_in: int,\n            n_out: int,\n            bn: bool = False,\n            dp: float = 0.0,\n            act: Type[torch.nn.Module] = None,\n    ) -&gt; torch.nn.Sequential:\n        layers = [torch.nn.Linear(n_in, n_out)]\n        if act is not None:\n            layers.append(act())\n        if bn:\n            layers.append(torch.nn.BatchNorm1d(n_out))\n        if dp &gt; 0:\n            layers.append(torch.nn.Dropout(p=dp))\n        return torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = Sequential(*layers)\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = norm\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.loss_fn","title":"loss_fn  <code>instance-attribute</code>","text":"<pre><code>loss_fn = loss\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.lr","title":"lr  <code>instance-attribute</code>","text":"<pre><code>lr = lr\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>def forward(self, x):\n    return self.layers(self.norm(x))\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    del batch_idx\n    x = batch\n    loss = self.loss_fn(self, x)\n    return loss\n</code></pre>"},{"location":"api/torchebm/core/score_matching/classes/ScoreMatching/#torchebm.core.score_matching.ScoreMatching.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> Source code in <code>torchebm/core/score_matching.py</code> <pre><code>def configure_optimizers(self):\n    return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"api/torchebm/core/trainer/","title":"Torchebm &gt; Core &gt; Trainer","text":""},{"location":"api/torchebm/core/trainer/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/trainer/#classes","title":"Classes","text":"<ul> <li><code>ContrastiveDivergenceTrainer</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/trainer/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/trainer/#torchebm.core.trainer","title":"torchebm.core.trainer","text":""},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/","title":"ContrastiveDivergenceTrainer","text":""},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#methods-and-attributes","title":"Methods and Attributes","text":"Source code in <code>torchebm/core/trainer.py</code> <pre><code>class ContrastiveDivergenceTrainer:\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        sampler: BaseSampler,\n        learning_rate: float = 0.01,\n    ):\n        self.energy_function = energy_function\n        self.sampler = sampler\n        self.optimizer = torch.optim.Adam(\n            self.energy_function.parameters(), lr=learning_rate\n        )\n\n    def train_step(self, real_data: torch.Tensor) -&gt; dict:\n        self.optimizer.zero_grad()\n\n        # Positive phase\n        positive_energy = self.energy_function(real_data)\n\n        # Negative phase\n        initial_samples = torch.randn_like(real_data)\n        negative_samples = self.sampler.sample(\n            self.energy_function, initial_samples, num_steps=10\n        )\n        negative_energy = self.energy_function(negative_samples)\n\n        # Compute loss\n        loss = positive_energy.mean() - negative_energy.mean()\n\n        # Backpropagation\n        loss.backward()\n        self.optimizer.step()\n\n        return {\n            \"loss\": loss.item(),\n            \"positive_energy\": positive_energy.mean().item(),\n            \"negative_energy\": negative_energy.mean().item(),\n        }\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = sampler\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/torchebm/core/trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.trainer.ContrastiveDivergenceTrainer.train_step","title":"train_step","text":"<pre><code>train_step(real_data: Tensor) -&gt; dict\n</code></pre> Source code in <code>torchebm/core/trainer.py</code> <pre><code>def train_step(self, real_data: torch.Tensor) -&gt; dict:\n    self.optimizer.zero_grad()\n\n    # Positive phase\n    positive_energy = self.energy_function(real_data)\n\n    # Negative phase\n    initial_samples = torch.randn_like(real_data)\n    negative_samples = self.sampler.sample(\n        self.energy_function, initial_samples, num_steps=10\n    )\n    negative_energy = self.energy_function(negative_samples)\n\n    # Compute loss\n    loss = positive_energy.mean() - negative_energy.mean()\n\n    # Backpropagation\n    loss.backward()\n    self.optimizer.step()\n\n    return {\n        \"loss\": loss.item(),\n        \"positive_energy\": positive_energy.mean().item(),\n        \"negative_energy\": negative_energy.mean().item(),\n    }\n</code></pre>"},{"location":"api/torchebm/cuda/","title":"Torchebm &gt; Cuda","text":""},{"location":"api/torchebm/cuda/#contents","title":"Contents","text":""},{"location":"api/torchebm/cuda/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/cuda/#torchebm.cuda","title":"torchebm.cuda","text":""},{"location":"api/torchebm/losses/","title":"Torchebm &gt; Losses","text":""},{"location":"api/torchebm/losses/#contents","title":"Contents","text":""},{"location":"api/torchebm/losses/#modules","title":"Modules","text":"<ul> <li>Contrastive_divergence</li> </ul>"},{"location":"api/torchebm/losses/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/losses/#torchebm.losses","title":"torchebm.losses","text":""},{"location":"api/torchebm/losses/contrastive_divergence/","title":"Torchebm &gt; Losses &gt; Contrastive_divergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#contents","title":"Contents","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#classes","title":"Classes","text":"<ul> <li><code>ContrastiveDivergence</code> - No description available.</li> <li><code>ContrastiveDivergenceBase</code> - No description available.</li> <li><code>ParallelTemperingCD</code> - No description available.</li> <li><code>PersistentContrastiveDivergence</code> - No description available.</li> </ul>"},{"location":"api/torchebm/losses/contrastive_divergence/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence","title":"torchebm.losses.contrastive_divergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/","title":"ContrastiveDivergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ContrastiveDivergenceBase</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ContrastiveDivergence(ContrastiveDivergenceBase):\n    def __init__(self, k=1):\n        super().__init__(k)\n\n    def sample(self, energy_model, x_pos):\n        x_neg = x_pos.clone().detach()\n        for _ in range(self.k):\n            x_neg = energy_model.gibbs_step(\n                x_neg\n            )  # todo: implement `gibbs_step` in energy_model\n        return x_neg\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.sample","title":"sample","text":"<pre><code>sample(energy_model, x_pos)\n</code></pre> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>def sample(self, energy_model, x_pos):\n    x_neg = x_pos.clone().detach()\n    for _ in range(self.k):\n        x_neg = energy_model.gibbs_step(\n            x_neg\n        )  # todo: implement `gibbs_step` in energy_model\n    return x_neg\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/","title":"ContrastiveDivergenceBase","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Loss</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ContrastiveDivergenceBase(Loss):\n    def __init__(self, k=1):\n        super().__init__()\n        self.k = k  # Number of sampling steps\n\n    @abstractmethod\n    def sample(self, energy_model, x_pos):\n        \"\"\"Abstract method: Generate negative samples from the energy model.\n        Args:\n            energy_model: Energy-based model (e.g., RBM)\n            x_pos: Positive samples (data)\n        Returns:\n            x_neg: Negative samples (model samples)\n        \"\"\"\n        raise NotImplementedError\n\n    def forward(self, energy_model, x_pos):\n        \"\"\"Compute the CD loss: E(x_pos) - E(x_neg)\"\"\"\n        x_neg = self.sample(energy_model, x_pos)\n        loss = energy_model(x_pos).mean() - energy_model(x_neg).mean()\n        return loss\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#torchebm.losses.contrastive_divergence.ContrastiveDivergenceBase.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k = k\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#torchebm.losses.contrastive_divergence.ContrastiveDivergenceBase.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(energy_model, x_pos)\n</code></pre> <p>Abstract method: Generate negative samples from the energy model. Args:     energy_model: Energy-based model (e.g., RBM)     x_pos: Positive samples (data) Returns:     x_neg: Negative samples (model samples)</p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>@abstractmethod\ndef sample(self, energy_model, x_pos):\n    \"\"\"Abstract method: Generate negative samples from the energy model.\n    Args:\n        energy_model: Energy-based model (e.g., RBM)\n        x_pos: Positive samples (data)\n    Returns:\n        x_neg: Negative samples (model samples)\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergenceBase/#torchebm.losses.contrastive_divergence.ContrastiveDivergenceBase.forward","title":"forward","text":"<pre><code>forward(energy_model, x_pos)\n</code></pre> <p>Compute the CD loss: E(x_pos) - E(x_neg)</p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>def forward(self, energy_model, x_pos):\n    \"\"\"Compute the CD loss: E(x_pos) - E(x_neg)\"\"\"\n    x_neg = self.sample(energy_model, x_pos)\n    loss = energy_model(x_pos).mean() - energy_model(x_neg).mean()\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/","title":"ParallelTemperingCD","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ContrastiveDivergenceBase</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ParallelTemperingCD(ContrastiveDivergenceBase):\n    def __init__(self, temps=[1.0, 0.5], k=5):\n        super().__init__(k)\n        self.temps = temps  # List of temperatures\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/#torchebm.losses.contrastive_divergence.ParallelTemperingCD.temps","title":"temps  <code>instance-attribute</code>","text":"<pre><code>temps = temps\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/","title":"PersistentContrastiveDivergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ContrastiveDivergenceBase</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class PersistentContrastiveDivergence(ContrastiveDivergenceBase):\n    def __init__(self, buffer_size=100):\n        super().__init__(k=1)\n        self.buffer = None  # Persistent chain state\n        self.buffer_size = buffer_size\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#torchebm.losses.contrastive_divergence.PersistentContrastiveDivergence.buffer","title":"buffer  <code>instance-attribute</code>","text":"<pre><code>buffer = None\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#torchebm.losses.contrastive_divergence.PersistentContrastiveDivergence.buffer_size","title":"buffer_size  <code>instance-attribute</code>","text":"<pre><code>buffer_size = buffer_size\n</code></pre>"},{"location":"api/torchebm/models/","title":"Torchebm &gt; Models","text":""},{"location":"api/torchebm/models/#contents","title":"Contents","text":""},{"location":"api/torchebm/models/#modules","title":"Modules","text":"<ul> <li>Base_model</li> </ul>"},{"location":"api/torchebm/models/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/models/#torchebm.models","title":"torchebm.models","text":""},{"location":"api/torchebm/models/base_model/","title":"Torchebm &gt; Models &gt; Base_model","text":""},{"location":"api/torchebm/models/base_model/#contents","title":"Contents","text":""},{"location":"api/torchebm/models/base_model/#classes","title":"Classes","text":"<ul> <li><code>BaseModel</code> - Base class for models.</li> </ul>"},{"location":"api/torchebm/models/base_model/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/models/base_model/#torchebm.models.base_model","title":"torchebm.models.base_model","text":""},{"location":"api/torchebm/models/base_model/classes/BaseModel/","title":"BaseModel","text":""},{"location":"api/torchebm/models/base_model/classes/BaseModel/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for models.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>sampler</code> <code>BaseSampler</code> <p>Sampler to use for sampling.</p> required <p>Methods:</p> Name Description <code>energy</code> <p>Compute the energy of the input.</p> <code>sample</code> <p>Sample from the model.</p> <code>train_step</code> <p>Perform a single training step</p> Source code in <code>torchebm/models/base_model.py</code> <pre><code>class BaseModel(ABC):\n    \"\"\"\n    Base class for models.\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        sampler (BaseSampler): Sampler to use for sampling.\n\n    Methods:\n        energy(x): Compute the energy of the input.\n        sample(num_samples): Sample from the model.\n        train_step(real_data): Perform a single training step\n    \"\"\"\n\n    def __init__(self, energy_function: EnergyFunction, sampler: BaseSampler):\n        self.energy_function = energy_function\n        self.sampler = sampler\n\n    @abstractmethod\n    def energy(self, x: torch.Tensor) -&gt; torch.Tensor:\n        pass\n\n    @abstractmethod\n    def sample(self, num_samples: int) -&gt; torch.Tensor:\n        pass\n\n    @abstractmethod\n    def train_step(self, real_data: torch.Tensor) -&gt; dict:\n        pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = sampler\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.energy","title":"energy  <code>abstractmethod</code>","text":"<pre><code>energy(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef energy(self, x: torch.Tensor) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(num_samples: int) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef sample(self, num_samples: int) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.train_step","title":"train_step  <code>abstractmethod</code>","text":"<pre><code>train_step(real_data: Tensor) -&gt; dict\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef train_step(self, real_data: torch.Tensor) -&gt; dict:\n    pass\n</code></pre>"},{"location":"api/torchebm/samplers/","title":"Torchebm &gt; Samplers","text":""},{"location":"api/torchebm/samplers/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/#modules","title":"Modules","text":"<ul> <li>Langevin_dynamics</li> <li>Mcmc</li> </ul>"},{"location":"api/torchebm/samplers/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/#torchebm.samplers","title":"torchebm.samplers","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/","title":"Torchebm &gt; Samplers &gt; Langevin_dynamics","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#classes","title":"Classes","text":"<ul> <li><code>LangevinDynamics</code> - Langevin Dynamics sampler implementing discretized gradient-based MCMC.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics","title":"torchebm.samplers.langevin_dynamics","text":"<p>Langevin Dynamics Sampler Module.</p> <p>This module provides an implementation of the Langevin Dynamics algorithm, a gradient-based Markov Chain Monte Carlo (MCMC) method. It leverages stochastic differential equations to sample from complex probability distributions, making it a lightweight yet effective tool for Bayesian inference and generative modeling.</p> <p>Key Features</p> <ul> <li>Gradient-based sampling with stochastic updates.</li> <li>Customizable step sizes and noise scales for flexible tuning.</li> <li>Optional diagnostics and trajectory tracking for analysis.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>LangevinDynamics</code> <p>Core class implementing the Langevin Dynamics sampler.</p>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--usage-example","title":"Usage Example","text":"<p>Sampling from a Custom Energy Function</p> <pre><code>from torchebm.samplers.mcmc.langevin import LangevinDynamics\nfrom torchebm.core.energy_function import GaussianEnergy\nimport torch\n\n# Define a 2D Gaussian energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize Langevin sampler\nsampler = LangevinDynamics(energy_fn, step_size=0.01, noise_scale=0.1)\n\n# Starting points for 5 chains\ninitial_state = torch.randn(5, 2)\n\n# Run sampling\nsamples, diagnostics = sampler.sample_chain(\n    x=initial_state, n_steps=100, n_samples=5, return_diagnostics=True\n)\nprint(f\"Samples shape: {samples.shape}\")\nprint(f\"Diagnostics keys: {diagnostics.shape}\")\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Langevin Dynamics Overview</p> <p>Langevin Dynamics simulates a stochastic process governed by the Langevin equation. For a state \\( x_t \\), the discretized update rule is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla U(x_t) + \\sqrt{2\\eta} \\epsilon_t \\] <ul> <li>\\( U(x) \\): Potential energy, where \\( U(x) = -\\log p(x) \\) and \\( p(x) \\) is the target distribution.</li> <li>\\( \\eta \\): Step size controlling the gradient descent.</li> <li>\\( \\epsilon_t \\sim \\mathcal{N}(0, I) \\): Gaussian noise introducing stochasticity.</li> </ul> <p>Over time, this process converges to samples from the Boltzmann distribution:</p> \\[ p(x) \\propto e^{-U(x)} \\] <p>Why Use Langevin Dynamics?</p> <ul> <li>Simplicity: Requires only first-order gradients, making it computationally lighter than methods like HMC.</li> <li>Exploration: The noise term prevents the sampler from getting stuck in local minima.</li> <li>Flexibility: Applicable to a wide range of energy-based models and score-based generative tasks.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--practical-considerations","title":"Practical Considerations","text":"<p>Parameter Tuning Guide</p> <ul> <li>Step Size (\\(\\eta\\)):<ul> <li>Too large: Instability and divergence</li> <li>Too small: Slow convergence</li> <li>Rule of thumb: Start with \\(\\eta \\approx 10^{-3}\\) to \\(10^{-5}\\)</li> </ul> </li> <li>Noise Scale (\\(\\beta^{-1/2}\\)):<ul> <li>Controls exploration-exploitation tradeoff</li> <li>Higher values help escape local minima</li> </ul> </li> <li>Decay Rate (future implementation):<ul> <li>Momentum-like term for accelerated convergence</li> </ul> </li> </ul> <p>Diagnostics Interpretation</p> <p>Use <code>return_diagnostics=True</code> to monitor: - Mean/Variance: Track distribution stationarity - Energy Gradients: Check for vanishing/exploding gradients - Autocorrelation: Assess mixing efficiency</p> <p>When to Choose Langevin Over HMC?</p> Criterion Langevin HMC Computational Cost Lower Higher Tuning Complexity Simpler More involved High Dimensions Efficient More efficient Multimodal Targets May need annealing Better exploration <p>How to Diagnose Sampling?</p> <p>Check diagnostics for: - Sample mean and variance convergence. - Gradient magnitudes (should stabilize). - Energy trends over iterations.</p> Further Reading <ul> <li>Langevin Dynamics Basics</li> <li>Score-Based Models and Langevin</li> <li>Practical Langevin Tutorial</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/","title":"LangevinDynamics","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Langevin Dynamics sampler implementing discretized gradient-based MCMC.</p> <p>This class implements the Langevin Dynamics algorithm, a gradient-based MCMC method that samples from a target distribution defined by an energy function. It uses a stochastic update rule combining gradient descent with Gaussian noise to explore the energy landscape.</p> <p>Each step updates the state \\(x_t\\) according to the discretized Langevin equation:</p> \\[x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t\\] <p>where \\(\\epsilon_t \\sim \\mathcal{N}(0, I)\\) and \\(\\eta\\) is the step size.</p> <p>This process generates samples that asymptotically follow the Boltzmann distribution:</p> \\[p(x) \\propto e^{-U(x)}\\] <p>where \\(U(x)\\) defines the energy landscape.</p> <p>Algorithm Summary</p> <ol> <li>If <code>x</code> is not provided, initialize it with Gaussian noise.</li> <li>Iteratively update <code>x</code> for <code>n_steps</code> using <code>self.langevin_step()</code>.</li> <li>Optionally track trajectory (<code>return_trajectory=True</code>).</li> <li>Optionally collect diagnostics such as mean, variance, and energy gradients.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>step_size</code> <code>float</code> <p>Step size for updates.</p> <code>0.001</code> <code>noise_scale</code> <code>float</code> <p>Scale of the noise.</p> <code>1.0</code> <code>decay</code> <code>float</code> <p>Damping coefficient (not supported yet).</p> <code>0.0</code> <code>dtype</code> <code>dtype</code> <p>Data type to use for the computations.</p> <code>float32</code> <code>device</code> <code>str</code> <p>Device to run the computations on (e.g., \"cpu\" or \"cuda\").</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>For invalid parameter ranges</p> <p>Methods:</p> Name Description <code>langevin_step</code> <p>Perform a Langevin step.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics</p> <p>Basic Usage</p> <pre><code># Define energy function\nenergy_fn = QuadraticEnergy(A=torch.eye(2), b=torch.zeros(2))\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=0.1\n)\n\n# Sample 100 points from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=2,\n    n_steps=50,\n    n_samples=100\n)\n</code></pre> <p>Parameter Relationships</p> <p>The effective temperature is controlled by: \\(\\(\\text{Temperature} = \\frac{\\text{noise_scale}^2}{2 \\cdot \\text{step_size}}\\)\\) Adjust both parameters together to maintain constant temperature.</p> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>class LangevinDynamics(BaseSampler):\n    r\"\"\"\n    Langevin Dynamics sampler implementing discretized gradient-based MCMC.\n\n    This class implements the Langevin Dynamics algorithm, a gradient-based MCMC method that samples from a target\n    distribution defined by an energy function. It uses a stochastic update rule combining gradient descent with Gaussian noise to explore the energy landscape.\n\n    Each step updates the state $x_t$ according to the discretized Langevin equation:\n\n    $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n    where $\\epsilon_t \\sim \\mathcal{N}(0, I)$ and $\\eta$ is the step size.\n\n    This process generates samples that asymptotically follow the Boltzmann distribution:\n\n\n    $$p(x) \\propto e^{-U(x)}$$\n\n    where $U(x)$ defines the energy landscape.\n\n    !!! note \"Algorithm Summary\"\n\n        1. If `x` is not provided, initialize it with Gaussian noise.\n        2. Iteratively update `x` for `n_steps` using `self.langevin_step()`.\n        3. Optionally track trajectory (`return_trajectory=True`).\n        4. Optionally collect diagnostics such as mean, variance, and energy gradients.\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        step_size (float): Step size for updates.\n        noise_scale (float): Scale of the noise.\n        decay (float): Damping coefficient (not supported yet).\n        dtype (torch.dtype): Data type to use for the computations.\n        device (str): Device to run the computations on (e.g., \"cpu\" or \"cuda\").\n\n    Raises:\n        ValueError: For invalid parameter ranges\n\n    Methods:\n        langevin_step(prev_x, noise): Perform a Langevin step.\n        sample_chain(x, dim, n_steps, n_samples, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(dim, n_steps, n_samples): Initialize the diagnostics\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Define energy function\n        energy_fn = QuadraticEnergy(A=torch.eye(2), b=torch.zeros(2))\n\n        # Initialize sampler\n        sampler = LangevinDynamics(\n            energy_function=energy_fn,\n            step_size=0.01,\n            noise_scale=0.1\n        )\n\n        # Sample 100 points from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=2,\n            n_steps=50,\n            n_samples=100\n        )\n        ```\n    !!! warning \"Parameter Relationships\"\n        The effective temperature is controlled by:\n        $$\\text{Temperature} = \\frac{\\text{noise_scale}^2}{2 \\cdot \\text{step_size}}$$\n        Adjust both parameters together to maintain constant temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        step_size: float = 1e-3,\n        noise_scale: float = 1.0,\n        decay: float = 0.0,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n    ):\n        super().__init__(energy_function, dtype, device)\n\n        if step_size &lt;= 0 or noise_scale &lt;= 0:\n            raise ValueError(\"step_size and noise_scale must be positive\")\n        if not 0 &lt;= decay &lt;= 1:\n            raise ValueError(\"decay must be between 0 and 1\")\n\n        if device is not None:\n            self.device = torch.device(device)\n            energy_function = energy_function.to(self.device)\n        else:\n            self.device = torch.device(\"cpu\")\n        self.dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n        self.energy_function = energy_function\n        self.step_size = step_size\n        self.noise_scale = noise_scale\n        self.decay = decay\n\n    def langevin_step(self, prev_x: torch.Tensor, noise: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Perform a single Langevin dynamics update step.\n\n        Implements the discrete Langevin equation:\n\n        $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n        Args:\n            prev_x (torch.Tensor): Current state tensor of shape (batch_size, dim)\n            noise (torch.Tensor): Gaussian noise tensor of shape (batch_size, dim)\n\n        Returns:\n            torch.Tensor: Updated state tensor of same shape as prev_x\n\n        Example:\n            ```python\n            # Single step for 10 particles in 2D space\n            current_state = torch.randn(10, 2)\n            noise = torch.randn_like(current_state)\n            next_state = langevin.langevin_step(current_state, noise)\n            ```\n        \"\"\"\n\n        # gradient_fn = partial(self.energy_function.gradient)\n        # new_x = (\n        #     prev_x\n        #     - self.step_size * gradient_fn(prev_x)\n        #     + torch.sqrt(torch.tensor(2.0 * self.step_size, device=prev_x.device))\n        #     * noise\n        # )\n        # return new_x\n\n        gradient = self.energy_function.gradient(prev_x)\n\n        # Apply noise scaling correctly\n        scaled_noise = self.noise_scale * noise\n\n        # Apply proper step size and noise scaling\n        new_x = (\n            prev_x\n            - self.step_size * gradient\n            + torch.sqrt(torch.tensor(2.0 * self.step_size, device=prev_x.device))\n            * scaled_noise\n        )\n        return new_x\n\n    @torch.no_grad()\n    def sample_chain(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        \"\"\"\n        Generate Markov chain samples using Langevin dynamics.\n\n        Args:\n            x: Initial state to start the sampling from.\n            dim: Dimension of the state space.\n            n_steps: Number of steps to take between samples.\n            n_samples: Number of samples to generate.\n            return_trajectory: Whether to return the trajectory of the samples.\n            return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n        Returns:\n            Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n                - If `return_trajectory=False` and `return_diagnostics=False`, returns the final\n                  samples of shape `(n_samples, dim)`.\n                - If `return_trajectory=True`, returns a tensor of shape `(n_samples, n_steps, dim)`,\n                  containing the sampled trajectory.\n                - If `return_diagnostics=True`, returns a tuple `(samples, diagnostics)`, where\n                  `diagnostics` is a list of dictionaries storing per-step statistics.\n\n        Raises:\n            ValueError: If input dimensions mismatch\n\n        Note:\n            - Automatically handles device placement (CPU/GPU)\n            - Uses mixed-precision training when available\n            - Diagnostics include:\n                * Mean and variance across dimensions\n                * Energy gradients\n                * Noise statistics\n\n        Example:\n            ```python\n            # Generate 100 samples from 5 parallel chains\n            samples = sampler.sample_chain(\n                dim=32,\n                n_steps=500,\n                n_samples=100,\n                return_diagnostics=True\n            )\n            ```\n\n        \"\"\"\n        if x is None:\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(self.device)  # Initial batch\n\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        if return_diagnostics:\n            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n\n        with torch.amp.autocast(\n            device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"\n        ):\n            for i in range(n_steps):\n                # Generate fresh noise for each step\n                noise = torch.randn_like(x, device=self.device)\n\n                x = self.langevin_step(x, noise)\n\n                if return_trajectory:\n                    trajectory[:, i, :] = x\n\n                if return_diagnostics:\n                    # Handle mean and variance safely regardless of batch size\n                    if n_samples &gt; 1:\n                        mean_x = x.mean(dim=0, keepdim=True)\n                        var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                        var_x = torch.clamp(var_x, min=1e-10, max=1e10)\n                    else:\n                        # For single sample, just use the value and zeros for variance\n                        mean_x = x.clone()\n                        var_x = torch.zeros_like(x)\n\n                    # Compute energy values\n                    energy = self.energy_function(x)\n\n                    # Store the diagnostics safely\n                    for b in range(n_samples):\n                        diagnostics[i, 0, b, :] = mean_x[b if n_samples &gt; 1 else 0]\n                        diagnostics[i, 1, b, :] = var_x[b if n_samples &gt; 1 else 0]\n                        diagnostics[i, 2, b, :] = energy[b].reshape(-1)\n\n        if return_trajectory:\n            if return_diagnostics:\n                return trajectory, diagnostics\n            return trajectory\n        if return_diagnostics:\n            return x, diagnostics\n        return x\n\n    def _setup_diagnostics(\n        self, dim: int, n_steps: int, n_samples: int = None\n    ) -&gt; torch.Tensor:\n        if n_samples is not None:\n            return torch.empty(\n                (n_steps, 3, n_samples, dim), device=self.device, dtype=self.dtype\n            )\n        else:\n            return torch.empty((n_steps, 3, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device(device)\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = float16 if device == 'cuda' else float32\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.step_size","title":"step_size  <code>instance-attribute</code>","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.noise_scale","title":"noise_scale  <code>instance-attribute</code>","text":"<pre><code>noise_scale = noise_scale\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.decay","title":"decay  <code>instance-attribute</code>","text":"<pre><code>decay = decay\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.langevin_step","title":"langevin_step","text":"<pre><code>langevin_step(prev_x: Tensor, noise: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Perform a single Langevin dynamics update step.</p> <p>Implements the discrete Langevin equation:</p> \\[x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t\\] <p>Parameters:</p> Name Type Description Default <code>prev_x</code> <code>Tensor</code> <p>Current state tensor of shape (batch_size, dim)</p> required <code>noise</code> <code>Tensor</code> <p>Gaussian noise tensor of shape (batch_size, dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Updated state tensor of same shape as prev_x</p> Example <pre><code># Single step for 10 particles in 2D space\ncurrent_state = torch.randn(10, 2)\nnoise = torch.randn_like(current_state)\nnext_state = langevin.langevin_step(current_state, noise)\n</code></pre> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>def langevin_step(self, prev_x: torch.Tensor, noise: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Perform a single Langevin dynamics update step.\n\n    Implements the discrete Langevin equation:\n\n    $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n    Args:\n        prev_x (torch.Tensor): Current state tensor of shape (batch_size, dim)\n        noise (torch.Tensor): Gaussian noise tensor of shape (batch_size, dim)\n\n    Returns:\n        torch.Tensor: Updated state tensor of same shape as prev_x\n\n    Example:\n        ```python\n        # Single step for 10 particles in 2D space\n        current_state = torch.randn(10, 2)\n        noise = torch.randn_like(current_state)\n        next_state = langevin.langevin_step(current_state, noise)\n        ```\n    \"\"\"\n\n    # gradient_fn = partial(self.energy_function.gradient)\n    # new_x = (\n    #     prev_x\n    #     - self.step_size * gradient_fn(prev_x)\n    #     + torch.sqrt(torch.tensor(2.0 * self.step_size, device=prev_x.device))\n    #     * noise\n    # )\n    # return new_x\n\n    gradient = self.energy_function.gradient(prev_x)\n\n    # Apply noise scaling correctly\n    scaled_noise = self.noise_scale * noise\n\n    # Apply proper step size and noise scaling\n    new_x = (\n        prev_x\n        - self.step_size * gradient\n        + torch.sqrt(torch.tensor(2.0 * self.step_size, device=prev_x.device))\n        * scaled_noise\n    )\n    return new_x\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.sample_chain","title":"sample_chain","text":"<pre><code>sample_chain(x: Optional[Tensor] = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> <p>Generate Markov chain samples using Langevin dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Initial state to start the sampling from.</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of the state space.</p> <code>10</code> <code>n_steps</code> <code>int</code> <p>Number of steps to take between samples.</p> <code>100</code> <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>1</code> <code>return_trajectory</code> <code>bool</code> <p>Whether to return the trajectory of the samples.</p> <code>False</code> <code>return_diagnostics</code> <code>bool</code> <p>Whether to return the diagnostics of the sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]: - If <code>return_trajectory=False</code> and <code>return_diagnostics=False</code>, returns the final   samples of shape <code>(n_samples, dim)</code>. - If <code>return_trajectory=True</code>, returns a tensor of shape <code>(n_samples, n_steps, dim)</code>,   containing the sampled trajectory. - If <code>return_diagnostics=True</code>, returns a tuple <code>(samples, diagnostics)</code>, where   <code>diagnostics</code> is a list of dictionaries storing per-step statistics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input dimensions mismatch</p> Note <ul> <li>Automatically handles device placement (CPU/GPU)</li> <li>Uses mixed-precision training when available</li> <li>Diagnostics include:<ul> <li>Mean and variance across dimensions</li> <li>Energy gradients</li> <li>Noise statistics</li> </ul> </li> </ul> Example <pre><code># Generate 100 samples from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=32,\n    n_steps=500,\n    n_samples=100,\n    return_diagnostics=True\n)\n</code></pre> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>@torch.no_grad()\ndef sample_chain(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    thin: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    \"\"\"\n    Generate Markov chain samples using Langevin dynamics.\n\n    Args:\n        x: Initial state to start the sampling from.\n        dim: Dimension of the state space.\n        n_steps: Number of steps to take between samples.\n        n_samples: Number of samples to generate.\n        return_trajectory: Whether to return the trajectory of the samples.\n        return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n    Returns:\n        Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n            - If `return_trajectory=False` and `return_diagnostics=False`, returns the final\n              samples of shape `(n_samples, dim)`.\n            - If `return_trajectory=True`, returns a tensor of shape `(n_samples, n_steps, dim)`,\n              containing the sampled trajectory.\n            - If `return_diagnostics=True`, returns a tuple `(samples, diagnostics)`, where\n              `diagnostics` is a list of dictionaries storing per-step statistics.\n\n    Raises:\n        ValueError: If input dimensions mismatch\n\n    Note:\n        - Automatically handles device placement (CPU/GPU)\n        - Uses mixed-precision training when available\n        - Diagnostics include:\n            * Mean and variance across dimensions\n            * Energy gradients\n            * Noise statistics\n\n    Example:\n        ```python\n        # Generate 100 samples from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=32,\n            n_steps=500,\n            n_samples=100,\n            return_diagnostics=True\n        )\n        ```\n\n    \"\"\"\n    if x is None:\n        x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n    else:\n        x = x.to(self.device)  # Initial batch\n\n    if return_trajectory:\n        trajectory = torch.empty(\n            (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n        )\n\n    if return_diagnostics:\n        diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n\n    with torch.amp.autocast(\n        device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"\n    ):\n        for i in range(n_steps):\n            # Generate fresh noise for each step\n            noise = torch.randn_like(x, device=self.device)\n\n            x = self.langevin_step(x, noise)\n\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n            if return_diagnostics:\n                # Handle mean and variance safely regardless of batch size\n                if n_samples &gt; 1:\n                    mean_x = x.mean(dim=0, keepdim=True)\n                    var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                    var_x = torch.clamp(var_x, min=1e-10, max=1e10)\n                else:\n                    # For single sample, just use the value and zeros for variance\n                    mean_x = x.clone()\n                    var_x = torch.zeros_like(x)\n\n                # Compute energy values\n                energy = self.energy_function(x)\n\n                # Store the diagnostics safely\n                for b in range(n_samples):\n                    diagnostics[i, 0, b, :] = mean_x[b if n_samples &gt; 1 else 0]\n                    diagnostics[i, 1, b, :] = var_x[b if n_samples &gt; 1 else 0]\n                    diagnostics[i, 2, b, :] = energy[b].reshape(-1)\n\n    if return_trajectory:\n        if return_diagnostics:\n            return trajectory, diagnostics\n        return trajectory\n    if return_diagnostics:\n        return x, diagnostics\n    return x\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/","title":"Torchebm &gt; Samplers &gt; Mcmc","text":""},{"location":"api/torchebm/samplers/mcmc/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/mcmc/#classes","title":"Classes","text":"<ul> <li><code>HamiltonianMonteCarlo</code> - Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc","title":"torchebm.samplers.mcmc","text":"<p>Hamiltonian Monte Carlo Sampler Module.</p> <p>This module provides a robust implementation of the Hamiltonian Monte Carlo (HMC) algorithm, a powerful Markov Chain Monte Carlo (MCMC) technique. By leveraging Hamiltonian dynamics, HMC efficiently explores complex, high-dimensional probability distributions, making it ideal for Bayesian inference and statistical modeling.</p> <p>Key Features</p> <ul> <li>Efficient sampling using Hamiltonian dynamics.</li> <li>Customizable step sizes and leapfrog steps for fine-tuned performance.</li> <li>Diagnostic tools to monitor convergence and sampling quality.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>HamiltonianMonteCarlo</code> <p>Implements the Hamiltonian Monte Carlo sampler.</p>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--usage-example","title":"Usage Example","text":"<p>Sampling from a Gaussian Distribution</p> <pre><code>from torchebm.samplers.mcmc import HamiltonianMonteCarlo\nfrom torchebm.core.energy_function import GaussianEnergy\nimport torch\n\n# Define a 2D Gaussian energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize HMC sampler\nhmc = HamiltonianMonteCarlo(energy_fn, step_size=0.1, n_leapfrog_steps=10)\n\n# Starting points for 10 chains\ninitial_state = torch.randn(10, 2)\n\n# Run sampling\nsamples, diagnostics = hmc.sample_chain(initial_state, n_steps=100, return_diagnostics=True)\nprint(f\"Samples: {samples.shape}\")\nprint(f\"Diagnostics: {diagnostics.keys()}\")\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Hamiltonian Dynamics in HMC</p> <p>HMC combines statistical sampling with concepts from classical mechanics. It introduces an auxiliary momentum variable \\( p \\) and defines a Hamiltonian:</p> \\[ H(q, p) = U(q) + K(p) \\] <ul> <li>Potential Energy: \\( U(q) = -\\log \\pi(q) \\), where \\( \\pi(q) \\) is the target distribution.</li> <li>Kinetic Energy: \\( K(p) = \\frac{1}{2} p^T M^{-1} p \\), with \\( M \\) as the mass matrix (often set to the identity matrix).</li> </ul> <p>This formulation allows HMC to propose new states by simulating trajectories along the energy landscape.</p> <p>Why Hamiltonian Dynamics?</p> <ul> <li>Efficient Exploration: HMC uses gradient information to propose new states, allowing it to explore the state space more efficiently, especially in high-dimensional and complex distributions.</li> <li>Reduced Correlation: By simulating Hamiltonian dynamics, HMC reduces the correlation between successive samples, leading to faster convergence to the target distribution.</li> <li>High Acceptance Rate: The use of Hamiltonian dynamics and a Metropolis acceptance step ensures that proposed moves are accepted with high probability, provided the numerical integration is accurate.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--leapfrog-integration","title":"Leapfrog Integration","text":"<p>Numerical Simulation of Dynamics</p> <p>HMC approximates Hamiltonian trajectories using the leapfrog integrator, a symplectic method that preserves energy. The steps are:</p> <ol> <li>Momentum Half-Step:     $$     p_{t + \\frac{\\epsilon}{2}} = p_t - \\frac{\\epsilon}{2} \\nabla U(q_t)     $$</li> <li>Position Full-Step:     $$     q_{t + 1} = q_t + \\epsilon M^{-1} p_{t + \\frac{\\epsilon}{2}}     $$</li> <li>Momentum Half-Step:     $$     p_{t + 1} = p_{t + \\frac{\\epsilon}{2}} - \\frac{\\epsilon}{2} \\nabla U(q_{t + 1})     $$</li> </ol> <p>Here, \\( \\epsilon \\) is the step size, and the process is repeated for \\( L \\) leapfrog steps.</p>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--acceptance-step","title":"Acceptance Step","text":"<p>Metropolis-Hastings Correction</p> <p>After proposing a new state \\( (q_{t + 1}, p_{t + 1}) \\), HMC applies an acceptance criterion to ensure detailed balance:</p> \\[ \\alpha = \\min \\left( 1, \\exp \\left( H(q_t, p_t) - H(q_{t + 1}, p_{t + 1}) \\right) \\right) \\] <p>The proposal is accepted with probability \\( \\alpha \\), correcting for numerical errors in the leapfrog integration.</p>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--practical-considerations","title":"Practical Considerations","text":"<p>Tuning Parameters</p> <ul> <li>Step Size (\\( \\epsilon \\)): Too large a step size can lead to unstable trajectories; too small reduces efficiency.</li> <li>Number of Leapfrog Steps (\\( L \\)): Affects the distance traveled per proposal\u2014balance exploration vs. computational cost.</li> <li>Mass Matrix (\\( M \\)): Adjusting \\( M \\) can improve sampling in distributions with varying scales.</li> </ul> <p>How to Diagnose Issues?</p> <p>Use diagnostics to check: - Acceptance rates (ideal: 0.6\u20130.8). - Energy conservation (should be relatively stable). - Autocorrelation of samples (should decrease with lag).</p> <p>Common Pitfalls</p> <ul> <li>Low Acceptance Rate: If the acceptance rate is too low, it may indicate that the step size is too large or the number of leapfrog steps is too high. Try reducing the step size or decreasing the number of leapfrog steps.</li> <li>High Correlation Between Samples: If samples are highly correlated, it may indicate that the step size is too small or the number of leapfrog steps is too few. Increase the step size or the number of leapfrog steps to improve exploration.</li> <li>Divergence or NaN Values: Numerical instability or poor parameter choices can lead to divergent behavior or NaN values. Ensure that the energy function and its gradients are correctly implemented and that parameters are appropriately scaled.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/#torchebm.samplers.mcmc--advanced-insights","title":"Advanced Insights","text":"<p>Why HMC Outperforms Other MCMC Methods</p> <p>HMC's use of gradients and dynamics reduces random-walk behavior, making it particularly effective for: - High-dimensional spaces. - Multimodal distributions (with proper tuning). - Models with strong correlations between variables.</p> Further Reading <ul> <li>Hamiltonian Mechanics Explained</li> <li>Neal, R. M. (2011). \"MCMC using Hamiltonian dynamics.\" Handbook of Markov Chain Monte Carlo.</li> </ul>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/","title":"HamiltonianMonteCarlo","text":""},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.</p> <p>This class implements the Hamiltonian Monte Carlo algorithm, which uses concepts from Hamiltonian mechanics to generate more efficient proposals than traditional random-walk methods. By introducing an auxiliary momentum variable and simulating Hamiltonian dynamics, HMC can make distant proposals with high acceptance probability, particularly in high-dimensional spaces.</p> <p>The method works by: 1. Augmenting the state space with momentum variables 2. Simulating Hamiltonian dynamics using leapfrog integration 3. Accepting or rejecting proposals using a Metropolis-Hastings criterion</p> <p>Algorithm Summary</p> <ol> <li>If <code>x</code> is not provided, initialize it with Gaussian noise.</li> <li>For each step:    a. Sample momentum from Gaussian distribution.    b. Perform leapfrog integration for <code>n_leapfrog_steps</code> steps.    c. Accept or reject the proposal based on Metropolis-Hastings criterion.</li> <li>Optionally track trajectory and diagnostics.</li> </ol> <p>Key Advantages</p> <ul> <li>Efficiency: Performs well in high dimensions by avoiding random walk behavior</li> <li>Exploration: Can efficiently traverse complex probability landscapes</li> <li>Energy Conservation: Uses symplectic integrators that approximately preserve energy</li> <li>Adaptability: Can be adjusted through mass matrices to handle varying scales</li> </ul> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>EnergyFunction</code> <p>Energy function to sample from.</p> required <code>step_size</code> <code>float</code> <p>Step size for leapfrog updates.</p> <code>0.1</code> <code>n_leapfrog_steps</code> <code>int</code> <p>Number of leapfrog steps per proposal.</p> <code>10</code> <code>mass</code> <code>Optional[Tuple[float, Tensor]]</code> <p>Optional mass matrix or scalar for momentum sampling.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type to use for computations.</p> <code>float32</code> <code>device</code> <code>Optional[Union[Tuple[str, device]]]</code> <p>Device to run computations on.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>For invalid parameter ranges</p> <p>Methods:</p> Name Description <code>_initialize_momentum</code> <p>Generate initial momentum from Gaussian distribution.</p> <code>_compute_kinetic_energy</code> <p>Compute the kinetic energy of the momentum.</p> <code>_leapfrog_step</code> <p>Perform a single leapfrog step.</p> <code>_leapfrog_integration</code> <p>Perform full leapfrog integration.</p> <code>hmc_step</code> <p>Perform one HMC step with Metropolis-Hastings acceptance.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics.</p> <p>Basic Usage</p> <pre><code># Define energy function for a 2D Gaussian\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize HMC sampler\nsampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10\n)\n\n# Sample 100 points from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=2,\n    n_steps=100,\n    n_samples=5\n)\n</code></pre> <p>Parameter Relationships</p> <ul> <li>Decreasing <code>step_size</code> improves stability but may reduce mixing.</li> <li>Increasing <code>n_leapfrog_steps</code> allows exploring more distant regions but increases computation.</li> <li>The <code>mass</code> parameter can be tuned to match the geometry of the target distribution.</li> </ul> Source code in <code>torchebm/samplers/mcmc.py</code> <pre><code>class HamiltonianMonteCarlo(BaseSampler):\n    r\"\"\"\n    Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.\n\n    This class implements the Hamiltonian Monte Carlo algorithm, which uses concepts from\n    Hamiltonian mechanics to generate more efficient proposals than traditional random-walk\n    methods. By introducing an auxiliary momentum variable and simulating Hamiltonian dynamics,\n    HMC can make distant proposals with high acceptance probability, particularly in\n    high-dimensional spaces.\n\n    The method works by:\n    1. Augmenting the state space with momentum variables\n    2. Simulating Hamiltonian dynamics using leapfrog integration\n    3. Accepting or rejecting proposals using a Metropolis-Hastings criterion\n\n    !!! note \"Algorithm Summary\"\n        1. If `x` is not provided, initialize it with Gaussian noise.\n        2. For each step:\n           a. Sample momentum from Gaussian distribution.\n           b. Perform leapfrog integration for `n_leapfrog_steps` steps.\n           c. Accept or reject the proposal based on Metropolis-Hastings criterion.\n        3. Optionally track trajectory and diagnostics.\n\n    !!! tip \"Key Advantages\"\n        - **Efficiency**: Performs well in high dimensions by avoiding random walk behavior\n        - **Exploration**: Can efficiently traverse complex probability landscapes\n        - **Energy Conservation**: Uses symplectic integrators that approximately preserve energy\n        - **Adaptability**: Can be adjusted through mass matrices to handle varying scales\n\n    Args:\n        energy_function (EnergyFunction): Energy function to sample from.\n        step_size (float): Step size for leapfrog updates.\n        n_leapfrog_steps (int): Number of leapfrog steps per proposal.\n        mass (Optional[Tuple[float, torch.Tensor]]): Optional mass matrix or scalar for momentum sampling.\n        dtype (torch.dtype): Data type to use for computations.\n        device (Optional[Union[Tuple[str, torch.device]]]): Device to run computations on.\n\n    Raises:\n        ValueError: For invalid parameter ranges\n\n    Methods:\n        _initialize_momentum(shape): Generate initial momentum from Gaussian distribution.\n        _compute_kinetic_energy(p): Compute the kinetic energy of the momentum.\n        _leapfrog_step(position, momentum, gradient_fn): Perform a single leapfrog step.\n        _leapfrog_integration(position, momentum): Perform full leapfrog integration.\n        hmc_step(current_position): Perform one HMC step with Metropolis-Hastings acceptance.\n        sample_chain(x, dim, n_steps, n_samples, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(dim, n_steps, n_samples): Initialize the diagnostics.\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Define energy function for a 2D Gaussian\n        energy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n        # Initialize HMC sampler\n        sampler = HamiltonianMonteCarlo(\n            energy_function=energy_fn,\n            step_size=0.1,\n            n_leapfrog_steps=10\n        )\n\n        # Sample 100 points from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=2,\n            n_steps=100,\n            n_samples=5\n        )\n        ```\n\n    !!! warning \"Parameter Relationships\"\n        - Decreasing `step_size` improves stability but may reduce mixing.\n        - Increasing `n_leapfrog_steps` allows exploring more distant regions but increases computation.\n        - The `mass` parameter can be tuned to match the geometry of the target distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        step_size: float = 0.1,\n        n_leapfrog_steps: int = 10,\n        mass: Optional[Tuple[float, torch.Tensor]] = None,\n        dtype: torch.Tensor = torch.float32,\n        device: Optional[Union[Tuple[str, torch.device]]] = None,\n    ):\n        \"\"\"Initialize the Hamiltonian Monte Carlo sampler.\n\n        Args:\n            energy_function: Energy function to sample from.\n            step_size: Step size for leapfrog integration (epsilon in equations).\n            n_leapfrog_steps: Number of leapfrog steps per HMC trajectory.\n            mass: Optional mass parameter or matrix for momentum.\n                If float: Uses scalar mass for all dimensions.\n                If Tensor: Uses diagonal mass matrix.\n                If None: Uses identity mass matrix.\n            dtype: Data type for computations.\n            device: Device to run computations on (\"cpu\" or \"cuda\").\n\n        Raises:\n            ValueError: If step_size or n_leapfrog_steps is non-positive.\n        \"\"\"\n        super().__init__(energy_function=energy_function, dtype=dtype, device=device)\n        if step_size &lt;= 0:\n            raise ValueError(\"step_size must be positive\")\n        if n_leapfrog_steps &lt;= 0:\n            raise ValueError(\"n_leapfrog_steps must be positive\")\n\n        # Ensure device consistency: convert device to torch.device and move energy_function\n        if device is not None:\n            self.device = torch.device(device)\n            energy_function = energy_function.to(self.device)\n        else:\n            self.device = torch.device(\"cpu\")\n\n        self.dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n        self.step_size = step_size\n        self.n_leapfrog_steps = n_leapfrog_steps\n        self.energy_function = energy_function\n        if mass is not None and not isinstance(mass, float):\n            self.mass = mass.to(self.device)\n        else:\n            self.mass = mass\n\n    def _initialize_momentum(self, shape: torch.Size) -&gt; torch.Tensor:\n        \"\"\"Initialize momentum variables from Gaussian distribution.\n\n        For HMC, momentum variables are sampled from a multivariate Gaussian distribution\n        determined by the mass matrix. The kinetic energy is then:\n        K(p) = p^T M^(-1) p / 2\n\n        Args:\n            shape: Size of the momentum tensor to generate.\n\n        Returns:\n            Momentum tensor drawn from appropriate Gaussian distribution.\n\n        Note:\n            When using a mass matrix M, we sample from N(0, M) rather than\n            transforming samples from N(0, I).\n        \"\"\"\n        p = torch.randn(shape, dtype=self.dtype, device=self.device)\n\n        if self.mass is not None:\n            # Apply mass matrix (equivalent to sampling from N(0, M))\n            if isinstance(self.mass, float):\n                p = p * torch.sqrt(\n                    torch.tensor(self.mass, dtype=self.dtype, device=self.device)\n                )\n            else:\n                mass_sqrt = torch.sqrt(self.mass)\n                p = p * mass_sqrt.view(*([1] * (len(shape) - 1)), -1).expand_as(p)\n        return p\n\n    def _compute_kinetic_energy(self, p: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the kinetic energy given momentum.\n\n        The kinetic energy is defined as:\n        $$ K(p) = p^T M^(-1) p / 2 $$\n\n        Args:\n            p: Momentum tensor.\n\n        Returns:\n            Kinetic energy for each sample in the batch.\n        \"\"\"\n        if self.mass is None:\n            return 0.5 * torch.sum(p**2, dim=-1)\n        elif isinstance(self.mass, float):\n            return 0.5 * torch.sum(p**2, dim=-1) / self.mass\n        else:\n            return 0.5 * torch.sum(\n                p**2 / self.mass.view(*([1] * (len(p.shape) - 1)), -1), dim=-1\n            )\n\n    def _leapfrog_step(\n        self, position: torch.Tensor, momentum: torch.Tensor, gradient_fn: Callable\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a single leapfrog integration step.\n\n        Implements the symplectic leapfrog integrator for Hamiltonian dynamics:\n        1. Half-step momentum update: p(t+\u03b5/2) = p(t) - (\u03b5/2)\u2207U(q(t))\n        2. Full-step position update: q(t+\u03b5) = q(t) + \u03b5M^(-1)p(t+\u03b5/2)\n        3. Half-step momentum update: p(t+\u03b5) = p(t+\u03b5/2) - (\u03b5/2)\u2207U(q(t+\u03b5))\n\n        Args:\n            position: Current position tensor.\n            momentum: Current momentum tensor.\n            gradient_fn: Function to compute gradient of potential energy.\n\n        Returns:\n            Tuple of (new_position, new_momentum).\n        \"\"\"\n        # Calculate gradient for half-step momentum update with numerical safeguards\n        grad = gradient_fn(position)\n        # Clip extreme gradient values to prevent instability\n        grad = torch.clamp(grad, min=-1e6, max=1e6)\n\n        # Half-step momentum update\n        p_half = momentum - 0.5 * self.step_size * grad\n\n        # Full-step position update with mass matrix adjustment\n        if self.mass is None:\n            x_new = position + self.step_size * p_half\n        else:\n            if isinstance(self.mass, float):\n                # Ensure mass is positive to avoid division issues\n                safe_mass = max(self.mass, 1e-10)\n                x_new = position + self.step_size * p_half / safe_mass\n            else:\n                # Create safe mass tensor avoiding zeros or negative values\n                safe_mass = torch.clamp(self.mass, min=1e-10)\n                x_new = position + self.step_size * p_half / safe_mass.view(\n                    *([1] * (len(position.shape) - 1)), -1\n                )\n\n        # Half-step momentum update with gradient clamping\n        grad_new = gradient_fn(x_new)\n        grad_new = torch.clamp(grad_new, min=-1e6, max=1e6)\n        p_new = p_half - 0.5 * self.step_size * grad_new\n\n        return x_new, p_new\n\n    def _leapfrog_integration(\n        self, position: torch.Tensor, momentum: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a full leapfrog integration for n_leapfrog_steps.\n\n        Applies multiple leapfrog steps to simulate Hamiltonian dynamics\n        for a trajectory of specified length. This is the core of the HMC\n        proposal generation.\n\n        Args:\n            position: Initial position tensor.\n            momentum: Initial momentum tensor.\n\n        Returns:\n            Tuple of (final_position, final_momentum) after integration.\n        \"\"\"\n        gradient_fn = partial(self.energy_function.gradient)\n        x = position\n        p = momentum\n\n        # Add check for NaN values before starting integration\n        if torch.isnan(x).any() or torch.isnan(p).any():\n            # Replace NaN values with zeros\n            x = torch.nan_to_num(x, nan=0.0)\n            p = torch.nan_to_num(p, nan=0.0)\n\n        for _ in range(self.n_leapfrog_steps):\n            x, p = self._leapfrog_step(x, p, gradient_fn)\n\n            # Check for NaN values after each step\n            if torch.isnan(x).any() or torch.isnan(p).any():\n                # If NaN values appear, break the integration\n                # Replace NaN with zeros and return current state\n                x = torch.nan_to_num(x, nan=0.0)\n                p = torch.nan_to_num(p, nan=0.0)\n                break\n\n        return x, p\n\n    def hmc_step(\n        self, current_position: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a single HMC step with Metropolis-Hastings acceptance.\n\n        This implements the core HMC algorithm:\n        1. Sample initial momentum\n        2. Compute initial Hamiltonian\n        3. Perform leapfrog integration to propose new state\n        4. Compute final Hamiltonian\n        5. Accept/reject based on Metropolis-Hastings criterion\n\n        Args:\n            current_position: Current position tensor of shape (batch_size, dim).\n\n        Returns:\n            Tuple containing:\n            - new_position: Updated position tensor\n            - acceptance_prob: Probability of accepting each proposal\n            - accepted: Boolean mask indicating which proposals were accepted\n        \"\"\"\n        batch_size = current_position.shape[0]\n\n        # Sample initial momentum\n        current_momentum = self._initialize_momentum(current_position.shape)\n\n        # Compute current Hamiltonian: H = U(q) + K(p)\n        # Add numerical stability with clamping\n        current_energy = self.energy_function(current_position)\n        current_energy = torch.clamp(\n            current_energy, min=-1e10, max=1e10\n        )  # Prevent extreme energy values\n\n        current_kinetic = self._compute_kinetic_energy(current_momentum)\n        current_kinetic = torch.clamp(\n            current_kinetic, min=0, max=1e10\n        )  # Kinetic energy should be non-negative\n\n        current_hamiltonian = current_energy + current_kinetic\n\n        # Perform leapfrog integration to get proposal\n        proposed_position, proposed_momentum = self._leapfrog_integration(\n            current_position, current_momentum\n        )\n\n        # Compute proposed Hamiltonian with similar numerical stability\n        proposed_energy = self.energy_function(proposed_position)\n        proposed_energy = torch.clamp(proposed_energy, min=-1e10, max=1e10)\n\n        proposed_kinetic = self._compute_kinetic_energy(proposed_momentum)\n        proposed_kinetic = torch.clamp(proposed_kinetic, min=0, max=1e10)\n\n        proposed_hamiltonian = proposed_energy + proposed_kinetic\n\n        # Metropolis-Hastings acceptance criterion\n        # Clamp hamiltonian_diff to avoid overflow in exp()\n        hamiltonian_diff = current_hamiltonian - proposed_hamiltonian\n        hamiltonian_diff = torch.clamp(hamiltonian_diff, max=50, min=-50)\n\n        acceptance_prob = torch.min(\n            torch.ones(batch_size, device=self.device), torch.exp(hamiltonian_diff)\n        )\n\n        # Accept/reject based on acceptance probability\n        random_uniform = torch.rand(batch_size, device=self.device)\n        accepted = random_uniform &lt; acceptance_prob\n        accepted_mask = accepted.float().view(\n            -1, *([1] * (len(current_position.shape) - 1))\n        )\n\n        # Update position based on acceptance\n        new_position = (\n            accepted_mask * proposed_position + (1.0 - accepted_mask) * current_position\n        )\n\n        return new_position, acceptance_prob, accepted\n\n    @torch.no_grad()\n    def sample_chain(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = None,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Generate samples using Hamiltonian Monte Carlo.\n\n        Runs an HMC chain for a specified number of steps, optionally returning\n        the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian\n        dynamics with leapfrog integration to propose samples efficiently, particularly\n        in high-dimensional spaces.\n\n        Args:\n            x: Initial state to start sampling from. If None, random initialization is used.\n            dim: Dimension of the state space when x is None. If None, will attempt to infer from the energy function.\n            n_steps: Number of HMC steps to perform.\n            n_samples: Number of parallel chains to run.\n            return_trajectory: If True, return the entire trajectory of samples.\n            return_diagnostics: If True, return diagnostics about the sampling process.\n\n        Returns:\n            If return_trajectory=False and return_diagnostics=False:\n                Tensor of shape (n_samples, dim) with final samples.\n            If return_trajectory=True and return_diagnostics=False:\n                Tensor of shape (n_samples, n_steps, dim) with the trajectory of all samples.\n            If return_diagnostics=True:\n                Tuple of (samples, diagnostics) where diagnostics contains information about\n                the sampling process, including mean, variance, energy values, and acceptance rates.\n\n        Note:\n            This method uses automatic mixed precision when available on CUDA devices\n            to improve performance while maintaining numerical stability for the\n            Hamiltonian dynamics simulation.\n\n        Example:\n            ```python\n            # Run 10 parallel chains for 1000 steps\n            samples, diagnostics = hmc.sample_chain(\n                dim=10,\n                n_steps=1000,\n                n_samples=10,\n                return_diagnostics=True\n            )\n\n            # Plot acceptance rates\n            import matplotlib.pyplot as plt\n            plt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\n            plt.ylabel('Acceptance Rate')\n            plt.xlabel('Step')\n            plt.show()\n            ```\n        \"\"\"\n        if x is None:\n            # If dim is not provided, try to infer from the energy function\n            if dim is None:\n                # Check if it's GaussianEnergy which has mean attribute\n                if hasattr(self.energy_function, \"mean\"):\n                    dim = self.energy_function.mean.shape[0]\n                else:\n                    raise ValueError(\n                        \"dim must be provided when x is None and cannot be inferred from the energy function\"\n                    )\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(self.device)\n\n        # Get dimension from x for later use\n        dim = x.shape[1]\n\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        if return_diagnostics:\n            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n            acceptance_rates = torch.zeros(\n                n_steps, device=self.device, dtype=self.dtype\n            )\n\n        with torch.amp.autocast(\n            device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"\n        ):\n            for i in range(n_steps):\n                # Perform single HMC step\n                x, acceptance_prob, accepted = self.hmc_step(x)\n\n                if return_trajectory:\n                    trajectory[:, i, :] = x\n\n                if return_diagnostics:\n                    # Calculate diagnostics with numerical stability safeguards\n\n                    if n_samples &gt; 1:\n                        # mean_x = x.mean(dim=0).unsqueeze(0).expand_as(x)\n                        mean_x = x.mean(dim=0, keepdim=True)\n\n                        # Clamp variance calculations to prevent NaN values\n                        # First compute variance in a numerically stable way\n                        # and then clamp to ensure positive finite values\n                        # x_centered = x - mean_x\n                        # var_x = torch.mean(x_centered**2, dim=0)\n                        var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                        var_x = torch.clamp(\n                            var_x, min=1e-10, max=1e10\n                        )  # Prevent zero/extreme variances\n                        # var_x = var_x.unsqueeze(0).expand_as(x)\n                    else:\n                        # For single sample, mean and variance are trivial\n                        mean_x = x.clone()\n                        var_x = torch.zeros_like(x)\n\n                    # Energy values (ensure finite values)\n                    energy = self.energy_function(\n                        x\n                    )  # assumed to have shape (n_samples,)\n                    energy = torch.clamp(\n                        energy, min=-1e10, max=1e10\n                    )  # Prevent extreme energy values\n                    energy = energy.unsqueeze(1).expand_as(x)\n\n                    # Acceptance rate is already between 0 and 1\n                    acceptance_rate = accepted.float().mean()\n                    acceptance_rate_expanded = torch.ones_like(x) * acceptance_rate\n\n                    # Stack diagnostics\n                    diagnostics[i, 0, :, :] = mean_x\n                    diagnostics[i, 1, :, :] = var_x\n                    diagnostics[i, 2, :, :] = energy\n                    diagnostics[i, 3, :, :] = acceptance_rate_expanded\n\n        if return_trajectory:\n            if return_diagnostics:\n                return trajectory, diagnostics  # , acceptance_rates\n            return trajectory\n\n        if return_diagnostics:\n            return x, diagnostics  # , acceptance_rates\n\n        return x\n\n    def _setup_diagnostics(\n        self, dim: int, n_steps: int, n_samples: int = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Initialize diagnostics tensor to track HMC sampling metrics.\n\n        Creates a tensor to store diagnostics information during sampling, including:\n        - Mean of samples (dimension 0)\n        - Variance of samples (dimension 1)\n        - Energy values (dimension 2)\n        - Acceptance rates (dimension 3)\n\n        Args:\n            dim: Dimensionality of the state space.\n            n_steps: Number of sampling steps.\n            n_samples: Number of parallel chains (if None, assumed to be 1).\n\n        Returns:\n            Empty tensor of shape (n_steps, 4, n_samples, dim) to store diagnostics.\n        \"\"\"\n        if n_samples is not None:\n            return torch.empty(\n                (n_steps, 4, n_samples, dim), device=self.device, dtype=self.dtype\n            )\n        else:\n            return torch.empty((n_steps, 4, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device(device)\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = float16 if device == 'cuda' else float32\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.step_size","title":"step_size  <code>instance-attribute</code>","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.n_leapfrog_steps","title":"n_leapfrog_steps  <code>instance-attribute</code>","text":"<pre><code>n_leapfrog_steps = n_leapfrog_steps\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.mass","title":"mass  <code>instance-attribute</code>","text":"<pre><code>mass = to(device)\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.hmc_step","title":"hmc_step","text":"<pre><code>hmc_step(current_position: Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Perform a single HMC step with Metropolis-Hastings acceptance.</p> <p>This implements the core HMC algorithm: 1. Sample initial momentum 2. Compute initial Hamiltonian 3. Perform leapfrog integration to propose new state 4. Compute final Hamiltonian 5. Accept/reject based on Metropolis-Hastings criterion</p> <p>Parameters:</p> Name Type Description Default <code>current_position</code> <code>Tensor</code> <p>Current position tensor of shape (batch_size, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple containing:</p> <code>Tensor</code> <ul> <li>new_position: Updated position tensor</li> </ul> <code>Tensor</code> <ul> <li>acceptance_prob: Probability of accepting each proposal</li> </ul> <code>Tuple[Tensor, Tensor, Tensor]</code> <ul> <li>accepted: Boolean mask indicating which proposals were accepted</li> </ul> Source code in <code>torchebm/samplers/mcmc.py</code> <pre><code>def hmc_step(\n    self, current_position: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Perform a single HMC step with Metropolis-Hastings acceptance.\n\n    This implements the core HMC algorithm:\n    1. Sample initial momentum\n    2. Compute initial Hamiltonian\n    3. Perform leapfrog integration to propose new state\n    4. Compute final Hamiltonian\n    5. Accept/reject based on Metropolis-Hastings criterion\n\n    Args:\n        current_position: Current position tensor of shape (batch_size, dim).\n\n    Returns:\n        Tuple containing:\n        - new_position: Updated position tensor\n        - acceptance_prob: Probability of accepting each proposal\n        - accepted: Boolean mask indicating which proposals were accepted\n    \"\"\"\n    batch_size = current_position.shape[0]\n\n    # Sample initial momentum\n    current_momentum = self._initialize_momentum(current_position.shape)\n\n    # Compute current Hamiltonian: H = U(q) + K(p)\n    # Add numerical stability with clamping\n    current_energy = self.energy_function(current_position)\n    current_energy = torch.clamp(\n        current_energy, min=-1e10, max=1e10\n    )  # Prevent extreme energy values\n\n    current_kinetic = self._compute_kinetic_energy(current_momentum)\n    current_kinetic = torch.clamp(\n        current_kinetic, min=0, max=1e10\n    )  # Kinetic energy should be non-negative\n\n    current_hamiltonian = current_energy + current_kinetic\n\n    # Perform leapfrog integration to get proposal\n    proposed_position, proposed_momentum = self._leapfrog_integration(\n        current_position, current_momentum\n    )\n\n    # Compute proposed Hamiltonian with similar numerical stability\n    proposed_energy = self.energy_function(proposed_position)\n    proposed_energy = torch.clamp(proposed_energy, min=-1e10, max=1e10)\n\n    proposed_kinetic = self._compute_kinetic_energy(proposed_momentum)\n    proposed_kinetic = torch.clamp(proposed_kinetic, min=0, max=1e10)\n\n    proposed_hamiltonian = proposed_energy + proposed_kinetic\n\n    # Metropolis-Hastings acceptance criterion\n    # Clamp hamiltonian_diff to avoid overflow in exp()\n    hamiltonian_diff = current_hamiltonian - proposed_hamiltonian\n    hamiltonian_diff = torch.clamp(hamiltonian_diff, max=50, min=-50)\n\n    acceptance_prob = torch.min(\n        torch.ones(batch_size, device=self.device), torch.exp(hamiltonian_diff)\n    )\n\n    # Accept/reject based on acceptance probability\n    random_uniform = torch.rand(batch_size, device=self.device)\n    accepted = random_uniform &lt; acceptance_prob\n    accepted_mask = accepted.float().view(\n        -1, *([1] * (len(current_position.shape) - 1))\n    )\n\n    # Update position based on acceptance\n    new_position = (\n        accepted_mask * proposed_position + (1.0 - accepted_mask) * current_position\n    )\n\n    return new_position, acceptance_prob, accepted\n</code></pre>"},{"location":"api/torchebm/samplers/mcmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.mcmc.HamiltonianMonteCarlo.sample_chain","title":"sample_chain","text":"<pre><code>sample_chain(x: Optional[Tensor] = None, dim: int = None, n_steps: int = 100, n_samples: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Generate samples using Hamiltonian Monte Carlo.</p> <p>Runs an HMC chain for a specified number of steps, optionally returning the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian dynamics with leapfrog integration to propose samples efficiently, particularly in high-dimensional spaces.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Initial state to start sampling from. If None, random initialization is used.</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of the state space when x is None. If None, will attempt to infer from the energy function.</p> <code>None</code> <code>n_steps</code> <code>int</code> <p>Number of HMC steps to perform.</p> <code>100</code> <code>n_samples</code> <code>int</code> <p>Number of parallel chains to run.</p> <code>1</code> <code>return_trajectory</code> <code>bool</code> <p>If True, return the entire trajectory of samples.</p> <code>False</code> <code>return_diagnostics</code> <code>bool</code> <p>If True, return diagnostics about the sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>If return_trajectory=False and return_diagnostics=False: Tensor of shape (n_samples, dim) with final samples.</p> <code>Tensor</code> <p>If return_trajectory=True and return_diagnostics=False: Tensor of shape (n_samples, n_steps, dim) with the trajectory of all samples.</p> <code>Tuple[Tensor, Tensor]</code> <p>If return_diagnostics=True: Tuple of (samples, diagnostics) where diagnostics contains information about the sampling process, including mean, variance, energy values, and acceptance rates.</p> Note <p>This method uses automatic mixed precision when available on CUDA devices to improve performance while maintaining numerical stability for the Hamiltonian dynamics simulation.</p> Example <pre><code># Run 10 parallel chains for 1000 steps\nsamples, diagnostics = hmc.sample_chain(\n    dim=10,\n    n_steps=1000,\n    n_samples=10,\n    return_diagnostics=True\n)\n\n# Plot acceptance rates\nimport matplotlib.pyplot as plt\nplt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\nplt.ylabel('Acceptance Rate')\nplt.xlabel('Step')\nplt.show()\n</code></pre> Source code in <code>torchebm/samplers/mcmc.py</code> <pre><code>@torch.no_grad()\ndef sample_chain(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = None,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Generate samples using Hamiltonian Monte Carlo.\n\n    Runs an HMC chain for a specified number of steps, optionally returning\n    the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian\n    dynamics with leapfrog integration to propose samples efficiently, particularly\n    in high-dimensional spaces.\n\n    Args:\n        x: Initial state to start sampling from. If None, random initialization is used.\n        dim: Dimension of the state space when x is None. If None, will attempt to infer from the energy function.\n        n_steps: Number of HMC steps to perform.\n        n_samples: Number of parallel chains to run.\n        return_trajectory: If True, return the entire trajectory of samples.\n        return_diagnostics: If True, return diagnostics about the sampling process.\n\n    Returns:\n        If return_trajectory=False and return_diagnostics=False:\n            Tensor of shape (n_samples, dim) with final samples.\n        If return_trajectory=True and return_diagnostics=False:\n            Tensor of shape (n_samples, n_steps, dim) with the trajectory of all samples.\n        If return_diagnostics=True:\n            Tuple of (samples, diagnostics) where diagnostics contains information about\n            the sampling process, including mean, variance, energy values, and acceptance rates.\n\n    Note:\n        This method uses automatic mixed precision when available on CUDA devices\n        to improve performance while maintaining numerical stability for the\n        Hamiltonian dynamics simulation.\n\n    Example:\n        ```python\n        # Run 10 parallel chains for 1000 steps\n        samples, diagnostics = hmc.sample_chain(\n            dim=10,\n            n_steps=1000,\n            n_samples=10,\n            return_diagnostics=True\n        )\n\n        # Plot acceptance rates\n        import matplotlib.pyplot as plt\n        plt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\n        plt.ylabel('Acceptance Rate')\n        plt.xlabel('Step')\n        plt.show()\n        ```\n    \"\"\"\n    if x is None:\n        # If dim is not provided, try to infer from the energy function\n        if dim is None:\n            # Check if it's GaussianEnergy which has mean attribute\n            if hasattr(self.energy_function, \"mean\"):\n                dim = self.energy_function.mean.shape[0]\n            else:\n                raise ValueError(\n                    \"dim must be provided when x is None and cannot be inferred from the energy function\"\n                )\n        x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n    else:\n        x = x.to(self.device)\n\n    # Get dimension from x for later use\n    dim = x.shape[1]\n\n    if return_trajectory:\n        trajectory = torch.empty(\n            (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n        )\n\n    if return_diagnostics:\n        diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n        acceptance_rates = torch.zeros(\n            n_steps, device=self.device, dtype=self.dtype\n        )\n\n    with torch.amp.autocast(\n        device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"\n    ):\n        for i in range(n_steps):\n            # Perform single HMC step\n            x, acceptance_prob, accepted = self.hmc_step(x)\n\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n            if return_diagnostics:\n                # Calculate diagnostics with numerical stability safeguards\n\n                if n_samples &gt; 1:\n                    # mean_x = x.mean(dim=0).unsqueeze(0).expand_as(x)\n                    mean_x = x.mean(dim=0, keepdim=True)\n\n                    # Clamp variance calculations to prevent NaN values\n                    # First compute variance in a numerically stable way\n                    # and then clamp to ensure positive finite values\n                    # x_centered = x - mean_x\n                    # var_x = torch.mean(x_centered**2, dim=0)\n                    var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                    var_x = torch.clamp(\n                        var_x, min=1e-10, max=1e10\n                    )  # Prevent zero/extreme variances\n                    # var_x = var_x.unsqueeze(0).expand_as(x)\n                else:\n                    # For single sample, mean and variance are trivial\n                    mean_x = x.clone()\n                    var_x = torch.zeros_like(x)\n\n                # Energy values (ensure finite values)\n                energy = self.energy_function(\n                    x\n                )  # assumed to have shape (n_samples,)\n                energy = torch.clamp(\n                    energy, min=-1e10, max=1e10\n                )  # Prevent extreme energy values\n                energy = energy.unsqueeze(1).expand_as(x)\n\n                # Acceptance rate is already between 0 and 1\n                acceptance_rate = accepted.float().mean()\n                acceptance_rate_expanded = torch.ones_like(x) * acceptance_rate\n\n                # Stack diagnostics\n                diagnostics[i, 0, :, :] = mean_x\n                diagnostics[i, 1, :, :] = var_x\n                diagnostics[i, 2, :, :] = energy\n                diagnostics[i, 3, :, :] = acceptance_rate_expanded\n\n    if return_trajectory:\n        if return_diagnostics:\n            return trajectory, diagnostics  # , acceptance_rates\n        return trajectory\n\n    if return_diagnostics:\n        return x, diagnostics  # , acceptance_rates\n\n    return x\n</code></pre>"},{"location":"api/torchebm/utils/","title":"Torchebm &gt; Utils","text":""},{"location":"api/torchebm/utils/#contents","title":"Contents","text":""},{"location":"api/torchebm/utils/#modules","title":"Modules","text":"<ul> <li>Visualization</li> </ul>"},{"location":"api/torchebm/utils/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/utils/#torchebm.utils","title":"torchebm.utils","text":""},{"location":"api/torchebm/utils/visualization/","title":"Torchebm &gt; Utils &gt; Visualization","text":""},{"location":"api/torchebm/utils/visualization/#contents","title":"Contents","text":""},{"location":"api/torchebm/utils/visualization/#functions","title":"Functions","text":"<ul> <li><code>plot_2d_energy_landscape()</code> - Plot a 2D energy landscape.</li> <li><code>plot_3d_energy_landscape()</code> - Plot a 3D surface visualization of a 2D energy landscape.</li> <li><code>plot_sample_trajectories()</code> - Plot sample trajectories, optionally on an energy landscape background.</li> <li><code>plot_samples_on_energy()</code> - Plot samples on a 2D energy landscape.</li> </ul>"},{"location":"api/torchebm/utils/visualization/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/utils/visualization/#torchebm.utils.visualization","title":"torchebm.utils.visualization","text":""},{"location":"blog/","title":"TorchEBM Blog","text":"<p>Welcome to the TorchEBM blog! Here you'll find latest news, tutorials, research insights, and updates about the project.</p>"},{"location":"blog/#categories","title":"Categories","text":"<ul> <li> <p> Tutorials</p> <p>Step-by-step guides to help you learn how to use TorchEBM effectively.</p> <p> View Tutorials</p> </li> <li> <p> Research</p> <p>Cutting-edge research using energy-based models and insights from the field.</p> <p> View Research</p> </li> <li> <p> Announcements</p> <p>Updates about TorchEBM releases, features, and roadmap.</p> <p> View Announcements</p> </li> <li> <p> Examples</p> <p>Real-world examples and case studies using TorchEBM.</p> <p> View Examples</p> </li> </ul>"},{"location":"blog/#recent-posts","title":"Recent Posts","text":""},{"location":"blog/hamiltonian-mechanics/","title":"Hamiltonian Mechanics","text":"<p> Hamiltonian mechanics is a way to describe how physical systems, like planets or pendulums, move over  time, focusing on energy rather than just forces. By reframing complex dynamics through energy lenses,  this 19th-century physics framework now powers cutting-edge generative AI. It uses generalized coordinates \\( q \\) (like position) and their  conjugate momenta \\( p \\) (related to momentum), forming a phase space that captures the system's state. This approach  is particularly useful for complex systems with many parts, making it easier to find patterns and conservation laws.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#mathematical-reformation-from-second-order-to-phase-flow","title":"Mathematical Reformation: From Second-Order to Phase Flow","text":"<p>Newton's \\( F = m\\ddot{q} \\) requires solving second-order differential equations, which become unwieldy for constrained systems or when identifying conserved quantities. </p> <p>The Core Idea</p> <p>Hamiltonian mechanics splits \\( \\ddot{q} = F(q)/m \\) into two first-order equations by introducing conjugate momentum \\( p \\):</p> \\[\\begin{align*} \\dot{q} = \\frac{\\partial H}{\\partial p} &amp; \\text{(Position)}, \\quad \\dot{p} = -\\frac{\\partial H}{\\partial q} &amp; \\text{(Momentum)} \\end{align*}\\] <p>It decomposes acceleration into complementary momentum/position flows. This phase space perspective reveals hidden geometric structure.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#lagrangian-prelude-action-principles","title":"Lagrangian Prelude: Action Principles","text":"<p>The Lagrangian \\( \\mathcal{L}(q, \\dot{q}) = K - U \\) leads to Euler-Lagrange equations via variational calculus: $$ \\frac{d}{dt}\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}} \\right) - \\frac{\\partial \\mathcal{L}}{\\partial q} = 0 $$</p> <p>Kinetic Energy Symbol</p> <p>Note that the \\( K \\) in the \\( \\mathcal{L}(q, \\dot{q}) = K - U \\) is also represented as \\( T \\).</p> <p>But these remain second-order. The critical leap comes through Legendre Transformation \\( (\\dot{q} \\rightarrow p) \\). The Hamiltonian is derived from the Lagrangian through a Legendre transformation by defining the conjugate momentum  as \\( p_i = \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}_i} \\); then the Hamiltonian can be written as: $$ H(q,p) = \\sum_i p_i \\dot{q}_i - \\mathcal{L}(q, \\dot{q}) $$</p> Annotated \\( H(q,p) \\) \\[ H(q,p) = \\sum_i \\underbrace{p_i}_{\\text{Conjugate Momentum}} \\underbrace{\\dot{q}_i}_{\\text{Generalized Velocity}} - \\underbrace{\\mathcal{L}(q, \\dot{q})}_{\\text{Lagrangian}} \\] <p>We can write \\( H(q,p) \\) more intuitively as: $$ H(q,p) = K(p) + U(q) $$</p> <p>Proof?</p> <p>T is the Kinetic energy; for simplicity, I'll replace it with K.</p> <p>The negative sign arises because we are subtracting the Lagrangian from the sum of the products of momenta and  velocities. This ensures that the Hamiltonian represents the total energy of the system for conservative systems,  where the Lagrangian is \\( K - U \\) and the Hamiltonian becomes \\( K + U \\).</p> <p>For a simple system where \\( K = \\frac{1}{2}m\\dot{q}^2 \\) and \\( U = U(q) \\), the Hamiltonian would be:</p> <ul> <li>Kinetic Energy: \\( K = \\frac{1}{2}m\\dot{q}^2 \\)</li> <li>Potential Energy: \\( U = U(q) \\)</li> <li>Lagrangian: \\( \\mathcal{L} = \\frac{1}{2}m\\dot{q}^2 - U(q) \\)</li> <li>Conjugate Momentum: \\( p = m\\dot{q} \\)</li> <li>Hamiltonian: \\( H = p\\dot{q} - \\mathcal{L} = p\\frac{p}{m} - \\left(\\frac{1}{2}m\\left(\\frac{p}{m}\\right)^2 - U(q)\\right) = \\frac{p^2}{m} - \\frac{p^2}{2m} + U(q) = \\frac{p^2}{2m} + U(q) = K(p) + U(q) \\)</li> </ul> <p>This flips the script: instead of \\( \\dot{q} \\)-centric dynamics, we get symplectic phase flow.</p> <p>Why This Matters</p> <p>The Hamiltonian becomes the system's total energy \\( H = K + U \\) for many physical systems.  It also provides a framework where time evolution is a canonical transformation -  a symmetry preserving the fundamental Poisson bracket structure \\( \\{q_i, p_j\\} = \\delta_{ij} \\).</p> Canonical and Non-Canonical Transformations <p>A canonical transformation is a change of variables that preserves the form of Hamilton's equations. It's like changing the map projection without altering the landscape.</p> <p>Consider a simple translation: $$ Q = q + a, \\quad P = p + b $$ This transformation preserves the Hamiltonian structure and Poisson bracket: \\( \\{Q, P\\} = \\{q + a, p + b\\} = \\{q, p\\} = 1 = \\delta_{ij} \\)</p> <ul> <li>Preserves Hamiltonian structure.</li> <li>Maintains Poisson bracket invariance.</li> <li>Time evolution can be viewed as a canonical transformation.</li> </ul> <p>On the other hand, non-canonical transformation changes the form of Hamilton's equations.</p> <p>For example, consider the transformation:</p> \\[ Q = q^3, \\quad P = p^3 \\] <p>The Poisson bracket is:</p> <p>\\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = 3q^2 \\cdot 3p^2 - 0 = 9q^2p^2 \\neq 1 \\)</p> How to calculate those formula? <p>The Poisson bracket of two functions \\( f \\) and \\( g \\) is defined as: \\( \\{f, g\\} = \\sum_i \\left( \\frac{\\partial f}{\\partial q_i} \\frac{\\partial g}{\\partial p_i} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i} \\right) \\)</p> <p>Transformation 1: \\( Q = q + a, \\quad P = p + b \\)</p> <p>Partial Derivatives: </p> <ul> <li>\\( \\frac{\\partial Q}{\\partial q} = 1 \\)</li> <li>\\( \\frac{\\partial Q}{\\partial p} = 0 \\)</li> <li>\\( \\frac{\\partial P}{\\partial q} = 0 \\)</li> <li>\\( \\frac{\\partial P}{\\partial p} = 1 \\)</li> </ul> <p>These derivatives can be represented in matrix form as: \\( \\frac{\\partial (Q, P)}{\\partial (q, p)} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\)</p> <p>This is a diagonal identity matrix, indicating that the transformation preserves the original structure.</p> <p>Poisson Bracket Calculation \\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = (1)(1) - (0)(0) = 1 \\)</p> <p>Transformation 2: \\( Q = q^3, \\quad P = p^3 \\)</p> <p>Partial Derivatives - \\( \\frac{\\partial Q}{\\partial q} = 3q^2 \\) - \\( \\frac{\\partial Q}{\\partial p} = 0 \\) - \\( \\frac{\\partial P}{\\partial q} = 0 \\) - \\( \\frac{\\partial P}{\\partial p} = 3p^2 \\)</p> <p>These derivatives can be represented as: \\( \\frac{\\partial (Q, P)}{\\partial (q, p)} = \\begin{pmatrix} 3q^2 &amp; 0 \\\\ 0 &amp; 3p^2 \\end{pmatrix} \\)</p> <p>This is a diagonal matrix but not the identity matrix, indicating that the transformation does not preserve the original structure.</p> <p>Poisson Bracket Calculation \\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = (3q^2)(3p^2) - (0)(0) = 9q^2p^2 \\)</p> <ul> <li>Transformation 1 preserves the Poisson bracket structure because it results in a constant value of 1, represented by an identity matrix.</li> <li>Transformation 2 does not preserve the Poisson bracket structure because the result depends on \\( q \\) and \\( p \\), represented by a non-identity diagonal matrix.</li> </ul> <p>This transformation is not canonical because it does not preserve the Poisson bracket structure.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#newton-vs-lagrange-vs-hamilton-a-philosophical-showdown","title":"Newton vs. Lagrange vs. Hamilton: A Philosophical Showdown","text":"Aspect Newtonian Lagrangian Hamiltonian State Variables Position \\( x \\) and velocity \\( \\dot{x} \\) Generalized coordinates \\( q \\) and velocities \\( \\dot{q} \\) Generalized coordinates \\( q \\) and conjugate momenta \\( p \\) Formulation Second-order differential equations \\( (F=ma) \\) Principle of least action (\\( \\delta \\int L \\, dt = 0 \\)) First-order differential equations from Hamiltonian function (Phase flow \\( (dH) \\)) Identifying Symmetries Manual identification or through specific methods Noether's theorem Canonical transformations and Poisson brackets Machine Learning Connection Physics-informed neural networks, simulations Optimal control, reinforcement learning Hamiltonian Monte Carlo (HMC) sampling, energy-based models Energy Conservation Not inherent (must be derived) Built-in through conservation laws Central (Hamiltonian is energy) General Coordinates Possible, but often cumbersome Natural fit Natural fit Time Reversibility Yes Yes Yes, especially in symplectic formulations","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltons-equations-the-geometry-of-phase-space","title":"Hamilton's Equations: The Geometry of Phase Space","text":"<p>The phase space is a mathematical space where we can represent the set of possible states of a physical system. For a system with \\( n \\) degrees of freedom, the phase space is a \\( 2n \\)-dimensional space, often visualized as a map where each point \\( (q, p) \\) represents a unique state. The evolution of the system is described by the motion of a point in this space, governed by Hamilton's equations.</p>      Your browser does not support the video tag.        Phase space portrait of a nonlinear pendulum showing oscillatory motion (closed orbits), rotational motion (wavy trajectories), and separatrices (red curves) connecting unstable equilibrium points. Position (q) and momentum (p) dynamics illustrate energy conservation principles fundamental to Hamiltonian systems.   <p>This formulation offers several advantages. It makes it straightforward to identify conserved quantities and symmetries through canonical transformations and Poisson brackets, which provides deeper insights into the system's behavior. For instance, Liouville's theorem states that the volume in phase space occupied by an ensemble of systems remains constant over time, expressed as:</p> \\[ \\frac{\\partial \\rho}{\\partial t} + \\{\\rho, H\\} = 0 \\] <p>or equivalently:</p> \\[ \\frac{\\partial \\rho}{\\partial t} + \\sum_i \\left(\\frac{\\partial \\rho}{\\partial q_i}\\frac{\\partial H}{\\partial p_i} - \\frac{\\partial \\rho}{\\partial p_i}\\frac{\\partial H}{\\partial q_i}\\right) = 0 \\] <p>where \\( \\rho(q, p, t) \\) is the density function. This helps us to represent the phase space flows and how they  preserve area under symplectic transformations. Its relation to symplectic geometry enables mathematical properties that are directly relevant to many numerical methods. For instance, it enables Hamiltonian Monte Carlo to perform well in high-dimensions by defining MCMC strategies that increases the chances of accepting a sample (particle). </p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplecticity-the-sacred-invariant","title":"Symplecticity: The Sacred Invariant","text":"<p>Hamiltonian flows preserve the symplectic 2-form \\( \\omega = \\sum_i dq_i \\wedge dp_i \\). </p> Symplectic 2-form \\( \\omega \\) <p>The symplectic 2-form, denoted by \\( \\omega = \\sum_i dq_i \\wedge dp_i \\), is a mathematical object used in  symplectic geometry. It measures the area of parallelograms formed by vectors in the tangent space of a phase space.</p> <ul> <li>\\( dq_i \\) and \\( dp_i \\): Infinitesimal changes in position and momentum coordinates.</li> <li>\\( \\wedge \\): The wedge product, which combines differential forms in an antisymmetric way meaning that \\( dq_i \\wedge dp_i = -dp_i \\wedge dq_i \\).</li> <li>\\( \\sum_i \\): Sum over all degrees of freedom.</li> </ul> <p>Imagine a phase space where each point represents a state of a physical system. The symplectic form assigns a  value to each pair of vectors, effectively measuring the area of the parallelogram they span. This area is  preserved under Hamiltonian flows.</p> <p>Key Properties</p> <ol> <li>Closed: \\( d\\omega = 0 \\) which means its exterior derivative is zero \\( d\\omega=0 \\). This property ensures that the form does not change under continuous transformations.</li> <li>Non-degenerate: The form is non-degenerate if \\( d\\omega(X,Y)=0 \\) for all \\( Y \\)s, then \\( X=0 \\). This ensures that every vector has a unique \"partner\" vector such that their pairing under \\( \\omega \\) is non-zero.</li> </ol> <p>Example</p> <p>For a simple harmonic oscillator with one degree of freedom, \\( \\omega = dq \\wedge dp \\). This measures the area of parallelograms in the phase space spanned by vectors representing changes in position and momentum.</p> <p>A Very Simplistic PyTorch Code: While PyTorch doesn't directly handle differential forms, you can conceptually represent the symplectic form using tensors:</p> <pre><code>import torch\n\n# Conceptual representation of dq and dp as tensors\ndq = torch.tensor([1.0])  \ndp = torch.tensor([1.0])  \n\n# \"Wedge product\" conceptually represented using an outer product\nomega = torch.outer(dq, dp) - torch.outer(dp, dq)\n\nprint(omega)\n</code></pre> <p>This code illustrates the antisymmetric nature of the wedge product.</p> <p>Numerically, this means good integrators must respect:</p> \\[ \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} = J \\quad \\text{where } J = \\begin{pmatrix} 0 &amp; I \\\\ -I &amp; 0 \\end{pmatrix} \\] Breaking Down the Formula <ul> <li>Geometric numerical integration solves differential equations while preserving geometric properties of the system.</li> <li> <p>Symplecticity is a geometric property inherent to Hamiltonian systems. It ensures that the area of geometric  structures (e.g., parallelograms) in phase space \\( (q, p) \\) remains constant over time. This is encoded in the  symplectic form \\( \\omega = \\sum_i dq_i \\wedge dp_i \\).</p> </li> <li> <p>A numerical method is symplectic if it preserves \\( \\omega \\). The Jacobian matrix of the transformation  from \\( (q(t), p(t)) \\) to \\( (q(t + \\epsilon), p(t + \\epsilon)) \\) must satisfy the condition above.</p> </li> <li> <p>The Jacobian matrix \\( \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\)  quantifies how small changes in the initial state \\( (q(t), p(t)) \\) propagate to the next state \\( (q(t + \\epsilon), p(t + \\epsilon)) \\).</p> </li> <li> <p>\\( q(t) \\) and \\( p(t) \\) : Position and momentum at time \\( t \\).</p> </li> <li>\\( q(t + \\epsilon) \\) and \\( p(t + \\epsilon) \\) : Updated position and momentum after one time step \\( \\epsilon \\).</li> <li>\\( \\frac{\\partial}{\\partial (q(t), p(t))} \\) : Partial derivatives with respect to the initial state.</li> </ul>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#how-are-we-going-to-solve-it","title":"How are We Going to Solve it?","text":"<p>Numerical solvers for differential equations inevitably introduce errors that affect solution accuracy. These errors manifest as deviations from the true trajectory in phase space, particularly noticeable in energy-conserving systems like the harmonic oscillator. The errors fall into two main categories: local truncation error, arising from the approximation of continuous derivatives with discrete steps (proportional to \\( \\mathcal{O}(\\epsilon^n+1) \\) where \\( \\epsilon \\) is the step size and n depends on the method); and global accumulation error, which compounds over integration time.</p> <p>Forward Euler Method Fails at This!</p> <p>To overcome this, we turn to symplectic integrators\u2014methods that respect the underlying geometry of Hamiltonian  systems, leading us naturally to the Leapfrog Verlet method, a powerful symplectic alternative. \ud83d\ude80</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#key-issue-energy-drift-from-non-symplectic-updates","title":"Key Issue: Energy Drift from Non-Symplectic Updates","text":"<p>The forward Euler method (FEM) violates the geometric structure of Hamiltonian systems,  leading to energy drift in long-term simulations. Let\u2019s dissect why.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#forward-euler-in-hamiltonian-systems","title":"Forward Euler in Hamiltonian Systems","text":"<p>For a Hamiltonian \\( H(q, p) \\), the forward Euler updates position and momentum as: \\( q(t + \\epsilon) = q(t) + \\epsilon \\frac{\\partial H}{\\partial p}(q(t), p(t)),\\quad  p(t + \\epsilon) = p(t) - \\epsilon \\frac{\\partial H}{\\partial q}(q(t), p(t)) \\)</p> <p>Unlike Leapfrog Verlet, these updates do not split the position/momentum dependencies, breaking symplecticity.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#step-by-step-failure-harmonic-oscillator-example","title":"Step-by-Step Failure: Harmonic Oscillator Example","text":"<p>Hamiltonian: \\( H = \\frac{1}{2}(q^2 + p^2) \\quad \\text{(mass-spring system with m = k = 1 )} \\)</p> <p>Forward Euler Updates: \\( q(t + \\epsilon) = q(t) + \\epsilon p(t) \\quad \\text{(position update)} \\quad p(t + \\epsilon) = p(t) - \\epsilon q(t) \\quad \\text{(momentum update)} \\)</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#jacobian-matrix-analysis","title":"Jacobian Matrix Analysis","text":"<p>The Jacobian \\( M = \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\) becomes: \\( M = \\begin{pmatrix} 1 &amp; \\epsilon \\\\ -\\epsilon &amp; 1 \\end{pmatrix} \\)</p> <p>Symplectic Condition Check: Does \\( M^T J M = J \\), where \\( J = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\)?</p> <ol> <li> <p>Transpose \\( M^T \\): \\(    M^T = \\begin{pmatrix}    1 &amp; -\\epsilon \\\\    \\epsilon &amp; 1    \\end{pmatrix}    \\)</p> </li> <li> <p>Compute \\( J M \\): \\(       J M = \\begin{pmatrix}       -\\epsilon &amp; 1 \\\\       -1 &amp; -\\epsilon       \\end{pmatrix}       \\)</p> </li> <li> <p>Final Product \\( M^T J M \\): \\(       M^T J M = \\begin{pmatrix}       0 &amp; 1 + \\epsilon^2 \\\\       -1 - \\epsilon^2 &amp; 0       \\end{pmatrix} \\neq J       \\)</p> </li> </ol> <p>The result violates \\( M^T J M = J \\), proving symplecticity fails unless \\( \\epsilon = 0 \\).</p> \\[ \\boxed{\\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\neq J} \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#practical-consequences-energy-drift","title":"Practical Consequences: Energy Drift","text":"<p>Why This Matters</p> <ul> <li>Energy Drift: FEM artificially injects/dissipates energy over time because \\( H(q,p) \\) is not conserved.  </li> <li>Phase Space Distortion: Volume preservation fails, corrupting long-term trajectories.  </li> <li>Unusable for HMC: Sampling in Hamiltonian Monte Carlo relies on symplectic integrators to maintain detailed balance.  </li> </ul> <p>Example: Simulating a harmonic oscillator with FEM shows spiraling/non-closing orbits in phase space, unlike the stable ellipses from Leapfrog Verlet.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplectic-numerical-integrators","title":"Symplectic Numerical Integrators","text":"","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#leapfrog-verlet","title":"Leapfrog Verlet","text":"<p>For a separable Hamiltonian \\( H(q,p) = K(p) + U(q) \\), where the corresponding probability distribution is given by:</p> \\[ P(q,p) = \\frac{1}{Z} e^{-U(q)} e^{-K(p)}, \\] <p>the Leapfrog Verlet integrator proceeds as follows:</p> \\[ \\begin{aligned} p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) &amp;= p_{i}(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t)) \\\\ q_{i}(t + \\epsilon) &amp;= q_{i}(t) + \\epsilon \\frac{\\partial K}{\\partial p_{i}}\\left(p\\left(t + \\frac{\\epsilon}{2}\\right)\\right) \\\\ p_{i}(t + \\epsilon) &amp;= p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t + \\epsilon)) \\end{aligned} \\] <p>This St\u00f6rmer-Verlet scheme preserves symplecticity exactly, with local error \\( \\mathcal{O}(\\epsilon^3) \\) and global  error \\( \\mathcal{O}(\\epsilon^2) \\). You can read more about   numerical methods and analysis in Python here.</p> <p>How Exactly?</p> <ol> <li> <p>Leapfrog Verlet Update Equations For a separable Hamiltonian \\( H(q, p) = K(p) + U(q) \\), the method splits into three component-wise steps:</p> <ol> <li> <p>Half-step momentum update: \\(    p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) = p_{i}(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t))    \\)</p> </li> <li> <p>Full-step position update: \\(    q_{i}(t + \\epsilon) = q_{i}(t) + \\epsilon \\frac{\\partial K}{\\partial p_{i}}\\left(p\\left(t + \\frac{\\epsilon}{2}\\right)\\right)    \\)</p> </li> <li> <p>Full-step momentum update: \\(    p_{i}(t + \\epsilon) = p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t + \\epsilon))    \\)</p> </li> </ol> </li> <li> <p>Jacobian Matrix Calculation For the harmonic oscillator \\( H(q, p) = \\frac{1}{2}(q^2 + p^2) \\), the updates simplify to:  </p> \\[ q(t + \\epsilon) = q(t) + \\epsilon p(t) - \\frac{\\epsilon^2}{2} q(t), \\quad p(t + \\epsilon) = p(t) - \\epsilon q(t) - \\frac{\\epsilon^2}{2} p(t). \\] <p>The Jacobian matrix \\( M = \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\) becomes: \\( M = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; \\epsilon \\\\ -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix}. \\)</p> </li> <li> <p>Transpose of \\( M \\) The transpose \\( M^T \\) swaps off-diagonal terms: \\( M^T = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; -\\epsilon \\\\ \\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix}. \\)</p> </li> <li> <p>Verify \\( M^T J M = J \\) Let \\( J = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\). Compute \\( M^T J M \\):</p> <ol> <li> <p>Calculate: \\( J M = \\begin{pmatrix} -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\\\ -\\left(1 - \\frac{\\epsilon^2}{2}\\right) &amp; -\\epsilon \\end{pmatrix}. \\)</p> </li> <li> <p>Calculate: \\( M^T J M = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; -\\epsilon \\\\ \\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix} \\begin{pmatrix} -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\\\ -\\left(1 - \\frac{\\epsilon^2}{2}\\right) &amp; -\\epsilon \\end{pmatrix}. \\)</p> </li> </ol> <p>After matrix multiplication: \\( M^T J M = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} = J. \\)</p> </li> </ol> <p>The Leapfrog Verlet method satisfies \\( M^T J M = J \\), proving it preserves the symplectic structure. This matches its theoretical property as a symplectic integrator.</p> \\[ \\boxed{\\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} = J} \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#why-symplectic-matters","title":"Why Symplectic Matters","text":"<p>They're the reversible neural nets of physics simulations!</p> <p>Symplectic integrators like Leapfrog Verlet are critical for long-term stability in Hamiltonian systems.  </p> <ul> <li>Phase space preservation: The volume in \\( (q, p) \\)-space is conserved exactly, avoiding artificial energy drift.  </li> <li>Approximate energy conservation: While energy \\( H(q,p) \\) is not perfectly conserved (due to \\( \\mathcal{O}(\\epsilon^2) \\) error), it oscillates near the true value over exponentially long timescales.  </li> <li>Practical relevance: This makes symplectic integrators indispensable in molecular dynamics and Hamiltonian Monte Carlo (HMC), where accurate sampling relies on stable trajectories.  </li> </ul>  Comparison of numerical integration methods for a simple harmonic oscillator in phase space. Color gradients  indicate error magnitude with brighter colors showing larger divergence from the exact solution (white).  Euler's method (a) exhibits energy growth, Modified Euler's method (b) shows improved stability, while  Leapfrog maintains excellent energy conservation at small stepsize (c) but develops geometric distortion  at larger stepsize (d).  <p>Euler's method (first-order) systematically injects energy into the system, causing the characteristic outward spiral seen in the plots. Modified Euler's method (second-order) significantly reduces this energy drift. Most importantly, symplectic integrators like the Leapfrog method preserve the geometric structure of Hamiltonian systems even with relatively large step sizes by maintaining phase space volume conservation. This structural preservation is why Leapfrog remains the preferred method for long-time simulations in molecular dynamics and astronomy, where energy conservation is critical despite the visible polygon-like discretization artifacts at large step sizes.</p> <p>Non-symplectic methods (e.g., Euler-Maruyama) often fail catastrophically in these settings.</p> Integrator Symplecticity Order Type Local Error Global Error Suitable For Computational Cost Euler Method 1 Explicit O(\u03b5\u00b2) O(\u03b5) Quick prototypes and Short-term simulations of general ODEs Low Symplectically Euler 1 Explicit O(\u03b5\u00b2) O(\u03b5) Simple Hamiltonian systems Low Leapfrog (Verlet) 2 Explicit O(\u03b5\u00b3) O(\u03b5\u00b2) Molecular dynamics, Long-term simulations of Hamiltonian systems Moderate Runge-Kutta 4 4 Explicit O(\u03b5\u2075) O(\u03b5\u2074) Short-term accuracy, General ODEs, but not recommended for long-term Hamiltonian systems High Forest-Ruth Integrator 4 Explicit O(\u03b5\u2075) O(\u03b5\u2074) High-accuracy long-term simulations High Yoshida 6<sup>th</sup>-order 6 Explicit O(\u03b5\u2077) O(\u03b5\u2076) High-accuracy High Heun\u2019s Method (RK2) 2 Explicit O(\u03b5\u00b3) O(\u03b5\u00b2) General ODEs requiring moderate accuracy Moderate Third-order Runge-Kutta 3 Explicit O(\u03b5\u2074) O(\u03b5\u00b3) When higher accuracy than RK2 is needed without the cost of RK4 High Implicit Midpoint Rule 2 Implicit (solving equations) O(\u03b5\u00b3) O(\u03b5\u00b2) Hamiltonian systems, stiff problems High Fourth-order Adams-Bashforth 4 Multi-step (explicit) O(\u03b5\u2075) O(\u03b5\u2074) Non-stiff problems with smooth solutions, after initial steps Low Backward Euler Method 1 Implicit (solving equations) O(\u03b5\u00b2) O(\u03b5) Stiff problems, where stability is crucial High","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":"<p>Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that leverages Hamiltonian dynamics to  efficiently sample from complex probability distributions, particularly in Bayesian statistics and machine learning.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#from-phase-space-to-probability-space","title":"From Phase Space to Probability Space","text":"<p>HMC interprets target distribution \\( P(z) \\) as a Boltzmann distribution:</p> \\[ P(z) = \\frac{1}{Z} e^{\\frac{-E(z)}{T}} \\] <p>Substituting into this formulation, the Hamiltonian gives us a joint density:</p> \\[  P(q,p) = \\frac{1}{Z} e^{-U(q)} e^{-K(p)} \\quad \\text{where } U(q) = -\\log[p(q), p(q|D)] \\] <p>where \\( p(q|D) \\) is the likelihood of the given data \\( D \\) and T=1 and therefore removed. We estimate our posterior distribution using the potential energy \\( U(q) \\) since \\( P(q,p) \\) consists of two independent probability distributions.</p> <p>Augment with artificial momentum \\( p \\sim \\mathcal{N}(0,M) \\), then simulate Hamiltonian dynamics to propose new \\( q' \\)  based on the distribution of the position variables \\( U(q) \\) which acts as the \"potential energy\" of the target distribution \\( P(q) \\), thereby creating valleys at high-probability regions.</p> <p>For more on HMC, check out this explanation or  this tutorial.</p> <ul> <li>Physical Systems: \\( H(q,p) = U(q) + K(p) \\) represents total energy  </li> <li>Sampling Systems: \\( H(q,p) = -\\log P(q) + \\frac{1}{2}p^T M^{-1} p \\) defines exploration dynamics  </li> </ul> <p>The kinetic energy with the popular form of \\( K(p) = \\frac{1}{2}p^T M^{-1} p \\), often Gaussian,  injects momentum to traverse these landscapes. Crucially, the mass matrix \\( M \\) plays the role of a  preconditioner - diagonal \\( M \\) adapts to parameter scales, while dense \\( M \\) can align with correlation  structure. \\( M \\) is symmetric, positive definite and typically diagonal.</p> <p>What is Positive Definite?</p> <p>Positive Definite: For any non-zero vector \\( x \\), the expression \\( x^T M x \\) is always positive. This ensures stability and efficiency.</p> <p>      Illustration of different quadratic forms in two variables that shows how different covariance matrices      influence the shape of these forms. The plots depict:     a) Positive Definite Form: A bowl-shaped surface where all eigenvalues are positive, indicating a minimum.     b) Negative Definite Form: An inverted bowl where all eigenvalues are negative, indicating a maximum.     c) Indefinite Form: A saddle-shaped surface with both positive and negative eigenvalues, indicating neither a maximum nor a minimum.     Each subplot includes the matrix \\( M \\) and the corresponding quadratic form \\( Q(x) = x^T M x \\).  </p> \\[ x^T M x &gt; 0 \\] Kinetic Energy Choices <ul> <li>Gaussian (Standard HMC): \\( K(p) = \\frac{1}{2}p^T M^{-1} p \\)   Yields Euclidean trajectories, efficient for moderate dimensions.  </li> <li>Relativistic (Riemannian HMC): \\( K(p) = \\sqrt{p^T M^{-1} p + c^2} \\)   Limits maximum velocity, preventing divergences in ill-conditioned spaces.  </li> <li>Adaptive (Surrogate Gradients): Learn \\( K(p) \\) via neural networks to match target geometry.</li> </ul> <p>Key Intuition</p> <p>The Hamiltonian \\( H(q,p) = U(q) + \\frac{1}{2}p^T M^{-1} p \\) creates an energy landscape where momentum carries  the sampler through high-probability regions, avoiding random walk behavior.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#the-hmc-algorithm","title":"The HMC Algorithm","text":"<p>The algorithm involves:</p> <ol> <li> <p>Initialization: Start with an initial position \\( q_0 \\) and sample momentum \\( p_0 \\sim \\mathcal{N}(0,M) \\).</p> </li> <li> <p>Leapfrog Integration: Use the leapfrog method to approximate Hamiltonian dynamics. For a step size \\( \\epsilon \\) and L steps, update:</p> </li> <li> <p>Half-step momentum: \\( p(t + \\frac{\\epsilon}{2}) = p(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t)) \\)</p> </li> <li>Full-step position: \\( q(t + \\epsilon) = q(t) + \\epsilon \\frac{\\partial K}{\\partial p}(p(t + \\frac{\\epsilon}{2})) \\), where \\( K(p) = \\frac{1}{2} p^T M^{-1} p \\), so \\( \\frac{\\partial K}{\\partial p} = M^{-1} p \\)</li> <li>Full-step momentum: \\( p(t + \\epsilon) = p(t + \\frac{\\epsilon}{2}) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t + \\epsilon)) \\)</li> </ol> <p>This is repeated L times to get proposed \\( \\dot{q} \\) and \\( \\dot{p} \\).</p> <ol> <li>Metropolis-Hastings Acceptance: Accept the proposed \\( \\dot{q} \\) with probability \\( \\min(1, e^{H(q_0,p_0) - H(\\dot{q},\\dot{p})}) \\), where \\( H(q,p) = U(q) + K(p) \\).</li> </ol> <p>This process generates a Markov chain with stationary distribution \\( P(q) \\), leveraging Hamiltonian dynamics to take larger, more efficient steps compared to random-walk methods.</p> Why Better Than Random Walk? <p>HMC navigates high-dimensional spaces along energy contours -  like following mountain paths instead of wandering randomly!</p> Recap of the Hamilton's equations? \\[ \\begin{cases} \\dot{q} = \\nabla_p K(p) = M^{-1}p &amp; \\text{(Guided exploration)} \\\\ \\dot{p} = -\\nabla_q U(q) = \\nabla_q \\log P(q) &amp; \\text{(Bayesian updating)} \\end{cases} \\] <p>This coupled system drives \\( (q,p) \\) along iso-probability contours of \\( P(q) \\), with momentum rotating rather  than resetting at each step like in Random Walk Metropolis--think of following mountain paths instead of wandering randomly! The key parameters - integration time \\( \\tau = L\\epsilon \\) and step size \\( \\epsilon \\) - balance exploration vs. computational cost:  </p> <ul> <li>Short \\( \\tau \\): Local exploration, higher acceptance  </li> <li>Long \\( \\tau \\): Global moves, risk of U-turns (periodic orbits)  </li> </ul> <p>Key Parameters and Tuning</p> <p>Tuning \\( M \\) to match the covariance of \\( P(q) \\) (e.g., via warmup adaptation) and setting \\( \\tau \\sim \\mathcal{O}(1/\\lambda_{\\text{max}}) \\), where \\( \\lambda_{\\text{max}} \\) is the largest eigenvalue of \\( \\nabla^2 U \\), often yields optimal mixing.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#connection-with-energy-based-models","title":"Connection with Energy-Based Models","text":"<p>Energy-based models (EBMs) are a class of generative models that define a probability distribution over data points  using an energy function. The probability of a data point is proportional to \\( e^{-E(x)} \\), where \\( E(x) \\) is the  energy function. This formulation is directly analogous to the Boltzmann distribution in statistical physics, where the  probability is related to the energy of a state. In Hamiltonian mechanics, the Hamiltonian function \\( H(q, p) \\)  represents the total energy of the system, and the probability distribution in phase space is given  by \\( e^{-H(q,p)/T} \\), where \\( T \\) is the temperature.</p> <p>In EBMs, Hamiltonian Monte Carlo (HMC) is often used to sample from the model's distribution. HMC leverages  Hamiltonian dynamics to propose new states, which are then accepted or rejected based on the Metropolis-Hastings  criterion. This method is particularly effective for high-dimensional problems, as it reduces the correlation between  samples and allows for more efficient exploration of the state space. For instance, in image generation tasks, HMC  can sample from the distribution defined by the energy function, facilitating the generation of high-quality images.</p> <p>EBMs define probability through Hamiltonians:</p> \\[ p(x) = \\frac{1}{Z}e^{-E(x)} \\quad \\leftrightarrow \\quad H(q,p) = E(q) + K(p) \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#potential-research-directions","title":"Potential Research Directions","text":"","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplecticity-in-machine-learning-models","title":"Symplecticity in Machine Learning Models","text":"Overview of the Hamiltonian Neural Networks architecture. Image from the HNN paper. <p>Incorporate the symplectic structure of Hamiltonian mechanics into machine learning models to preserve properties  like energy conservation, which is crucial for long-term predictions. Generalizing Hamiltonian Neural Networks (HNNs),  as discussed in Hamiltonian Neural Networks, to more complex systems or developing new architectures that preserve symplecticity</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hmc-for-complex-distributions","title":"HMC for Complex Distributions:","text":"<p>HMC for sampling from complex, high-dimensional, and multimodal distributions, such as those encountered in deep learning. Combining HMC with other techniques, like parallel tempering, could handle distributions with multiple modes more  effectively.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#combining-hamiltonian-mechanics-with-other-ml-techniques","title":"Combining Hamiltonian Mechanics with Other ML Techniques:","text":"<p>Integrate Hamiltonian mechanics with reinforcement learning to guide exploration in continuous state and action spaces. Using it to model the environment could improve exploration strategies, as seen in potential applications in robotics.  Additionally, using Hamiltonian mechanics to define approximate posteriors in variational inference could lead to more  flexible and accurate approximations.   </p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltonian-gans","title":"Hamiltonian GANs","text":"<p>Employing Hamiltonian formalism as an inductive bias for the generation of physically plausible videos with neural networks. Imagine generator-discriminator dynamics governed by:</p> <p>The possibilities make phase space feel infinite...</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#want-to-team-up-on-this","title":"Want to Team Up on This \ud83e\udd13","text":"<p>If you are an ML researcher and are interested in collaborating on researching EBMs,  diffusion- and flow-based models, or have other relevant ideas in mind for generalization over out-of-distribution  data (downstream tasks can be anything in from molecular design to robotics motion planning to LLMs), please feel free to reach out!</p> <p>Also follow me on  Twitter /  BlueSky or  GitHub\u2014I\u2019m usually rambling about this stuff there.  Also on LinkedIn and  Medium /  TDS if you\u2019re curious.  To find more about my research interests, check out my  personal website.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#useful-links","title":"Useful Links","text":"<ul> <li>An Introduction to Multistep Methods: Leap-frog</li> <li>The beginners guide to Hamiltonian Monte Carlo</li> <li>Hamiltonian Monte Carlo</li> <li>Hamiltonian Monte Carlo - Stan - Stan explained</li> <li>Hamiltonian Mechanics For Dummies: An Intuitive Introduction.</li> <li>Hamiltonian mechanics Wikipedia page</li> <li>An introduction to Lagrangian and Hamiltonian mechanics Lecture notes</li> <li> <p>Hamiltonian Mechanics - Jeremy Tatum, University of Victoria</p> </li> <li> <p>Hamiltonian Neural Networks - Blog</p> </li> <li>Hamiltonian Neural Networks</li> <li>Other: Natural Intelligence - A blog by Sam Greydanus - Many interesting topics </li> </ul>","tags":["hamiltonian","sampling"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/","title":"Langevin Dynamics Sampling with TorchEBM","text":"<p>Langevin dynamics is a powerful sampling technique that allows us to draw samples from complex probability distributions. In this tutorial, we'll explore how to use TorchEBM's implementation of Langevin dynamics for sampling from various energy landscapes.</p>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#basic-example-sampling-from-a-2d-gaussian","title":"Basic Example: Sampling from a 2D Gaussian","text":"<p>Let's start with a simple example of sampling from a 2D Gaussian distribution:</p> Basic Langevin Dynamics Sampling<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create energy function for a 2D Gaussian\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndim = 2  # dimension of the state space\nn_steps = 100  # steps between samples\nn_samples = 1000  # num of samples\nmean = torch.tensor([1.0, -1.0])\ncov = torch.tensor([[1.0, 0.5], [0.5, 2.0]])\nenergy_fn = GaussianEnergy(mean, cov, device=device)\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=0.1,\n    device=device,\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = sampler.sample_chain(\n    x=initial_state,\n    n_steps=n_steps,\n    n_samples=n_samples,\n)\n\n# Plot results\nsamples = samples.cpu().numpy()\nplt.figure(figsize=(10, 5))\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\nplt.title(\"Samples from 2D Gaussian using Langevin Dynamics\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.show()\n</code></pre>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#advanced-example-double-well-potential","title":"Advanced Example: Double Well Potential","text":"<p>For a more interesting example, let's sample from a double well potential, which has two local minima:</p> Double Well Energy Sampling<pre><code>from torchebm.core import DoubleWellEnergy\n\n# Create energy function and sampler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.001,\n    noise_scale=0.1,\n    decay=0.1,  # for stability\n    device=device,\n)\n\n# Generate trajectory with diagnostics\ninitial_state = torch.tensor([0.0], device=device)\ntrajectory, diagnostics = sampler.sample(\n    x=initial_state,\n    n_steps=1000,\n    return_trajectory=True,\n    return_diagnostics=True,\n)\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot trajectory\nax1.plot(trajectory[0, :, 0].cpu().numpy())\nax1.set_title(\"Single Chain Trajectory\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Position\")\n\n# Plot energy over time\nax2.plot(diagnostics[:, 2, 0, 0].cpu().numpy())\nax2.set_title(\"Energy Evolution\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Energy\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#key-benefits-of-torchebms-langevin-dynamics-implementation","title":"Key Benefits of TorchEBM's Langevin Dynamics Implementation","text":"<ol> <li>GPU Acceleration - Sampling is performed efficiently on GPUs when available</li> <li>Flexible API - Easy to use with various energy functions and initialization strategies</li> <li>Diagnostic Tools - Track energy, gradient norms, and acceptance rates during sampling</li> <li>Configurable Parameters - Fine-tune step size, noise scale, and decay for optimal performance</li> </ol>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#conclusion","title":"Conclusion","text":"<p>Langevin dynamics is a versatile sampling method for energy-based models, and TorchEBM makes it easy to use in your projects. Whether you're sampling from simple analytical distributions or complex neural network energy functions, the same API works seamlessly.</p> <p>Stay tuned for more tutorials on other samplers and energy functions! </p>","tags":["langevin","sampling","tutorial"]},{"location":"developer_guide/","title":"TorchEBM Developer Guide","text":"<p>Welcome to the TorchEBM developer guide! This comprehensive resource is designed to help you understand the project's architecture, contribute effectively, and follow best practices when working with the codebase.</p>"},{"location":"developer_guide/#getting-started-with-development","title":"Getting Started with Development","text":"<ul> <li> <p> Development Setup</p> <p>Set up your development environment and prepare for contribution.</p> <p> Development Setup</p> </li> <li> <p> Code Style</p> <p>Learn about the coding standards and style guidelines.</p> <p> Code Style</p> </li> <li> <p> Testing Guide</p> <p>Discover how to write and run tests for TorchEBM.</p> <p> Testing Guide</p> </li> <li> <p>:material-git-commit:{ .lg .middle } Commit Conventions</p> <p>Understand the commit message format and conventions.</p> <p> Commit Conventions</p> </li> </ul>"},{"location":"developer_guide/#understanding-the-architecture","title":"Understanding the Architecture","text":"<ul> <li> <p> Project Structure</p> <p>Explore the organization of the TorchEBM codebase.</p> <p> Project Structure</p> </li> <li> <p> Design Principles</p> <p>Learn about the guiding principles behind TorchEBM.</p> <p> Design Principles</p> </li> <li> <p> Core Components</p> <p>Understand the core components and their interactions.</p> <p> Core Components</p> </li> </ul>"},{"location":"developer_guide/#implementation-details","title":"Implementation Details","text":"<ul> <li> <p> Energy Functions</p> <p>Detailed information about energy function implementations.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Understand how samplers are implemented in TorchEBM.</p> <p> Samplers</p> </li> <li> <p> Loss Functions</p> <p>Learn about the implementation of various loss functions.</p> <p> Loss Functions</p> </li> <li> <p> Model Architecture</p> <p>Explore the implementation of neural network models.</p> <p> Model Architecture</p> </li> <li> <p>:material-gpu:{ .lg .middle } CUDA Optimizations</p> <p>Discover performance optimizations using CUDA.</p> <p> CUDA Optimizations</p> </li> </ul>"},{"location":"developer_guide/#contributing","title":"Contributing","text":"<ul> <li> <p> Contributing</p> <p>Guidelines for contributing to TorchEBM.</p> <p> Contributing</p> </li> <li> <p> API Design</p> <p>Learn about API design principles in TorchEBM.</p> <p> API Design</p> </li> <li> <p> API Generation</p> <p>Understand how API documentation is generated.</p> <p> API Generation</p> </li> <li> <p> Performance</p> <p>Best practices for optimizing performance.</p> <p> Performance</p> </li> </ul>"},{"location":"developer_guide/#contributing-process","title":"Contributing Process","text":"<p>The general process for contributing to TorchEBM involves:</p> <ol> <li> <p>Set up the development environment: Follow the development setup guide to prepare your workspace.</p> </li> <li> <p>Understand the codebase: Familiarize yourself with the project structure, design principles, and core components.</p> </li> <li> <p>Make changes: Implement your feature or bug fix, following the code style guidelines.</p> </li> <li> <p>Write tests: Add tests for your changes as described in the testing guide.</p> </li> <li> <p>Submit a pull request: Follow the contributing guidelines to submit your changes.</p> </li> </ol> <p>We welcome contributions from the community and are grateful for your help in improving TorchEBM!</p>"},{"location":"developer_guide/#development-philosophy","title":"Development Philosophy","text":"<p>TorchEBM aims to be:</p> <ul> <li>Modular: Components should be easy to combine and extend</li> <li>Performant: Critical operations should be optimized for speed</li> <li>User-friendly: APIs should be intuitive and well-documented</li> <li>Well-tested: Code should be thoroughly tested to ensure reliability</li> </ul> <p>Learn more about our Design Principles.</p>"},{"location":"developer_guide/#quick-reference","title":"Quick Reference","text":"<ul> <li>Installation for development: <code>pip install -e \".[dev]\"</code></li> <li>Run tests: <code>pytest</code></li> <li>Check code style: <code>black torchebm/ &amp;&amp; isort torchebm/ &amp;&amp; flake8 torchebm/</code></li> <li>Build documentation: <code>mkdocs serve</code></li> <li>Pre-commit hooks: <code>pre-commit install</code></li> </ul>"},{"location":"developer_guide/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during development, you can:</p> <ul> <li>Open an issue on GitHub</li> <li>Ask questions in the GitHub Discussions</li> <li>Reach out to maintainers via email or GitHub </li> </ul>"},{"location":"developer_guide/api_design/","title":"API Design","text":"<p>This document outlines the design principles and patterns used in TorchEBM's API, providing guidelines for contributors and insights for users building on top of the library.</p>"},{"location":"developer_guide/api_design/#api-design-philosophy","title":"API Design Philosophy","text":"<p>Design Goals</p> <p>TorchEBM's API is designed with these goals in mind:</p> <ol> <li>Intuitive: APIs should be easy to understand and use</li> <li>Consistent: Similar operations should have similar interfaces</li> <li>Pythonic: Follow Python conventions and best practices</li> <li>Flexible: Allow for customization and extension</li> <li>Type-Safe: Use type hints for better IDE support and error checking</li> </ol>"},{"location":"developer_guide/api_design/#core-abstractions","title":"Core Abstractions","text":"<ul> <li> <p> Energy Functions</p> <p>Energy functions define the energy landscape that characterizes a probability distribution.</p> <pre><code>class EnergyFunction(nn.Module):\n    def forward(self, x):\n        # Return energy values for inputs x\n        pass\n\n    def gradient(self, x):\n        # Return energy gradients for inputs x\n        pass\n</code></pre> </li> <li> <p> Samplers</p> <p>Samplers generate samples from energy functions via various algorithms.</p> <pre><code>class BaseSampler:\n    def __init__(self, energy_function, device=\"cpu\"):\n        self.energy_function = energy_function\n        self.device = device\n\n    def sample_chain(self, dim, n_steps, n_samples=1):\n        # Generate samples\n        pass\n</code></pre> </li> <li> <p> Loss Functions</p> <p>Loss functions are used to train energy-based models, often using samplers.</p> <pre><code>class ContrastiveDivergence(nn.Module):\n    def __init__(self, energy_fn, sampler, mcmc_steps=1):\n        self.energy_fn = energy_fn\n        self.sampler = sampler\n        self.mcmc_steps = mcmc_steps\n\n    def forward(self, data_samples):\n        # Compute loss\n        pass\n</code></pre> </li> <li> <p> Models</p> <p>Neural network models that can be used as energy functions.</p> <pre><code>class BaseModel(EnergyFunction):\n    def __init__(self):\n        super().__init__()\n        # Define model architecture\n\n    def forward(self, x):\n        # Compute energy\n        pass\n</code></pre> </li> </ul>"},{"location":"developer_guide/api_design/#interface-design-patterns","title":"Interface Design Patterns","text":""},{"location":"developer_guide/api_design/#method-naming-conventions","title":"Method Naming Conventions","text":"<p>TorchEBM follows consistent naming patterns:</p> Pattern Example Purpose <code>forward()</code> <code>energy_fn.forward(x)</code> Core computation (energy) <code>gradient()</code> <code>energy_fn.gradient(x)</code> Compute gradients <code>sample_chain()</code> <code>sampler.sample_chain(dim, n_steps)</code> Generate samples <code>step()</code> <code>sampler.step(x)</code> Single sampling step"},{"location":"developer_guide/api_design/#parameter-ordering","title":"Parameter Ordering","text":"<p>Parameters follow a consistent ordering pattern:</p> <ol> <li>Required parameters (e.g., input data, dimensions)</li> <li>Algorithm-specific parameters (e.g., step size, number of steps)</li> <li>Optional parameters with defaults (e.g., device, random seed)</li> </ol>"},{"location":"developer_guide/api_design/#return-types","title":"Return Types","text":"<p>Return values are consistently structured:</p> <ul> <li>Single values are returned directly</li> <li>Multiple return values use tuples</li> <li>Complex returns use dictionaries for named access</li> <li>Diagnostic information is returned in a separate dictionary</li> </ul> <p>Example: <pre><code># Return samples and diagnostics\nsamples, diagnostics = sampler.sample_chain(dim=2, n_steps=100)\n\n# Access diagnostic information\nacceptance_rate = diagnostics['acceptance_rate']\nenergy_trajectory = diagnostics['energy_trajectory']\n</code></pre></p>"},{"location":"developer_guide/api_design/#extension-patterns","title":"Extension Patterns","text":""},{"location":"developer_guide/api_design/#subclassing-base-classes","title":"Subclassing Base Classes","text":"<p>The primary extension pattern is to subclass the appropriate base class:</p> <pre><code>class MyCustomSampler(BaseSampler):\n    def __init__(self, energy_function, special_param, device=\"cpu\"):\n        super().__init__(energy_function, device)\n        self.special_param = special_param\n\n    def sample_chain(self, x, step_idx=None):\n        # Custom sampling logic\n        return x_new, diagnostics\n</code></pre>"},{"location":"developer_guide/api_design/#composition-pattern","title":"Composition Pattern","text":"<p>For more complex extensions, composition can be used:</p> <pre><code>class HybridSampler(BaseSampler):\n    def __init__(self, energy_function, sampler1, sampler2, switch_freq=10):\n        super().__init__(energy_function)\n        self.sampler1 = sampler1\n        self.sampler2 = sampler2\n        self.switch_freq = switch_freq\n\n    def sample_chain(self, x, step_idx=None):\n        # Choose sampler based on step index\n        if step_idx % self.switch_freq &lt; self.switch_freq // 2:\n            return self.sampler1.step(x, step_idx)\n        else:\n            return self.sampler2.step(x, step_idx)\n</code></pre>"},{"location":"developer_guide/api_design/#configuration-and-customization","title":"Configuration and Customization","text":""},{"location":"developer_guide/api_design/#constructor-parameters","title":"Constructor Parameters","text":"<p>Features are enabled and configured primarily through constructor parameters:</p> <pre><code># Configure through constructor parameters\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=0.1,\n    thinning=5,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n</code></pre>"},{"location":"developer_guide/api_design/#method-parameters","title":"Method Parameters","text":"<p>Runtime behavior is controlled through method parameters:</p> <pre><code># Control sampling behavior through method parameters\nsamples = sampler.sample_chain(\n    dim=2,\n    n_steps=1000,\n    n_samples=100,\n    initial_samples=None,  # If None, random initialization\n    burn_in=100,\n    verbose=True\n)\n</code></pre>"},{"location":"developer_guide/api_design/#handling-errors-and-edge-cases","title":"Handling Errors and Edge Cases","text":"<p>TorchEBM follows these practices for error handling:</p> <ol> <li>Input Validation: Validate inputs early and raise descriptive exceptions</li> <li>Graceful Degradation: Fall back to simpler algorithms when necessary</li> <li>Informative Exceptions: Provide clear error messages with suggestions</li> <li>Default Safety: Choose safe default values that work in most cases</li> </ol> <p>Example: <pre><code>def sample_chain(self, dim, n_steps, n_samples=1):\n    if n_steps &lt;= 0:\n        raise ValueError(\"n_steps must be positive\")\n\n    if dim &lt;= 0:\n        raise ValueError(\"dim must be positive\")\n\n    # Implementation\n</code></pre></p>"},{"location":"developer_guide/api_design/#api-evolution-guidelines","title":"API Evolution Guidelines","text":"<p>When evolving the API, we follow these guidelines:</p> <ol> <li>Backward Compatibility: Avoid breaking changes when possible</li> <li>Deprecation Cycle: Use deprecation warnings before removing features</li> <li>Default Arguments: Add new parameters with sensible defaults</li> <li>Feature Flags: Use boolean flags to enable/disable new features</li> </ol> <p>Example of deprecation: <pre><code>def old_method(self, param):\n    warnings.warn(\n        \"old_method is deprecated and will be removed in a future version. \"\n        \"Use new_method instead.\",\n        DeprecationWarning,\n        stacklevel=2\n    )\n    return self.new_method(param)\n</code></pre></p>"},{"location":"developer_guide/api_design/#documentation-standards","title":"Documentation Standards","text":"<p>All APIs should include:</p> <ul> <li>Docstrings for all public classes and methods</li> <li>Type hints for parameters and return values</li> <li>Examples showing common usage patterns</li> <li>Notes on performance implications</li> <li>References to relevant papers or algorithms</li> </ul> <p>Example: <pre><code>def sample_chain(\n    self, \n    dim: int, \n    n_steps: int, \n    n_samples: int = 1\n) -&gt; Tuple[torch.Tensor, Dict[str, Any]]:\n    \"\"\"\n    Generate samples using Langevin dynamics.\n\n    Args:\n        dim: Dimensionality of samples\n        n_steps: Number of sampling steps\n        n_samples: Number of parallel chains\n\n    Returns:\n        tuple: (samples, diagnostics)\n            - samples: Tensor of shape [n_samples, dim]\n            - diagnostics: Dict with sampling statistics\n\n    Example:\n        &gt;&gt;&gt; energy_fn = GaussianEnergy(torch.zeros(2), torch.eye(2))\n        &gt;&gt;&gt; sampler = LangevinDynamics(energy_fn, step_size=0.1)\n        &gt;&gt;&gt; samples, _ = sampler.sample_chain(dim=2, n_steps=100, n_samples=10)\n    \"\"\"\n    # Implementation\n</code></pre></p> <p>By following these API design principles, TorchEBM maintains a consistent, intuitive, and extensible interface for energy-based modeling in PyTorch. </p>"},{"location":"developer_guide/api_generation/","title":"Developer Guide for <code>torchebm</code>","text":""},{"location":"developer_guide/api_generation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Setting Up the Development Environment</li> <li>Prerequisites</li> <li>Cloning the Repository</li> <li>Installing Dependencies</li> <li>Code Style and Quality</li> <li>Code Formatting</li> <li>Linting</li> <li>Type Checking</li> <li>Testing</li> <li>Documentation</li> <li>Docstring Conventions</li> <li>API Documentation Generation</li> <li>Contributing</li> <li>Branching Strategy</li> <li>Commit Messages</li> <li>Pull Requests</li> <li>Additional Resources</li> </ol>"},{"location":"developer_guide/api_generation/#introduction","title":"Introduction","text":"<p>Welcome to the developer guide for <code>torchebm</code>, a Python library focused on components and algorithms for energy-based models. This document provides instructions and best practices for contributing to the project.</p>"},{"location":"developer_guide/api_generation/#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":""},{"location":"developer_guide/api_generation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed:</p> <ul> <li>Python: Version 3.9 or higher.</li> <li>Git: For version control.</li> <li>pip: Python package installer.</li> </ul>"},{"location":"developer_guide/api_generation/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Clone the repository using SSH:</p> <pre><code>git clone git@github.com:soran-ghaderi/torchebm.git\n</code></pre> <p>Or using HTTPS:</p> <pre><code>git clone https://github.com/soran-ghaderi/torchebm.git\n</code></pre> <p>Navigate to the project directory:</p> <pre><code>cd torchebm\n</code></pre>"},{"location":"developer_guide/api_generation/#installing-dependencies","title":"Installing Dependencies","text":"<p>Install the development dependencies:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>This command installs the package in editable mode along with all development dependencies specified in the <code>pyproject.toml</code> file.</p>"},{"location":"developer_guide/api_generation/#code-style-and-quality","title":"Code Style and Quality","text":"<p>Maintaining a consistent code style ensures readability and ease of collaboration.</p> <p>To streamline code formatting and quality checks in your development workflow, integrating tools like Black, isort, and mypy directly into your Integrated Development Environment (IDE) can be highly beneficial. Many modern IDEs, such as PyCharm, offer built-in support or plugins for these tools, enabling automatic code formatting and linting as you write.</p> <p>PyCharm Integration:</p> <ul> <li> <p>Black Integration: Starting from PyCharm 2023.2, Black integration is built-in. To enable it:</p> </li> <li> <p>Navigate to <code>Preferences</code> or <code>Settings</code> &gt; <code>Tools</code> &gt; <code>Black</code>.</p> </li> <li>Configure the settings as desired.</li> <li> <p>Ensure Black is installed in your environment:</p> <pre><code>pip install black\n</code></pre> </li> <li> <p>isort Integration: While PyCharm doesn't have built-in isort support, you can set it up using File Watchers:</p> </li> <li> <p>Install the File Watchers plugin if it's not already installed.</p> </li> <li>Navigate to <code>Preferences</code> or <code>Settings</code> &gt; <code>Tools</code> &gt; <code>File Watchers</code>.</li> <li> <p>Add a new watcher for isort with the following configuration:</p> <ul> <li>File Type: Python</li> <li>Scope: Current File</li> <li>Program: Path to isort executable (e.g., <code>$PyInterpreterDirectory$/isort</code>)</li> <li>Arguments: <code>$FilePath$</code></li> <li>Output Paths to Refresh: <code>$FilePath$</code></li> <li>Working Directory: <code>$ProjectFileDir$</code></li> </ul> </li> <li> <p>mypy Integration: To integrate mypy:</p> </li> <li> <p>Install mypy in your environment:</p> <pre><code>pip install mypy\n</code></pre> </li> <li> <p>Set up a File Watcher in PyCharm similar to isort, replacing the program path with the mypy executable path.</p> </li> </ul> <p>VSCode Integration:</p> <p>For Visual Studio Code users, extensions are available for seamless integration:</p> <ul> <li>Black Formatter: Install the \"Python\" extension by Microsoft, which supports Black formatting.</li> <li>isort: Use the \"Python\" extension's settings to enable isort integration.</li> <li>mypy: Install the \"mypy\" extension for real-time type checking.</li> </ul> <p>By configuring your IDE with these tools, you ensure consistent code quality and adhere to the project's coding standards effortlessly. </p> <p>If you prefer to do these steps manually, please read the following steps, if not, you can ignore this section.</p>"},{"location":"developer_guide/api_generation/#code-formatting","title":"Code Formatting","text":"<p>We use Black for code formatting. To format your code, run:</p> <pre><code>black .\n</code></pre>"},{"location":"developer_guide/api_generation/#linting","title":"Linting","text":"<p>isort is used for sorting imports. To sort imports, execute:</p> <pre><code>isort .\n</code></pre>"},{"location":"developer_guide/api_generation/#type-checking","title":"Type Checking","text":"<p>mypy is employed for static type checking. To check types, run:</p> <pre><code>mypy torchebm/\n</code></pre>"},{"location":"developer_guide/api_generation/#testing","title":"Testing","text":"<p>We utilize pytest for testing. To run tests, execute:</p> <pre><code>pytest\n</code></pre> <p>For test coverage, use:</p> <pre><code>pytest --cov=torchebm\n</code></pre>"},{"location":"developer_guide/api_generation/#documentation","title":"Documentation","text":""},{"location":"developer_guide/api_generation/#docstring-conventions","title":"Docstring Conventions","text":"<p>All docstrings should adhere to the Google style guide. For example:</p> <pre><code>def function_name(param1, param2):\n    \"\"\"Short description of the function.\n\n    Longer description if needed.\n\n    Args:\n        param1 (type): Description of param1.\n        param2 (type): Description of param2.\n\n    Returns:\n        type: Description of return value.\n\n    Raises:\n        ExceptionType: Explanation of when this exception is raised.\n\n    Examples:\n        result = function_name(1, \"test\")\n    \"\"\"\n    # Function implementation\n</code></pre>"},{"location":"developer_guide/api_generation/#api-documentation-generation","title":"API Documentation Generation","text":"<p>The API documentation is automatically generated from docstrings using <code>generate_api_docs.py</code>, MkDocs, and the MkDocstrings plugin.</p> <p>To update the API documentation:</p> <ol> <li> <p>Run the API documentation generator script:</p> <pre><code>python gen_ref_pages.py\n</code></pre> </li> <li> <p>Build the documentation to preview changes:</p> <pre><code>mkdocs serve\n</code></pre> </li> </ol>"},{"location":"developer_guide/api_generation/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please follow the guidelines below.</p>"},{"location":"developer_guide/api_generation/#branching-strategy","title":"Branching Strategy","text":"<ul> <li>main: Contains stable code.</li> <li>feature/branch-name: For developing new features.</li> <li>bugfix/branch-name: For fixing bugs.</li> </ul>"},{"location":"developer_guide/api_generation/#commit-messages","title":"Commit Messages","text":"<p>Use clear and concise commit messages. Follow the format:</p> <pre><code>Subject line (imperative, 50 characters or less)\n\nOptional detailed description, wrapped at 72 characters.\n</code></pre>"},{"location":"developer_guide/api_generation/#pull-requests","title":"Pull Requests","text":"<p>Before submitting a pull request:</p> <ol> <li>Ensure all tests pass.</li> <li>Update documentation if applicable.</li> <li>Adhere to code style guidelines.</li> </ol>"},{"location":"developer_guide/api_generation/#additional-resources","title":"Additional Resources","text":"<ul> <li>Python's PEP 8 Style Guide</li> <li>Google Python Style Guide</li> <li>Black Code Formatter</li> <li>pytest Documentation</li> </ul> <p>By following this guide, you will help maintain the quality and consistency of the <code>torchebm</code> library. Happy coding! </p>"},{"location":"developer_guide/architecture/","title":"Architecture","text":"<p>This document outlines the architecture and design principles of TorchEBM, providing insights into how the library is structured and how its components interact.</p>"},{"location":"developer_guide/architecture/#core-components","title":"Core Components","text":"<p>TorchEBM is designed around several key components that work together to provide a flexible and powerful framework for energy-based modeling:</p>"},{"location":"developer_guide/architecture/#energy-functions","title":"Energy Functions","text":"<p>The <code>EnergyFunction</code> base class defines the interface for all energy functions. It provides methods for:</p> <ul> <li>Computing energy values</li> <li>Computing gradients</li> <li>Handling batches of inputs</li> <li>CUDA acceleration</li> </ul> <p>Implemented energy functions include Gaussian, Double Well, Rastrigin, Rosenbrock, and more.</p>"},{"location":"developer_guide/architecture/#samplers","title":"Samplers","text":"<p>The <code>BaseSampler</code> class defines the interface for sampling algorithms. Key features include:</p> <ul> <li>Generating samples from energy functions</li> <li>Running sampling chains</li> <li>Collecting diagnostics</li> <li>Parallelized sampling</li> </ul> <p>Implemented samplers include Langevin Dynamics and Hamiltonian Monte Carlo.</p>"},{"location":"developer_guide/architecture/#loss-functions","title":"Loss Functions","text":"<p>Loss functions are used to train energy-based models. They include:</p> <ul> <li>Contrastive Divergence (CD)</li> <li>Persistent Contrastive Divergence (PCD)</li> <li>Parallel Tempering Contrastive Divergence</li> <li>Score Matching (planned)</li> </ul>"},{"location":"developer_guide/architecture/#models","title":"Models","text":"<p>Neural network models that can be used as energy functions:</p> <ul> <li>Base model interfaces</li> <li>Integration with PyTorch modules</li> <li>Support for custom architectures</li> <li>GPU acceleration</li> </ul>"},{"location":"developer_guide/architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TD\n    A[Energy Functions] --&gt; C[Samplers]\n    B[Models] --&gt; A\n    C --&gt; D[Loss Functions]\n    D --&gt; B\n    E[CUDA Accelerators] --&gt; A\n    E --&gt; C\n    F[Utils] --&gt; A\n    F --&gt; B\n    F --&gt; C\n    F --&gt; D</code></pre>"},{"location":"developer_guide/architecture/#design-principles","title":"Design Principles","text":"<p>Key Design Principles</p> <p>TorchEBM follows these core design principles:</p> <ol> <li>Modularity: Components can be used independently and combined flexibly</li> <li>Extensibility: Easy to add new energy functions, samplers, and loss functions</li> <li>Performance: Optimized for both CPU and GPU execution</li> <li>Compatibility: Seamless integration with PyTorch ecosystem</li> <li>Usability: Clear, consistent API with comprehensive documentation</li> </ol>"},{"location":"developer_guide/architecture/#component-interactions","title":"Component Interactions","text":""},{"location":"developer_guide/architecture/#energy-function-and-sampler-interaction","title":"Energy Function and Sampler Interaction","text":"<p>The energy function provides the landscape that the sampler traverses:</p> <pre><code># Energy function computes energy and gradients\nenergy = energy_fn(x)  # Forward pass\ngradient = energy_fn.gradient(x)  # Gradient computation\n\n# Sampler uses gradients for updates\nx_new = x - step_size * gradient + noise\n</code></pre>"},{"location":"developer_guide/architecture/#sampler-and-loss-function-interaction","title":"Sampler and Loss Function Interaction","text":"<p>Samplers are used by loss functions to generate negative samples during training:</p> <pre><code># Loss function uses sampler to generate negative samples\nnegative_samples = sampler.sample_chain(x_init, n_steps=10)\n\n# Loss computation uses both data samples and negative samples\nloss = loss_fn(data_samples, negative_samples)\n</code></pre>"},{"location":"developer_guide/architecture/#module-organization","title":"Module Organization","text":"<p>TorchEBM's codebase is organized into the following modules:</p> Module Description Key Classes <code>torchebm.core</code> Core functionality and base classes <code>EnergyFunction</code>, <code>BaseSampler</code> <code>torchebm.samplers</code> Sampling algorithms <code>LangevinDynamics</code>, <code>HamiltonianMonteCarlo</code> <code>torchebm.losses</code> Loss functions <code>ContrastiveDivergence</code>, <code>PersistentContrastiveDivergence</code> <code>torchebm.models</code> Neural network models <code>BaseModel</code> <code>torchebm.cuda</code> CUDA-accelerated implementations Various CUDA kernels <code>torchebm.utils</code> Utility functions and helpers Visualization tools, diagnostics"},{"location":"developer_guide/architecture/#performance-considerations","title":"Performance Considerations","text":"<p>TorchEBM is designed with performance in mind:</p> <ul> <li>Vectorization: Operations are vectorized for efficient batch processing</li> <li>GPU Acceleration: Most operations can run on CUDA devices</li> <li>Memory Management: Careful memory management to avoid unnecessary allocations</li> <li>Parallel Sampling: Samples can be generated in parallel for better utilization of hardware</li> </ul>"},{"location":"developer_guide/architecture/#extension-points","title":"Extension Points","text":"<p>TorchEBM is designed to be extended in several ways:</p> <ol> <li>Custom Energy Functions: Create your own energy functions by subclassing <code>EnergyFunction</code></li> <li>Custom Samplers: Implement new sampling algorithms by subclassing <code>BaseSampler</code></li> <li>Custom Loss Functions: Create new training objectives for energy-based models</li> <li>Neural Network Energy Functions: Use neural networks as energy functions</li> </ol> <p>For more details on implementing extensions, see our API Design documentation. </p>"},{"location":"developer_guide/code_style/","title":"Code Style Guide","text":"<p>Consistent Style</p> <p>Following a consistent code style ensures our codebase remains readable and maintainable. This guide outlines the style conventions used in TorchEBM.</p>"},{"location":"developer_guide/code_style/#python-style-guidelines","title":"Python Style Guidelines","text":"<p>TorchEBM follows PEP 8 with some project-specific guidelines.</p>"},{"location":"developer_guide/code_style/#automatic-formatting","title":"Automatic Formatting","text":"<p>We use several tools to automatically format and check our code:</p> <ul> <li> <p> Black</p> <p>Automatic code formatter with a focus on consistency.</p> <pre><code>black torchebm/\n</code></pre> </li> <li> <p> isort</p> <p>Sorts imports alphabetically and separates them into sections.</p> <pre><code>isort torchebm/\n</code></pre> </li> <li> <p> Flake8</p> <p>Linter to catch logical and stylistic issues.</p> <pre><code>flake8 torchebm/\n</code></pre> </li> </ul>"},{"location":"developer_guide/code_style/#code-structure","title":"Code Structure","text":"Function DefinitionsClass Definitions <pre><code>def function_name(\n    param1: type,\n    param2: type,\n    param3: Optional[type] = None\n) -&gt; ReturnType:\n    \"\"\"Short description of the function.\n\n    More detailed explanation if needed.\n\n    Args:\n        param1: Description of parameter 1\n        param2: Description of parameter 2\n        param3: Description of parameter 3\n\n    Returns:\n        Description of the return value\n\n    Raises:\n        ExceptionType: When and why this exception is raised\n    \"\"\"\n    # Function implementation\n    pass\n</code></pre> <pre><code>class ClassName(BaseClass):\n    \"\"\"Short description of the class.\n\n    More detailed explanation if needed.\n\n    Args:\n        attr1: Description of attribute 1\n        attr2: Description of attribute 2\n    \"\"\"\n\n    def __init__(\n        self,\n        attr1: type,\n        attr2: type = default_value\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            attr1: Description of attribute 1\n            attr2: Description of attribute 2\n        \"\"\"\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def method_name(self, param: type) -&gt; ReturnType:\n        \"\"\"Short description of the method.\n\n        Args:\n            param: Description of parameter\n\n        Returns:\n            Description of the return value\n        \"\"\"\n        # Method implementation\n        pass\n</code></pre>"},{"location":"developer_guide/code_style/#naming-conventions","title":"Naming Conventions","text":""},{"location":"developer_guide/code_style/#classes","title":"Classes","text":"<p>Use <code>CamelCase</code> for class names:</p> <pre><code>class EnergyFunction:\n    pass\n\nclass LangevinDynamics:\n    pass\n</code></pre>"},{"location":"developer_guide/code_style/#functions-and-variables","title":"Functions and Variables","text":"<p>Use <code>snake_case</code> for functions and variables:</p> <pre><code>def compute_energy(x):\n    pass\n\nsample_count = 1000\n</code></pre>"},{"location":"developer_guide/code_style/#constants","title":"Constants","text":"<p>Use <code>UPPER_CASE</code> for constants:</p> <pre><code>DEFAULT_STEP_SIZE = 0.01\nMAX_ITERATIONS = 1000\n</code></pre>"},{"location":"developer_guide/code_style/#documentation-style","title":"Documentation Style","text":"<p>TorchEBM uses Google-style docstrings for all code documentation.</p> <p>Docstring Example</p> <pre><code>def sample_chain(\n    self, \n    dim: int, \n    n_steps: int, \n    n_samples: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"Generate samples using a Markov chain of specified length.\n\n    Args:\n        dim: Dimensionality of samples\n        n_steps: Number of steps in the chain\n        n_samples: Number of parallel chains to run\n\n    Returns:\n        Tensor of shape (n_samples, dim) containing final samples\n\n    Examples:\n        &gt;&gt;&gt; energy_fn = GaussianEnergy(torch.zeros(2), torch.eye(2))\n        &gt;&gt;&gt; sampler = LangevinDynamics(energy_fn, step_size=0.01)\n        &gt;&gt;&gt; samples = sampler.sample_chain(dim=2, n_steps=100, n_samples=10)\n    \"\"\"\n</code></pre>"},{"location":"developer_guide/code_style/#type-annotations","title":"Type Annotations","text":"<p>We use Python's type hints to improve code readability and enable static type checking:</p> <pre><code>from typing import Optional, List, Union, Dict, Tuple, Callable\n\ndef function(\n    tensor: torch.Tensor,\n    scale: float = 1.0,\n    use_cuda: bool = False,\n    callback: Optional[Callable[[torch.Tensor], None]] = None\n) -&gt; Tuple[torch.Tensor, float]:\n    # Implementation\n    pass\n</code></pre>"},{"location":"developer_guide/code_style/#cuda-code-style","title":"CUDA Code Style","text":"<p>For CUDA extensions, we follow these conventions:</p> File OrganizationCUDA Naming Conventions <pre><code>torchebm/cuda/\n\u251c\u2500\u2500 kernels/\n\u2502   \u251c\u2500\u2500 kernel_name.cu\n\u2502   \u2514\u2500\u2500 kernel_name.cuh\n\u251c\u2500\u2500 bindings.cpp\n\u2514\u2500\u2500 __init__.py\n</code></pre> <pre><code>// Function names in snake_case\n__global__ void compute_energy_kernel(float* input, float* output, int n) {\n    // Implementation\n}\n\n// Constants in UPPER_CASE\n#define BLOCK_SIZE 256\n</code></pre>"},{"location":"developer_guide/code_style/#imports-organization","title":"Imports Organization","text":"<p>Organize imports in the following order:</p> <ol> <li>Standard library imports</li> <li>Related third-party imports</li> <li>Local application/library specific imports</li> </ol> <pre><code># Standard library\nimport os\nimport sys\nfrom typing import Optional, Dict\n\n# Third-party\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# Local application\nfrom torchebm.core import EnergyFunction\nfrom torchebm.utils import get_device\n</code></pre>"},{"location":"developer_guide/code_style/#comments","title":"Comments","text":"<ul> <li>Use comments sparingly - prefer self-documenting code with clear variable names</li> <li>Add comments for complex algorithms or non-obvious implementations</li> <li>Update comments when you change code</li> </ul> <p>Good Comments Example</p> <pre><code># Correcting for numerical instability by adding a small epsilon\nnormalized_weights = weights / (torch.sum(weights, dim=1, keepdim=True) + 1e-8)\n</code></pre>"},{"location":"developer_guide/code_style/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>TorchEBM uses pre-commit hooks to enforce code style. Make sure to install them as described in the Development Setup guide.</p>"},{"location":"developer_guide/code_style/#recommended-editor-settings","title":"Recommended Editor Settings","text":"VS CodePyCharm <pre><code>{\n  \"editor.formatOnSave\": true,\n  \"editor.codeActionsOnSave\": {\n    \"source.organizeImports\": true\n  },\n  \"python.linting.enabled\": true,\n  \"python.linting.flake8Enabled\": true,\n  \"python.formatting.provider\": \"black\"\n}\n</code></pre> <ol> <li>Install Black and isort plugins</li> <li>Configure Code Style for Python to match PEP 8</li> <li>Set Black as the external formatter</li> <li>Enable \"Reformat code on save\"</li> <li>Configure isort for import optimization</li> </ol>"},{"location":"developer_guide/code_style/#style-enforcement","title":"Style Enforcement","text":"<p>Our CI pipeline checks for style compliance. Pull requests failing style checks will be automatically rejected.</p> <p>CI Pipeline Failure</p> <p>If your PR fails CI due to style issues, run the following commands locally to fix them:</p> <pre><code># Format code with Black\nblack torchebm/\n\n# Sort imports\nisort torchebm/\n\n# Run flake8\nflake8 torchebm/\n\n# Run mypy for type checking\nmypy torchebm/\n</code></pre>"},{"location":"developer_guide/commit_conventions/","title":"Commit Message Conventions","text":"<p>TorchEBM follows a specific format for commit messages to maintain a clear project history and generate meaningful changelogs. This document outlines the conventions that all contributors should follow when making commits to the project.</p>"},{"location":"developer_guide/commit_conventions/#format","title":"Format","text":"<p>Each commit message should have a specific format:</p> <ol> <li>The first line should be a maximum of 50-60 characters</li> <li>It should begin with an emoji followed by a type, then a colon and a brief description</li> <li>Any further details should be in the subsequent lines, separated by an empty line</li> </ol>"},{"location":"developer_guide/commit_conventions/#types-and-emojis","title":"Types and Emojis","text":"<p>We use the following types and emojis to categorize commits:</p> Type Emoji Description feat \u2728 Introduces a new feature fix \ud83d\udc1b Patches a bug in the codebase docs \ud83d\udcd6 Changes related to documentation style \ud83d\udc8e Changes that do not affect the meaning of the code (formatting) refactor \ud83d\udce6 Code changes that neither fix a bug nor add a feature perf \ud83d\ude80 Improvements to performance test \ud83d\udea8 Adding or correcting tests build \ud83d\udc77 Changes affecting the build system or external dependencies ci \ud83d\udcbb Changes to Continuous Integration configuration chore \ud83c\udfab Miscellaneous changes that don't modify source or test files revert \ud83d\udd19 Reverts a previous commit"},{"location":"developer_guide/commit_conventions/#examples","title":"Examples","text":"<pre><code>\u2728 feat: add Hamiltonian Monte Carlo sampler\n</code></pre> <pre><code>\ud83d\udc1b fix: correct gradient calculation in Langevin dynamics\n\nThis fixes an issue where gradients were not being properly scaled\nby the step size, leading to instability in long sampling chains.\n</code></pre> <pre><code>\ud83d\udcd6 docs: improve installation instructions\n\nUpdate pip installation command and add conda installation option.\n</code></pre>"},{"location":"developer_guide/commit_conventions/#version-bumping-and-releasing","title":"Version Bumping and Releasing","text":"<p>For Maintainers</p> <p>Version bumping and release tags are primarily for project maintainers. As a contributor, you don't need to worry about these when submitting pull requests. Project maintainers will handle versioning and releases.</p> <p>For project maintainers, our CI/CD workflow supports the following tags:</p> <ul> <li>Use <code>#major</code> for breaking changes requiring a major version bump (e.g., 1.0.0 to 2.0.0)</li> <li>Use <code>#minor</code> for new features requiring a minor version bump (e.g., 1.0.0 to 1.1.0)</li> <li>Default is patch level for bug fixes (e.g., 1.0.0 to 1.0.1)</li> <li>Include <code>#release</code> to trigger a release to PyPI</li> </ul> <p>Example (for maintainers): <pre><code>\u2728 feat: add comprehensive API for custom energy functions #minor #release\n</code></pre></p>"},{"location":"developer_guide/commit_conventions/#best-practices","title":"Best Practices","text":"<ol> <li>Be descriptive but concise in your commit message</li> <li>Focus on the why, not just the what</li> <li>Use present tense (\"add feature\" not \"added feature\")</li> <li>Separate commits logically - one commit per logical change</li> <li>Reference issues in commit messages when appropriate (e.g., \"Fixes #123\")</li> </ol> <p>Following these conventions helps maintain a clean project history and facilitates automated changelog generation. </p>"},{"location":"developer_guide/contributing/","title":"Contributing to TorchEBM","text":"<p>Thank you for your interest in contributing to TorchEBM! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"developer_guide/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We expect all contributors to follow our Code of Conduct. Please be respectful and inclusive in your interactions with others.</p>"},{"location":"developer_guide/contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are many ways to contribute to TorchEBM:</p> <ol> <li>Report bugs: Report bugs or issues by opening an issue on GitHub</li> <li>Suggest features: Suggest new features or improvements</li> <li>Improve documentation: Fix typos, clarify explanations, or add examples</li> <li>Write code: Implement new features, fix bugs, or improve performance</li> <li>Review pull requests: Help review code from other contributors</li> </ol>"},{"location":"developer_guide/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"developer_guide/contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/your-username/torchebm.git\ncd torchebm\n</code></pre></li> <li>Set the original repository as an upstream remote:    <pre><code>git remote add upstream https://github.com/soran-ghaderi/torchebm.git\n</code></pre></li> <li>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> </ol>"},{"location":"developer_guide/contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Create a new branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes</li> <li>Run tests to make sure your changes don't break existing functionality:    <pre><code>pytest\n</code></pre></li> <li>Add and commit your changes using our commit message conventions</li> </ol>"},{"location":"developer_guide/contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 for Python code style with some modifications:</p> <ul> <li>Line length limit: 88 characters</li> <li>Use double quotes for strings</li> <li>Follow naming conventions:</li> <li>Classes: <code>CamelCase</code></li> <li>Functions and variables: <code>snake_case</code></li> <li>Constants: <code>UPPER_CASE</code></li> </ul> <p>We use <code>black</code> for code formatting and <code>isort</code> for sorting imports.</p>"},{"location":"developer_guide/contributing/#commit-message-conventions","title":"Commit Message Conventions","text":"<p>We follow a specific format for commit messages to make the project history clear and generate meaningful changelogs. Each commit message should have a specific format:</p> <p>The first line should be max 50-60 chars. Any further details should be in the next lines separated by an empty line.</p> <ul> <li>\u2728 feat: Introduces a new feature</li> <li>\ud83d\udc1b fix: Patches a bug in the codebase</li> <li>\ud83d\udcd6 docs: Changes related to documentation</li> <li>\ud83d\udc8e style: Changes that do not affect the meaning of the code (formatting)</li> <li>\ud83d\udce6 refactor: Code changes that neither fix a bug nor add a feature</li> <li>\ud83d\ude80 perf: Improvements to performance</li> <li>\ud83d\udea8 test: Adding or correcting tests</li> <li>\ud83d\udc77 build: Changes affecting the build system or external dependencies</li> <li>\ud83d\udcbb ci: Changes to Continuous Integration configuration</li> <li>\ud83c\udfab chore: Miscellaneous changes that don't modify source or test files</li> <li>\ud83d\udd19 revert: Reverts a previous commit</li> </ul> <p>Example: <pre><code>\u2728 feat: new feature implemented\n\nThe details of the commit (if any) go here.\n</code></pre></p> <p>For version bumping, include one of these tags in your commit message: - Use <code>#major</code> for breaking changes - Use <code>#minor</code> for new features - Default is patch level for bug fixes</p> <p>For releasing to PyPI, include <code>#release</code> in your commit message.</p> <p>For more detailed information about our commit message conventions, please see our Commit Message Conventions guide.</p>"},{"location":"developer_guide/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Push your changes to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Create a pull request on GitHub</li> <li>In your pull request description, explain the changes and link to any related issues</li> <li>Wait for a review and address any feedback</li> </ol>"},{"location":"developer_guide/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Keep pull requests focused on a single task</li> <li>Add tests for new features or bug fixes</li> <li>Update documentation as needed</li> <li>Ensure all tests pass</li> <li>Follow the code style guidelines</li> </ul>"},{"location":"developer_guide/contributing/#issue-guidelines","title":"Issue Guidelines","text":"<p>When opening an issue, please provide:</p> <ul> <li>A clear and descriptive title</li> <li>A detailed description of the issue</li> <li>Steps to reproduce (for bugs)</li> <li>Expected vs. actual behavior</li> <li>Version information (TorchEBM, PyTorch, Python, CUDA)</li> <li>Any relevant code snippets or error messages</li> </ul>"},{"location":"developer_guide/contributing/#implementing-new-features","title":"Implementing New Features","text":""},{"location":"developer_guide/contributing/#samplers","title":"Samplers","text":"<p>When implementing a new sampler:</p> <ol> <li>Create a new file in <code>torchebm/samplers/</code></li> <li>Extend the <code>BaseSampler</code> class</li> <li>Implement the required methods:</li> <li><code>__init__</code>: Initialize the sampler with appropriate parameters</li> <li><code>step</code>: Implement the sampling step</li> <li><code>sample_chain</code>: Implement a full sampling chain (or use the default implementation)</li> <li>Add tests in <code>tests/samplers/</code></li> <li>Update documentation</li> </ol> <p>Example:</p> <pre><code>from torchebm.core import BaseSampler\nimport torch\n\nclass MySampler(BaseSampler):\n    def __init__(self, energy_function, param1, param2, device=\"cpu\"):\n        super().__init__(energy_function, device)\n        self.param1 = param1\n        self.param2 = param2\n\n    def sample_chain(self, x, step_idx=None):\n        # Implement your sampling algorithm here\n        # x shape: [n_samples, dim]\n\n        # Your sampler logic\n        x_new = ...\n\n        # Return updated samples and any diagnostics\n        return x_new, {\"diagnostic1\": value1, \"diagnostic2\": value2}\n</code></pre>"},{"location":"developer_guide/contributing/#energy-functions","title":"Energy Functions","text":"<p>When implementing a new energy function:</p> <ol> <li>Create a new class in <code>torchebm/core/energy_function.py</code> or a new file in <code>torchebm/core/</code></li> <li>Extend the <code>EnergyFunction</code> class</li> <li>Implement the required methods:</li> <li><code>__init__</code>: Initialize the energy function with appropriate parameters</li> <li><code>forward</code>: Compute the energy value for a given input</li> <li>Add tests in <code>tests/core/</code></li> <li>Update documentation</li> </ol> <p>Example:</p> <pre><code>from torchebm.core import EnergyFunction\nimport torch\n\nclass MyEnergyFunction(EnergyFunction):\n    def __init__(self, param1, param2):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def forward(self, x):\n        # Implement your energy function here\n        # x shape: [batch_size, dimension]\n        # Return shape: [batch_size]\n        return torch.sum(self.param1 * x**2 + self.param2 * torch.sin(x), dim=-1)\n</code></pre>"},{"location":"developer_guide/contributing/#loss-functions","title":"Loss Functions","text":"<p>When implementing a new loss function:</p> <ol> <li>Create a new class in <code>torchebm/losses/</code></li> <li>Implement the required methods:</li> <li><code>__init__</code>: Initialize the loss function with appropriate parameters</li> <li><code>forward</code>: Compute the loss value</li> <li>Add tests in <code>tests/losses/</code></li> <li>Update documentation</li> </ol> <p>Example:</p> <pre><code>import torch\nimport torch.nn as nn\n\nclass MyLossFunction(nn.Module):\n    def __init__(self, param1, param2):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def forward(self, model, data_samples):\n        # Implement your loss function here\n        # Return a scalar loss value\n        return loss\n</code></pre>"},{"location":"developer_guide/contributing/#documentation-guidelines","title":"Documentation Guidelines","text":"<ul> <li>Use clear, concise language</li> <li>Include examples for complex functionality</li> <li>Document parameters, return values, and exceptions</li> <li>Add docstrings to classes and functions</li> <li>Update the roadmap when implementing new features</li> </ul>"},{"location":"developer_guide/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help or have questions:</p> <ul> <li>Check existing documentation</li> <li>Search for similar issues on GitHub</li> <li>Ask for help in your pull request or issue</li> </ul>"},{"location":"developer_guide/contributing/#thank-you","title":"Thank You!","text":"<p>Thank you for contributing to TorchEBM! Your help is greatly appreciated and makes the library better for everyone. </p>"},{"location":"developer_guide/core_components/","title":"Core Components","text":"<p>Building Blocks</p> <p>TorchEBM is built around several core components that form the foundation of the library. This guide provides in-depth information about these components and how they interact.</p>"},{"location":"developer_guide/core_components/#component-overview","title":"Component Overview","text":"<ul> <li> <p> Energy Functions</p> <p>Define the energy landscape for probability distributions.</p> <pre><code>energy = energy_fn(x)  # Evaluate energy at point x\n</code></pre> </li> <li> <p> Samplers</p> <p>Generate samples from energy-based distributions.</p> <pre><code>samples = sampler.sample(n_samples=1000)  # Generate 1000 samples\n</code></pre> </li> <li> <p> Loss Functions</p> <p>Train energy-based models from data.</p> <pre><code>loss = loss_fn(model, data_samples)  # Compute training loss\n</code></pre> </li> <li> <p>:material-neural-network:{ .lg .middle } Models</p> <p>Parameterize energy functions with neural networks.</p> <pre><code>model = EnergyModel(network=nn.Sequential(...))\n</code></pre> </li> </ul>"},{"location":"developer_guide/core_components/#energy-functions","title":"Energy Functions","text":"<p>Energy functions are the core building block of TorchEBM. They define a scalar energy value for each point in the sample space.</p>"},{"location":"developer_guide/core_components/#base-energy-function","title":"Base Energy Function","text":"<p>The <code>EnergyFunction</code> class is the foundation for all energy functions:</p> <pre><code>class EnergyFunction(nn.Module):\n    \"\"\"Base class for all energy functions.\n\n    An energy function maps points in the sample space to scalar energy values.\n    Lower energy corresponds to higher probability density.\n    \"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy for input points.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        raise NotImplementedError\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score function (gradient of energy) for input points.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size, dim) containing score values\n        \"\"\"\n        x = x.requires_grad_(True)\n        energy = self.forward(x)\n        return torch.autograd.grad(energy.sum(), x, create_graph=True)[0]\n</code></pre>"},{"location":"developer_guide/core_components/#analytical-energy-functions","title":"Analytical Energy Functions","text":"<p>TorchEBM provides several analytical energy functions for testing and benchmarking:</p> Gaussian EnergyDouble Well Energy <pre><code>class GaussianEnergy(EnergyFunction):\n    \"\"\"Gaussian energy function.\n\n    Energy function defined by a multivariate Gaussian distribution:\n    E(x) = 0.5 * (x - mean)^T * precision * (x - mean)\n    \"\"\"\n\n    def __init__(self, mean: torch.Tensor, cov: torch.Tensor):\n        \"\"\"Initialize Gaussian energy function.\n\n        Args:\n            mean: Mean vector of shape (dim,)\n            cov: Covariance matrix of shape (dim, dim)\n        \"\"\"\n        super().__init__()\n        self.register_buffer(\"mean\", mean)\n        self.register_buffer(\"cov\", cov)\n        self.register_buffer(\"precision\", torch.inverse(cov))\n        self._dim = mean.size(0)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute Gaussian energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        centered = x - self.mean\n        return 0.5 * torch.sum(\n            centered * (self.precision @ centered.T).T,\n            dim=1\n        )\n</code></pre> <pre><code>class DoubleWellEnergy(EnergyFunction):\n    \"\"\"Double well energy function.\n\n    Energy function with two local minima:\n    E(x) = a * (x^2 - b)^2\n    \"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 2.0):\n        \"\"\"Initialize double well energy function.\n\n        Args:\n            a: Scale parameter\n            b: Parameter controlling the distance between wells\n        \"\"\"\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute double well energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        return self.a * torch.sum((x**2 - self.b)**2, dim=1)\n</code></pre>"},{"location":"developer_guide/core_components/#composite-energy-functions","title":"Composite Energy Functions","text":"<p>Energy functions can be composed to create more complex landscapes:</p> <pre><code>class CompositeEnergy(EnergyFunction):\n    \"\"\"Composite energy function.\n\n    Combines multiple energy functions through addition.\n    \"\"\"\n\n    def __init__(self, energy_functions: List[EnergyFunction], weights: Optional[List[float]] = None):\n        \"\"\"Initialize composite energy function.\n\n        Args:\n            energy_functions: List of energy functions to combine\n            weights: Optional weights for each energy function\n        \"\"\"\n        super().__init__()\n        self.energy_functions = nn.ModuleList(energy_functions)\n        if weights is None:\n            weights = [1.0] * len(energy_functions)\n        self.weights = weights\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute composite energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        return sum(w * f(x) for w, f in zip(self.weights, self.energy_functions))\n</code></pre>"},{"location":"developer_guide/core_components/#samplers","title":"Samplers","text":"<p>Samplers generate samples from energy-based distributions. They provide methods to initialize and update samples based on the energy landscape.</p>"},{"location":"developer_guide/core_components/#base-sampler","title":"Base Sampler","text":"<p>The <code>Sampler</code> class is the foundation for all sampling algorithms:</p> <pre><code>class Sampler(ABC):\n    \"\"\"Base class for all samplers.\n\n    A sampler generates samples from an energy-based distribution.\n    \"\"\"\n\n    def __init__(self, energy_function: EnergyFunction):\n        \"\"\"Initialize sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n        \"\"\"\n        self.energy_function = energy_function\n\n    @abstractmethod\n    def sample(self, n_samples: int, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of shape (n_samples, dim) containing samples\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using a Markov chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of shape (n_samples, dim) containing final samples\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/core_components/#langevin-dynamics","title":"Langevin Dynamics","text":"<p>The <code>LangevinDynamics</code> sampler implements Langevin Monte Carlo:</p> <pre><code>class LangevinDynamics(Sampler):\n    \"\"\"Langevin dynamics sampler.\n\n    Uses Langevin dynamics to sample from an energy-based distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        step_size: float = 0.01,\n        noise_scale: float = 1.0\n    ):\n        \"\"\"Initialize Langevin dynamics sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            step_size: Step size for updates\n            noise_scale: Scale of noise added at each step\n        \"\"\"\n        super().__init__(energy_function)\n        self.step_size = step_size\n        self.noise_scale = noise_scale\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Langevin dynamics.\n\n        Args:\n            x: Current samples of shape (n_samples, dim)\n\n        Returns:\n            Updated samples of shape (n_samples, dim)\n        \"\"\"\n        # Compute score (gradient of energy)\n        score = self.energy_function.score(x)\n\n        # Update samples\n        noise = torch.randn_like(x) * np.sqrt(2 * self.step_size * self.noise_scale)\n        x_new = x - self.step_size * score + noise\n\n        return x_new\n\n    def sample_chain(\n        self,\n        dim: int,\n        n_steps: int,\n        n_samples: int = 1,\n        initial_samples: Optional[torch.Tensor] = None,\n        return_trajectory: bool = False\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Generate samples using a Langevin dynamics chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            initial_samples: Optional initial samples\n            return_trajectory: Whether to return the full trajectory\n\n        Returns:\n            Tensor of shape (n_samples, dim) containing final samples,\n            or a tuple of (samples, trajectory) if return_trajectory is True\n        \"\"\"\n        # Initialize samples\n        if initial_samples is None:\n            x = torch.randn(n_samples, dim)\n        else:\n            x = initial_samples.clone()\n\n        # Initialize trajectory if needed\n        if return_trajectory:\n            trajectory = torch.zeros(n_steps + 1, n_samples, dim)\n            trajectory[0] = x\n\n        # Run chain\n        for i in range(n_steps):\n            x = self.sample_step(x)\n            if return_trajectory:\n                trajectory[i + 1] = x\n\n        if return_trajectory:\n            return x, trajectory\n        else:\n            return x\n\n    def sample(self, n_samples: int, dim: int, n_steps: int = 100, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\n\n        Args:\n            n_samples: Number of samples to generate\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            **kwargs: Additional parameters passed to sample_chain\n\n        Returns:\n            Tensor of shape (n_samples, dim) containing samples\n        \"\"\"\n        return self.sample_chain(dim=dim, n_steps=n_steps, n_samples=n_samples, **kwargs)\n</code></pre>"},{"location":"developer_guide/core_components/#loss-functions","title":"Loss Functions","text":"<p>Loss functions are used to train energy-based models from data. They provide methods to compute gradients for model updates.</p>"},{"location":"developer_guide/core_components/#base-loss-function","title":"Base Loss Function","text":"<p>The <code>Loss</code> class is the foundation for all loss functions:</p> <pre><code>class Loss(ABC):\n    \"\"\"Base class for all loss functions.\n\n    A loss function computes a loss value for an energy-based model.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(\n        self,\n        model: nn.Module,\n        data_samples: torch.Tensor,\n        **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute loss for the model.\n\n        Args:\n            model: Energy-based model\n            data_samples: Samples from the target distribution\n            **kwargs: Additional loss-specific parameters\n\n        Returns:\n            Scalar loss value\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/core_components/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>The <code>ContrastiveDivergence</code> loss implements the contrastive divergence algorithm:</p> <pre><code>class ContrastiveDivergence(Loss):\n    \"\"\"Contrastive divergence loss.\n\n    Uses contrastive divergence to train energy-based models.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler: Sampler,\n        k: int = 1,\n        batch_size: Optional[int] = None\n    ):\n        \"\"\"Initialize contrastive divergence loss.\n\n        Args:\n            sampler: Sampler to generate model samples\n            k: Number of sampling steps (CD-k)\n            batch_size: Optional batch size for sampling\n        \"\"\"\n        super().__init__()\n        self.sampler = sampler\n        self.k = k\n        self.batch_size = batch_size\n\n    def __call__(\n        self,\n        model: nn.Module,\n        data_samples: torch.Tensor,\n        **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute contrastive divergence loss.\n\n        Args:\n            model: Energy-based model\n            data_samples: Samples from the target distribution\n            **kwargs: Additional parameters passed to the sampler\n\n        Returns:\n            Scalar loss value\n        \"\"\"\n        # Get data statistics\n        batch_size = self.batch_size or data_samples.size(0)\n        dim = data_samples.size(1)\n\n        # Set the model as the sampler's energy function\n        self.sampler.energy_function = model\n\n        # Generate model samples\n        model_samples = self.sampler.sample_chain(\n            dim=dim,\n            n_steps=self.k,\n            n_samples=batch_size,\n            **kwargs\n        )\n\n        # Compute energies\n        data_energy = model(data_samples).mean()\n        model_energy = model(model_samples).mean()\n\n        # Compute loss\n        loss = data_energy - model_energy\n\n        return loss\n</code></pre>"},{"location":"developer_guide/core_components/#models","title":"Models","text":"<p>Models parameterize energy functions using neural networks.</p>"},{"location":"developer_guide/core_components/#energy-model","title":"Energy Model","text":"<p>The <code>EnergyModel</code> class wraps a neural network as an energy function:</p> <pre><code>class EnergyModel(EnergyFunction):\n    \"\"\"Neural network-based energy model.\n\n    Uses a neural network to parameterize an energy function.\n    \"\"\"\n\n    def __init__(self, network: nn.Module):\n        \"\"\"Initialize energy model.\n\n        Args:\n            network: Neural network that outputs scalar energy values\n        \"\"\"\n        super().__init__()\n        self.network = network\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy using the neural network.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"developer_guide/core_components/#component-interactions","title":"Component Interactions","text":"<p>The following diagram illustrates how the core components interact:</p> <pre><code>graph TD\n    A[Energy Function] --&gt;|Defines landscape| B[Sampler]\n    B --&gt;|Generates samples| C[Training Process]\n    D[Loss Function] --&gt;|Guides training| C\n    C --&gt;|Updates| E[Energy Model]\n    E --&gt;|Parameterizes| A</code></pre>"},{"location":"developer_guide/core_components/#typical-usage-flow","title":"Typical Usage Flow","text":"<ol> <li>Define an energy function - Either analytical or neural network-based</li> <li>Create a sampler - Using the energy function</li> <li>Generate samples - Using the sampler</li> <li>Train a model - Using the loss function and sampler</li> <li>Use the trained model - For tasks like generation or density estimation</li> </ol> <pre><code># Define energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Create sampler\nsampler = LangevinDynamics(energy_function=energy_fn, step_size=0.01)\n\n# Generate samples\nsamples = sampler.sample_chain(dim=2, n_steps=1000, n_samples=100)\n\n# Create and train a model\nmodel = EnergyModel(network=MLP(input_dim=2, hidden_dims=[32, 32], output_dim=1))\nloss_fn = ContrastiveDivergence(sampler=sampler, k=10)\n\n# Training loop\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nfor epoch in range(100):\n    optimizer.zero_grad()\n    loss = loss_fn(model, data_samples)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"developer_guide/core_components/#extension-points","title":"Extension Points","text":"<p>TorchEBM is designed to be extensible at several points:</p> <ul> <li>New Energy Functions - Create by subclassing <code>EnergyFunction</code></li> <li>New Samplers - Create by subclassing <code>Sampler</code></li> <li>New Loss Functions - Create by subclassing <code>Loss</code></li> <li>New Models - Create by subclassing <code>EnergyModel</code> or using custom networks</li> </ul>"},{"location":"developer_guide/core_components/#component-lifecycle","title":"Component Lifecycle","text":"<p>Each component in TorchEBM has a typical lifecycle:</p> <ol> <li>Initialization - Configure the component with parameters</li> <li>Usage - Use the component to perform its intended function</li> <li>Composition - Combine with other components</li> <li>Extension - Extend with new functionality</li> </ol> <p>Understanding this lifecycle helps when implementing new components or extending existing ones.</p>"},{"location":"developer_guide/core_components/#best-practices","title":"Best Practices","text":"<p>When working with TorchEBM components, follow these best practices:</p> <ul> <li>Energy Functions: Ensure they're properly normalized for stable training</li> <li>Samplers: Check mixing time and adjust parameters accordingly</li> <li>Loss Functions: Monitor training stability and adjust hyperparameters</li> <li>Models: Use appropriate architecture for the problem domain</li> </ul> <p>Performance Optimization</p> <p>For large-scale applications, consider using CUDA-optimized implementations and batch processing for better performance.</p> <ul> <li> <p> Energy Functions</p> <p>Learn about energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Explore sampler implementation details.</p> <p> Samplers</p> </li> <li> <p> Loss Functions</p> <p>Understand loss function implementation details.</p> <p> Loss Functions</p> </li> </ul>"},{"location":"developer_guide/cuda_optimizations/","title":"CUDA Optimizations","text":"<p>Performance Engineering</p> <p>TorchEBM leverages CUDA to accelerate performance-critical operations. This guide explains the CUDA optimization strategies and how to implement new CUDA kernels.</p>"},{"location":"developer_guide/cuda_optimizations/#overview","title":"Overview","text":"<p>CUDA optimizations in TorchEBM focus on accelerating three main performance bottlenecks:</p> <ul> <li> <p>:material-gradient:{ .lg .middle } Score Function Computation</p> <p>Computing gradients of energy functions can be computationally intensive, especially for large batches or complex energy functions.</p> </li> <li> <p> Sampling Operations</p> <p>Sampling algorithms like Langevin dynamics require many iterations of score computation and updates.</p> </li> <li> <p> Energy Evaluation</p> <p>Evaluating energy functions on large batches of samples during training or inference.</p> </li> </ul>"},{"location":"developer_guide/cuda_optimizations/#cuda-architecture","title":"CUDA Architecture","text":"<p>TorchEBM's CUDA implementation follows a layered architecture:</p> <pre><code>torchebm/\n\u2514\u2500\u2500 cuda/\n    \u251c\u2500\u2500 __init__.py             # Package exports\n    \u251c\u2500\u2500 ops.py                  # Python interface to CUDA operations\n    \u251c\u2500\u2500 utils.py                # CUDA utilities\n    \u251c\u2500\u2500 bindings.cpp            # PyTorch C++ bindings\n    \u2514\u2500\u2500 kernels/                # CUDA kernel implementations\n        \u251c\u2500\u2500 score_function.cu   # Score function kernel\n        \u251c\u2500\u2500 langevin_step.cu    # Langevin dynamics step kernel\n        \u251c\u2500\u2500 energy_kernels.cu   # Energy function kernels\n        \u2514\u2500\u2500 include/            # Header files\n            \u251c\u2500\u2500 common.cuh      # Common utilities\n            \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#pytorch-c-extension","title":"PyTorch C++ Extension","text":"<p>TorchEBM's CUDA functionality is built on PyTorch's C++ extension mechanism:</p> <pre><code># In setup.py\nfrom torch.utils.cpp_extension import CUDAExtension, BuildExtension\n\nsetup(\n    name=\"torchebm\",\n    ext_modules=[\n        CUDAExtension(\n            \"torchebm.cuda.kernels\",\n            sources=[\n                \"torchebm/cuda/bindings.cpp\",\n                \"torchebm/cuda/kernels/score_function.cu\",\n                \"torchebm/cuda/kernels/langevin_step.cu\",\n                \"torchebm/cuda/kernels/energy_kernels.cu\",\n            ],\n            include_dirs=[\"torchebm/cuda/kernels/include\"],\n            extra_compile_args={\"cxx\": [\"-O3\"], \"nvcc\": [\"-O3\"]}\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension}\n)\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#score-function-optimization","title":"Score Function Optimization","text":"<p>The score function (gradient of energy) computation is optimized with CUDA:</p>"},{"location":"developer_guide/cuda_optimizations/#python-interface","title":"Python Interface","text":"<pre><code>def cuda_score(energy_fn, x, create_graph=False):\n    \"\"\"CUDA-optimized score function computation.\n\n    Args:\n        energy_fn: Energy function\n        x: Input tensor of shape (batch_size, dim)\n        create_graph: Whether to create gradient graph\n\n    Returns:\n        Score tensor of shape (batch_size, dim)\n    \"\"\"\n    # Check if energy function has custom CUDA implementation\n    if hasattr(energy_fn, \"cuda_score_impl\") and torch.cuda.is_available():\n        return energy_fn.cuda_score_impl(x, create_graph)\n\n    # Fall back to standard implementation for common energy functions\n    if isinstance(energy_fn, GaussianEnergy) and torch.cuda.is_available():\n        return _gaussian_score_cuda(energy_fn, x)\n\n    # Fall back to autograd\n    return score_function(energy_fn, x, create_graph)\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#cuda-kernel","title":"CUDA Kernel","text":"<pre><code>// In score_function.cu\n__global__ void gaussian_score_kernel(\n    const float* x,        // Input samples (batch_size * dim)\n    const float* mean,     // Mean vector (dim)\n    const float* precision,// Precision matrix (dim * dim)\n    float* score,          // Output score (batch_size * dim)\n    int batch_size,        // Batch size\n    int dim                // Dimensionality\n) {\n    // Get sample index from CUDA thread\n    int sample_idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check if this thread processes a valid sample\n    if (sample_idx &lt; batch_size) {\n        // Compute centered sample (x - mean)\n        float centered[MAX_DIM];  // Use shared memory for better performance\n        for (int d = 0; d &lt; dim; ++d) {\n            centered[d] = x[sample_idx * dim + d] - mean[d];\n        }\n\n        // Compute Precision * (x - mean)\n        for (int d = 0; d &lt; dim; ++d) {\n            float sum = 0.0f;\n            for (int j = 0; j &lt; dim; ++j) {\n                sum += precision[d * dim + j] * centered[j];\n            }\n            // Score is -Precision * (x - mean)\n            score[sample_idx * dim + d] = -sum;\n        }\n    }\n}\n\n// C++ binding function\ntorch::Tensor gaussian_score_cuda(\n    torch::Tensor x,\n    torch::Tensor mean,\n    torch::Tensor precision\n) {\n    // Get dimensions\n    int batch_size = x.size(0);\n    int dim = x.size(1);\n\n    // Create output tensor\n    auto score = torch::empty_like(x);\n\n    // Configure CUDA kernel\n    const int threads_per_block = 256;\n    const int blocks = (batch_size + threads_per_block - 1) / threads_per_block;\n\n    // Launch kernel\n    gaussian_score_kernel&lt;&lt;&lt;blocks, threads_per_block&gt;&gt;&gt;(\n        x.data_ptr&lt;float&gt;(),\n        mean.data_ptr&lt;float&gt;(),\n        precision.data_ptr&lt;float&gt;(),\n        score.data_ptr&lt;float&gt;(),\n        batch_size,\n        dim\n    );\n\n    return score;\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#langevin-dynamics-optimization","title":"Langevin Dynamics Optimization","text":"<p>Langevin dynamics sampling is accelerated using CUDA kernels:</p>"},{"location":"developer_guide/cuda_optimizations/#python-interface_1","title":"Python Interface","text":"<pre><code>class CUDALangevinDynamics(LangevinDynamics):\n    \"\"\"CUDA-optimized Langevin dynamics sampler.\"\"\"\n\n    def __init__(self, energy_function, step_size=0.01, noise_scale=1.0):\n        super().__init__(energy_function, step_size, noise_scale)\n\n    def sample_step(self, x):\n        \"\"\"Perform one step of Langevin dynamics with CUDA optimization.\"\"\"\n        if not torch.cuda.is_available() or not x.is_cuda:\n            # Fall back to CPU implementation\n            return super().sample_step(x)\n\n        # Use optimized CUDA implementation\n        return langevin_step_cuda(\n            x,\n            self.energy_function,\n            self.step_size,\n            self.noise_scale\n        )\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#cuda-kernel_1","title":"CUDA Kernel","text":"<pre><code>// In langevin_step.cu\n__global__ void langevin_step_kernel(\n    const float* x,        // Input samples (batch_size * dim)\n    const float* score,    // Score function values (batch_size * dim)\n    float* x_new,          // Updated samples (batch_size * dim)\n    float step_size,       // Step size parameter\n    float noise_scale,     // Noise scale parameter\n    float* noise,          // Random noise (batch_size * dim)\n    int batch_size,        // Batch size\n    int dim                // Dimensionality\n) {\n    // Get global thread ID\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check bounds\n    if (idx &lt; batch_size * dim) {\n        // Compute Langevin update\n        // x_new = x - step_size * score + sqrt(2 * step_size * noise_scale) * noise\n        float noise_factor = sqrt(2.0f * step_size * noise_scale);\n        x_new[idx] = x[idx] - step_size * score[idx] + noise_factor * noise[idx];\n    }\n}\n\n// C++ binding function\ntorch::Tensor langevin_step_cuda(\n    torch::Tensor x,\n    torch::Tensor score,\n    float step_size,\n    float noise_scale\n) {\n    // Get dimensions\n    int batch_size = x.size(0);\n    int dim = x.size(1);\n\n    // Generate random noise\n    auto noise = torch::randn_like(x);\n\n    // Create output tensor\n    auto x_new = torch::empty_like(x);\n\n    // Configure CUDA kernel\n    const int threads_per_block = 256;\n    const int total_elements = batch_size * dim;\n    const int blocks = (total_elements + threads_per_block - 1) / threads_per_block;\n\n    // Launch kernel\n    langevin_step_kernel&lt;&lt;&lt;blocks, threads_per_block&gt;&gt;&gt;(\n        x.data_ptr&lt;float&gt;(),\n        score.data_ptr&lt;float&gt;(),\n        x_new.data_ptr&lt;float&gt;(),\n        step_size,\n        noise_scale,\n        noise.data_ptr&lt;float&gt;(),\n        batch_size,\n        dim\n    );\n\n    return x_new;\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#energy-function-optimization","title":"Energy Function Optimization","text":"<p>Energy function evaluation is optimized for specific analytical energy functions:</p>"},{"location":"developer_guide/cuda_optimizations/#gaussian-energy","title":"Gaussian Energy","text":"<pre><code>// In energy_kernels.cu\n__global__ void gaussian_energy_kernel(\n    const float* x,        // Input samples (batch_size * dim)\n    const float* mean,     // Mean vector (dim)\n    const float* precision,// Precision matrix (dim * dim)\n    float* energy,         // Output energy (batch_size)\n    int batch_size,        // Batch size\n    int dim                // Dimensionality\n) {\n    // Get sample index\n    int sample_idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check bounds\n    if (sample_idx &lt; batch_size) {\n        // Compute centered values\n        float centered[MAX_DIM];\n        for (int d = 0; d &lt; dim; ++d) {\n            centered[d] = x[sample_idx * dim + d] - mean[d];\n        }\n\n        // Compute quadratic form: centered^T * precision * centered\n        float quadratic_sum = 0.0f;\n        for (int i = 0; i &lt; dim; ++i) {\n            float row_sum = 0.0f;\n            for (int j = 0; j &lt; dim; ++j) {\n                row_sum += precision[i * dim + j] * centered[j];\n            }\n            quadratic_sum += centered[i] * row_sum;\n        }\n\n        // Energy is 0.5 * quadratic_sum\n        energy[sample_idx] = 0.5f * quadratic_sum;\n    }\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#memory-optimization-techniques","title":"Memory Optimization Techniques","text":"<p>TorchEBM uses several memory optimization techniques:</p>"},{"location":"developer_guide/cuda_optimizations/#shared-memory-usage","title":"Shared Memory Usage","text":"<pre><code>__global__ void optimized_kernel(...) {\n    // Declare shared memory for frequently accessed data\n    __shared__ float shared_data[BLOCK_SIZE];\n\n    // Load data into shared memory\n    shared_data[threadIdx.x] = global_data[blockIdx.x * blockDim.x + threadIdx.x];\n    __syncthreads();\n\n    // Use shared memory for computation\n    // ...\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#memory-coalescing","title":"Memory Coalescing","text":"<pre><code>// Good: Coalesced memory access\n__global__ void coalesced_kernel(float* data, float* result, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; size) {\n        result[idx] = data[idx] * 2.0f;\n    }\n}\n\n// Avoid: Non-coalesced memory access\n__global__ void noncoalesced_kernel(float* data, float* result, int width, int height) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row &lt; height) {\n        for (int col = 0; col &lt; width; ++col) {\n            // Non-coalesced access pattern\n            result[row * width + col] = data[row * width + col] * 2.0f;\n        }\n    }\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#reducing-register-pressure","title":"Reducing Register Pressure","text":"<pre><code>__global__ void optimized_kernel(...) {\n    // Use local variables instead of arrays where possible\n    float x1, x2, x3, x4;\n\n    // Process in chunks to reduce register usage\n    // ...\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#thread-block-organization","title":"Thread Block Organization","text":"<p>CUDA kernels in TorchEBM are organized to maximize performance:</p> <pre><code>// Compute optimal block size based on problem dimensions\nint compute_block_size(int dim) {\n    // Power of 2 for better performance\n    if (dim &lt;= 32) return 32;\n    if (dim &lt;= 64) return 64;\n    if (dim &lt;= 128) return 128;\n    return 256;\n}\n\n// Launch kernel with optimal configuration\nvoid launch_kernel(int batch_size, int dim) {\n    int block_size = compute_block_size(dim);\n    int grid_size = (batch_size + block_size - 1) / block_size;\n\n    my_kernel&lt;&lt;&lt;grid_size, block_size&gt;&gt;&gt;(/* args */);\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#custom-cuda-kernels-for-special-energy-functions","title":"Custom CUDA Kernels for Special Energy Functions","text":"<p>TorchEBM includes specialized CUDA kernels for common energy functions:</p> <pre><code>// Specialized kernel for Rosenbrock function\n__global__ void rosenbrock_energy_kernel(\n    const float* x,\n    float* energy,\n    float a,\n    float b,\n    int batch_size,\n    int dim\n) {\n    int sample_idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (sample_idx &lt; batch_size) {\n        float sum = 0.0f;\n\n        for (int i = 0; i &lt; dim - 1; ++i) {\n            float x_i = x[sample_idx * dim + i];\n            float x_i_plus_1 = x[sample_idx * dim + i + 1];\n\n            float term1 = b * (x_i_plus_1 - x_i * x_i) * (x_i_plus_1 - x_i * x_i);\n            float term2 = (x_i - a) * (x_i - a);\n\n            sum += term1 + term2;\n        }\n\n        energy[sample_idx] = sum;\n    }\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>The following benchmarks demonstrate the performance gains from CUDA optimization:</p> <ul> <li> <p> Score Function Computation</p> <ul> <li>CPU Implementation: 100 ms</li> <li>CUDA Implementation: 5 ms</li> <li>Speedup: 20x</li> </ul> </li> <li> <p> Langevin Dynamics Sampling</p> <ul> <li>CPU Implementation: 2000 ms</li> <li>CUDA Implementation: 200 ms</li> <li>Speedup: 10x</li> </ul> </li> <li> <p> Energy Evaluation</p> <ul> <li>CPU Implementation: 80 ms</li> <li>CUDA Implementation: 6 ms</li> <li>Speedup: 13x</li> </ul> </li> </ul>"},{"location":"developer_guide/cuda_optimizations/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>TorchEBM supports mixed precision training:</p> <pre><code>def mixed_precision_score(energy_fn, x):\n    \"\"\"Compute score with mixed precision.\"\"\"\n    # Cast to half precision for computation\n    x_half = x.half()\n    x_half.requires_grad_(True)\n\n    # Compute energy in half precision\n    with torch.cuda.amp.autocast():\n        energy = energy_fn(x_half)\n\n    # Compute gradient in full precision\n    score = torch.autograd.grad(energy.sum(), x_half)[0].float()\n\n    return score\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#multi-gpu-support","title":"Multi-GPU Support","text":"<p>TorchEBM provides utilities for multi-GPU operation:</p> <pre><code>def distribute_sampling(energy_fn, n_samples, n_steps, device_ids):\n    \"\"\"Distribute sampling across multiple GPUs.\"\"\"\n    # Distribute samples across devices\n    samples_per_device = n_samples // len(device_ids)\n\n    results = []\n    for i, device_id in enumerate(device_ids):\n        device = torch.device(f\"cuda:{device_id}\")\n\n        # Create sampler on device\n        sampler = LangevinDynamics(energy_fn).to(device)\n\n        # Compute samples for this device\n        samples = sampler.sample_chain(\n            dim=energy_fn.dim,\n            n_steps=n_steps,\n            n_samples=samples_per_device\n        )\n\n        results.append(samples)\n\n    # Gather results from all devices\n    return torch.cat(results, dim=0)\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#cuda-stream-management","title":"CUDA Stream Management","text":"<p>TorchEBM uses CUDA streams for concurrent execution:</p> <pre><code>def parallel_score_computation(energy_fn, samples_list):\n    \"\"\"Compute scores for multiple sample batches in parallel.\"\"\"\n    # Create streams for parallel execution\n    streams = [torch.cuda.Stream() for _ in range(len(samples_list))]\n\n    # Start computation in separate streams\n    results = []\n    for i, samples in enumerate(samples_list):\n        with torch.cuda.stream(streams[i]):\n            score = energy_fn.score(samples)\n            results.append(score)\n\n    # Synchronize streams\n    for stream in streams:\n        stream.synchronize()\n\n    return results\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#implementing-custom-cuda-kernels","title":"Implementing Custom CUDA Kernels","text":"<p>To add a new CUDA kernel to TorchEBM:</p> <ol> <li>Create a new <code>.cu</code> file in the <code>torchebm/cuda/kernels/</code> directory</li> <li>Implement the CUDA kernel and C++ binding function</li> <li>Add the source file to the <code>CUDAExtension</code> in <code>setup.py</code></li> <li>Create a Python interface in <code>torchebm/cuda/ops.py</code></li> </ol> <p>Example of a custom kernel implementation:</p> <pre><code>// In custom_kernel.cu\n#include &lt;torch/extension.h&gt;\n#include \"common.cuh\"\n\n// CUDA kernel\n__global__ void custom_kernel(...) {\n    // Kernel implementation\n}\n\n// C++ binding function\ntorch::Tensor custom_kernel_cuda(...) {\n    // Binding implementation\n    // ...\n    return result;\n}\n\n// Register function for Python binding\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"custom_kernel\", &amp;custom_kernel_cuda, \"Custom kernel implementation\");\n}\n</code></pre>"},{"location":"developer_guide/cuda_optimizations/#troubleshooting-cuda-issues","title":"Troubleshooting CUDA Issues","text":"<p>Common CUDA issues and solutions:</p> <p>Common Pitfalls</p> <ul> <li>Check for proper error handling in CUDA code</li> <li>Beware of race conditions in kernel execution</li> <li>Ensure correct synchronization between CPU and GPU</li> <li>Verify tensor memory layouts match expectations</li> </ul>"},{"location":"developer_guide/cuda_optimizations/#memory-errors","title":"Memory Errors","text":"<ul> <li>Check for memory leaks</li> <li>Reduce batch size</li> <li>Use torch.cuda.empty_cache()</li> <li>Monitor memory usage with torch.cuda.memory_summary()</li> </ul>"},{"location":"developer_guide/cuda_optimizations/#performance-issues","title":"Performance Issues","text":"<ul> <li>Use CUDA profiling tools</li> <li>Check for serialized operations</li> <li>Optimize memory access patterns</li> <li>Reduce kernel launch overhead</li> </ul>"},{"location":"developer_guide/cuda_optimizations/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Understand the core components of TorchEBM.</p> <p> Core Components</p> </li> <li> <p> Energy Functions</p> <p>Learn about energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> CUDA Programming</p> <p>NVIDIA's CUDA programming guide.</p> <p> CUDA Guide</p> </li> </ul>"},{"location":"developer_guide/design_principles/","title":"Design Principles","text":"<p>Project Philosophy</p> <p>TorchEBM is built on a set of core design principles that guide its development. Understanding these principles will help you contribute in a way that aligns with the project's vision.</p>"},{"location":"developer_guide/design_principles/#core-philosophy","title":"Core Philosophy","text":"<p>TorchEBM aims to be:</p> <ul> <li> <p> Performant</p> <p>High-performance implementations that leverage PyTorch's capabilities and CUDA acceleration.</p> </li> <li> <p> Modular</p> <p>Components that can be easily combined, extended, and customized.</p> </li> <li> <p> Intuitive</p> <p>Clear, well-documented APIs that are easy to understand and use.</p> </li> <li> <p> Educational</p> <p>Serves as both a practical tool and a learning resource for energy-based modeling.</p> </li> </ul>"},{"location":"developer_guide/design_principles/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"developer_guide/design_principles/#composable-base-classes","title":"Composable Base Classes","text":"<p>TorchEBM is built around a set of extensible base classes that provide common interface:</p> <pre><code>class EnergyFunction(nn.Module):\n    \"\"\"Base class for all energy functions.\"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy for input x.\"\"\"\n        raise NotImplementedError\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score (gradient of energy) for input x.\"\"\"\n        x = x.requires_grad_(True)\n        energy = self.forward(x)\n        return torch.autograd.grad(energy.sum(), x, create_graph=True)[0]\n</code></pre> <p>This design allows for:</p> <ul> <li>Composition: Combining energy functions via addition, multiplication, etc.</li> <li>Extension: Creating new energy functions by subclassing</li> <li>Integration: Using energy functions with any sampler that follows the interface</li> </ul>"},{"location":"developer_guide/design_principles/#factory-methods","title":"Factory Methods","text":"<p>Factory methods create configured instances with sensible defaults:</p> <pre><code>@classmethod\ndef create_standard(cls, dim: int = 2) -&gt; 'GaussianEnergy':\n    \"\"\"Create a standard Gaussian energy function.\"\"\"\n    return cls(mean=torch.zeros(dim), cov=torch.eye(dim))\n</code></pre>"},{"location":"developer_guide/design_principles/#configuration-through-constructor","title":"Configuration through Constructor","text":"<p>Classes are configured through their constructor rather than setter methods:</p> <pre><code>sampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=1.0\n)\n</code></pre> <p>This approach:</p> <ul> <li>Makes the configuration explicit and clear</li> <li>Encourages immutability of key parameters</li> <li>Simplifies object creation and usage</li> </ul>"},{"location":"developer_guide/design_principles/#method-chaining","title":"Method Chaining","text":"<p>Methods return the object itself when appropriate to allow method chaining:</p> <pre><code>result = (\n    sampler\n    .set_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    .set_seed(42)\n    .sample_chain(dim=2, n_steps=1000)\n)\n</code></pre>"},{"location":"developer_guide/design_principles/#lazily-evaluated-operations","title":"Lazily-Evaluated Operations","text":"<p>Computations are performed lazily when possible to avoid unnecessary work:</p> <pre><code># Create a sampler with a sampling trajectory\nsampler = LangevinDynamics(energy_fn)\ntrajectory = sampler.sample_trajectory(dim=2, n_steps=1000)\n\n# Compute statistics only when needed\nmean = trajectory.mean()  # Computation happens here\nvariance = trajectory.variance()  # Computation happens here\n</code></pre>"},{"location":"developer_guide/design_principles/#architecture-principles","title":"Architecture Principles","text":""},{"location":"developer_guide/design_principles/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Components have clearly defined responsibilities:</p> <ul> <li>Energy Functions: Define the energy landscape</li> <li>Samplers: Generate samples from energy functions</li> <li>Losses: Train energy functions from data</li> <li>Models: Parameterize energy functions using neural networks</li> <li>Utils: Provide supporting functionality</li> </ul>"},{"location":"developer_guide/design_principles/#minimizing-dependencies","title":"Minimizing Dependencies","text":"<p>Each module has minimal dependencies on other modules:</p> <ul> <li>Core modules (e.g., <code>core</code>, <code>samplers</code>) don't depend on higher-level modules</li> <li>Utility modules are designed to be used by all other modules</li> <li>CUDA implementations are separated to allow for CPU-only usage</li> </ul>"},{"location":"developer_guide/design_principles/#consistent-error-handling","title":"Consistent Error Handling","text":"<p>Error handling follows consistent patterns:</p> <ul> <li>Use descriptive error messages that suggest solutions</li> <li>Validate inputs early with helpful validation errors</li> <li>Provide debug information when operations fail</li> </ul> <pre><code>def validate_dimensions(tensor: torch.Tensor, expected_dims: int) -&gt; None:\n    \"\"\"Validate that tensor has the expected number of dimensions.\"\"\"\n    if tensor.dim() != expected_dims:\n        raise ValueError(\n            f\"Expected tensor with {expected_dims} dimensions, \"\n            f\"but got tensor with shape {tensor.shape}\"\n        )\n</code></pre>"},{"location":"developer_guide/design_principles/#consistent-api-design","title":"Consistent API Design","text":"<p>APIs are designed consistently across the library:</p> <ul> <li>Similar operations have similar interfaces</li> <li>Parameters follow consistent naming conventions</li> <li>Return types are consistent and well-documented</li> </ul>"},{"location":"developer_guide/design_principles/#progressive-disclosure","title":"Progressive Disclosure","text":"<p>Simple use cases are simple, while advanced functionality is available but not required:</p> <pre><code># Simple usage\nsampler = LangevinDynamics(energy_fn)\nsamples = sampler.sample(n_samples=1000)\n\n# Advanced usage\nsampler = LangevinDynamics(\n    energy_fn,\n    step_size=0.01,\n    noise_scale=1.0,\n    step_size_schedule=LinearSchedule(0.01, 0.001),\n    metropolis_correction=True\n)\nsamples = sampler.sample(\n    n_samples=1000,\n    initial_samples=initial_x,\n    callback=logging_callback\n)\n</code></pre>"},{"location":"developer_guide/design_principles/#implementation-principles","title":"Implementation Principles","text":""},{"location":"developer_guide/design_principles/#pytorch-first","title":"PyTorch First","text":"<p>TorchEBM is built on PyTorch and follows PyTorch patterns:</p> <ul> <li>Use PyTorch's tensor operations whenever possible</li> <li>Follow PyTorch's model design patterns (e.g., <code>nn.Module</code>)</li> <li>Leverage PyTorch's autograd for gradient computation</li> <li>Support both CPU and CUDA execution</li> </ul>"},{"location":"developer_guide/design_principles/#vectorized-operations","title":"Vectorized Operations","text":"<p>Operations are vectorized where possible for efficiency:</p> <pre><code># Good: Vectorized operations\ndef compute_pairwise_distances(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    return torch.cdist(x, y, p=2)\n\n# Avoid: Explicit loops\ndef compute_pairwise_distances_slow(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    result = torch.zeros(x.size(0), y.size(0))\n    for i in range(x.size(0)):\n        for j in range(y.size(0)):\n            result[i, j] = torch.norm(x[i] - y[j])\n    return result\n</code></pre>"},{"location":"developer_guide/design_principles/#cuda-optimization","title":"CUDA Optimization","text":"<p>Performance-critical operations are optimized with CUDA when appropriate:</p> <ul> <li>CPU implementations as fallback</li> <li>CUDA implementations for performance</li> <li>Automatic selection based on available hardware</li> </ul>"},{"location":"developer_guide/design_principles/#type-annotations","title":"Type Annotations","text":"<p>Code uses type annotations for clarity and static analysis:</p> <pre><code>def sample_chain(\n    self,\n    dim: int,\n    n_steps: int,\n    n_samples: int = 1,\n    initial_samples: Optional[torch.Tensor] = None,\n    return_trajectory: bool = False\n) -&gt; Union[torch.Tensor, Trajectory]:\n    \"\"\"Generate samples using a Markov chain.\"\"\"\n    # Implementation\n</code></pre>"},{"location":"developer_guide/design_principles/#testing-principles","title":"Testing Principles","text":"<ul> <li>Unit Testing: Individual components are thoroughly tested</li> <li>Integration Testing: Component interactions are tested</li> <li>Property Testing: Properties of algorithms are tested</li> <li>Numerical Testing: Numerical algorithms are tested for stability and accuracy</li> </ul>"},{"location":"developer_guide/design_principles/#documentation-principles","title":"Documentation Principles","text":"<p>Documentation is comprehensive and includes:</p> <ul> <li>API Documentation: Clear documentation of all public APIs</li> <li>Tutorials: Step-by-step guides for common tasks</li> <li>Examples: Real-world examples of using the library</li> <li>Theory: Explanations of the underlying mathematical concepts</li> </ul>"},{"location":"developer_guide/design_principles/#future-compatibility","title":"Future Compatibility","text":"<p>TorchEBM is designed with future compatibility in mind:</p> <ul> <li>API Stability: Breaking changes are minimized and clearly documented</li> <li>Feature Flags: Experimental features are clearly marked</li> <li>Deprecation Warnings: Deprecated features emit warnings before removal</li> </ul>"},{"location":"developer_guide/design_principles/#contributing-guidelines","title":"Contributing Guidelines","text":"<p>When contributing to TorchEBM, adhere to these design principles:</p> <ul> <li>Make sure new components follow existing patterns</li> <li>Keep interfaces consistent with the rest of the library</li> <li>Write thorough tests for new functionality</li> <li>Document public APIs clearly</li> <li>Optimize for readability and maintainability</li> </ul> <p>Design Example: Adding a New Sampler</p> <p>When adding a new sampler:</p> <ol> <li>Subclass the <code>Sampler</code> base class</li> <li>Implement required methods (<code>sample</code>, <code>sample_chain</code>)</li> <li>Follow the existing parameter naming conventions</li> <li>Add comprehensive documentation</li> <li>Write tests that verify the sampler's properties</li> <li>Optimize performance-critical sections</li> </ol> <ul> <li> <p> Project Structure</p> <p>Understand how the project is organized.</p> <p> Project Structure</p> </li> <li> <p> Core Components</p> <p>Learn about the core components in detail.</p> <p> Core Components</p> </li> <li> <p> Code Style</p> <p>Follow the project's coding standards.</p> <p> Code Style</p> </li> </ul>"},{"location":"developer_guide/development_setup/","title":"Development Setup","text":"<p>Quick Start</p> <p>If you're just getting started with TorchEBM development, this guide will help you set up your environment properly.</p>"},{"location":"developer_guide/development_setup/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the TorchEBM development environment, make sure you have the following:</p> <ul> <li> <p> Python 3.9+</p> <p>TorchEBM requires Python 3.9 or higher.</p> <p> Install Python</p> </li> <li> <p> Git</p> <p>You'll need Git for version control.</p> <p> Install Git</p> </li> <li> <p> GitHub Account</p> <p>For contributing to the repository.</p> <p> Create GitHub Account</p> </li> </ul>"},{"location":"developer_guide/development_setup/#setting-up-your-environment","title":"Setting Up Your Environment","text":""},{"location":"developer_guide/development_setup/#1-fork-and-clone-the-repository","title":"1. Fork and Clone the Repository","text":"GitHub UIGitHub CLI <ol> <li>Navigate to TorchEBM repository</li> <li>Click the Fork button in the top-right corner</li> <li>Clone your fork to your local machine:    <pre><code>git clone https://github.com/YOUR-USERNAME/torchebm.git\ncd torchebm\n</code></pre></li> </ol> <pre><code>gh repo fork soran-ghaderi/torchebm --clone=true\ncd torchebm\n</code></pre>"},{"location":"developer_guide/development_setup/#2-set-up-virtual-environment","title":"2. Set Up Virtual Environment","text":"<p>It's recommended to use a virtual environment to manage dependencies:</p> venvconda <pre><code>python -m venv venv\n# Activate on Windows\nvenv\\Scripts\\activate\n# Activate on macOS/Linux\nsource venv/bin/activate\n</code></pre> <pre><code>conda create -n torchebm python=3.9\nconda activate torchebm\n</code></pre>"},{"location":"developer_guide/development_setup/#3-install-development-dependencies","title":"3. Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This will install TorchEBM in development mode along with all development dependencies.</p>"},{"location":"developer_guide/development_setup/#development-workflow","title":"Development Workflow","text":""},{"location":"developer_guide/development_setup/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"developer_guide/development_setup/#2-make-changes","title":"2. Make Changes","text":"<p>Make your changes to the codebase.</p>"},{"location":"developer_guide/development_setup/#3-run-tests","title":"3. Run Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"developer_guide/development_setup/#4-commit-changes","title":"4. Commit Changes","text":"<p>Follow our commit conventions.</p> <pre><code>git commit -m \"feat: add new feature\"\n</code></pre>"},{"location":"developer_guide/development_setup/#5-push-changes-and-create-a-pull-request","title":"5. Push Changes and Create a Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then go to GitHub and create a pull request.</p>"},{"location":"developer_guide/development_setup/#documentation-development","title":"Documentation Development","text":"<p>To preview documentation locally:</p> <pre><code>pip install -e \".[docs]\"\nmkdocs serve\n</code></pre> <p>This will start a local server at http://127.0.0.1:8000/.</p>"},{"location":"developer_guide/development_setup/#common-issues","title":"Common Issues","text":"<p>Missing CUDA?</p> <p>If you're developing CUDA extensions, ensure you have the right CUDA toolkit installed:</p> <pre><code>pip install torch==1.12.0+cu116 -f https://download.pytorch.org/whl/cu116/torch_stable.html\n</code></pre>"},{"location":"developer_guide/development_setup/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Code Style</p> <p>Learn about our coding standards.</p> <p> Code Style</p> </li> <li> <p> Testing Guide</p> <p>Learn how to write effective tests.</p> <p> Testing Guide</p> </li> <li> <p> API Design</p> <p>Understand our API design principles.</p> <p> API Design</p> </li> </ul>"},{"location":"developer_guide/implementation_energy/","title":"Energy Functions Implementation","text":"<p>Implementation Details</p> <p>This guide provides detailed information about the implementation of energy functions in TorchEBM, including mathematical foundations, code structure, and optimization techniques.</p>"},{"location":"developer_guide/implementation_energy/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Energy-based models define a probability distribution through an energy function:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>where \\(E(x)\\) is the energy function and \\(Z = \\int e^{-E(x)} dx\\) is the normalization constant (partition function).</p> <p>The score function is the gradient of the log-probability:</p> \\[\\nabla_x \\log p(x) = -\\nabla_x E(x)\\] <p>This relationship is fundamental to many sampling and training methods in TorchEBM.</p>"},{"location":"developer_guide/implementation_energy/#base-energy-function-implementation","title":"Base Energy Function Implementation","text":"<p>The <code>EnergyFunction</code> base class provides the foundation for all energy functions:</p> <pre><code>class EnergyFunction(nn.Module):\n    \"\"\"Base class for all energy functions.\"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy for input x.\"\"\"\n        raise NotImplementedError\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score (gradient of energy) for input x.\"\"\"\n        x = x.requires_grad_(True)\n        energy = self.forward(x)\n        return torch.autograd.grad(energy.sum(), x, create_graph=True)[0]\n</code></pre> <p>Key design decisions:</p> <ol> <li>PyTorch <code>nn.Module</code> Base: Allows energy functions to have learnable parameters and use PyTorch's optimization tools</li> <li>Automatic Differentiation: Uses PyTorch's autograd for computing the score function</li> <li>Batched Computation: All methods support batched inputs for efficiency</li> </ol>"},{"location":"developer_guide/implementation_energy/#analytical-energy-functions","title":"Analytical Energy Functions","text":"<p>TorchEBM includes several analytical energy functions for testing and benchmarking. Here are detailed implementations of some key ones:</p>"},{"location":"developer_guide/implementation_energy/#gaussian-energy","title":"Gaussian Energy","text":"<p>The Gaussian energy function is defined as:</p> \\[E(x) = \\frac{1}{2}(x - \\mu)^T\\Sigma^{-1}(x - \\mu)\\] <p>Where \\(\\mu\\) is the mean vector and \\(\\Sigma\\) is the covariance matrix.</p> <pre><code>class GaussianEnergy(EnergyFunction):\n    \"\"\"Gaussian energy function.\"\"\"\n\n    def __init__(self, mean: torch.Tensor, cov: torch.Tensor):\n        \"\"\"Initialize Gaussian energy function.\n\n        Args:\n            mean: Mean vector of shape (dim,)\n            cov: Covariance matrix of shape (dim, dim)\n        \"\"\"\n        super().__init__()\n        self.register_buffer(\"mean\", mean)\n        self.register_buffer(\"cov\", cov)\n        self.register_buffer(\"precision\", torch.inverse(cov))\n        self._dim = mean.size(0)\n\n        # Compute log determinant for normalization (optional)\n        self.register_buffer(\"log_det\", torch.logdet(cov))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute Gaussian energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        # Ensure x has the right shape\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        # Center the data\n        centered = x - self.mean\n\n        # Compute quadratic form efficiently\n        return 0.5 * torch.sum(\n            centered * torch.matmul(centered, self.precision),\n            dim=1\n        )\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score function analytically.\n\n        This is more efficient than using automatic differentiation.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size, dim) containing score values\n        \"\"\"\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        return -torch.matmul(x - self.mean, self.precision)\n</code></pre> <p>Implementation notes:</p> <ul> <li>We precompute the precision matrix (inverse covariance) for efficiency</li> <li>A specialized <code>score</code> method is provided that uses the analytical formula rather than automatic differentiation</li> <li>Input shape handling ensures both single samples and batches work correctly</li> </ul>"},{"location":"developer_guide/implementation_energy/#double-well-energy","title":"Double Well Energy","text":"<p>The double well energy function creates a bimodal distribution:</p> \\[E(x) = a(x^2 - b)^2\\] <pre><code>class DoubleWellEnergy(EnergyFunction):\n    \"\"\"Double well energy function.\"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 2.0):\n        \"\"\"Initialize double well energy function.\n\n        Args:\n            a: Scale parameter controlling depth of wells\n            b: Parameter controlling the distance between wells\n        \"\"\"\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute double well energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        # Compute (x^2 - b)^2 for each dimension, then sum\n        return self.a * torch.sum((x**2 - self.b)**2, dim=1)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#rosenbrock-energy","title":"Rosenbrock Energy","text":"<p>The Rosenbrock function is a challenging test case with a narrow curved valley:</p> \\[E(x) = \\sum_{i=1}^{d-1} \\left[ a(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\] <pre><code>class RosenbrockEnergy(EnergyFunction):\n    \"\"\"Rosenbrock energy function.\"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 100.0):\n        \"\"\"Initialize Rosenbrock energy function.\n\n        Args:\n            a: Scale parameter for the first term\n            b: Scale parameter for the second term (usually 100)\n        \"\"\"\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute Rosenbrock energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        batch_size, dim = x.shape\n        energy = torch.zeros(batch_size, device=x.device)\n\n        for i in range(dim - 1):\n            term1 = self.b * (x[:, i+1] - x[:, i]**2)**2\n            term2 = (x[:, i] - 1)**2\n            energy += term1 + term2\n\n        return energy\n</code></pre>"},{"location":"developer_guide/implementation_energy/#composite-energy-functions","title":"Composite Energy Functions","text":"<p>TorchEBM supports composing energy functions to create more complex landscapes:</p> <pre><code>class CompositeEnergy(EnergyFunction):\n    \"\"\"Composite energy function.\"\"\"\n\n    def __init__(\n        self,\n        energy_functions: List[EnergyFunction],\n        weights: Optional[List[float]] = None,\n        operation: str = \"sum\"\n    ):\n        \"\"\"Initialize composite energy function.\n\n        Args:\n            energy_functions: List of energy functions to combine\n            weights: Optional weights for each energy function\n            operation: How to combine energy functions (\"sum\", \"product\", \"min\", \"max\")\n        \"\"\"\n        super().__init__()\n        self.energy_functions = nn.ModuleList(energy_functions)\n\n        if weights is None:\n            weights = [1.0] * len(energy_functions)\n        self.register_buffer(\"weights\", torch.tensor(weights))\n\n        if operation not in [\"sum\", \"product\", \"min\", \"max\"]:\n            raise ValueError(f\"Unknown operation: {operation}\")\n        self.operation = operation\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute composite energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        energies = [f(x) * w for f, w in zip(self.energy_functions, self.weights)]\n\n        if self.operation == \"sum\":\n            return torch.sum(torch.stack(energies), dim=0)\n        elif self.operation == \"product\":\n            return torch.prod(torch.stack(energies), dim=0)\n        elif self.operation == \"min\":\n            return torch.min(torch.stack(energies), dim=0)[0]\n        elif self.operation == \"max\":\n            return torch.max(torch.stack(energies), dim=0)[0]\n</code></pre>"},{"location":"developer_guide/implementation_energy/#neural-network-energy-functions","title":"Neural Network Energy Functions","text":"<p>Neural networks can parameterize energy functions for flexibility:</p> <pre><code>class MLPEnergy(EnergyFunction):\n    \"\"\"Multi-layer perceptron energy function.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dims: List[int],\n        activation: Callable = nn.SiLU\n    ):\n        \"\"\"Initialize MLP energy function.\n\n        Args:\n            input_dim: Input dimensionality\n            hidden_dims: List of hidden layer dimensions\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n\n        # Build MLP layers\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(activation())\n            prev_dim = hidden_dim\n\n        # Final layer with scalar output\n        layers.append(nn.Linear(prev_dim, 1))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy using the MLP.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"developer_guide/implementation_energy/#efficient-gradient-computation","title":"Efficient Gradient Computation","text":"<p>For gradients, TorchEBM provides optimized implementations:</p> <pre><code>def efficient_grad(energy_fn: EnergyFunction, x: torch.Tensor, create_graph: bool = False) -&gt; torch.Tensor:\n    \"\"\"Compute gradient of energy function efficiently.\n\n    Args:\n        energy_fn: Energy function\n        x: Input tensor of shape (batch_size, dim)\n        create_graph: Whether to create gradient graph (for higher-order gradients)\n\n    Returns:\n        Gradient tensor of shape (batch_size, dim)\n    \"\"\"\n    x.requires_grad_(True)\n    with torch.enable_grad():\n        energy = energy_fn(x)\n\n    grad = torch.autograd.grad(\n        energy.sum(), x, create_graph=create_graph\n    )[0]\n\n    return grad\n</code></pre>"},{"location":"developer_guide/implementation_energy/#cuda-implementations","title":"CUDA Implementations","text":"<p>For performance-critical operations, TorchEBM includes CUDA implementations:</p> <pre><code>def cuda_score_function(energy_fn, x):\n    \"\"\"CUDA-optimized score function computation.\"\"\"\n    # Use energy_fn's custom CUDA implementation if available\n    if hasattr(energy_fn, 'cuda_score') and torch.cuda.is_available():\n        return energy_fn.cuda_score(x)\n    else:\n        # Fall back to autograd\n        return energy_fn.score(x)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#factory-methods","title":"Factory Methods","text":"<p>Factory methods provide convenient ways to create energy functions:</p> <pre><code>@classmethod\ndef create_standard_gaussian(cls, dim: int) -&gt; 'GaussianEnergy':\n    \"\"\"Create a standard Gaussian energy function.\n\n    Args:\n        dim: Dimensionality\n\n    Returns:\n        GaussianEnergy with zero mean and identity covariance\n    \"\"\"\n    return cls(mean=torch.zeros(dim), cov=torch.eye(dim))\n\n@classmethod\ndef from_samples(cls, samples: torch.Tensor, regularization: float = 1e-4) -&gt; 'GaussianEnergy':\n    \"\"\"Create a Gaussian energy function from data samples.\n\n    Args:\n        samples: Data samples of shape (n_samples, dim)\n        regularization: Small value added to diagonal for numerical stability\n\n    Returns:\n        GaussianEnergy fit to the samples\n    \"\"\"\n    mean = samples.mean(dim=0)\n    cov = torch.cov(samples.T) + regularization * torch.eye(samples.size(1))\n    return cls(mean=mean, cov=cov)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#implementation-challenges-and-solutions","title":"Implementation Challenges and Solutions","text":""},{"location":"developer_guide/implementation_energy/#numerical-stability","title":"Numerical Stability","text":"<p>Energy functions must be numerically stable:</p> <pre><code>class NumericallyStableEnergy(EnergyFunction):\n    \"\"\"Energy function with numerical stability considerations.\"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy with numerical stability.\n\n        Uses log-sum-exp trick for numerical stability.\n        \"\"\"\n        # Example of numerical stability in computation\n        terms = self.compute_terms(x)\n        max_term = torch.max(terms, dim=1, keepdim=True)[0]\n        stable_energy = max_term + torch.log(torch.sum(\n            torch.exp(terms - max_term), dim=1\n        ))\n        return stable_energy\n</code></pre>"},{"location":"developer_guide/implementation_energy/#handling-multi-modal-distributions","title":"Handling Multi-Modal Distributions","text":"<p>For multi-modal distributions:</p> <pre><code>class MixtureEnergy(EnergyFunction):\n    \"\"\"Mixture of energy functions.\"\"\"\n\n    def __init__(self, components: List[EnergyFunction], weights: Optional[List[float]] = None):\n        \"\"\"Initialize mixture energy function.\n\n        Args:\n            components: List of component energy functions\n            weights: Optional weights for each component\n        \"\"\"\n        super().__init__()\n        self.components = nn.ModuleList(components)\n\n        if weights is None:\n            weights = [1.0] * len(components)\n        self.register_buffer(\"log_weights\", torch.log(torch.tensor(weights)))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute mixture energy using log-sum-exp for stability.\"\"\"\n        energies = torch.stack([f(x) for f in self.components], dim=1)\n        weighted_energies = -self.log_weights - energies\n\n        # Use log-sum-exp trick for numerical stability\n        max_val = torch.max(weighted_energies, dim=1, keepdim=True)[0]\n        stable_energy = -max_val - torch.log(torch.sum(\n            torch.exp(weighted_energies - max_val), dim=1\n        ))\n\n        return stable_energy\n</code></pre>"},{"location":"developer_guide/implementation_energy/#testing-energy-functions","title":"Testing Energy Functions","text":"<p>TorchEBM includes comprehensive testing utilities for energy functions:</p> <pre><code>def test_energy_function(energy_fn: EnergyFunction, dim: int, n_samples: int = 1000) -&gt; dict:\n    \"\"\"Test an energy function for correctness and properties.\n\n    Args:\n        energy_fn: Energy function to test\n        dim: Input dimensionality\n        n_samples: Number of test samples\n\n    Returns:\n        Dictionary with test results\n    \"\"\"\n    # Generate random samples\n    x = torch.randn(n_samples, dim)\n\n    # Test energy computation\n    energy = energy_fn(x)\n    assert energy.shape == (n_samples,)\n\n    # Test score computation\n    score = energy_fn.score(x)\n    assert score.shape == (n_samples, dim)\n\n    # Test gradient consistency\n    manual_grad = torch.autograd.grad(\n        energy_fn(x).sum(), x, create_graph=True\n    )[0]\n    assert torch.allclose(score, -manual_grad, atol=1e-5, rtol=1e-5)\n\n    return {\n        \"energy_mean\": energy.mean().item(),\n        \"energy_std\": energy.std().item(),\n        \"score_mean\": score.mean().item(),\n        \"score_std\": score.std().item(),\n    }\n</code></pre>"},{"location":"developer_guide/implementation_energy/#best-practices-for-custom-energy-functions","title":"Best Practices for Custom Energy Functions","text":"<p>When implementing custom energy functions, follow these best practices:</p> <p>Custom Energy Function Example</p> <pre><code>class CustomEnergy(EnergyFunction):\n    \"\"\"Custom energy function example.\"\"\"\n\n    def __init__(self, scale: float = 1.0):\n        super().__init__()\n        self.scale = scale\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Ensure correct input shape\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        # Compute energy using vectorized operations\n        return self.scale * torch.sum(torch.sin(x) ** 2, dim=1)\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Analytical gradient\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        return -2 * self.scale * torch.sin(x) * torch.cos(x)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#do","title":"Do","text":"<ul> <li>Implement a custom <code>score</code> method if an analytical gradient is available</li> <li>Use vectorized operations for performance</li> <li>Register parameters and buffers properly</li> <li>Handle batched inputs consistently</li> <li>Add factory methods for common use cases</li> </ul>"},{"location":"developer_guide/implementation_energy/#dont","title":"Don't","text":"<ul> <li>Use loops when vectorized operations are possible</li> <li>Recompute values that could be cached</li> <li>Modify inputs in-place</li> <li>Forget to handle edge cases</li> <li>Ignore numerical stability</li> </ul>"},{"location":"developer_guide/implementation_energy/#debugging-energy-functions","title":"Debugging Energy Functions","text":"<p>Common issues with energy functions include:</p> <ol> <li>NaN/Inf Values: Check for division by zero or log of negative numbers</li> <li>Poor Sampling: Energy may not be well-defined or have numerical issues</li> <li>Training Instability: Energy might grow unbounded or collapse</li> </ol> <p>Debugging techniques:</p> <pre><code>def debug_energy_function(energy_fn: EnergyFunction, x: torch.Tensor) -&gt; None:\n    \"\"\"Debug an energy function for common issues.\"\"\"\n    # Check for NaN/Inf in energy\n    energy = energy_fn(x)\n    if torch.isnan(energy).any() or torch.isinf(energy).any():\n        print(\"Warning: Energy contains NaN or Inf values\")\n\n    # Check for NaN/Inf in score\n    score = energy_fn.score(x)\n    if torch.isnan(score).any() or torch.isinf(score).any():\n        print(\"Warning: Score contains NaN or Inf values\")\n\n    # Check score magnitude\n    score_norm = torch.norm(score, dim=1)\n    if (score_norm &gt; 1e3).any():\n        print(\"Warning: Score has very large values\")\n\n    # Check energy range\n    if energy.max() - energy.min() &gt; 1e6:\n        print(\"Warning: Energy has a very large range\")\n</code></pre>"},{"location":"developer_guide/implementation_energy/#advanced-topics","title":"Advanced Topics","text":""},{"location":"developer_guide/implementation_energy/#spherical-energy-functions","title":"Spherical Energy Functions","text":"<p>Energy functions on constrained domains:</p> <pre><code>class SphericalEnergy(EnergyFunction):\n    \"\"\"Energy function defined on a unit sphere.\"\"\"\n\n    def __init__(self, base_energy: EnergyFunction):\n        \"\"\"Initialize spherical energy function.\n\n        Args:\n            base_energy: Base energy function\n        \"\"\"\n        super().__init__()\n        self.base_energy = base_energy\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy on unit sphere.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        # Project to unit sphere\n        x_normalized = F.normalize(x, p=2, dim=1)\n        return self.base_energy(x_normalized)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#energy-from-density-model","title":"Energy from Density Model","text":"<p>Creating an energy function from a density model:</p> <pre><code>class DensityModelEnergy(EnergyFunction):\n    \"\"\"Energy function from a density model.\"\"\"\n\n    def __init__(self, density_model: Callable):\n        \"\"\"Initialize energy function from density model.\n\n        Args:\n            density_model: Model that computes log probability\n        \"\"\"\n        super().__init__()\n        self.density_model = density_model\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy as negative log probability.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        log_prob = self.density_model.log_prob(x)\n        return -log_prob\n</code></pre>"},{"location":"developer_guide/implementation_energy/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Samplers</p> <p>Explore how samplers work with energy functions.</p> <p> Samplers</p> </li> <li> <p> Code Style</p> <p>Follow coding standards when implementing energy functions.</p> <p> Code Style</p> </li> </ul>"},{"location":"developer_guide/implementation_losses/","title":"Loss Functions Implementation","text":"<p>Implementation Details</p> <p>This guide provides detailed information about the implementation of loss functions in TorchEBM, including mathematical foundations, code structure, and optimization techniques.</p>"},{"location":"developer_guide/implementation_losses/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Energy-based models can be trained using various loss functions, each with different properties. The primary goal is to shape the energy landscape such that observed data has low energy while other regions have high energy.</p>"},{"location":"developer_guide/implementation_losses/#base-loss-implementation","title":"Base Loss Implementation","text":"<p>The <code>Loss</code> base class provides the foundation for all loss functions:</p> <pre><code>from abc import ABC, abstractmethod\nimport torch\nfrom typing import Optional, Dict, Any, Tuple\n\nfrom torchebm.core import EnergyFunction\n\nclass Loss(ABC):\n    \"\"\"Base class for all loss functions.\"\"\"\n\n    def __init__(self, energy_function: EnergyFunction):\n        \"\"\"Initialize loss with an energy function.\n\n        Args:\n            energy_function: The energy function to train\n        \"\"\"\n        self.energy_function = energy_function\n\n    @abstractmethod\n    def __call__(\n        self, \n        pos_samples: torch.Tensor, \n        neg_samples: torch.Tensor, \n        **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Negative samples from the model distribution\n            **kwargs: Additional loss-specific parameters\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/implementation_losses/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background","title":"Mathematical Background","text":"<p>The MLE approach aims to maximize the log-likelihood of the data under the model:</p> \\[\\mathcal{L}_{\\text{MLE}} = \\mathbb{E}_{p_{\\text{data}}(x)}[E(x)] - \\mathbb{E}_{p_{\\text{model}}(x)}[E(x)]\\]"},{"location":"developer_guide/implementation_losses/#implementation","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import EnergyFunction\nfrom torchebm.losses.base import Loss\n\nclass MLELoss(Loss):\n    \"\"\"Maximum Likelihood Estimation loss.\"\"\"\n\n    def __init__(\n        self, \n        energy_function: EnergyFunction,\n        alpha: float = 1.0,\n        regularization: Optional[str] = None,\n        reg_strength: float = 0.0\n    ):\n        \"\"\"Initialize MLE loss.\n\n        Args:\n            energy_function: Energy function to train\n            alpha: Weight for the negative phase\n            regularization: Type of regularization ('l1', 'l2', or None)\n            reg_strength: Strength of regularization\n        \"\"\"\n        super().__init__(energy_function)\n        self.alpha = alpha\n        self.regularization = regularization\n        self.reg_strength = reg_strength\n\n    def __call__(\n        self, \n        pos_samples: torch.Tensor, \n        neg_samples: torch.Tensor, \n        **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the MLE loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Negative samples from the model distribution\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Compute loss components\n        pos_term = pos_energy.mean()\n        neg_term = neg_energy.mean()\n\n        # Full loss\n        loss = pos_term - self.alpha * neg_term\n\n        # Add regularization if specified\n        reg_loss = torch.tensor(0.0, device=pos_energy.device)\n        if self.regularization is not None and self.reg_strength &gt; 0:\n            if self.regularization == 'l2':\n                for param in self.energy_function.parameters():\n                    reg_loss += torch.sum(param ** 2)\n            elif self.regularization == 'l1':\n                for param in self.energy_function.parameters():\n                    reg_loss += torch.sum(torch.abs(param))\n\n            loss = loss + self.reg_strength * reg_loss\n\n        # Metrics to track\n        metrics = {\n            'pos_energy': pos_term.detach(),\n            'neg_energy': neg_term.detach(),\n            'energy_gap': (neg_term - pos_term).detach(),\n            'loss': loss.detach(),\n            'reg_loss': reg_loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#contrastive-divergence-cd","title":"Contrastive Divergence (CD)","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_1","title":"Mathematical Background","text":"<p>Contrastive Divergence is a variant of MLE that uses a specific sampling scheme where negative samples are obtained by starting from positive samples and running MCMC for a few steps:</p> \\[\\mathcal{L}_{\\text{CD}} = \\mathbb{E}_{p_{\\text{data}}(x)}[E(x)] - \\mathbb{E}_{p_{K}(x|x_{\\text{data}})}[E(x)]\\] <p>where \\(p_{K}(x|x_{\\text{data}})\\) is the distribution after \\(K\\) steps of MCMC starting from data samples.</p>"},{"location":"developer_guide/implementation_losses/#implementation_1","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple, Optional\n\nfrom torchebm.core import EnergyFunction\nfrom torchebm.samplers import Sampler, LangevinDynamics\nfrom torchebm.losses.base import Loss\n\nclass ContrastiveDivergenceLoss(Loss):\n    \"\"\"Contrastive Divergence loss.\"\"\"\n\n    def __init__(\n        self, \n        energy_function: EnergyFunction,\n        sampler: Optional[Sampler] = None,\n        n_steps: int = 10,\n        alpha: float = 1.0\n    ):\n        \"\"\"Initialize CD loss.\n\n        Args:\n            energy_function: Energy function to train\n            sampler: Sampler for generating negative samples\n            n_steps: Number of sampling steps for negative samples\n            alpha: Weight for the negative phase\n        \"\"\"\n        super().__init__(energy_function)\n        self.sampler = sampler or LangevinDynamics(energy_function)\n        self.n_steps = n_steps\n        self.alpha = alpha\n\n    def __call__(\n        self, \n        pos_samples: torch.Tensor, \n        neg_samples: Optional[torch.Tensor] = None, \n        **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the CD loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Optional negative samples (if None, will be generated)\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Generate negative samples if not provided\n        if neg_samples is None:\n            with torch.no_grad():\n                neg_samples = self.sampler.sample_chain(\n                    pos_samples.shape[1],\n                    self.n_steps,\n                    n_samples=pos_samples.shape[0],\n                    initial_samples=pos_samples.detach()\n                )\n\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Compute loss components\n        pos_term = pos_energy.mean()\n        neg_term = neg_energy.mean()\n\n        # Full loss\n        loss = pos_term - self.alpha * neg_term\n\n        # Metrics to track\n        metrics = {\n            'pos_energy': pos_term.detach(),\n            'neg_energy': neg_term.detach(),\n            'energy_gap': (neg_term - pos_term).detach(),\n            'loss': loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#noise-contrastive-estimation-nce","title":"Noise Contrastive Estimation (NCE)","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_2","title":"Mathematical Background","text":"<p>NCE treats the problem as a binary classification task, distinguishing between data samples and noise samples:</p> \\[\\mathcal{L}_{\\text{NCE}} = -\\mathbb{E}_{p_{\\text{data}}(x)}[\\log \\sigma(f_\\theta(x))] - \\mathbb{E}_{p_{\\text{noise}}(x)}[\\log (1 - \\sigma(f_\\theta(x)))]\\] <p>where \\(f_\\theta(x) = -E(x) - \\log Z\\) and \\(\\sigma\\) is the sigmoid function.</p>"},{"location":"developer_guide/implementation_losses/#implementation_2","title":"Implementation","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import EnergyFunction\nfrom torchebm.losses.base import Loss\n\nclass NCELoss(Loss):\n    \"\"\"Noise Contrastive Estimation loss.\"\"\"\n\n    def __init__(\n        self, \n        energy_function: EnergyFunction,\n        log_partition: float = 0.0,\n        learn_partition: bool = True\n    ):\n        \"\"\"Initialize NCE loss.\n\n        Args:\n            energy_function: Energy function to train\n            log_partition: Initial value of log partition function\n            learn_partition: Whether to learn the partition function\n        \"\"\"\n        super().__init__(energy_function)\n        if learn_partition:\n            self.log_z = torch.nn.Parameter(torch.tensor([log_partition], dtype=torch.float32))\n        else:\n            self.register_buffer('log_z', torch.tensor([log_partition], dtype=torch.float32))\n        self.learn_partition = learn_partition\n\n    def __call__(\n        self, \n        pos_samples: torch.Tensor, \n        neg_samples: torch.Tensor, \n        **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the NCE loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Negative samples from noise distribution\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Compute logits\n        pos_logits = -pos_energy - self.log_z\n        neg_logits = -neg_energy - self.log_z\n\n        # Binary classification loss\n        pos_loss = F.binary_cross_entropy_with_logits(\n            pos_logits, \n            torch.ones_like(pos_logits)\n        )\n        neg_loss = F.binary_cross_entropy_with_logits(\n            neg_logits, \n            torch.zeros_like(neg_logits)\n        )\n\n        # Full loss\n        loss = pos_loss + neg_loss\n\n        # Metrics to track\n        metrics = {\n            'pos_loss': pos_loss.detach(),\n            'neg_loss': neg_loss.detach(),\n            'loss': loss.detach(),\n            'log_z': self.log_z.detach(),\n            'pos_energy': pos_energy.mean().detach(),\n            'neg_energy': neg_energy.mean().detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#score-matching","title":"Score Matching","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_3","title":"Mathematical Background","text":"<p>Score Matching minimizes the difference between the model's score function (gradient of log-probability) and the data score:</p> \\[\\mathcal{L}_{\\text{SM}} = \\frac{1}{2}\\mathbb{E}_{p_{\\text{data}}(x)}\\left[\\left\\|\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\text{model}}(x)\\right\\|^2\\right]\\] <p>This can be simplified to:</p> \\[\\mathcal{L}_{\\text{SM}} = \\mathbb{E}_{p_{\\text{data}}(x)}\\left[\\text{tr}(\\nabla_x^2 E(x)) + \\frac{1}{2}\\|\\nabla_x E(x)\\|^2\\right]\\]"},{"location":"developer_guide/implementation_losses/#implementation_3","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import EnergyFunction\nfrom torchebm.losses.base import Loss\n\nclass ScoreMatchingLoss(Loss):\n    \"\"\"Score Matching loss.\"\"\"\n\n    def __init__(\n        self, \n        energy_function: EnergyFunction,\n        implicit: bool = True\n    ):\n        \"\"\"Initialize Score Matching loss.\n\n        Args:\n            energy_function: Energy function to train\n            implicit: Whether to use implicit score matching\n        \"\"\"\n        super().__init__(energy_function)\n        self.implicit = implicit\n\n    def _compute_explicit_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute explicit score matching loss.\n\n        This requires computing both the score and the Hessian trace.\n\n        Args:\n            x: Input samples of shape (n_samples, dim)\n\n        Returns:\n            Loss value\n        \"\"\"\n        x.requires_grad_(True)\n\n        # Compute energy\n        energy = self.energy_function(x)\n\n        # Compute score (first derivatives)\n        score = torch.autograd.grad(\n            energy.sum(), x, create_graph=True\n        )[0]\n\n        # Compute trace of Hessian (second derivatives)\n        trace = 0.0\n        for i in range(x.shape[1]):\n            grad_score_i = torch.autograd.grad(\n                score[:, i].sum(), x, create_graph=True\n            )[0]\n            trace += grad_score_i[:, i]\n\n        # Compute squared norm of score\n        score_norm = torch.sum(score ** 2, dim=1)\n\n        # Full loss\n        loss = trace + 0.5 * score_norm\n\n        return loss.mean()\n\n    def _compute_implicit_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute implicit score matching loss.\n\n        This avoids computing the Hessian trace.\n\n        Args:\n            x: Input samples of shape (n_samples, dim)\n\n        Returns:\n            Loss value\n        \"\"\"\n        # Add noise to inputs\n        x_noise = x + torch.randn_like(x) * 0.01\n        x_noise.requires_grad_(True)\n\n        # Compute energy and its gradient\n        energy = self.energy_function(x_noise)\n        score = torch.autograd.grad(\n            energy.sum(), x_noise, create_graph=True\n        )[0]\n\n        # Compute loss as squared difference between gradient and vector field\n        vector_field = (x_noise - x) / (0.01 ** 2)\n        loss = 0.5 * torch.sum((score + vector_field) ** 2, dim=1)\n\n        return loss.mean()\n\n    def __call__(\n        self, \n        pos_samples: torch.Tensor, \n        neg_samples: torch.Tensor = None, \n        **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the Score Matching loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Not used in Score Matching\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Compute loss based on method\n        if self.implicit:\n            loss = self._compute_implicit_score_matching(pos_samples)\n        else:\n            loss = self._compute_explicit_score_matching(pos_samples)\n\n        # Metrics to track\n        metrics = {\n            'loss': loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#denoising-score-matching","title":"Denoising Score Matching","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_4","title":"Mathematical Background","text":"<p>Denoising Score Matching is a variant of score matching that adds noise to the data and tries to predict the score of the noisy distribution:</p> \\[\\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{p_{\\text{data}}(x)}\\mathbb{E}_{q_\\sigma(\\tilde{x}|x)}\\left[\\left\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x) - \\nabla_{\\tilde{x}} \\log p_{\\text{model}}(\\tilde{x})\\right\\|^2\\right]\\] <p>where \\(q_\\sigma(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}|x, \\sigma^2\\mathbf{I})\\).</p>"},{"location":"developer_guide/implementation_losses/#implementation_4","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple, Union, List\n\nfrom torchebm.core import EnergyFunction\nfrom torchebm.losses.base import Loss\n\nclass DenoisingScoreMatchingLoss(Loss):\n    \"\"\"Denoising Score Matching loss.\"\"\"\n\n    def __init__(\n        self, \n        energy_function: EnergyFunction,\n        sigma: Union[float, List[float]] = 0.01\n    ):\n        \"\"\"Initialize DSM loss.\n\n        Args:\n            energy_function: Energy function to train\n            sigma: Noise level(s) for denoising\n        \"\"\"\n        super().__init__(energy_function)\n        if isinstance(sigma, (int, float)):\n            self.sigma = [float(sigma)]\n        else:\n            self.sigma = sigma\n\n    def __call__(\n        self, \n        pos_samples: torch.Tensor, \n        neg_samples: torch.Tensor = None, \n        **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the DSM loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Not used in DSM\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        total_loss = 0.0\n        metrics = {}\n\n        for i, sigma in enumerate(self.sigma):\n            # Add noise to inputs\n            noise = torch.randn_like(pos_samples) * sigma\n            x_noisy = pos_samples + noise\n\n            # Compute score of model\n            x_noisy.requires_grad_(True)\n            energy = self.energy_function(x_noisy)\n            score_model = torch.autograd.grad(\n                energy.sum(), x_noisy, create_graph=True\n            )[0]\n\n            # Target score (gradient of log density of noise model)\n            # For Gaussian noise, this is -(x_noisy - pos_samples) / sigma^2\n            score_target = -noise / (sigma ** 2)\n\n            # Compute loss\n            loss_sigma = 0.5 * torch.sum((score_model + score_target) ** 2, dim=1).mean()\n            total_loss += loss_sigma\n\n            metrics[f'loss_sigma_{sigma}'] = loss_sigma.detach()\n\n        # Average loss over all noise levels\n        avg_loss = total_loss / len(self.sigma)\n        metrics['loss'] = avg_loss.detach()\n\n        return avg_loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#slicedscorematching","title":"SlicedScoreMatching","text":"<pre><code>import torch\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import EnergyFunction\nfrom torchebm.losses.base import Loss\n\nclass SlicedScoreMatchingLoss(Loss):\n    \"\"\"Sliced Score Matching loss.\"\"\"\n\n    def __init__(\n        self, \n        energy_function: EnergyFunction,\n        n_projections: int = 1\n    ):\n        \"\"\"Initialize SSM loss.\n\n        Args:\n            energy_function: Energy function to train\n            n_projections: Number of random projections\n        \"\"\"\n        super().__init__(energy_function)\n        self.n_projections = n_projections\n\n    def __call__(\n        self, \n        pos_samples: torch.Tensor, \n        neg_samples: torch.Tensor = None, \n        **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the SSM loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Not used in SSM\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        x = pos_samples.detach().requires_grad_(True)\n\n        # Compute energy\n        energy = self.energy_function(x)\n\n        # Compute score (first derivatives)\n        score = torch.autograd.grad(\n            energy.sum(), x, create_graph=True\n        )[0]\n\n        total_loss = 0.0\n        for _ in range(self.n_projections):\n            # Generate random vectors\n            v = torch.randn_like(x)\n            v = v / torch.norm(v, p=2, dim=1, keepdim=True)\n\n            # Compute directional derivative\n            Jv = torch.sum(score * v, dim=1)\n\n            # Compute second directional derivative\n            J2v = torch.autograd.grad(\n                Jv.sum(), x, create_graph=True\n            )[0]\n\n            # Compute sliced score matching loss terms\n            loss_1 = torch.sum(J2v * v, dim=1)\n            loss_2 = 0.5 * torch.sum(score ** 2, dim=1)\n\n            # Full loss\n            loss = loss_1 + loss_2\n            total_loss += loss.mean()\n\n        # Average loss over projections\n        avg_loss = total_loss / self.n_projections\n\n        # Metrics to track\n        metrics = {\n            'loss': avg_loss.detach()\n        }\n\n        return avg_loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#performance-optimizations","title":"Performance Optimizations","text":"<p>For computationally intensive loss functions like Score Matching, we can use vectorized operations and CUDA optimizations:</p> <pre><code>def batched_hessian_trace(energy_function, x, batch_size=16):\n    \"\"\"Compute the trace of the Hessian in batches to save memory.\"\"\"\n    x.requires_grad_(True)\n    trace = torch.zeros(x.size(0), device=x.device)\n\n    # Compute energy and score\n    energy = energy_function(x)\n    score = torch.autograd.grad(\n        energy.sum(), x, create_graph=True\n    )[0]\n\n    # Compute trace of Hessian in batches\n    for i in range(0, x.size(1), batch_size):\n        end_i = min(i + batch_size, x.size(1))\n        sub_dims = list(range(i, end_i))\n\n        for j in sub_dims:\n            # Compute diagonal elements of Hessian\n            grad_score_j = torch.autograd.grad(\n                score[:, j].sum(), x, create_graph=True\n            )[0]\n            trace += grad_score_j[:, j]\n\n    return trace\n</code></pre>"},{"location":"developer_guide/implementation_losses/#factory-methods","title":"Factory Methods","text":"<p>Factory methods simplify loss creation:</p> <pre><code>def create_loss(\n    loss_type: str,\n    energy_function: EnergyFunction,\n    **kwargs\n) -&gt; Loss:\n    \"\"\"Create a loss function instance.\n\n    Args:\n        loss_type: Type of loss function\n        energy_function: Energy function to train\n        **kwargs: Loss-specific parameters\n\n    Returns:\n        Loss instance\n    \"\"\"\n    if loss_type.lower() == 'mle':\n        return MLELoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'cd':\n        return ContrastiveDivergenceLoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'nce':\n        return NCELoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'sm':\n        return ScoreMatchingLoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'dsm':\n        return DenoisingScoreMatchingLoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'ssm':\n        return SlicedScoreMatchingLoss(energy_function, **kwargs)\n    else:\n        raise ValueError(f\"Unknown loss type: {loss_type}\")\n</code></pre>"},{"location":"developer_guide/implementation_losses/#testing-loss-functions","title":"Testing Loss Functions","text":"<p>For testing loss implementations:</p> <pre><code>def validate_loss_gradients(\n    loss_fn: Loss,\n    dim: int = 2,\n    n_samples: int = 10,\n    seed: int = 42\n) -&gt; bool:\n    \"\"\"Validate that loss function produces valid gradients.\n\n    Args:\n        loss_fn: Loss function to test\n        dim: Dimensionality of test samples\n        n_samples: Number of test samples\n        seed: Random seed\n\n    Returns:\n        True if validation passes, False otherwise\n    \"\"\"\n    torch.manual_seed(seed)\n\n    # Generate test samples\n    pos_samples = torch.randn(n_samples, dim)\n    neg_samples = torch.randn(n_samples, dim)\n\n    # Ensure parameters require grad\n    for param in loss_fn.energy_function.parameters():\n        param.requires_grad_(True)\n\n    # Compute loss\n    loss, _ = loss_fn(pos_samples, neg_samples)\n\n    # Check if loss is scalar\n    if not isinstance(loss, torch.Tensor) or loss.numel() != 1:\n        print(f\"Loss is not a scalar: {loss}\")\n        return False\n\n    # Check if loss produces gradients\n    try:\n        loss.backward()\n        has_grad = all(p.grad is not None for p in loss_fn.energy_function.parameters())\n        if not has_grad:\n            print(\"Some parameters did not receive gradients\")\n            return False\n    except Exception as e:\n        print(f\"Error during backward pass: {e}\")\n        return False\n\n    return True\n</code></pre>"},{"location":"developer_guide/implementation_losses/#best-practices-for-custom-loss-functions","title":"Best Practices for Custom Loss Functions","text":"<p>When implementing custom loss functions, follow these best practices:</p> <p>Custom Loss Example</p> <pre><code>class CustomLoss(Loss):\n    \"\"\"Custom loss example.\"\"\"\n\n    def __init__(self, energy_function, alpha=1.0, beta=0.5):\n        super().__init__(energy_function)\n        self.alpha = alpha\n        self.beta = beta\n\n    def __call__(self, pos_samples, neg_samples, **kwargs):\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Custom loss logic\n        loss = (pos_energy.mean() - self.alpha * neg_energy.mean()) + \\\n               self.beta * torch.abs(pos_energy.mean() - neg_energy.mean())\n\n        # Return loss and metrics\n        metrics = {\n            'pos_energy': pos_energy.mean().detach(),\n            'neg_energy': neg_energy.mean().detach(),\n            'loss': loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#do","title":"Do","text":"<ul> <li>Subclass the <code>Loss</code> base class</li> <li>Return both the loss and metrics dictionary</li> <li>Validate inputs</li> <li>Use autograd for derivatives</li> <li>Consider numerical stability</li> </ul>"},{"location":"developer_guide/implementation_losses/#dont","title":"Don't","text":"<ul> <li>Modify input tensors in-place</li> <li>Compute unnecessary gradients</li> <li>Forget to detach metrics</li> <li>Mix device types</li> <li>Ignore potential NaN values</li> </ul>"},{"location":"developer_guide/implementation_losses/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Energy Functions</p> <p>Explore energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Understand sampler implementation details.</p> <p> Implementation Samplers</p> </li> </ul>"},{"location":"developer_guide/implementation_models/","title":"Models Implementation","text":"<p>Implementation Details</p> <p>This guide explains the implementation of neural network models in TorchEBM, including architecture designs, training workflows, and integration with energy functions.</p>"},{"location":"developer_guide/implementation_models/#base-model-architecture","title":"Base Model Architecture","text":"<p>The <code>BaseModel</code> class provides the foundation for all neural networks in TorchEBM:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom typing import Tuple, List, Dict, Any, Optional, Union\n\nclass BaseModel(nn.Module):\n    \"\"\"Base class for all neural network models.\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dims: List[int], activation: Optional[nn.Module] = None):\n        \"\"\"Initialize base model.\n\n        Args:\n            input_dim: Input dimension\n            hidden_dims: List of hidden dimensions\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dims = hidden_dims\n        self.activation = activation or nn.ReLU()\n\n        # Build network architecture\n        self._build_network()\n\n    def _build_network(self):\n        \"\"\"Build the neural network architecture.\"\"\"\n        layers = []\n\n        # Input layer\n        prev_dim = self.input_dim\n\n        # Hidden layers\n        for dim in self.hidden_dims:\n            layers.append(nn.Linear(prev_dim, dim))\n            layers.append(self.activation)\n            prev_dim = dim\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the network.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim)\n\n        Returns:\n            Output tensor\n        \"\"\"\n        return self.network(x)\n</code></pre>"},{"location":"developer_guide/implementation_models/#mlp-energy-model","title":"MLP Energy Model","text":"<pre><code>class MLPEnergyModel(BaseModel):\n    \"\"\"Multi-layer perceptron energy model.\"\"\"\n\n    def __init__(\n        self, \n        input_dim: int, \n        hidden_dims: List[int], \n        activation: Optional[nn.Module] = None,\n        use_spectral_norm: bool = False\n    ):\n        \"\"\"Initialize MLP energy model.\n\n        Args:\n            input_dim: Input dimension\n            hidden_dims: List of hidden dimensions\n            activation: Activation function\n            use_spectral_norm: Whether to use spectral normalization\n        \"\"\"\n        super().__init__(input_dim, hidden_dims, activation)\n        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n\n        # Apply spectral normalization if requested\n        if use_spectral_norm:\n            self._apply_spectral_norm()\n\n    def _apply_spectral_norm(self):\n        \"\"\"Apply spectral normalization to all linear layers.\"\"\"\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear):\n                setattr(\n                    self, \n                    name, \n                    nn.utils.spectral_norm(module)\n                )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass to compute energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim)\n\n        Returns:\n            Energy values of shape (batch_size,)\n        \"\"\"\n        features = super().forward(x)\n        energy = self.output_layer(features)\n        return energy.squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#convolutional-energy-models","title":"Convolutional Energy Models","text":"<p>For image data, convolutional architectures are more appropriate:</p> <pre><code>class ConvEnergyModel(nn.Module):\n    \"\"\"Convolutional energy model for image data.\"\"\"\n\n    def __init__(\n        self,\n        input_channels: int,\n        image_size: int,\n        channels: List[int] = [32, 64, 128, 256],\n        kernel_size: int = 3,\n        activation: Optional[nn.Module] = None\n    ):\n        \"\"\"Initialize convolutional energy model.\n\n        Args:\n            input_channels: Number of input channels\n            image_size: Size of input images (assumed square)\n            channels: List of channel dimensions for conv layers\n            kernel_size: Size of convolutional kernel\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n        self.input_channels = input_channels\n        self.image_size = image_size\n        self.activation = activation or nn.LeakyReLU(0.2)\n\n        # Build convolutional layers\n        layers = []\n        in_channels = input_channels\n\n        for out_channels in channels:\n            layers.append(\n                nn.Conv2d(\n                    in_channels, \n                    out_channels, \n                    kernel_size=kernel_size,\n                    stride=2,\n                    padding=kernel_size // 2\n                )\n            )\n            layers.append(self.activation)\n            in_channels = out_channels\n\n        self.conv_net = nn.Sequential(*layers)\n\n        # Calculate feature size after convolutions\n        feature_size = image_size // (2 ** len(channels))\n\n        # Final layers\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_channels * feature_size * feature_size, 128),\n            self.activation,\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass to compute energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, channels, height, width)\n\n        Returns:\n            Energy values of shape (batch_size,)\n        \"\"\"\n        # Ensure correct input shape\n        if len(x.shape) == 2:\n            x = x.view(-1, self.input_channels, self.image_size, self.image_size)\n\n        features = self.conv_net(x)\n        energy = self.fc(features)\n        return energy.squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#neural-energy-functions","title":"Neural Energy Functions","text":"<p>Neural networks can be used to create energy functions:</p> <pre><code>from torchebm.core import EnergyFunction\n\nclass NeuralEnergyFunction(EnergyFunction):\n    \"\"\"Energy function implemented using a neural network.\"\"\"\n\n    def __init__(self, model: nn.Module):\n        \"\"\"Initialize neural energy function.\n\n        Args:\n            model: Neural network model\n        \"\"\"\n        super().__init__()\n        self.model = model\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy values for inputs.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Energy values\n        \"\"\"\n        return self.model(x)\n</code></pre>"},{"location":"developer_guide/implementation_models/#advanced-architectures","title":"Advanced Architectures","text":""},{"location":"developer_guide/implementation_models/#residual-blocks","title":"Residual Blocks","text":"<pre><code>class ResidualBlock(nn.Module):\n    \"\"\"Residual block for energy models.\"\"\"\n\n    def __init__(self, dim: int, activation: nn.Module = nn.ReLU()):\n        \"\"\"Initialize residual block.\n\n        Args:\n            dim: Feature dimension\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Linear(dim, dim),\n            activation,\n            nn.Linear(dim, dim)\n        )\n        self.activation = activation\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass with residual connection.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Output tensor\n        \"\"\"\n        return x + self.block(x)\n\nclass ResNetEnergyModel(nn.Module):\n    \"\"\"ResNet-style energy model.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 128,\n        n_blocks: int = 4,\n        activation: nn.Module = nn.ReLU()\n    ):\n        \"\"\"Initialize ResNet energy model.\n\n        Args:\n            input_dim: Input dimension\n            hidden_dim: Hidden dimension\n            n_blocks: Number of residual blocks\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n\n        # Input projection\n        self.input_proj = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            activation\n        )\n\n        # Residual blocks\n        self.blocks = nn.ModuleList([\n            ResidualBlock(hidden_dim, activation)\n            for _ in range(n_blocks)\n        ])\n\n        # Output projection\n        self.output_proj = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through ResNet energy model.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Energy values\n        \"\"\"\n        h = self.input_proj(x)\n\n        for block in self.blocks:\n            h = block(h)\n\n        energy = self.output_proj(h)\n        return energy.squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#integration-with-trainers","title":"Integration with Trainers","text":"<p>Models are integrated with trainer classes:</p> <pre><code>class EBMTrainer:\n    \"\"\"Trainer for energy-based models.\"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        sampler: \"Sampler\",\n        optimizer: torch.optim.Optimizer,\n        loss_fn: \"Loss\"\n    ):\n        \"\"\"Initialize EBM trainer.\n\n        Args:\n            energy_function: Energy function to train\n            sampler: Sampler for negative samples\n            optimizer: Optimizer for model parameters\n            loss_fn: Loss function\n        \"\"\"\n        self.energy_function = energy_function\n        self.sampler = sampler\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n\n    def train_step(\n        self,\n        pos_samples: torch.Tensor,\n        neg_samples: Optional[torch.Tensor] = None\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Perform one training step.\n\n        Args:\n            pos_samples: Positive samples from data\n            neg_samples: Optional negative samples\n\n        Returns:\n            Dictionary of metrics\n        \"\"\"\n        # Generate negative samples if not provided\n        if neg_samples is None:\n            with torch.no_grad():\n                neg_samples = self.sampler.sample(\n                    n_samples=pos_samples.shape[0],\n                    dim=pos_samples.shape[1]\n                )\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Compute loss\n        loss, metrics = self.loss_fn(pos_samples, neg_samples)\n\n        # Backward and optimize\n        loss.backward()\n        self.optimizer.step()\n\n        return metrics\n</code></pre>"},{"location":"developer_guide/implementation_models/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"developer_guide/implementation_models/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\nclass MixedPrecisionEBMTrainer(EBMTrainer):\n    \"\"\"Trainer with mixed precision for faster training.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scaler = GradScaler()\n\n    def train_step(\n        self,\n        pos_samples: torch.Tensor,\n        neg_samples: Optional[torch.Tensor] = None\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Perform one training step with mixed precision.\"\"\"\n        # Generate negative samples if not provided\n        if neg_samples is None:\n            with torch.no_grad():\n                neg_samples = self.sampler.sample(\n                    n_samples=pos_samples.shape[0],\n                    dim=pos_samples.shape[1]\n                )\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Forward pass with mixed precision\n        with autocast():\n            loss, metrics = self.loss_fn(pos_samples, neg_samples)\n\n        # Backward and optimize with gradient scaling\n        self.scaler.scale(loss).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n\n        return metrics\n</code></pre>"},{"location":"developer_guide/implementation_models/#best-practices-for-custom-models","title":"Best Practices for Custom Models","text":"<p>When implementing custom models, follow these best practices:</p> <p>Custom Model Example</p> <pre><code>class CustomEnergyModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SiLU(),  # SiLU/Swish activation\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Initialize weights properly\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.orthogonal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#do","title":"Do","text":"<ul> <li>Subclass appropriate base classes</li> <li>Handle device placement correctly</li> <li>Use proper initialization</li> <li>Consider normalization techniques</li> <li>Document architecture clearly</li> </ul>"},{"location":"developer_guide/implementation_models/#dont","title":"Don't","text":"<ul> <li>Create overly complex architectures</li> <li>Ignore numerical stability</li> <li>Forget to validate inputs</li> <li>Mix different PyTorch versions</li> <li>Ignore gradient flow issues</li> </ul>"},{"location":"developer_guide/implementation_models/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Energy Functions</p> <p>Explore energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Understand sampler implementation details.</p> <p> Implementation Samplers</p> </li> </ul>"},{"location":"developer_guide/implementation_samplers/","title":"Samplers Implementation","text":"<p>Implementation Details</p> <p>This guide provides detailed information about the implementation of sampling algorithms in TorchEBM, including mathematical foundations, code structure, and optimization techniques.</p>"},{"location":"developer_guide/implementation_samplers/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Sampling algorithms in energy-based models aim to generate samples from the distribution:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>where \\(E(x)\\) is the energy function and \\(Z = \\int e^{-E(x)} dx\\) is the normalization constant.</p>"},{"location":"developer_guide/implementation_samplers/#base-sampler-implementation","title":"Base Sampler Implementation","text":"<p>The <code>Sampler</code> base class provides the foundation for all sampling algorithms:</p> <pre><code>from abc import ABC, abstractmethod\nimport torch\nfrom typing import Optional, Union, Tuple\n\nfrom torchebm.core import EnergyFunction\n\nclass Sampler(ABC):\n    \"\"\"Base class for all sampling algorithms.\"\"\"\n\n    def __init__(self, energy_function: EnergyFunction):\n        \"\"\"Initialize sampler with an energy function.\n\n        Args:\n            energy_function: The energy function to sample from\n        \"\"\"\n        self.energy_function = energy_function\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def to(self, device):\n        \"\"\"Move sampler to specified device.\"\"\"\n        self.device = device\n        return self\n\n    @abstractmethod\n    def sample(self, n_samples: int, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of shape (n_samples, dim) containing samples\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using a Markov chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of shape (n_samples, dim) containing final samples\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#langevin-dynamics","title":"Langevin Dynamics","text":""},{"location":"developer_guide/implementation_samplers/#mathematical-background","title":"Mathematical Background","text":"<p>Langevin dynamics uses the score function (gradient of log-probability) to guide sampling with Brownian motion:</p> \\[dx_t = -\\nabla E(x_t)dt + \\sqrt{2}dW_t\\] <p>where \\(W_t\\) is the Wiener process (Brownian motion).</p>"},{"location":"developer_guide/implementation_samplers/#implementation","title":"Implementation","text":"<pre><code>import torch\nimport numpy as np\nfrom typing import Optional, Union, Tuple\n\nfrom torchebm.core import EnergyFunction\nfrom torchebm.samplers.base import Sampler\n\nclass LangevinDynamics(Sampler):\n    \"\"\"Langevin dynamics sampler.\"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        step_size: float = 0.01,\n        noise_scale: float = 1.0\n    ):\n        \"\"\"Initialize Langevin dynamics sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            step_size: Step size for updates\n            noise_scale: Scale of noise added at each step\n        \"\"\"\n        super().__init__(energy_function)\n        self.step_size = step_size\n        self.noise_scale = noise_scale\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Langevin dynamics.\n\n        Args:\n            x: Current samples of shape (n_samples, dim)\n\n        Returns:\n            Updated samples of shape (n_samples, dim)\n        \"\"\"\n        # Compute score (gradient of log probability)\n        score = -self.energy_function.score(x)\n\n        # Add drift term and noise\n        noise = torch.randn_like(x) * np.sqrt(2 * self.step_size * self.noise_scale)\n        x_new = x + self.step_size * score + noise\n\n        return x_new\n\n    def sample_chain(\n        self,\n        dim: int,\n        n_steps: int,\n        n_samples: int = 1,\n        initial_samples: Optional[torch.Tensor] = None,\n        return_trajectory: bool = False\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Generate samples using a Langevin dynamics chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            initial_samples: Optional initial samples\n            return_trajectory: Whether to return the full trajectory\n\n        Returns:\n            Samples or (samples, trajectory)\n        \"\"\"\n        # Initialize samples\n        if initial_samples is None:\n            x = torch.randn(n_samples, dim, device=self.device)\n        else:\n            x = initial_samples.clone().to(self.device)\n\n        # Initialize trajectory if needed\n        if return_trajectory:\n            trajectory = torch.zeros(n_steps + 1, n_samples, dim, device=self.device)\n            trajectory[0] = x\n\n        # Run sampling chain\n        for i in range(n_steps):\n            x = self.sample_step(x)\n            if return_trajectory:\n                trajectory[i + 1] = x\n\n        if return_trajectory:\n            return x, trajectory\n        else:\n            return x\n\n    def sample(self, n_samples: int, dim: int, n_steps: int = 100, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\"\"\"\n        return self.sample_chain(dim=dim, n_steps=n_steps, n_samples=n_samples, **kwargs)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":""},{"location":"developer_guide/implementation_samplers/#mathematical-background_1","title":"Mathematical Background","text":"<p>Hamiltonian Monte Carlo (HMC) introduces momentum variables to help explore the distribution more efficiently:</p> \\[H(x, p) = E(x) + \\frac{1}{2}p^Tp\\] <p>where \\(p\\) is the momentum variable and \\(H\\) is the Hamiltonian.</p>"},{"location":"developer_guide/implementation_samplers/#implementation_1","title":"Implementation","text":"<pre><code>class HamiltonianMonteCarlo(Sampler):\n    \"\"\"Hamiltonian Monte Carlo sampler.\"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        step_size: float = 0.1,\n        n_leapfrog_steps: int = 10,\n        mass_matrix: Optional[torch.Tensor] = None\n    ):\n        \"\"\"Initialize HMC sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            step_size: Step size for leapfrog integration\n            n_leapfrog_steps: Number of leapfrog steps\n            mass_matrix: Mass matrix for momentum (identity by default)\n        \"\"\"\n        super().__init__(energy_function)\n        self.step_size = step_size\n        self.n_leapfrog_steps = n_leapfrog_steps\n        self.mass_matrix = mass_matrix\n\n    def _leapfrog_step(self, x: torch.Tensor, p: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Perform one leapfrog step.\n\n        Args:\n            x: Position tensor of shape (n_samples, dim)\n            p: Momentum tensor of shape (n_samples, dim)\n\n        Returns:\n            New position and momentum\n        \"\"\"\n        # Half step for momentum\n        grad_x = self.energy_function.score(x)\n        p = p - 0.5 * self.step_size * grad_x\n\n        # Full step for position\n        if self.mass_matrix is not None:\n            x = x + self.step_size * torch.matmul(p, self.mass_matrix)\n        else:\n            x = x + self.step_size * p\n\n        # Half step for momentum\n        grad_x = self.energy_function.score(x)\n        p = p - 0.5 * self.step_size * grad_x\n\n        return x, p\n\n    def _compute_hamiltonian(self, x: torch.Tensor, p: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Hamiltonian value.\n\n        Args:\n            x: Position tensor of shape (n_samples, dim)\n            p: Momentum tensor of shape (n_samples, dim)\n\n        Returns:\n            Hamiltonian value of shape (n_samples,)\n        \"\"\"\n        energy = self.energy_function(x)\n\n        if self.mass_matrix is not None:\n            kinetic = 0.5 * torch.sum(p * torch.matmul(p, self.mass_matrix), dim=1)\n        else:\n            kinetic = 0.5 * torch.sum(p * p, dim=1)\n\n        return energy + kinetic\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of HMC.\n\n        Args:\n            x: Current samples of shape (n_samples, dim)\n\n        Returns:\n            Updated samples of shape (n_samples, dim)\n        \"\"\"\n        # Sample initial momentum\n        p = torch.randn_like(x)\n\n        # Compute initial Hamiltonian\n        x_old, p_old = x.clone(), p.clone()\n        h_old = self._compute_hamiltonian(x_old, p_old)\n\n        # Leapfrog integration\n        x_new, p_new = x_old.clone(), p_old.clone()\n        for _ in range(self.n_leapfrog_steps):\n            x_new, p_new = self._leapfrog_step(x_new, p_new)\n\n        # Metropolis-Hastings correction\n        h_new = self._compute_hamiltonian(x_new, p_new)\n        accept_prob = torch.exp(h_old - h_new)\n        accept = torch.rand_like(accept_prob) &lt; accept_prob\n\n        # Accept or reject\n        x_out = torch.where(accept.unsqueeze(1), x_new, x_old)\n\n        return x_out\n\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using an HMC chain.\"\"\"\n        # Implementation similar to LangevinDynamics.sample_chain\n        pass\n\n    def sample(self, n_samples: int, dim: int, n_steps: int = 100, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\"\"\"\n        return self.sample_chain(dim=dim, n_steps=n_steps, n_samples=n_samples, **kwargs)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#metropolis-hastings-sampler","title":"Metropolis-Hastings Sampler","text":"<pre><code>class MetropolisHastings(Sampler):\n    \"\"\"Metropolis-Hastings sampler.\"\"\"\n\n    def __init__(\n        self,\n        energy_function: EnergyFunction,\n        proposal_scale: float = 0.1\n    ):\n        \"\"\"Initialize Metropolis-Hastings sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            proposal_scale: Scale of proposal distribution\n        \"\"\"\n        super().__init__(energy_function)\n        self.proposal_scale = proposal_scale\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Metropolis-Hastings.\n\n        Args:\n            x: Current samples of shape (n_samples, dim)\n\n        Returns:\n            Updated samples of shape (n_samples, dim)\n        \"\"\"\n        # Compute energy of current state\n        energy_x = self.energy_function(x)\n\n        # Propose new state\n        proposal = x + self.proposal_scale * torch.randn_like(x)\n\n        # Compute energy of proposed state\n        energy_proposal = self.energy_function(proposal)\n\n        # Compute acceptance probability\n        accept_prob = torch.exp(energy_x - energy_proposal)\n        accept = torch.rand_like(accept_prob) &lt; accept_prob\n\n        # Accept or reject\n        x_new = torch.where(accept.unsqueeze(1), proposal, x)\n\n        return x_new\n\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using a Metropolis-Hastings chain.\"\"\"\n        # Implementation similar to LangevinDynamics.sample_chain\n        pass\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"developer_guide/implementation_samplers/#cuda-acceleration","title":"CUDA Acceleration","text":"<p>For performance-critical operations, we implement CUDA-optimized versions:</p> <pre><code>from torchebm.cuda import langevin_step_cuda\n\nclass CUDALangevinDynamics(LangevinDynamics):\n    \"\"\"CUDA-optimized Langevin dynamics sampler.\"\"\"\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Langevin dynamics with CUDA optimization.\"\"\"\n        if not torch.cuda.is_available() or not x.is_cuda:\n            return super().sample_step(x)\n\n        return langevin_step_cuda(\n            x, \n            self.energy_function,\n            self.step_size,\n            self.noise_scale\n        )\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#batch-processing","title":"Batch Processing","text":"<p>To handle large numbers of samples efficiently:</p> <pre><code>def batch_sample_chain(\n    sampler: Sampler,\n    dim: int,\n    n_steps: int,\n    n_samples: int,\n    batch_size: int = 1000\n) -&gt; torch.Tensor:\n    \"\"\"Sample in batches to avoid memory issues.\"\"\"\n    samples = []\n\n    for i in range(0, n_samples, batch_size):\n        batch_n = min(batch_size, n_samples - i)\n        batch_samples = sampler.sample_chain(\n            dim=dim,\n            n_steps=n_steps,\n            n_samples=batch_n\n        )\n        samples.append(batch_samples)\n\n    return torch.cat(samples, dim=0)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#best-practices-for-custom-samplers","title":"Best Practices for Custom Samplers","text":"<p>When implementing custom samplers, follow these best practices:</p> <p>Custom Sampler Example</p> <pre><code>class CustomSampler(Sampler):\n    \"\"\"Custom sampler example.\"\"\"\n\n    def __init__(self, energy_function, step_size=0.01):\n        super().__init__(energy_function)\n        self.step_size = step_size\n\n    def sample_step(self, x):\n        # Custom sampling logic\n        return x + self.step_size * torch.randn_like(x)\n\n    def sample_chain(self, dim, n_steps, n_samples=1):\n        # Initialize\n        x = torch.randn(n_samples, dim, device=self.device)\n\n        # Run chain\n        for _ in range(n_steps):\n            x = self.sample_step(x)\n\n        return x\n\n    def sample(self, n_samples, dim, n_steps=100):\n        return self.sample_chain(dim, n_steps, n_samples)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#do","title":"Do","text":"<ul> <li>Subclass the <code>Sampler</code> base class</li> <li>Implement both <code>sample</code> and <code>sample_chain</code> methods</li> <li>Handle device placement correctly</li> <li>Support batched execution</li> <li>Add diagnostics when appropriate</li> </ul>"},{"location":"developer_guide/implementation_samplers/#dont","title":"Don't","text":"<ul> <li>Modify input tensors in-place</li> <li>Allocate new tensors unnecessarily</li> <li>Ignore numerical stability</li> <li>Forget to validate inputs</li> <li>Implement complex logic in sampling loops</li> </ul>"},{"location":"developer_guide/implementation_samplers/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Energy Functions</p> <p>Explore energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Loss Functions</p> <p>Understand loss function implementation details.</p> <p> Loss Functions</p> </li> </ul>"},{"location":"developer_guide/performance/","title":"Performance Optimization","text":"<p>This document provides guidance on optimizing the performance of TorchEBM for both development and usage.</p>"},{"location":"developer_guide/performance/#performance-considerations","title":"Performance Considerations","text":"<p>Key Performance Areas</p> <p>When working with TorchEBM, pay special attention to these performance-critical areas:</p> <ol> <li>Sampling algorithms: These are iterative and typically the most compute-intensive</li> <li>Gradient calculations: Computing energy gradients is fundamental to many algorithms</li> <li>Batch processing: Effective vectorization for parallel processing</li> <li>GPU utilization: Proper device management and memory usage</li> </ol>"},{"location":"developer_guide/performance/#vectorization-techniques","title":"Vectorization Techniques","text":""},{"location":"developer_guide/performance/#batched-operations","title":"Batched Operations","text":"<p>TorchEBM extensively uses batching to improve performance:</p> <pre><code># Instead of looping over samples\nfor i in range(n_samples):\n    energy_i = energy_function(x[i])  # Slow\n\n# Use batched computation\nenergy = energy_function(x)  # Fast\n</code></pre>"},{"location":"developer_guide/performance/#parallel-sampling","title":"Parallel Sampling","text":"<p>Sample multiple chains in parallel by using batch dimensions:</p> <pre><code># Initialize batch of samples\nx = torch.randn(n_samples, dim, device=device)\n\n# One sampling step (all chains update together)\nx_new, _ = sampler.step(x)\n</code></pre>"},{"location":"developer_guide/performance/#gpu-acceleration","title":"GPU Acceleration","text":"<p>TorchEBM is designed to work efficiently on GPUs:</p>"},{"location":"developer_guide/performance/#device-management","title":"Device Management","text":"<pre><code># Create energy function and move to appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = GaussianEnergy(mean, cov).to(device)\n\n# Create sampler with the same device\nsampler = LangevinDynamics(energy_fn, device=device)\n\n# Generate samples (automatically on the correct device)\nsamples, _ = sampler.sample_chain(dim=2, n_steps=1000, n_samples=10000)\n</code></pre>"},{"location":"developer_guide/performance/#memory-management","title":"Memory Management","text":"<p>Memory management is critical for performance, especially on GPUs:</p> <pre><code># Avoid creating new tensors in loops\nfor step in range(n_steps):\n    # Bad: Creates new tensors each iteration\n    x = x - step_size * energy_fn.gradient(x) + noise_scale * torch.randn_like(x)\n\n    # Good: In-place operations\n    grad = energy_fn.gradient(x)\n    x.sub_(step_size * grad)\n    x.add_(noise_scale * torch.randn_like(x))\n</code></pre>"},{"location":"developer_guide/performance/#custom-cuda-kernels","title":"Custom CUDA Kernels","text":"<p>For the most performance-critical operations, TorchEBM provides custom CUDA kernels:</p> <pre><code># Standard PyTorch implementation\ndef langevin_step_pytorch(x, energy_fn, step_size, noise_scale):\n    grad = energy_fn.gradient(x)\n    noise = torch.randn_like(x) * noise_scale\n    return x - step_size * grad + noise\n\n# Using custom CUDA kernel when available\nfrom torchebm.cuda import langevin_step_cuda\n\ndef langevin_step(x, energy_fn, step_size, noise_scale):\n    if x.is_cuda and torch.cuda.is_available():\n        return langevin_step_cuda(x, energy_fn, step_size, noise_scale)\n    else:\n        return langevin_step_pytorch(x, energy_fn, step_size, noise_scale)\n</code></pre>"},{"location":"developer_guide/performance/#sampling-efficiency","title":"Sampling Efficiency","text":"<p>Sampling efficiency can be improved using several techniques:</p> <ul> <li> <p> Step Size Adaptation</p> <p>Automatically adjust step sizes based on acceptance rates or other metrics.</p> <pre><code># Adaptive step size example\nif acceptance_rate &lt; 0.3:\n    step_size *= 0.9  # Decrease step size\nelif acceptance_rate &gt; 0.7:\n    step_size *= 1.1  # Increase step size\n</code></pre> </li> <li> <p> Burn-in Period</p> <p>Discard initial samples to reduce the impact of initialization.</p> <pre><code># Run burn-in period\nx = torch.randn(n_samples, dim)\nfor _ in range(burn_in_steps):\n    x, _ = sampler.step(x)\n\n# Start collecting samples\nsamples = []\nfor _ in range(n_steps):\n    x, _ = sampler.step(x)\n    samples.append(x.clone())\n</code></pre> </li> <li> <p> Thinning</p> <p>Reduce correlation between samples by keeping only every Nth sample.</p> <pre><code># Collect samples with thinning\nsamples = []\nfor i in range(n_steps):\n    x, _ = sampler.step(x)\n    if i % thinning == 0:\n        samples.append(x.clone())\n</code></pre> </li> <li> <p> Warm Starting</p> <p>Initialize sampling from a distribution close to the target.</p> <pre><code># Warm start from approximate distribution\nx = approximate_sampler.sample(n_samples, dim)\nsamples = sampler.sample_chain(\n    n_steps=n_steps, \n    initial_samples=x\n)\n</code></pre> </li> </ul>"},{"location":"developer_guide/performance/#profiling-and-benchmarking","title":"Profiling and Benchmarking","text":"<p>To identify performance bottlenecks, TorchEBM includes profiling utilities:</p> <pre><code>from torchebm.utils.profiling import profile_sampling\n\n# Profile a sampling run\nprofiling_results = profile_sampling(\n    sampler, \n    dim=10, \n    n_steps=1000, \n    n_samples=100\n)\n\n# Print results\nprint(f\"Total time: {profiling_results['total_time']:.2f} seconds\")\nprint(f\"Time per step: {profiling_results['time_per_step']:.5f} seconds\")\nprint(\"Component breakdown:\")\nfor component, time_pct in profiling_results['component_times'].items():\n    print(f\"  {component}: {time_pct:.1f}%\")\n</code></pre>"},{"location":"developer_guide/performance/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Here are some performance benchmarks for common operations:</p> Operation CPU Time (ms) GPU Time (ms) Speedup Langevin step (1,000 samples, dim=10) 8.2 0.41 20x HMC step (1,000 samples, dim=10) 15.4 0.76 20.3x Energy gradient (10,000 samples, dim=100) 42.1 1.8 23.4x Full sampling (10,000 samples, 100 steps) 820 38 21.6x"},{"location":"developer_guide/performance/#performance-tips-and-best-practices","title":"Performance Tips and Best Practices","text":"<p>Common Pitfalls</p> <p>Avoid these common performance issues:</p> <ol> <li>Unnecessary CPU-GPU transfers: Keep data on the same device</li> <li>Small batch sizes: Too small batches underutilize hardware</li> <li>Unneeded gradient tracking: Disable gradients when not training</li> <li>Excessive logging: Logging every step can significantly slow down sampling</li> </ol>"},{"location":"developer_guide/performance/#general-tips","title":"General Tips","text":"<ol> <li>Use the right device: Always move computation to GPU when available</li> <li>Batch processing: Process data in batches rather than individually</li> <li>Reuse tensors: Avoid creating new tensors in inner loops</li> <li>Monitor memory: Use <code>torch.cuda.memory_summary()</code> to track memory usage</li> </ol>"},{"location":"developer_guide/performance/#sampling-tips","title":"Sampling Tips","text":"<ol> <li>Tune step sizes: Optimal step sizes balance exploration and stability</li> <li>Parallel chains: Use multiple chains to improve sample diversity</li> <li>Adaptive methods: Use adaptive samplers for complex distributions</li> <li>Mixed precision: Consider using mixed precision for larger models</li> </ol>"},{"location":"developer_guide/performance/#algorithm-specific-optimizations","title":"Algorithm-Specific Optimizations","text":""},{"location":"developer_guide/performance/#langevin-dynamics","title":"Langevin Dynamics","text":"<pre><code># Optimize step size for Langevin dynamics\n# Rule of thumb: step_size \u2248 O(d^(-1/3)) where d is dimension\nstep_size = min(0.01, 0.1 * dim**(-1/3))\n\n# Noise scale should be sqrt(2 * step_size) for standard Langevin\nnoise_scale = np.sqrt(2 * step_size)\n</code></pre>"},{"location":"developer_guide/performance/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":"<pre><code># Optimize HMC parameters\n# Leapfrog steps should scale with dimension\nn_leapfrog_steps = max(5, int(np.sqrt(dim)))\n\n# Step size should decrease with dimension\nstep_size = min(0.01, 0.05 * dim**(-1/4))\n</code></pre>"},{"location":"developer_guide/performance/#multi-gpu-scaling","title":"Multi-GPU Scaling","text":"<p>For extremely large sampling tasks, TorchEBM supports multi-GPU execution:</p> <pre><code># Distribution across GPUs using DataParallel\nimport torch.nn as nn\n\nclass ParallelSampler(nn.DataParallel):\n    def __init__(self, sampler, device_ids=None):\n        super().__init__(sampler, device_ids=device_ids)\n        self.module = sampler\n\n    def sample_chain(self, dim, n_steps, n_samples):\n        # Distribute samples across GPUs\n        return self.forward(dim, n_steps, n_samples)\n\n# Create parallel sampler\ndevices = list(range(torch.cuda.device_count()))\nparallel_sampler = ParallelSampler(sampler, device_ids=devices)\n\n# Generate samples using all available GPUs\nsamples = parallel_sampler.sample_chain(dim=100, n_steps=1000, n_samples=100000)\n</code></pre>"},{"location":"developer_guide/performance/#conclusion","title":"Conclusion","text":"<p>Performance optimization in TorchEBM involves careful attention to vectorization, GPU acceleration, memory management, and algorithm-specific tuning. By following these guidelines, you can achieve significant speedups in your energy-based modeling workflows. </p>"},{"location":"developer_guide/project_structure/","title":"Project Structure","text":"<p>Codebase Organization</p> <p>Understanding the TorchEBM project structure helps you navigate the codebase and contribute effectively. This guide provides an overview of the repository organization.</p>"},{"location":"developer_guide/project_structure/#repository-overview","title":"Repository Overview","text":"<p>The TorchEBM repository is organized as follows:</p> <pre><code>torchebm/\n\u251c\u2500\u2500 torchebm/              # Main package source code\n\u2502   \u251c\u2500\u2500 core/              # Core functionality and base classes\n\u2502   \u251c\u2500\u2500 samplers/          # Sampling algorithms implementation\n\u2502   \u251c\u2500\u2500 losses/            # Loss functions for training\n\u2502   \u251c\u2500\u2500 models/            # Neural network model implementations\n\u2502   \u251c\u2500\u2500 cuda/              # CUDA optimized implementations\n\u2502   \u2514\u2500\u2500 utils/             # Utility functions and helpers\n\u251c\u2500\u2500 tests/                 # Test directory\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 examples/              # Example applications\n\u251c\u2500\u2500 benchmarks/            # Performance benchmarks\n\u251c\u2500\u2500 setup.py               # Package setup script\n\u2514\u2500\u2500 README.md              # Project README\n</code></pre>"},{"location":"developer_guide/project_structure/#main-package-structure","title":"Main Package Structure","text":""},{"location":"developer_guide/project_structure/#torchebmcore","title":"<code>torchebm.core</code>","text":"<p>Contains the core functionality including base classes and essential components:</p> <ul> <li><code>base.py</code> - Base classes for the entire library</li> <li><code>energy_function.py</code> - Base energy function class and interface</li> <li><code>analytical_functions.py</code> - Analytical energy function implementations</li> <li><code>distributions.py</code> - Probability distribution implementations</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmsamplers","title":"<code>torchebm.samplers</code>","text":"<p>Implementations of various sampling algorithms:</p> <ul> <li><code>base.py</code> - Base sampler class</li> <li><code>langevin_dynamics.py</code> - Langevin dynamics implementation</li> <li><code>hmc.py</code> - Hamiltonian Monte Carlo</li> <li><code>metropolis_hastings.py</code> - Metropolis-Hastings</li> <li>[Other sampler implementations]</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmlosses","title":"<code>torchebm.losses</code>","text":"<p>Loss functions for training energy-based models:</p> <ul> <li><code>base.py</code> - Base loss class</li> <li><code>contrastive_divergence.py</code> - Contrastive divergence implementations</li> <li><code>score_matching.py</code> - Score matching methods</li> <li>[Other loss implementations]</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmmodels","title":"<code>torchebm.models</code>","text":"<p>Neural network model implementations:</p> <ul> <li><code>mlp.py</code> - Multi-layer perceptron energy models</li> <li><code>cnn.py</code> - Convolutional neural network energy models</li> <li><code>ebm.py</code> - Generic energy-based model implementations</li> <li>[Other model architectures]</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmcuda","title":"<code>torchebm.cuda</code>","text":"<p>CUDA-optimized implementations for performance-critical operations:</p> <ul> <li><code>kernels/</code> - CUDA kernel implementations</li> <li><code>bindings.cpp</code> - PyTorch C++ bindings</li> <li><code>ops.py</code> - Python interfaces to CUDA operations</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmutils","title":"<code>torchebm.utils</code>","text":"<p>Utility functions and helpers:</p> <ul> <li><code>device.py</code> - Device management utilities</li> <li><code>visualization.py</code> - Visualization tools</li> <li><code>data.py</code> - Data loading and processing utilities</li> <li><code>logging.py</code> - Logging utilities</li> </ul>"},{"location":"developer_guide/project_structure/#tests-structure","title":"Tests Structure","text":"<p>The tests directory mirrors the package structure:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/                  # Unit tests\n\u2502   \u251c\u2500\u2500 core/              # Tests for core module\n\u2502   \u251c\u2500\u2500 samplers/          # Tests for samplers module\n\u2502   \u251c\u2500\u2500 losses/            # Tests for losses module\n\u2502   \u2514\u2500\u2500 utils/             # Tests for utilities\n\u251c\u2500\u2500 integration/           # Integration tests\n\u251c\u2500\u2500 performance/           # Performance benchmarks\n\u251c\u2500\u2500 conftest.py            # Pytest configuration and fixtures\n\u2514\u2500\u2500 utils.py               # Test utilities\n</code></pre>"},{"location":"developer_guide/project_structure/#documentation-structure","title":"Documentation Structure","text":"<p>The documentation is built with MkDocs and organized as follows:</p> <pre><code>docs/\n\u251c\u2500\u2500 index.md               # Home page\n\u251c\u2500\u2500 getting_started.md     # Quick start guide\n\u251c\u2500\u2500 guides/                # User guides\n\u251c\u2500\u2500 api/                   # API reference (auto-generated)\n\u251c\u2500\u2500 examples/              # Example documentation\n\u251c\u2500\u2500 developer_guide/       # Developer documentation\n\u2502   \u251c\u2500\u2500 contributing.md    # Contributing guidelines\n\u2502   \u251c\u2500\u2500 code_style.md      # Code style guide\n\u2502   \u2514\u2500\u2500 ...                # Other developer docs\n\u2514\u2500\u2500 assets/                # Static assets\n    \u251c\u2500\u2500 images/            # Images\n    \u251c\u2500\u2500 stylesheets/       # Custom CSS\n    \u2514\u2500\u2500 javascripts/       # Custom JavaScript\n</code></pre>"},{"location":"developer_guide/project_structure/#examples-structure","title":"Examples Structure","text":"<p>Example applications to demonstrate TorchEBM usage:</p> <pre><code>examples/\n\u251c\u2500\u2500 basic/                 # Basic usage examples\n\u251c\u2500\u2500 advanced/              # Advanced examples\n\u2514\u2500\u2500 real_world/            # Real-world applications\n</code></pre>"},{"location":"developer_guide/project_structure/#dependencies-and-requirements","title":"Dependencies and Requirements","text":"<p>TorchEBM has the following dependencies:</p> <ul> <li> <p> PyTorch</p> <p>Primary framework for tensor operations and automatic differentiation.</p> </li> <li> <p>:material-numpy:{ .lg .middle } NumPy</p> <p>Used for numerical operations when PyTorch isn't needed.</p> </li> <li> <p> Matplotlib</p> <p>For visualization capabilities.</p> </li> <li> <p> tqdm</p> <p>For progress bars during long operations.</p> </li> </ul>"},{"location":"developer_guide/project_structure/#entry-points","title":"Entry Points","text":"<p>The main entry points to the library are:</p> <ul> <li><code>torchebm.core</code> - Import core functionality</li> <li><code>torchebm.samplers</code> - Import samplers</li> <li><code>torchebm.losses</code> - Import loss functions</li> <li><code>torchebm.models</code> - Import neural network models</li> </ul> <p>Example of typical import patterns:</p> <pre><code># Import core components\nfrom torchebm.core import EnergyFunction, GaussianEnergy\n\n# Import samplers\nfrom torchebm.samplers import LangevinDynamics, HamiltonianMC\n\n# Import loss functions\nfrom torchebm.losses import ContrastiveDivergence\n\n# Import utilities\nfrom torchebm.utils import visualize_samples\n</code></pre>"},{"location":"developer_guide/project_structure/#package-management","title":"Package Management","text":"<p>TorchEBM uses setuptools for package management with setup.py:</p> <pre><code># setup.py excerpt\nsetup(\n    name=\"torchebm\",\n    version=__version__,\n    description=\"Energy-Based Modeling library for PyTorch\",\n    packages=find_packages(),\n    install_requires=[\n        \"torch&gt;=1.9.0\",\n        \"numpy&gt;=1.20.0\",\n        \"matplotlib&gt;=3.4.0\",\n        \"tqdm&gt;=4.60.0\",\n    ],\n    # Additional configuration...\n)\n</code></pre>"},{"location":"developer_guide/project_structure/#version-control-structure","title":"Version Control Structure","text":"<ul> <li>We follow Conventional Commits for our commit messages</li> <li>Feature branches should be named <code>feature/feature-name</code></li> <li>Bugfix branches should be named <code>bugfix/bug-name</code></li> <li>Release branches should be named <code>release/vX.Y.Z</code></li> </ul> <p>Finding Your Way Around</p> <p>When contributing to TorchEBM, start by exploring the relevant directory for your feature. For example, if you're adding a new sampler, look at the existing implementations in <code>torchebm/samplers/</code> to understand the pattern. </p>"},{"location":"developer_guide/testing_guide/","title":"Testing Guide","text":"<p>Quality Assurance</p> <p>Comprehensive testing is essential for maintaining the reliability and stability of TorchEBM. This guide outlines our testing approach and best practices.</p>"},{"location":"developer_guide/testing_guide/#testing-philosophy","title":"Testing Philosophy","text":"<p>TorchEBM follows test-driven development principles where appropriate, especially for core functionality. Our testing strategy includes:</p> <ul> <li> <p> Unit Tests</p> <p>Test individual components in isolation to ensure they work correctly.</p> </li> <li> <p> Integration Tests</p> <p>Test combinations of components to ensure they work together seamlessly.</p> </li> <li> <p> Performance Tests</p> <p>Measure the speed and resource usage of critical operations.</p> </li> <li> <p> Numerical Tests</p> <p>Verify numerical correctness of algorithms against known results.</p> </li> </ul>"},{"location":"developer_guide/testing_guide/#test-directory-structure","title":"Test Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                # Unit tests\n\u2502   \u251c\u2500\u2500 core/            # Tests for core module\n\u2502   \u251c\u2500\u2500 samplers/        # Tests for samplers module\n\u2502   \u251c\u2500\u2500 losses/          # Tests for losses module\n\u2502   \u2514\u2500\u2500 utils/           # Tests for utilities\n\u251c\u2500\u2500 integration/         # Integration tests\n\u251c\u2500\u2500 performance/         # Performance benchmarks\n\u251c\u2500\u2500 conftest.py          # Pytest configuration and fixtures\n\u2514\u2500\u2500 utils.py             # Test utilities\n</code></pre>"},{"location":"developer_guide/testing_guide/#running-tests","title":"Running Tests","text":"Basic UsageCoverageParallel Execution <pre><code># Run all tests\npytest\n\n# Run specific tests\npytest tests/unit/core/\npytest tests/unit/samplers/test_langevin.py\n\n# Run specific test class\npytest tests/unit/core/test_energy.py::TestGaussianEnergy\n\n# Run specific test method\npytest tests/unit/core/test_energy.py::TestGaussianEnergy::test_energy_computation\n</code></pre> <pre><code># Run tests with coverage\npytest --cov=torchebm\n\n# Generate HTML coverage report\npytest --cov=torchebm --cov-report=html\n</code></pre> <pre><code># Run tests in parallel (4 processes)\npytest -n 4\n</code></pre>"},{"location":"developer_guide/testing_guide/#writing-tests","title":"Writing Tests","text":"<p>We use pytest for all our tests. Here are guidelines for writing effective tests:</p>"},{"location":"developer_guide/testing_guide/#test-class-structure","title":"Test Class Structure","text":"<pre><code>import pytest\nimport torch\nfrom torchebm.core import GaussianEnergy\n\nclass TestGaussianEnergy:\n    @pytest.fixture\n    def energy_fn(self):\n        \"\"\"Fixture to create a standard Gaussian energy function.\"\"\"\n        return GaussianEnergy(\n            mean=torch.zeros(2),\n            cov=torch.eye(2)\n        )\n\n    def test_energy_computation(self, energy_fn):\n        \"\"\"Test that energy is correctly computed for known inputs.\"\"\"\n        x = torch.zeros(2)\n        energy = energy_fn(x)\n        assert energy.item() == 0.0\n\n        x = torch.ones(2)\n        energy = energy_fn(x)\n        assert torch.isclose(energy, torch.tensor(1.0))\n</code></pre>"},{"location":"developer_guide/testing_guide/#test-naming-conventions","title":"Test Naming Conventions","text":"<ul> <li>Test files should be named <code>test_*.py</code></li> <li>Test classes should be named <code>Test*</code></li> <li>Test methods should be named <code>test_*</code></li> <li>Use descriptive names that indicate what's being tested</li> </ul>"},{"location":"developer_guide/testing_guide/#parametrized-tests","title":"Parametrized Tests","text":"<p>Use <code>pytest.mark.parametrize</code> for testing multiple inputs:</p> <pre><code>import pytest\nimport torch\nfrom torchebm.core import GaussianEnergy\n\nclass TestGaussianEnergy:\n    @pytest.mark.parametrize(\"mean,cov,x,expected\", [\n        (torch.zeros(2), torch.eye(2), torch.zeros(2), 0.0),\n        (torch.zeros(2), torch.eye(2), torch.ones(2), 1.0),\n        (torch.ones(2), torch.eye(2), torch.zeros(2), 1.0),\n    ])\n    def test_energy_parametrized(self, mean, cov, x, expected):\n        energy_fn = GaussianEnergy(mean=mean, cov=cov)\n        energy = energy_fn(x)\n        assert torch.isclose(energy, torch.tensor(expected))\n</code></pre>"},{"location":"developer_guide/testing_guide/#fixtures","title":"Fixtures","text":"<p>Use fixtures for common setup code:</p> <pre><code>import pytest\nimport torch\n\n@pytest.fixture\ndef device():\n    \"\"\"Return the default device for testing.\"\"\"\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@pytest.fixture\ndef precision():\n    \"\"\"Return the default precision for comparison.\"\"\"\n    return 1e-5\n</code></pre>"},{"location":"developer_guide/testing_guide/#testing-cuda-code","title":"Testing CUDA Code","text":"<p>When testing CUDA code, follow these guidelines:</p> <pre><code>import pytest\nimport torch\nfrom torchebm.cuda import cuda_function\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA not available\")\ndef test_cuda_function():\n    # Prepare test data\n    x = torch.randn(100, device=\"cuda\")\n\n    # Call function\n    result = cuda_function(x)\n\n    # Verify result\n    expected = x * 2  # Hypothetical expected result\n    assert torch.allclose(result, expected)\n</code></pre>"},{"location":"developer_guide/testing_guide/#mocking","title":"Mocking","text":"<p>Use <code>unittest.mock</code> or <code>pytest-mock</code> for mocking dependencies:</p> <pre><code>def test_with_mock(mocker):\n    # Mock an expensive function\n    mock_compute = mocker.patch(\"torchebm.utils.compute_expensive_function\")\n    mock_compute.return_value = torch.tensor(1.0)\n\n    # Test code that uses the mocked function\n    # ...\n\n    # Verify the mock was called correctly\n    mock_compute.assert_called_once_with(torch.tensor(0.0))\n</code></pre>"},{"location":"developer_guide/testing_guide/#property-based-testing","title":"Property-Based Testing","text":"<p>For complex functions, consider using property-based testing with Hypothesis:</p> <pre><code>import hypothesis.strategies as st\nfrom hypothesis import given\nimport torch\nfrom torchebm.core import GaussianEnergy\n\n@given(\n    x=st.lists(st.floats(min_value=-10, max_value=10), min_size=2, max_size=2).map(torch.tensor)\n)\ndef test_gaussian_energy_properties(x):\n    \"\"\"Test properties of Gaussian energy function.\"\"\"\n    energy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n    # Property: energy is non-negative for standard Gaussian\n    energy = energy_fn(x)\n    assert energy &gt;= 0\n\n    # Property: energy is minimized at the mean\n    energy_at_mean = energy_fn(torch.zeros(2))\n    assert energy &gt;= energy_at_mean\n</code></pre>"},{"location":"developer_guide/testing_guide/#performance-testing","title":"Performance Testing","text":"<p>For critical components, include performance tests:</p> <pre><code>import pytest\nimport time\nimport torch\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.core import GaussianEnergy\n\n@pytest.mark.performance\ndef test_langevin_performance():\n    \"\"\"Test the performance of Langevin dynamics sampling.\"\"\"\n    energy_fn = GaussianEnergy(mean=torch.zeros(10), cov=torch.eye(10))\n    sampler = LangevinDynamics(energy_function=energy_fn, step_size=0.01)\n\n    # Warm-up\n    sampler.sample_chain(dim=10, n_steps=10, n_samples=100)\n\n    # Timed test\n    start_time = time.time()\n    sampler.sample_chain(dim=10, n_steps=1000, n_samples=1000)\n    end_time = time.time()\n\n    elapsed = end_time - start_time\n    print(f\"Sampling took {elapsed:.4f} seconds\")\n\n    # Ensure performance meets requirements\n    assert elapsed &lt; 2.0  # Adjust threshold as needed\n</code></pre>"},{"location":"developer_guide/testing_guide/#test-coverage-requirements","title":"Test Coverage Requirements","text":"<p>TorchEBM aims for high test coverage:</p> <ul> <li>Core modules: 90%+ coverage</li> <li>Samplers and losses: 85%+ coverage</li> <li>Utilities: 80%+ coverage</li> <li>CUDA code: 75%+ coverage</li> </ul> <p>Use <code>pytest-cov</code> to measure coverage:</p> <pre><code>pytest --cov=torchebm --cov-report=term-missing\n</code></pre>"},{"location":"developer_guide/testing_guide/#continuous-integration","title":"Continuous Integration","text":"<p>Our CI pipeline automatically runs tests on every pull request:</p> <ul> <li>All tests must pass before a PR can be merged</li> <li>Coverage should not decrease</li> <li>Performance tests should not show significant regressions</li> </ul> <p>Local CI</p> <p>Before submitting a PR, run the full test suite locally to ensure it passes:</p> <pre><code># Install test dependencies\npip install -e \".[test]\"\n\n# Run all tests\npytest\n\n# Check coverage\npytest --cov=torchebm\n</code></pre>"},{"location":"developer_guide/testing_guide/#resources","title":"Resources","text":"<ul> <li> <p> pytest Documentation</p> <p>Comprehensive guide to pytest features.</p> <p> pytest Docs</p> </li> <li> <p> pytest-cov</p> <p>Coverage plugin for pytest.</p> <p> pytest-cov Docs</p> </li> <li> <p> Hypothesis</p> <p>Property-based testing for Python.</p> <p> Hypothesis Docs</p> </li> </ul>"},{"location":"examples/","title":"TorchEBM Examples","text":"<p>This section contains practical examples that demonstrate how to use TorchEBM for energy-based modeling. Each example is fully tested and focuses on a specific use case or feature.</p> <ul> <li> <p> Energy Functions</p> <p>Explore and visualize various energy functions and their properties.</p> <p> Energy Functions</p> </li> <li> <p> Langevin Dynamics</p> <p>Sample from various distributions using Langevin dynamics.</p> <p> Langevin Dynamics</p> </li> <li> <p> Hamiltonian Monte Carlo</p> <p>Learn to use Hamiltonian Monte Carlo for efficient sampling.</p> <p> Hamiltonian Monte Carlo</p> </li> <li> <p> Visualization Tools</p> <p>Advanced visualization tools for energy landscapes and sampling results.</p> <p> Visualization</p> </li> </ul>"},{"location":"examples/#example-structure","title":"Example Structure","text":"<p>Example Format</p> <p>Each example follows a consistent structure to help you understand and apply the concepts:</p> <ol> <li>Overview: Brief explanation of the example and its purpose</li> <li>Code: Complete, runnable code for the example</li> <li>Explanation: Detailed explanation of key concepts and code sections</li> <li>Extensions: Suggestions for extending or modifying the example</li> </ol>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>All examples can be run using the examples main.py script:</p> <pre><code># Clone the repository\ngit clone https://github.com/soran-ghaderi/torchebm.git\ncd torchebm\n\n# Set up your environment\npip install -e .\n\n# List all available examples\npython examples/main.py --list\n\n# Run a specific example\npython examples/main.py samplers/langevin/visualization_trajectory\n</code></pre>"},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<p>To run these examples, you'll need:</p> <ul> <li>Python 3.7+</li> <li>PyTorch 1.9+</li> <li>NumPy</li> <li>Matplotlib</li> </ul> <p>If you haven't installed TorchEBM yet, see the Installation guide.</p>"},{"location":"examples/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Most examples support GPU acceleration and will automatically use CUDA if available:</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = GaussianEnergy(mean, cov).to(device)\n</code></pre>"},{"location":"examples/#key-example-files","title":"Key Example Files","text":"<p>You'll find examples organized into the following categories:</p> Category Description Key Files <code>core/energy_functions/</code> Energy function visualization and properties <code>landscape_2d.py</code>, <code>multimodal.py</code>, <code>parametric.py</code> <code>samplers/langevin/</code> Langevin dynamics sampling examples <code>gaussian_sampling.py</code>, <code>multimodal_sampling.py</code>, <code>visualization_trajectory.py</code> <code>samplers/hmc/</code> Hamiltonian Monte Carlo examples <code>gaussian_sampling.py</code>, <code>advanced.py</code>, <code>mass_matrix.py</code> <code>visualization/</code> Advanced visualization tools <code>basic/contour_plots.py</code>, <code>advanced/trajectory_animation.py</code>, <code>advanced/parallel_chains.py</code>"},{"location":"examples/#additional-resources","title":"Additional Resources","text":"<p>For more in-depth information about the concepts demonstrated in these examples, see:</p> <ul> <li>Energy Functions Guide</li> <li>Samplers Guide</li> <li>API Reference</li> </ul>"},{"location":"examples/#whats-next","title":"What's Next?","text":"<p>After exploring these examples, you might want to:</p> <ol> <li>Check out the API Reference for detailed documentation</li> <li>Read the Developer Guide to learn about contributing</li> <li>Look at the roadmap for upcoming features </li> </ol>"},{"location":"examples/energy_visualization/","title":"Energy Landscape Visualization","text":"<p>This example demonstrates how to visualize various energy function landscapes in TorchEBM.</p> <p>Key Concepts Covered</p> <ul> <li>Visualizing energy functions in 2D</li> <li>Comparing different energy landscapes</li> <li>Using matplotlib for 3D visualization</li> </ul>"},{"location":"examples/energy_visualization/#overview","title":"Overview","text":"<p>Energy-based models rely on energy functions that define landscapes over the sample space. Visualizing these landscapes helps us understand the behavior of different energy functions and the challenges in sampling from them.</p> <p>This example shows how to plot various built-in energy functions from TorchEBM: - Rosenbrock - Ackley - Rastrigin - Double Well - Gaussian - Harmonic</p>"},{"location":"examples/energy_visualization/#code-example","title":"Code Example","text":"<pre><code>import numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom torchebm.core.energy_function import (\n    RosenbrockEnergy, AckleyEnergy, RastriginEnergy, \n    DoubleWellEnergy, GaussianEnergy, HarmonicEnergy\n)\n\ndef plot_energy_function(energy_fn, x_range, y_range, title):\n    x = np.linspace(x_range[0], x_range[1], 100)\n    y = np.linspace(y_range[0], y_range[1], 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.zeros_like(X)\n\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n            Z[i, j] = energy_fn(point).item()\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X, Y, Z, cmap='viridis')\n    ax.set_title(title)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('Energy')\n    plt.show()\n\nenergy_functions = [\n    (RosenbrockEnergy(), [-2, 2], [-1, 3], 'Rosenbrock Energy Function'),\n    (AckleyEnergy(), [-5, 5], [-5, 5], 'Ackley Energy Function'),\n    (RastriginEnergy(), [-5, 5], [-5, 5], 'Rastrigin Energy Function'),\n    (DoubleWellEnergy(), [-2, 2], [-2, 2], 'Double Well Energy Function'),\n    (GaussianEnergy(torch.tensor([0.0, 0.0]), \n                   torch.tensor([[1.0, 0.0], [0.0, 1.0]])), \n     [-3, 3], [-3, 3], 'Gaussian Energy Function'),\n    (HarmonicEnergy(), [-3, 3], [-3, 3], 'Harmonic Energy Function')\n]\n\n# Plot each energy function\nfor energy_fn, x_range, y_range, title in energy_functions:\n    plot_energy_function(energy_fn, x_range, y_range, title)\n</code></pre>"},{"location":"examples/energy_visualization/#energy-function-characteristics","title":"Energy Function Characteristics","text":""},{"location":"examples/energy_visualization/#rosenbrock-energy","title":"Rosenbrock Energy","text":"<p>The Rosenbrock function creates a long, narrow, banana-shaped valley. The global minimum is inside the valley at (1,1), but finding it is challenging because the valley is very flat and the gradient provides little guidance.</p> <p>Mathematical Definition: \\(E(x) = \\sum_{i=1}^{n-1} \\left[ a(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\)</p>"},{"location":"examples/energy_visualization/#ackley-energy","title":"Ackley Energy","text":"<p>The Ackley function is characterized by a nearly flat outer region and a large hole at the center. It has many local minima but only one global minimum at (0,0).</p> <p>Mathematical Definition: \\(E(x) = -a \\exp\\left(-b\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_i^2}\\right)\\) \\(- \\exp\\left(\\frac{1}{d}\\sum_{i=1}^{d}\\cos(c x_i)\\right) + a + \\exp(1)\\)</p>"},{"location":"examples/energy_visualization/#rastrigin-energy","title":"Rastrigin Energy","text":"<p>The Rastrigin function has many regularly distributed local minima, making it highly multimodal. Its surface looks like an \"egg carton.\" The global minimum is at (0,0).</p> <p>Mathematical Definition: \\(E(x) = An + \\sum_{i=1}^n \\left[ x_i^2 - A\\cos(2\\pi x_i) \\right]\\)</p>"},{"location":"examples/energy_visualization/#double-well-energy","title":"Double Well Energy","text":"<p>The Double Well function features two distinct minima (wells) separated by an energy barrier. It's a classic example of a bimodal distribution and is often used to test sampling algorithms' ability to traverse energy barriers.</p> <p>Mathematical Definition: \\(E(x) = a(x^2 - b)^2\\)</p>"},{"location":"examples/energy_visualization/#gaussian-and-harmonic-energy","title":"Gaussian and Harmonic Energy","text":"<p>The Gaussian energy function represents a simple quadratic energy landscape with a single minimum. It corresponds to a multivariate Gaussian distribution in the probability space.</p> <p>Mathematical Definition:</p> <p>\\(E(x) = \\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\)</p>"},{"location":"examples/energy_visualization/#harmonic-energy","title":"Harmonic Energy","text":"<p>The Harmonic energy function represents a simple quadratic potential (like a spring). It has a single global minimum at the origin and is convex everywhere. </p> <p>Mathematical Definition:</p> <p>\\(E(x) = \\frac{1}{2}\\sum_{i=1}^{d}x_i^2\\)</p>"},{"location":"examples/energy_visualization/#visualization-results","title":"Visualization Results","text":"<p>Below are the visualizations of the energy functions described above. Each visualization shows both a 3D surface plot and a 2D contour plot to help understand the landscape structure.</p>"},{"location":"examples/energy_visualization/#rosenbrock-energy_1","title":"Rosenbrock Energy","text":"<p> The Rosenbrock function creates a long, narrow, banana-shaped valley. Finding the global minimum at (1,1) is challenging because the valley is very flat and provides little gradient information.</p>"},{"location":"examples/energy_visualization/#ackley-energy_1","title":"Ackley Energy","text":"<p>The Ackley function has a nearly flat outer region and a large hole at the center. It contains many local minima but only one global minimum at (0,0).</p>"},{"location":"examples/energy_visualization/#rastrigin-energy_1","title":"Rastrigin Energy","text":"<p>The Rastrigin function's \"egg carton\" surface has many regularly distributed local minima, making it highly multimodal. The global minimum is at (0,0).</p>"},{"location":"examples/energy_visualization/#double-well-energy_1","title":"Double Well Energy","text":"<p>The Double Well energy features two distinct minima (wells) separated by an energy barrier, making it ideal for testing sampling algorithms' ability to traverse energy barriers.</p>"},{"location":"examples/energy_visualization/#gaussian-energy","title":"Gaussian Energy","text":"<p>The Gaussian energy function has a simple quadratic landscape with a single minimum, corresponding to a multivariate Gaussian distribution.</p> <p>Note</p> <p>Visualizing these energy functions helps understand why some distributions are more challenging to sample from than others. For instance, the narrow valleys in the Rosenbrock function or the many local minima in the Rastrigin function make it difficult for sampling algorithms to efficiently explore the full distribution.</p>"},{"location":"examples/energy_visualization/#extensions","title":"Extensions","text":"<p>You can extend this example to:</p> <ol> <li>Create custom energy functions by implementing the <code>EnergyFunction</code> interface</li> <li>Visualize energy functions in higher dimensions using projection techniques</li> <li>Animate sampling trajectories on top of these energy landscapes</li> </ol> <p>For more advanced energy landscape visualizations, including contour plots and comparing sampling algorithms, see the Langevin Sampler Trajectory example. </p>"},{"location":"examples/hmc/","title":"Hamiltonian Monte Carlo Sampling","text":"<p>This example demonstrates how to use the Hamiltonian Monte Carlo (HMC) sampler in TorchEBM to efficiently sample from energy landscapes.</p> <p>Key Concepts Covered</p> <ul> <li>Basic usage of Hamiltonian Monte Carlo</li> <li>High-dimensional sampling</li> <li>Working with diagnostics</li> <li>GPU acceleration</li> <li>Custom mass matrix configuration</li> </ul>"},{"location":"examples/hmc/#overview","title":"Overview","text":"<p>Hamiltonian Monte Carlo (HMC) is an advanced Markov Chain Monte Carlo (MCMC) method that uses the geometry of the energy landscape to make more efficient sampling proposals. By incorporating gradient information and simulating Hamiltonian dynamics, HMC can explore distributions more efficiently than random-walk methods, particularly in high dimensions.</p>"},{"location":"examples/hmc/#basic-example","title":"Basic Example","text":"<p>The following example shows how to sample from a 2D Gaussian distribution using HMC:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.mcmc import HamiltonianMonteCarlo\n\n# Create energy function for a 2D Gaussian\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndim = 2  # dimension of the state space\nn_steps = 100  # steps between samples\nn_samples = 1000  # num of samples\nmean = torch.tensor([1.0, -1.0], device=device)\ncov = torch.tensor([[1.0, 0.5], [0.5, 2.0]], device=device)\nenergy_fn = GaussianEnergy(mean, cov)\n\n# Initialize HMC sampler\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=5,\n    device=device\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = hmc_sampler.sample_chain(\n    x=initial_state,\n    n_steps=n_steps\n)\n\n# Plot results\nsamples = samples.cpu().numpy()\nplt.figure(figsize=(10, 5))\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\nplt.title(\"Samples from 2D Gaussian using HMC\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.show()\n</code></pre>"},{"location":"examples/hmc/#sample-distribution-visualization","title":"Sample Distribution Visualization","text":"<p>This plot shows 1000 samples from a 2D Gaussian distribution generated using Hamiltonian Monte Carlo. Note how the samples efficiently cover the target distribution's high-probability regions. The samples reflect the covariance structure with the characteristic elliptical shape around the mean [1.0, -1.0], represented by the red point and dashed ellipse.</p>"},{"location":"examples/hmc/#how-hmc-works","title":"How HMC Works","text":"<p>Hamiltonian Monte Carlo uses principles from physics to improve sampling efficiency:</p> <ol> <li>Hamiltonian System: Introduces momentum variables alongside position variables</li> <li>Leapfrog Integration: Simulates the Hamiltonian dynamics using a symplectic integrator</li> <li>Metropolis Acceptance: Ensures detailed balance by accepting/rejecting proposals</li> <li>Momentum Resampling: Periodically resamples momentum to explore different directions</li> </ol> <p>The HMC update consists of these steps:</p> <ol> <li>Sample momentum variables from a Gaussian distribution</li> <li>Simulate Hamiltonian dynamics using leapfrog integration</li> <li>Compute the acceptance probability based on the change in total energy</li> <li>Accept or reject the proposal based on the acceptance probability</li> <li>Repeat from step 1</li> </ol>"},{"location":"examples/hmc/#key-parameters","title":"Key Parameters","text":"<p>The HMC sampler has several important parameters:</p> Parameter Description <code>step_size</code> Size of each leapfrog step - controls the discretization granularity <code>n_leapfrog_steps</code> Number of leapfrog steps per proposal - controls trajectory length <code>mass</code> Optional parameter to adjust the momentum distribution (defaults to identity) <code>device</code> Device to run computations on (\"cpu\" or \"cuda\")"},{"location":"examples/hmc/#working-with-diagnostics","title":"Working with Diagnostics","text":"<p>HMC provides several diagnostic metrics to monitor sampling quality. The diagnostics tensor includes information about the sampling process:</p> <pre><code>final_samples, diagnostics = hmc_sampler.sample_chain(\n    n_samples=n_samples,\n    n_steps=n_steps,\n    dim=dim,\n    return_trajectory=True,\n    return_diagnostics=True,\n)\n\n# The diagnostics tensor has shape (n_steps, 4, n_samples, dim) and contains:\nmean_values = diagnostics[:, 0, :, :]  # Mean of samples at each step\nvariance_values = diagnostics[:, 1, :, :]  # Variance of samples at each step \nenergy_values = diagnostics[:, 2, :, :]  # Energy values of samples\nacceptance_rates = diagnostics[:, 3, :, :]  # Acceptance rates for each sample\n\n# To get the overall acceptance rate at the last step:\noverall_acceptance_rate = acceptance_rates[-1].mean()\n</code></pre> <p>Good Acceptance Rates</p> <p>For HMC, an acceptance rate between 60-90% typically indicates good performance. If the rate is too low, the step size should be decreased. If it's too high, the step size might be inefficiently small.</p>"},{"location":"examples/hmc/#performance-considerations","title":"Performance Considerations","text":"<ul> <li> <p> GPU Acceleration</p> <p>HMC can benefit significantly from GPU acceleration, especially for large sample sizes or high-dimensional problems.</p> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn, \n    step_size=0.1, \n    n_leapfrog_steps=10, \n    device=device\n)\n</code></pre> </li> <li> <p> Parameter Tuning</p> <p>The performance of HMC is sensitive to the choice of step size and number of leapfrog steps.</p> <pre><code># For higher dimensions, use more leapfrog steps\nn_leapfrog_steps = max(5, int(np.sqrt(dim)))\n\n# Step size should decrease with dimension\nstep_size = min(0.1, 0.5 * dim**(-0.25))\n</code></pre> </li> <li> <p> Warm-up Period</p> <p>Consider discarding initial samples to allow the chain to reach the target distribution.</p> <pre><code># Run 100 warm-up steps before collecting samples\nwarm_up_samples = hmc_sampler.sample_chain(\n    x=initial_state, n_steps=100\n)\n# Use the final state as the starting point\nsamples = hmc_sampler.sample_chain(\n    x=warm_up_samples, n_steps=1000\n)\n</code></pre> </li> <li> <p> Parallel Chains</p> <p>Run multiple chains in parallel to improve exploration and assess convergence.</p> <pre><code># Run 10 chains in parallel\nn_chains = 10\nsamples = hmc_sampler.sample_chain(\n    dim=dim, n_steps=1000, n_samples=n_chains\n)\n</code></pre> </li> </ul>"},{"location":"examples/hmc/#comparison-with-langevin-dynamics","title":"Comparison with Langevin Dynamics","text":"<p>HMC differs from Langevin dynamics in several important ways:</p> Feature HMC Langevin Dynamics Computational Cost Higher (multiple gradient evaluations per step) Lower (one gradient evaluation per step) Exploration Efficiency More efficient, especially in high dimensions Less efficient, more random walk behavior Parameters to Tune Step size and number of leapfrog steps Step size and noise scale Acceptance Step Uses Metropolis acceptance No explicit acceptance step Autocorrelation Typically lower Typically higher"},{"location":"examples/hmc/#using-custom-mass-matrix","title":"Using Custom Mass Matrix","text":"<p>The mass parameter in HMC affects how momentum is sampled and can improve sampling efficiency for certain distributions:</p> <pre><code># Custom mass parameter (diagonal values)\nmass = torch.tensor([0.1, 1.0], device=device)\n\n# Initialize HMC sampler with custom mass\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10,\n    mass=mass,\n    device=device\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = hmc_sampler.sample_chain(\n    x=initial_state,\n    n_steps=n_steps\n)\n</code></pre> <p>The mass parameter can be provided as either:</p> <ul> <li>A scalar value (float) that's applied to all dimensions</li> <li>A tensor of values for a diagonal mass matrix</li> </ul> <p>When using a diagonal mass matrix, each dimension can have different momentum scaling. This can be useful when dimensions have different scales or variances.</p>"},{"location":"examples/hmc/#custom-mass-matrix-results","title":"Custom Mass Matrix Results","text":"<p>This plot shows samples from the same 2D Gaussian distribution using HMC with a custom diagonal mass parameter [0.1, 1.0]. The mass parameter affects the sampling dynamics, allowing more efficient exploration of the distribution. The red point indicates the mean, and the dashed ellipse represents the 2\u03c3 confidence region.</p>"},{"location":"examples/hmc/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"<p>The following visualization compares standard HMC with HMC using a custom mass parameter:</p> <p></p> <p>This side-by-side comparison shows standard HMC (left) and HMC with a custom mass parameter (right) sampling from the same Gaussian distribution. Both methods effectively sample the distribution, but with slightly different dynamics due to the mass parameter configuration.</p>"},{"location":"examples/hmc/#conclusion","title":"Conclusion","text":"<p>Hamiltonian Monte Carlo provides efficient sampling for complex, high-dimensional distributions. It leverages gradient information to make informed proposals, resulting in faster mixing and lower autocorrelation compared to simpler methods. While it requires more computation per step than methods like Langevin dynamics, it often requires fewer steps overall to achieve the same sampling quality.</p> <p>By adjusting parameters like the mass value, step size, and number of leapfrog steps, you can optimize HMC for specific sampling tasks and distribution characteristics. </p>"},{"location":"examples/langevin_dynamics/","title":"Langevin Dynamics Sampling","text":"<p>This example demonstrates how to use the Langevin Dynamics sampler in TorchEBM to generate samples from various energy functions.</p> <p>Key Concepts Covered</p> <ul> <li>Basic usage of Langevin Dynamics</li> <li>Parallel sampling with multiple chains</li> <li>Performance considerations</li> <li>Working with diagnostics</li> </ul>"},{"location":"examples/langevin_dynamics/#overview","title":"Overview","text":"<p>Langevin dynamics is a powerful sampling method that uses gradients of the energy function to guide the exploration of the state space, combined with random noise to ensure proper exploration. It's particularly useful for sampling from complex, high-dimensional distributions.</p>"},{"location":"examples/langevin_dynamics/#basic-example","title":"Basic Example","text":"<p>The following example shows how to sample from a 2D Gaussian distribution using Langevin dynamics:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\ndef basic_example():\n    \"\"\"\n    Simple Langevin dynamics sampling from a 2D Gaussian distribution.\n    \"\"\"\n    # Create energy function for a 2D Gaussian\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dim = 2  # dimension of the state space\n    n_steps = 100  # steps between samples\n    n_samples = 1000  # num of samples\n    mean = torch.tensor([1.0, -1.0])\n    cov = torch.tensor([[1.0, 0.5], [0.5, 2.0]])\n    energy_fn = GaussianEnergy(mean, cov, device=device)\n\n    # Initialize sampler\n    sampler = LangevinDynamics(\n        energy_function=energy_fn,\n        step_size=0.01,\n        noise_scale=0.1,\n        device=device,\n    )\n\n    # Generate samples\n    initial_state = torch.zeros(n_samples, dim, device=device)\n    samples = sampler.sample_chain(\n        x=initial_state,\n        n_steps=n_steps,\n        n_samples=n_samples,\n    )\n\n    # Plot results\n    samples = samples.cpu().numpy()\n    plt.figure(figsize=(10, 5))\n    plt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\n    plt.title(\"Samples from 2D Gaussian using Langevin Dynamics\")\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.show()\n</code></pre>"},{"location":"examples/langevin_dynamics/#high-dimensional-sampling","title":"High-Dimensional Sampling","text":"<p>Langevin dynamics scales well to high-dimensional spaces. Here's an example sampling from a 10D Gaussian:</p> <pre><code>import torch\nimport time\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\ndef langevin_gaussain_sampling():\n    energy_fn = GaussianEnergy(mean=torch.zeros(10), cov=torch.eye(10))\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Initialize Langevin dynamics model\n    langevin_sampler = LangevinDynamics(\n        energy_function=energy_fn, step_size=5e-3, device=device\n    ).to(device)\n\n    # Initial state: batch of 100 samples, 10-dimensional space\n    ts = time.time()\n    # Run Langevin sampling for 500 steps\n    final_x = langevin_sampler.sample_chain(\n        dim=10, n_steps=500, n_samples=10000, return_trajectory=False\n    )\n\n    print(final_x.shape)  # Output: (10000, 10)  (final state)\n    print(\"Time taken: \", time.time() - ts)\n\n    # Sample with diagnostics and trajectory\n    n_samples = 250\n    n_steps = 500\n    dim = 10\n    final_samples, diagnostics = langevin_sampler.sample_chain(\n        n_samples=n_samples,\n        n_steps=n_steps,\n        dim=dim,\n        return_trajectory=True,\n        return_diagnostics=True,\n    )\n    print(final_samples.shape)  # Output: (250, 500, 10)\n    print(diagnostics.shape)  # (500, 3, 250, 10)\n</code></pre>"},{"location":"examples/langevin_dynamics/#working-with-diagnostics","title":"Working with Diagnostics","text":"<p>TorchEBM can return diagnostic information during sampling to monitor the sampling process:</p> <pre><code>from typing import Tuple\nimport torch\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\nfrom torchebm.core import HarmonicEnergy\n\ndef sampling_utilities_example():\n    \"\"\"\n    Example demonstrating various utility features:\n    1. Chain thinning (future updates)\n    2. Device management\n    3. Custom diagnostics\n    4. Convergence checking\n    \"\"\"\n\n    # Initialize sampler with GPU support if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    sampler = LangevinDynamics(\n        energy_function=HarmonicEnergy(), step_size=0.01, noise_scale=0.1\n    ).to(device)\n\n    # Generate samples with diagnostics\n    initial_state = torch.tensor([2.0], device=device)\n    samples, diagnostics = sampler.sample_chain(\n        x=initial_state,\n        n_steps=50,\n        n_samples=1000,\n        return_diagnostics=True,\n    )\n\n    # Custom analysis of results\n    def analyze_convergence(\n        samples: torch.Tensor, diagnostics: list\n    ) -&gt; Tuple[float, float]:\n        \"\"\"Example utility function to analyze convergence.\"\"\"\n        mean = samples.mean().item()\n        std = samples.std().item()\n        return mean, std\n\n    mean, std = analyze_convergence(samples, diagnostics)\n    print(f\"Sample Statistics - Mean: {mean:.3f}, Std: {std:.3f}\")\n</code></pre>"},{"location":"examples/langevin_dynamics/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/langevin_dynamics/#gpu-acceleration","title":"GPU Acceleration","text":"<p>TorchEBM's Langevin dynamics sampler works efficiently on both CPU and GPU. When available, using a GPU can significantly accelerate sampling, especially for high-dimensional distributions or large sample sizes.</p> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsampler = LangevinDynamics(\n    energy_function=energy_fn, \n    step_size=0.01,\n    device=device\n).to(device)\n</code></pre>"},{"location":"examples/langevin_dynamics/#parallel-sampling","title":"Parallel Sampling","text":"<p>The sampler automatically handles parallel sampling when you specify <code>n_samples &gt; 1</code>. This parallelism is particularly efficient on GPUs.</p> <pre><code># Generate 1000 samples in parallel\nsamples = sampler.sample_chain(\n    dim=10, \n    n_steps=100, \n    n_samples=1000\n)\n</code></pre>"},{"location":"examples/langevin_dynamics/#advanced-parameters","title":"Advanced Parameters","text":"<p>The Langevin dynamics sampler supports several parameters for fine-tuning:</p> Parameter Description <code>step_size</code> Controls the size of update steps <code>noise_scale</code> Controls the amount of random exploration <code>decay</code> Optional decay factor for step size during sampling <code>thinning</code> How many steps to skip between saved samples <code>return_trajectory</code> Whether to return the entire sampling trajectory <code>return_diagnostics</code> Whether to collect and return diagnostic information"},{"location":"examples/langevin_dynamics/#key-considerations","title":"Key Considerations","text":"<p>When using Langevin dynamics, keep in mind:</p> <ol> <li>Step Size: Too large can cause instability, too small can make sampling inefficient</li> <li>Burn-in Period: Initial samples may be far from the target distribution</li> <li>Energy Gradient: Ensure your energy function has a well-defined gradient</li> <li>Tuning: Optimal parameters depend on the specific energy landscape</li> </ol>"},{"location":"examples/langevin_dynamics/#conclusion","title":"Conclusion","text":"<p>Langevin dynamics is a versatile sampling approach suitable for many energy-based models. It combines the efficiency of gradient-based methods with the exploration capability of stochastic methods, making it an excellent choice for complex distributions.</p> <p>For a more visual exploration of Langevin dynamics, see the Langevin Sampler Trajectory example that visualizes sampling trajectories overlaid on energy landscapes.</p>"},{"location":"examples/langevin_dynamics/#visualization-result","title":"Visualization Result","text":"<p>This plot shows 1000 samples from a 2D Gaussian distribution generated using Langevin dynamics. The samples are concentrated around the mean [1.0, -1.0] and reflect the covariance structure with the characteristic elliptical shape.</p>"},{"location":"examples/langevin_dynamics/#working-with-double-well-energy-and-diagnostics","title":"Working with Double Well Energy and Diagnostics","text":"<p>Here's an example showing a trajectory from a Double Well energy function along with energy diagnostics:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create Double Well energy function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.1,\n    noise_scale=0.3,\n    device=device\n)\n\n# Generate samples with trajectory and diagnostics\ninitial_state = torch.tensor([-1.5], device=device).view(1, 1)\ntrajectory, diagnostics = sampler.sample_chain(\n    x=initial_state,\n    n_steps=5000,\n    return_trajectory=True,\n    return_diagnostics=True\n)\n\n# Plot trajectory and energy\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nax1.plot(trajectory[0, :, 0].cpu().numpy())\nax1.set_title(\"Single Chain Trajectory\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Position\")\nax2.plot(diagnostics[:, 2, 0, 0].cpu().numpy())\nax2.set_title(\"Energy Evolution\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Energy\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/langevin_dynamics/#diagnostics-visualization","title":"Diagnostics Visualization","text":"<p>The left plot shows a single sampling chain trajectory in a Double Well energy landscape. The trajectory moves between the two wells over time. The right plot shows the corresponding energy evolution during sampling, with drops indicating transitions between wells. </p>"},{"location":"examples/langevin_trajectory/","title":"Langevin Sampler Trajectory","text":"<p>This example demonstrates how to visualize the trajectories of Langevin dynamics samplers on multimodal energy landscapes.</p> <p>Key Concepts Covered</p> <ul> <li>Creating custom energy functions</li> <li>Visualizing energy landscapes</li> <li>Tracking and plotting sampling trajectories</li> <li>Working with multimodal distributions</li> </ul>"},{"location":"examples/langevin_trajectory/#overview","title":"Overview","text":"<p>Visualizing sampling trajectories helps understand how different sampling algorithms explore the energy landscape. This example creates a multimodal energy function and visualizes multiple sampling chains as they traverse the landscape.</p>"},{"location":"examples/langevin_trajectory/#multimodal-energy-function","title":"Multimodal Energy Function","text":"<p>First, we define a custom energy function with multiple local minima:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\nclass MultimodalEnergy:\n    \"\"\"\n    A 2D energy function with multiple local minima to demonstrate sampling behavior.\n    \"\"\"\n\n    def __init__(self, device=None, dtype=torch.float32):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.dtype = dtype\n\n        # Define centers and weights for multiple Gaussian components\n        self.centers = torch.tensor(\n            [[-1.0, -1.0], [1.0, 1.0], [-0.5, 1.0], [1.0, -0.5]],\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n        self.weights = torch.tensor(\n            [1.0, 0.8, 0.6, 0.7], device=self.device, dtype=self.dtype\n        )\n\n    def __call__(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Ensure input has correct dtype and shape\n        x = x.to(dtype=self.dtype)\n        if x.dim() == 1:\n            x = x.view(1, -1)\n\n        # Calculate distance to each center\n        dists = torch.cdist(x, self.centers)\n\n        # Calculate energy as negative log of mixture of Gaussians\n        energy = -torch.log(\n            torch.sum(self.weights * torch.exp(-0.5 * dists.pow(2)), dim=-1)\n        )\n\n        return energy\n\n    def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Ensure input has correct dtype and shape\n        x = x.to(dtype=self.dtype)\n        if x.dim() == 1:\n            x = x.view(1, -1)\n\n        # Calculate distances and Gaussian components\n        diff = x.unsqueeze(1) - self.centers\n        exp_terms = torch.exp(-0.5 * torch.sum(diff.pow(2), dim=-1))\n        weights_exp = self.weights * exp_terms\n\n        # Calculate gradient\n        normalizer = torch.sum(weights_exp, dim=-1, keepdim=True)\n        gradient = torch.sum(\n            weights_exp.unsqueeze(-1) * diff / normalizer.unsqueeze(-1), dim=1\n        )\n\n        return gradient\n\n    def to(self, device):\n        self.device = device\n        self.centers = self.centers.to(device)\n        self.weights = self.weights.to(device)\n        return self\n</code></pre>"},{"location":"examples/langevin_trajectory/#visualization-function","title":"Visualization Function","text":"<p>Next, we create a function to visualize the energy landscape and multiple Langevin dynamics sampling trajectories:</p> <pre><code>def visualize_energy_landscape_and_sampling():\n    # Set up device and dtype\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dtype = torch.float32\n\n    # Create energy function\n    energy_fn = MultimodalEnergy(device=device, dtype=dtype)\n\n    # Initialize the standard Langevin dynamics sampler from the library\n    sampler = LangevinDynamics(\n        energy_function=energy_fn, \n        step_size=0.01, \n        noise_scale=0.1, \n        device=device\n    )\n\n    # Create grid for energy landscape visualization\n    x = np.linspace(-3, 3, 100)\n    y = np.linspace(-3, 3, 100)\n    X, Y = np.meshgrid(x, y)\n\n    # Calculate energy values\n    grid_points = torch.tensor(\n        np.stack([X.flatten(), Y.flatten()], axis=1), device=device, dtype=dtype\n    )\n    energy_values = energy_fn(grid_points).cpu().numpy().reshape(X.shape)\n\n    # Set up sampling parameters\n    dim = 2  # 2D energy function\n    n_steps = 200\n\n    # Create figure\n    plt.figure(figsize=(10, 8))\n\n    # Plot energy landscape with clear contours\n    contour = plt.contour(X, Y, energy_values, levels=20, cmap=\"viridis\")\n    plt.colorbar(contour, label=\"Energy\")\n\n    # Run multiple independent chains from different starting points\n    n_chains = 5\n\n    # Define distinct colors for the chains\n    colors = plt.cm.tab10(np.linspace(0, 1, n_chains))\n\n    # Generate seeds for random starting positions to make chains start in different areas\n    seeds = [42, 123, 456, 789, 999]\n\n    for i, seed in enumerate(seeds):\n        # Set the seed for reproducibility\n        torch.manual_seed(seed)\n\n        # Run one chain using the standard API\n        trajectory = sampler.sample_chain(\n            dim=dim,              # 2D space\n            n_samples=1,          # Single chain\n            n_steps=n_steps,      # Number of steps\n            return_trajectory=True  # Return full trajectory\n        )\n\n        # Extract trajectory data\n        traj_np = trajectory.cpu().numpy().squeeze(0)  # Remove n_samples dimension\n\n        # Plot the trajectory\n        plt.plot(\n            traj_np[:, 0], \n            traj_np[:, 1], \n            'o-', \n            color=colors[i], \n            alpha=0.6, \n            markersize=3,\n            label=f\"Chain {i+1}\"\n        )\n\n        # Mark the start and end points\n        plt.plot(traj_np[0, 0], traj_np[0, 1], 'o', color=colors[i], markersize=8)\n        plt.plot(traj_np[-1, 0], traj_np[-1, 1], '*', color=colors[i], markersize=10)\n\n    # Add labels and title\n    plt.title(\"Energy Landscape and Langevin Dynamics Sampling Trajectories\")\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n</code></pre>"},{"location":"examples/langevin_trajectory/#running-the-example","title":"Running the Example","text":"<p>To run the example, simply execute:</p> <pre><code>if __name__ == \"__main__\":\n    print(\"Running energy landscape visualization...\")\n    visualize_energy_landscape_and_sampling()\n</code></pre>"},{"location":"examples/langevin_trajectory/#expected-results","title":"Expected Results","text":"<p>When you run this example, you'll see a contour plot of the energy landscape with multiple chains of Langevin dynamics samples overlaid. The visualization shows:</p> <ul> <li>Energy landscape: Contour lines representing the multimodal energy function</li> <li>Multiple sampling chains: Different colored trajectories starting from random initial points</li> <li>Trajectory progression: You can see how samples move from high-energy regions to low-energy regions</li> </ul> <p></p> <p>The key insights from this visualization:</p> <ol> <li>Sampling chains are attracted to areas of low energy (high probability)</li> <li>Chains can get trapped in local minima and have difficulty crossing energy barriers</li> <li>The stochastic nature of Langevin dynamics helps chains occasionally escape local minima</li> <li>Sampling efficiency depends on starting points and energy landscape geometry</li> </ol>"},{"location":"examples/langevin_trajectory/#understanding-multimodal-sampling","title":"Understanding Multimodal Sampling","text":"<p>Multimodal distributions present special challenges for sampling algorithms:</p> <p>Challenges in Multimodal Sampling</p> <ol> <li>Energy barriers: Chains must overcome barriers between modes</li> <li>Mode-hopping: Chains may have difficulty transitioning between distant modes</li> <li>Mixing time: The time required to adequately explore all modes increases</li> <li>Mode coverage: Some modes may be missed entirely during finite sampling</li> </ol> <p>The visualization helps understand these challenges by showing:</p> <ul> <li>How chains explore the space around each mode</li> <li>Whether chains successfully transition between modes</li> <li>If certain modes are favored over others</li> <li>The impact of initialization on the final sampling distribution</li> </ul>"},{"location":"examples/langevin_trajectory/#api-usage-notes","title":"API Usage Notes","text":"<p>This example demonstrates several key aspects of using the TorchEBM library:</p> <ol> <li>Creating custom energy functions: How to implement a custom energy function with gradient support</li> <li>Using the Langevin dynamics sampler: Using the standard library API</li> <li>Parallel chain sampling: Running multiple chains to explore different areas of the space</li> <li>Trajectory tracking: Enabling <code>return_trajectory=True</code> to record the full sampling path</li> </ol> <p>The standard pattern for using <code>LangevinDynamics.sample_chain</code> is:</p> <pre><code># Initialize the sampler\nsampler = LangevinDynamics(energy_function=my_energy_fn, step_size=0.01)\n\n# Run sampling with trajectory tracking\ntrajectory = sampler.sample_chain(\n    dim=2,                 # Dimension of the space\n    n_samples=10,          # Number of parallel chains\n    n_steps=100,           # Number of steps to run\n    return_trajectory=True # Return the full trajectory rather than just final points\n)\n</code></pre>"},{"location":"examples/langevin_trajectory/#extensions-and-variations","title":"Extensions and Variations","text":"<p>This example can be extended in various ways:</p> <ol> <li>Compare different samplers: Add HMC or other samplers for comparison</li> <li>Vary step size and noise: Show the impact of different parameters</li> <li>Use more complex energy functions: Create energy functions with more challenging landscapes</li> <li>Add diagnostics visualization: Plot energy evolution and other metrics alongside trajectories</li> </ol>"},{"location":"examples/langevin_trajectory/#visualization-results","title":"Visualization Results","text":"<p>When running the example, you'll see a visualization of the energy landscape with multiple sampling chains:</p> <p></p> <p>This visualization shows a multimodal energy landscape (contour lines) with five independent Langevin dynamics sampling chains (colored trajectories). Each chain starts from a random position (marked by a circle) and evolves through 200 steps (ending at the stars). The trajectories show how the chains are attracted to the energy function's local minima. Note how some chains follow the gradient to the nearest minimum, while others may explore multiple regions of the space.</p>"},{"location":"examples/core/energy_functions/","title":"Energy Function Examples","text":"<p>This section demonstrates the various energy functions available in TorchEBM and how to visualize them.</p>"},{"location":"examples/core/energy_functions/#basic-energy-landscapes","title":"Basic Energy Landscapes","text":"<p>The <code>landscape_2d.py</code> example shows how to create and visualize basic energy functions:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create the energy function\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Create a grid for visualization\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Create 3D surface plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\nsurf = ax.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.8)\n</code></pre>"},{"location":"examples/core/energy_functions/#multimodal-energy-functions","title":"Multimodal Energy Functions","text":"<p>The <code>multimodal.py</code> example demonstrates more complex energy functions with multiple local minima:</p> <pre><code>class MultimodalEnergy:\n    \"\"\"\n    A 2D energy function with multiple local minima to demonstrate sampling behavior.\n    \"\"\"\n    def __init__(self, device=None, dtype=torch.float32):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.dtype = dtype\n\n        # Define centers and weights for multiple Gaussian components\n        self.centers = torch.tensor(\n            [[-1.0, -1.0], [1.0, 1.0], [-0.5, 1.0], [1.0, -0.5]],\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n        self.weights = torch.tensor(\n            [1.0, 0.8, 0.6, 0.7], device=self.device, dtype=self.dtype\n        )\n\n    def __call__(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Calculate energy as negative log of mixture of Gaussians\n        dists = torch.cdist(x, self.centers)\n        energy = -torch.log(\n            torch.sum(self.weights * torch.exp(-0.5 * dists.pow(2)), dim=-1)\n        )\n        return energy\n</code></pre>"},{"location":"examples/core/energy_functions/#parametric-energy-functions","title":"Parametric Energy Functions","text":"<p>The <code>parametric.py</code> example shows how to create energy functions with adjustable parameters:</p> <pre><code># Create a figure with multiple subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\n# Calculate energy landscapes for different barrier heights\nbarrier_heights = [0.5, 1.0, 2.0, 4.0]\n\nfor i, barrier_height in enumerate(barrier_heights):\n    # Create energy function with the specified barrier height\n    energy_fn = DoubleWellEnergy(barrier_height=barrier_height)\n\n    # Compute energy values\n    # ...\n\n    # Create contour plot\n    contour = axes[i].contourf(X, Y, Z, 50, cmap=\"viridis\")\n    fig.colorbar(contour, ax=axes[i], label=\"Energy\")\n    axes[i].set_title(f\"Double Well Energy (Barrier Height = {barrier_height})\")\n</code></pre>"},{"location":"examples/core/energy_functions/#running-the-examples","title":"Running the Examples","text":"<p>To run these examples:</p> <pre><code># List available energy function examples\npython examples/main.py --list\n\n# Run a specific example\npython examples/main.py core/energy_functions/landscape_2d\npython examples/main.py core/energy_functions/multimodal\npython examples/main.py core/energy_functions/parametric\n</code></pre>"},{"location":"examples/core/energy_functions/#additional-resources","title":"Additional Resources","text":"<p>For more information on energy functions, see:</p> <ul> <li>API Reference: Energy Functions</li> <li>Guide: Energy Functions </li> </ul>"},{"location":"examples/samplers/hmc/","title":"Hamiltonian Monte Carlo Sampling","text":"<p>This example demonstrates how to use the Hamiltonian Monte Carlo (HMC) sampler in TorchEBM to efficiently sample from energy landscapes.</p> <p>Key Concepts Covered</p> <ul> <li>Basic usage of Hamiltonian Monte Carlo</li> <li>High-dimensional sampling</li> <li>Working with diagnostics</li> <li>GPU acceleration</li> <li>Custom mass matrix configuration</li> </ul>"},{"location":"examples/samplers/hmc/#overview","title":"Overview","text":"<p>Hamiltonian Monte Carlo (HMC) is an advanced Markov Chain Monte Carlo (MCMC) method that uses the geometry of the energy landscape to make more efficient sampling proposals. By incorporating gradient information and simulating Hamiltonian dynamics, HMC can explore distributions more efficiently than random-walk methods, particularly in high dimensions.</p>"},{"location":"examples/samplers/hmc/#basic-example","title":"Basic Example","text":"<p>The following example shows how to sample from a 2D Gaussian distribution using HMC:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.mcmc import HamiltonianMonteCarlo\n\n# Create energy function for a 2D Gaussian\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndim = 2  # dimension of the state space\nn_steps = 100  # steps between samples\nn_samples = 1000  # num of samples\nmean = torch.tensor([1.0, -1.0], device=device)\ncov = torch.tensor([[1.0, 0.5], [0.5, 2.0]], device=device)\nenergy_fn = GaussianEnergy(mean, cov)\n\n# Initialize HMC sampler\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=5,\n    device=device\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = hmc_sampler.sample_chain(\n    x=initial_state,\n    n_steps=n_steps\n)\n\n# Plot results\nsamples = samples.cpu().numpy()\nplt.figure(figsize=(10, 5))\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\nplt.title(\"Samples from 2D Gaussian using HMC\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.show()\n</code></pre>"},{"location":"examples/samplers/hmc/#sample-distribution-visualization","title":"Sample Distribution Visualization","text":"<p>This plot shows 1000 samples from a 2D Gaussian distribution generated using Hamiltonian Monte Carlo. Note how the samples efficiently cover the target distribution's high-probability regions. The samples reflect the covariance structure with the characteristic elliptical shape around the mean [1.0, -1.0], represented by the red point and dashed ellipse.</p>"},{"location":"examples/samplers/hmc/#how-hmc-works","title":"How HMC Works","text":"<p>Hamiltonian Monte Carlo uses principles from physics to improve sampling efficiency:</p> <ol> <li>Hamiltonian System: Introduces momentum variables alongside position variables</li> <li>Leapfrog Integration: Simulates the Hamiltonian dynamics using a symplectic integrator</li> <li>Metropolis Acceptance: Ensures detailed balance by accepting/rejecting proposals</li> <li>Momentum Resampling: Periodically resamples momentum to explore different directions</li> </ol> <p>The HMC update consists of these steps:</p> <ol> <li>Sample momentum variables from a Gaussian distribution</li> <li>Simulate Hamiltonian dynamics using leapfrog integration</li> <li>Compute the acceptance probability based on the change in total energy</li> <li>Accept or reject the proposal based on the acceptance probability</li> <li>Repeat from step 1</li> </ol>"},{"location":"examples/samplers/hmc/#key-parameters","title":"Key Parameters","text":"<p>The HMC sampler has several important parameters:</p> Parameter Description <code>step_size</code> Size of each leapfrog step - controls the discretization granularity <code>n_leapfrog_steps</code> Number of leapfrog steps per proposal - controls trajectory length <code>mass</code> Optional parameter to adjust the momentum distribution (defaults to identity) <code>device</code> Device to run computations on (\"cpu\" or \"cuda\")"},{"location":"examples/samplers/hmc/#working-with-diagnostics","title":"Working with Diagnostics","text":"<p>HMC provides several diagnostic metrics to monitor sampling quality. The diagnostics tensor includes information about the sampling process:</p> <pre><code>final_samples, diagnostics = hmc_sampler.sample_chain(\n    n_samples=n_samples,\n    n_steps=n_steps,\n    dim=dim,\n    return_trajectory=True,\n    return_diagnostics=True,\n)\n\n# The diagnostics tensor has shape (n_steps, 4, n_samples, dim) and contains:\nmean_values = diagnostics[:, 0, :, :]  # Mean of samples at each step\nvariance_values = diagnostics[:, 1, :, :]  # Variance of samples at each step \nenergy_values = diagnostics[:, 2, :, :]  # Energy values of samples\nacceptance_rates = diagnostics[:, 3, :, :]  # Acceptance rates for each sample\n\n# To get the overall acceptance rate at the last step:\noverall_acceptance_rate = acceptance_rates[-1].mean()\n</code></pre> <p>Good Acceptance Rates</p> <p>For HMC, an acceptance rate between 60-90% typically indicates good performance. If the rate is too low, the step size should be decreased. If it's too high, the step size might be inefficiently small.</p>"},{"location":"examples/samplers/hmc/#performance-considerations","title":"Performance Considerations","text":"<ul> <li> <p> GPU Acceleration</p> <p>HMC can benefit significantly from GPU acceleration, especially for large sample sizes or high-dimensional problems.</p> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn, \n    step_size=0.1, \n    n_leapfrog_steps=10, \n    device=device\n)\n</code></pre> </li> <li> <p> Parameter Tuning</p> <p>The performance of HMC is sensitive to the choice of step size and number of leapfrog steps.</p> <pre><code># For higher dimensions, use more leapfrog steps\nn_leapfrog_steps = max(5, int(np.sqrt(dim)))\n\n# Step size should decrease with dimension\nstep_size = min(0.1, 0.5 * dim**(-0.25))\n</code></pre> </li> <li> <p> Warm-up Period</p> <p>Consider discarding initial samples to allow the chain to reach the target distribution.</p> <pre><code># Run 100 warm-up steps before collecting samples\nwarm_up_samples = hmc_sampler.sample_chain(\n    x=initial_state, n_steps=100\n)\n# Use the final state as the starting point\nsamples = hmc_sampler.sample_chain(\n    x=warm_up_samples, n_steps=1000\n)\n</code></pre> </li> <li> <p> Parallel Chains</p> <p>Run multiple chains in parallel to improve exploration and assess convergence.</p> <pre><code># Run 10 chains in parallel\nn_chains = 10\nsamples = hmc_sampler.sample_chain(\n    dim=dim, n_steps=1000, n_samples=n_chains\n)\n</code></pre> </li> </ul>"},{"location":"examples/samplers/hmc/#comparison-with-langevin-dynamics","title":"Comparison with Langevin Dynamics","text":"<p>HMC differs from Langevin dynamics in several important ways:</p> Feature HMC Langevin Dynamics Computational Cost Higher (multiple gradient evaluations per step) Lower (one gradient evaluation per step) Exploration Efficiency More efficient, especially in high dimensions Less efficient, more random walk behavior Parameters to Tune Step size and number of leapfrog steps Step size and noise scale Acceptance Step Uses Metropolis acceptance No explicit acceptance step Autocorrelation Typically lower Typically higher"},{"location":"examples/samplers/hmc/#using-custom-mass-matrix","title":"Using Custom Mass Matrix","text":"<p>The mass parameter in HMC affects how momentum is sampled and can improve sampling efficiency for certain distributions:</p> <pre><code># Custom mass parameter (diagonal values)\nmass = torch.tensor([0.1, 1.0], device=device)\n\n# Initialize HMC sampler with custom mass\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10,\n    mass=mass,\n    device=device\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = hmc_sampler.sample_chain(\n    x=initial_state,\n    n_steps=n_steps\n)\n</code></pre> <p>The mass parameter can be provided as either:</p> <ul> <li>A scalar value (float) that's applied to all dimensions</li> <li>A tensor of values for a diagonal mass matrix</li> </ul> <p>When using a diagonal mass matrix, each dimension can have different momentum scaling. This can be useful when dimensions have different scales or variances.</p>"},{"location":"examples/samplers/hmc/#custom-mass-matrix-results","title":"Custom Mass Matrix Results","text":"<p>This plot shows samples from the same 2D Gaussian distribution using HMC with a custom diagonal mass parameter [0.1, 1.0]. The mass parameter affects the sampling dynamics, allowing more efficient exploration of the distribution. The red point indicates the mean, and the dashed ellipse represents the 2\u03c3 confidence region.</p>"},{"location":"examples/samplers/hmc/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"<p>The following visualization compares standard HMC with HMC using a custom mass parameter:</p> <p></p> <p>This side-by-side comparison shows standard HMC (left) and HMC with a custom mass parameter (right) sampling from the same Gaussian distribution. Both methods effectively sample the distribution, but with slightly different dynamics due to the mass parameter configuration.</p>"},{"location":"examples/samplers/hmc/#conclusion","title":"Conclusion","text":"<p>Hamiltonian Monte Carlo provides efficient sampling for complex, high-dimensional distributions. It leverages gradient information to make informed proposals, resulting in faster mixing and lower autocorrelation compared to simpler methods. While it requires more computation per step than methods like Langevin dynamics, it often requires fewer steps overall to achieve the same sampling quality.</p> <p>By adjusting parameters like the mass value, step size, and number of leapfrog steps, you can optimize HMC for specific sampling tasks and distribution characteristics. </p>"},{"location":"examples/samplers/langevin/","title":"Langevin Dynamics Sampling","text":"<p>This example demonstrates how to use the Langevin Dynamics sampler in TorchEBM to generate samples from various energy functions.</p> <p>Key Concepts Covered</p> <ul> <li>Basic usage of Langevin Dynamics</li> <li>Parallel sampling with multiple chains</li> <li>Performance considerations</li> <li>Working with diagnostics</li> </ul>"},{"location":"examples/samplers/langevin/#overview","title":"Overview","text":"<p>Langevin dynamics is a powerful sampling method that uses gradients of the energy function to guide the exploration of the state space, combined with random noise to ensure proper exploration. It's particularly useful for sampling from complex, high-dimensional distributions.</p>"},{"location":"examples/samplers/langevin/#basic-example","title":"Basic Example","text":"<p>The following example shows how to sample from a 2D Gaussian distribution using Langevin dynamics:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\ndef basic_example():\n    \"\"\"\n    Simple Langevin dynamics sampling from a 2D Gaussian distribution.\n    \"\"\"\n    # Create energy function for a 2D Gaussian\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dim = 2  # dimension of the state space\n    n_steps = 100  # steps between samples\n    n_samples = 1000  # num of samples\n    mean = torch.tensor([1.0, -1.0])\n    cov = torch.tensor([[1.0, 0.5], [0.5, 2.0]])\n    energy_fn = GaussianEnergy(mean, cov, device=device)\n\n    # Initialize sampler\n    sampler = LangevinDynamics(\n        energy_function=energy_fn,\n        step_size=0.01,\n        noise_scale=0.1,\n        device=device,\n    )\n\n    # Generate samples\n    initial_state = torch.zeros(n_samples, dim, device=device)\n    samples = sampler.sample_chain(\n        x=initial_state,\n        n_steps=n_steps,\n        n_samples=n_samples,\n    )\n\n    # Plot results\n    samples = samples.cpu().numpy()\n    plt.figure(figsize=(10, 5))\n    plt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\n    plt.title(\"Samples from 2D Gaussian using Langevin Dynamics\")\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.show()\n</code></pre>"},{"location":"examples/samplers/langevin/#high-dimensional-sampling","title":"High-Dimensional Sampling","text":"<p>Langevin dynamics scales well to high-dimensional spaces. Here's an example sampling from a 10D Gaussian:</p> <pre><code>import torch\nimport time\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\ndef langevin_gaussain_sampling():\n    energy_fn = GaussianEnergy(mean=torch.zeros(10), cov=torch.eye(10))\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Initialize Langevin dynamics model\n    langevin_sampler = LangevinDynamics(\n        energy_function=energy_fn, step_size=5e-3, device=device\n    ).to(device)\n\n    # Initial state: batch of 100 samples, 10-dimensional space\n    ts = time.time()\n    # Run Langevin sampling for 500 steps\n    final_x = langevin_sampler.sample_chain(\n        dim=10, n_steps=500, n_samples=10000, return_trajectory=False\n    )\n\n    print(final_x.shape)  # Output: (10000, 10)  (final state)\n    print(\"Time taken: \", time.time() - ts)\n\n    # Sample with diagnostics and trajectory\n    n_samples = 250\n    n_steps = 500\n    dim = 10\n    final_samples, diagnostics = langevin_sampler.sample_chain(\n        n_samples=n_samples,\n        n_steps=n_steps,\n        dim=dim,\n        return_trajectory=True,\n        return_diagnostics=True,\n    )\n    print(final_samples.shape)  # Output: (250, 500, 10)\n    print(diagnostics.shape)  # (500, 3, 250, 10)\n</code></pre>"},{"location":"examples/samplers/langevin/#working-with-diagnostics","title":"Working with Diagnostics","text":"<p>TorchEBM can return diagnostic information during sampling to monitor the sampling process:</p> <pre><code>from typing import Tuple\nimport torch\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\nfrom torchebm.core import HarmonicEnergy\n\ndef sampling_utilities_example():\n    \"\"\"\n    Example demonstrating various utility features:\n    1. Chain thinning (future updates)\n    2. Device management\n    3. Custom diagnostics\n    4. Convergence checking\n    \"\"\"\n\n    # Initialize sampler with GPU support if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    sampler = LangevinDynamics(\n        energy_function=HarmonicEnergy(), step_size=0.01, noise_scale=0.1\n    ).to(device)\n\n    # Generate samples with diagnostics\n    initial_state = torch.tensor([2.0], device=device)\n    samples, diagnostics = sampler.sample_chain(\n        x=initial_state,\n        n_steps=50,\n        n_samples=1000,\n        return_diagnostics=True,\n    )\n\n    # Custom analysis of results\n    def analyze_convergence(\n        samples: torch.Tensor, diagnostics: list\n    ) -&gt; Tuple[float, float]:\n        \"\"\"Example utility function to analyze convergence.\"\"\"\n        mean = samples.mean().item()\n        std = samples.std().item()\n        return mean, std\n\n    mean, std = analyze_convergence(samples, diagnostics)\n    print(f\"Sample Statistics - Mean: {mean:.3f}, Std: {std:.3f}\")\n</code></pre>"},{"location":"examples/samplers/langevin/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/samplers/langevin/#gpu-acceleration","title":"GPU Acceleration","text":"<p>TorchEBM's Langevin dynamics sampler works efficiently on both CPU and GPU. When available, using a GPU can significantly accelerate sampling, especially for high-dimensional distributions or large sample sizes.</p> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsampler = LangevinDynamics(\n    energy_function=energy_fn, \n    step_size=0.01,\n    device=device\n).to(device)\n</code></pre>"},{"location":"examples/samplers/langevin/#parallel-sampling","title":"Parallel Sampling","text":"<p>The sampler automatically handles parallel sampling when you specify <code>n_samples &gt; 1</code>. This parallelism is particularly efficient on GPUs.</p> <pre><code># Generate 1000 samples in parallel\nsamples = sampler.sample_chain(\n    dim=10, \n    n_steps=100, \n    n_samples=1000\n)\n</code></pre>"},{"location":"examples/samplers/langevin/#advanced-parameters","title":"Advanced Parameters","text":"<p>The Langevin dynamics sampler supports several parameters for fine-tuning:</p> Parameter Description <code>step_size</code> Controls the size of update steps <code>noise_scale</code> Controls the amount of random exploration <code>decay</code> Optional decay factor for step size during sampling <code>thinning</code> How many steps to skip between saved samples <code>return_trajectory</code> Whether to return the entire sampling trajectory <code>return_diagnostics</code> Whether to collect and return diagnostic information"},{"location":"examples/samplers/langevin/#key-considerations","title":"Key Considerations","text":"<p>When using Langevin dynamics, keep in mind:</p> <ol> <li>Step Size: Too large can cause instability, too small can make sampling inefficient</li> <li>Burn-in Period: Initial samples may be far from the target distribution</li> <li>Energy Gradient: Ensure your energy function has a well-defined gradient</li> <li>Tuning: Optimal parameters depend on the specific energy landscape</li> </ol>"},{"location":"examples/samplers/langevin/#conclusion","title":"Conclusion","text":"<p>Langevin dynamics is a versatile sampling approach suitable for many energy-based models. It combines the efficiency of gradient-based methods with the exploration capability of stochastic methods, making it an excellent choice for complex distributions.</p> <p>For a more visual exploration of Langevin dynamics, see the Langevin Sampler Trajectory example that visualizes sampling trajectories overlaid on energy landscapes.</p>"},{"location":"examples/samplers/langevin/#visualization-result","title":"Visualization Result","text":"<p>This plot shows 1000 samples from a 2D Gaussian distribution generated using Langevin dynamics. The samples are concentrated around the mean [1.0, -1.0] and reflect the covariance structure with the characteristic elliptical shape.</p>"},{"location":"examples/samplers/langevin/#working-with-double-well-energy-and-diagnostics","title":"Working with Double Well Energy and Diagnostics","text":"<p>Here's an example showing a trajectory from a Double Well energy function along with energy diagnostics:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create Double Well energy function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.1,\n    noise_scale=0.3,\n    device=device\n)\n\n# Generate samples with trajectory and diagnostics\ninitial_state = torch.tensor([-1.5], device=device).view(1, 1)\ntrajectory, diagnostics = sampler.sample_chain(\n    x=initial_state,\n    n_steps=5000,\n    return_trajectory=True,\n    return_diagnostics=True\n)\n\n# Plot trajectory and energy\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nax1.plot(trajectory[0, :, 0].cpu().numpy())\nax1.set_title(\"Single Chain Trajectory\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Position\")\nax2.plot(diagnostics[:, 2, 0, 0].cpu().numpy())\nax2.set_title(\"Energy Evolution\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Energy\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/samplers/langevin/#diagnostics-visualization","title":"Diagnostics Visualization","text":"<p>The left plot shows a single sampling chain trajectory in a Double Well energy landscape. The trajectory moves between the two wells over time. The right plot shows the corresponding energy evolution during sampling, with drops indicating transitions between wells. </p>"},{"location":"examples/visualization/","title":"Visualization Tools","text":"<p>This section demonstrates various visualization tools and techniques available in TorchEBM for visualizing energy functions and sampling processes.</p>"},{"location":"examples/visualization/#basic-visualizations","title":"Basic Visualizations","text":""},{"location":"examples/visualization/#contour-plots","title":"Contour Plots","text":"<p>The <code>contour_plots.py</code> example demonstrates basic contour plots for energy functions:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create the energy function\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Create a grid for visualization\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Create contour plot\nplt.figure(figsize=(10, 8))\ncontour = plt.contourf(X, Y, Z, 50, cmap=\"viridis\")\nplt.colorbar(label=\"Energy\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Double Well Energy Landscape\")\n</code></pre>"},{"location":"examples/visualization/#distribution-comparison","title":"Distribution Comparison","text":"<p>The <code>distribution_comparison.py</code> example compares sampled distributions to their ground truth:</p> <pre><code># Create figure with multiple plots\nfig = plt.figure(figsize=(15, 5))\n\n# Ground truth contour\nax1 = fig.add_subplot(131)\ncontour = ax1.contourf(X, Y, Z, 50, cmap=\"Blues\")\nfig.colorbar(contour, ax=ax1, label=\"Density\")\nax1.set_title(\"Ground Truth Density\")\n\n# Sample density (using kernel density estimation)\nax2 = fig.add_subplot(132)\nh = ax2.hist2d(samples_np[:, 0], samples_np[:, 1], bins=50, cmap=\"Reds\", density=True)\nfig.colorbar(h[3], ax=ax2, label=\"Density\")\nax2.set_title(\"Sampled Distribution\")\n\n# Scatter plot of samples\nax3 = fig.add_subplot(133)\nax3.scatter(samples_np[:, 0], samples_np[:, 1], alpha=0.5, s=3)\nax3.set_title(\"Sample Points\")\n</code></pre>"},{"location":"examples/visualization/#advanced-visualizations","title":"Advanced Visualizations","text":""},{"location":"examples/visualization/#trajectory-animation","title":"Trajectory Animation","text":"<p>The <code>trajectory_animation.py</code> example visualizes sampling trajectories on energy landscapes:</p> <pre><code># Extract trajectory coordinates\ntraj_x = trajectory[0, :, 0].numpy()\ntraj_y = trajectory[0, :, 1].numpy()\n\n# Plot trajectory with colormap based on step number\nplt.figure(figsize=(10, 8))\ncontour = plt.contourf(X, Y, Z, 50, cmap=\"viridis\", alpha=0.7)  # Energy landscape\npoints = plt.scatter(\n    traj_x, traj_y, c=np.arange(len(traj_x)), cmap=\"plasma\", s=5, alpha=0.7\n)\nplt.colorbar(points, label=\"Sampling Step\")\n\n# Plot arrows to show direction of trajectory\nstep = 50  # Plot an arrow every 50 steps\nplt.quiver(\n    traj_x[:-1:step],\n    traj_y[:-1:step],\n    traj_x[1::step] - traj_x[:-1:step],\n    traj_y[1::step] - traj_y[:-1:step],\n    scale_units=\"xy\",\n    angles=\"xy\",\n    scale=1,\n    color=\"red\",\n    alpha=0.7,\n)\n</code></pre>"},{"location":"examples/visualization/#parallel-chains","title":"Parallel Chains","text":"<p>The <code>parallel_chains.py</code> example visualizes multiple sampling chains:</p> <pre><code># Plot contour\nplt.figure(figsize=(12, 10))\ncontour = plt.contourf(X, Y, Z, 50, cmap=\"viridis\", alpha=0.7)\nplt.colorbar(label=\"Energy\")\n\n# Plot each trajectory with a different color\ncolors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\"]\nfor i in range(num_chains):\n    traj_x = trajectories[i, :, 0].numpy()\n    traj_y = trajectories[i, :, 1].numpy()\n\n    plt.plot(traj_x, traj_y, alpha=0.7, linewidth=1, c=colors[i], label=f\"Chain {i+1}\")\n\n    # Mark start and end points\n    plt.scatter(traj_x[0], traj_y[0], c=\"black\", s=50, marker=\"o\")\n    plt.scatter(traj_x[-1], traj_y[-1], c=colors[i], s=100, marker=\"*\")\n</code></pre>"},{"location":"examples/visualization/#energy-over-time","title":"Energy Over Time","text":"<p>The <code>energy_over_time.py</code> example tracks energy values during sampling:</p> <pre><code># Track the trajectory and energy manually\ntrajectory = torch.zeros((1, n_steps, dim))\nenergy_values = torch.zeros(n_steps)\ncurrent_sample = initial_point.clone()\n\n# Run the sampling steps and store each position and energy\nfor i in range(n_steps):\n    current_sample = sampler.langevin_step(\n        current_sample, torch.randn_like(current_sample)\n    )\n    trajectory[:, i, :] = current_sample.clone().detach()\n    energy_values[i] = energy_fn(current_sample).item()\n\n# Plot energy evolution\nplt.figure(figsize=(10, 6))\nplt.plot(energy_values.numpy())\nplt.xlabel(\"Step\")\nplt.ylabel(\"Energy\")\nplt.title(\"Energy Evolution During Sampling\")\nplt.grid(True, alpha=0.3)\n</code></pre>"},{"location":"examples/visualization/#common-visualization-utilities","title":"Common Visualization Utilities","text":"<p>The <code>utils.py</code> file provides common visualization functions that can be reused across examples:</p> <pre><code>def plot_2d_energy_landscape(\n    energy_fn, \n    title=None, \n    x_range=(-3, 3), \n    y_range=(-3, 3), \n    resolution=100,\n    device=\"cpu\",\n    save_path=None\n):\n    \"\"\"\n    Plot a 2D energy landscape as a contour plot.\n\n    Args:\n        energy_fn: The energy function to visualize\n        title: Optional title for the plot\n        x_range: Range for x-axis (min, max)\n        y_range: Range for y-axis (min, max)\n        resolution: Number of points along each axis\n        device: Device for tensor calculations\n        save_path: Optional path to save the figure\n\n    Returns:\n        The figure object\n    \"\"\"\n    # Implementation details...\n\ndef plot_sample_trajectories(\n    trajectories, \n    energy_fn=None, \n    title=None,\n    device=\"cpu\",\n    save_path=None\n):\n    \"\"\"\n    Plot sample trajectories on an energy landscape.\n\n    Args:\n        trajectories: Tensor of shape (n_samples, n_steps, dim)\n        energy_fn: Optional energy function for background\n        title: Optional title for the plot\n        device: Device for tensor calculations\n        save_path: Optional path to save the figure\n\n    Returns:\n        The figure object\n    \"\"\"\n    # Implementation details...\n</code></pre>"},{"location":"examples/visualization/#running-the-examples","title":"Running the Examples","text":"<p>To run these examples:</p> <pre><code># List available visualization examples\npython examples/main.py --list\n\n# Run basic visualization examples\npython examples/main.py visualization/basic/contour_plots\npython examples/main.py visualization/basic/distribution_comparison\n\n# Run advanced visualization examples\npython examples/main.py visualization/advanced/trajectory_animation\npython examples/main.py visualization/advanced/parallel_chains\npython examples/main.py visualization/advanced/energy_over_time\n</code></pre>"},{"location":"examples/visualization/#additional-resources","title":"Additional Resources","text":"<p>For more information on visualization tools, see:</p> <ul> <li>Matplotlib Documentation</li> </ul>"},{"location":"guides/","title":"Getting Started with TorchEBM","text":"<p>Welcome to the TorchEBM guides section! These comprehensive guides will help you understand how to use TorchEBM effectively for your energy-based modeling tasks.</p>"},{"location":"guides/#core-concepts","title":"Core Concepts","text":"<ul> <li> <p> Energy Functions</p> <p>Learn about the foundation of energy-based models and how to work with energy functions in TorchEBM.</p> <p> Energy Functions Guide</p> </li> <li> <p> Samplers</p> <p>Discover how to generate samples from energy landscapes using various sampling algorithms.</p> <p> Samplers Guide</p> </li> <li> <p> Loss Functions</p> <p>Explore different loss functions for training energy-based models.</p> <p> Loss Functions Guide</p> </li> <li> <p> Custom Neural Networks</p> <p>Learn how to create and use neural networks as energy functions.</p> <p> Custom Neural Networks Guide</p> </li> <li> <p> Training EBMs</p> <p>Master the techniques for effectively training energy-based models.</p> <p> Training Guide</p> </li> <li> <p> Visualization</p> <p>Visualize energy landscapes and sampling results to gain insights.</p> <p> Visualization Guide</p> </li> </ul>"},{"location":"guides/#quick-start","title":"Quick Start","text":"<p>If you're new to energy-based models, we recommend the following learning path:</p> <ol> <li>Start with the Introduction to understand basic concepts</li> <li>Follow the Installation guide to set up TorchEBM</li> <li>Read the Energy Functions guide to understand the fundamentals</li> <li>Explore the Samplers guide to learn how to generate samples</li> <li>Study the Training guide to learn how to train your models</li> </ol>"},{"location":"guides/#basic-example","title":"Basic Example","text":"<p>Here's a simple example to get you started with TorchEBM:</p> <pre><code>import torch\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create an energy function (2D Gaussian)\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n\n# Create a sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Generate samples\nsamples = sampler.sample_chain(\n    dim=2, n_steps=100, n_samples=1000\n)\n\n# Print sample statistics\nprint(f\"Sample mean: {samples.mean(0)}\")\nprint(f\"Sample std: {samples.std(0)}\")\n</code></pre>"},{"location":"guides/#common-patterns","title":"Common Patterns","text":"<p>Here are some common patterns you'll encounter throughout the guides:</p>"},{"location":"guides/#energy-function-definition","title":"Energy Function Definition","text":"<pre><code>from torchebm.core import EnergyFunction\nimport torch\n\nclass MyEnergyFunction(EnergyFunction):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return torch.sum(x**2, dim=-1)\n</code></pre>"},{"location":"guides/#sampler-usage","title":"Sampler Usage","text":"<pre><code>from torchebm.samplers.langevin_dynamics import LangevinDynamics\n\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\nsamples = sampler.sample_chain(\n    dim=2, n_steps=100, n_samples=1000\n)\n</code></pre>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<p>Once you're familiar with the basics, you can:</p> <ul> <li>Explore detailed Examples that demonstrate TorchEBM in action</li> <li>Check the API Reference for comprehensive documentation</li> <li>Learn how to contribute to TorchEBM in the Developer Guide</li> </ul> <p>Remember that all examples in these guides are tested with the latest version of TorchEBM, and you can run them in your own environment to gain hands-on experience. </p>"},{"location":"guides/custom_neural_networks/","title":"Custom Neural Network Energy Functions","text":"<p>Energy-based models (EBMs) are extremely flexible, and one of their key advantages is that the energy function can be parameterized using neural networks. This guide explains how to create and use neural network-based energy functions in TorchEBM.</p>"},{"location":"guides/custom_neural_networks/#overview","title":"Overview","text":"<p>Neural networks provide a powerful way to represent complex energy landscapes that can't be easily defined analytically. By using neural networks as energy functions:</p> <ul> <li>You can capture complex, high-dimensional distributions</li> <li>The energy function can be learned from data</li> <li>You gain the expressivity of modern deep learning architectures</li> </ul>"},{"location":"guides/custom_neural_networks/#basic-neural-network-energy-function","title":"Basic Neural Network Energy Function","text":"<p>To create a neural network-based energy function in TorchEBM, you need to subclass the <code>EnergyFunction</code> base class and implement the <code>forward</code> method:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import EnergyFunction\n\nclass NeuralNetEnergyFunction(EnergyFunction):\n    def __init__(self, input_dim, hidden_dim=128):\n        super().__init__()\n\n        # Define neural network architecture\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Softplus(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Softplus(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Softplus(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x has shape (batch_size, input_dim)\n        # Output should have shape (batch_size,)\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"guides/custom_neural_networks/#design-considerations","title":"Design Considerations","text":"<p>When designing neural network energy functions, consider the following:</p>"},{"location":"guides/custom_neural_networks/#network-architecture","title":"Network Architecture","text":"<p>The choice of architecture depends on the data type and complexity:</p> <ul> <li>MLPs: Good for generic, low-dimensional data</li> <li>CNNs: Effective for images and data with spatial structure</li> <li>Transformers: Useful for sequential data or when attention mechanisms are beneficial</li> <li>Graph Neural Networks: For data with graph structure</li> </ul>"},{"location":"guides/custom_neural_networks/#output-requirements","title":"Output Requirements","text":"<p>Remember the following key points:</p> <ol> <li>The energy function should output a scalar value for each sample in the batch</li> <li>Lower energy values should correspond to higher probability density</li> <li>The neural network must be differentiable for gradient-based sampling methods to work</li> </ol>"},{"location":"guides/custom_neural_networks/#scale-and-normalization","title":"Scale and Normalization","text":"<p>Energy values should be properly scaled to avoid numerical issues:</p> <ul> <li>Very large energy values can cause instability in sampling</li> <li>Energy functions that grow too quickly may cause sampling algorithms to fail</li> </ul>"},{"location":"guides/custom_neural_networks/#example-mlp-energy-function-for-2d-data","title":"Example: MLP Energy Function for 2D Data","text":"<p>Here's a complete example with a simple MLP energy function:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import EnergyFunction\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass MLPEnergyFunction(EnergyFunction):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super().__init__()\n\n        # Define neural network architecture\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Initialize with small weights\n        for m in self.network.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight, gain=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # Ensure x is batched\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n\n        # Forward pass through network\n        return self.network(x).squeeze(-1)\n\n# Create the energy function\nenergy_fn = MLPEnergyFunction()\n\n# Define parameters we want the network to learn\n# Let's create a \"four peaks\" energy landscape\ndef target_energy(x, y):\n    return -2.0 * torch.exp(-0.2 * ((x - 2)**2 + (y - 2)**2)) \\\n           -3.0 * torch.exp(-0.2 * ((x + 2)**2 + (y - 2)**2)) \\\n           -1.0 * torch.exp(-0.3 * ((x - 2)**2 + (y + 2)**2)) \\\n           -4.0 * torch.exp(-0.2 * ((x + 2)**2 + (y + 2)**2)) \\\n           + 0.1 * (x**2 + y**2)\n\n# Generate training data from the target distribution\ndef generate_training_data(n_samples=10000):\n    # Sample uniformly from a grid\n    x = torch.linspace(-4, 4, 100)\n    y = torch.linspace(-4, 4, 100)\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    positions = torch.stack([X.flatten(), Y.flatten()], dim=1)\n\n    # Calculate target energy\n    energies = target_energy(positions[:, 0], positions[:, 1])\n\n    # Convert energies to probabilities (unnormalized)\n    probs = torch.exp(-energies)\n\n    # Normalize to create a distribution\n    probs = probs / probs.sum()\n\n    # Sample indices based on probability\n    indices = torch.multinomial(probs, n_samples, replacement=True)\n\n    # Return sampled positions\n    return positions[indices]\n\n# Generate training data\ntrain_data = generate_training_data(10000)\n\n# Set up optimizer\noptimizer = torch.optim.Adam(energy_fn.parameters(), lr=0.001)\n\n# Training loop\nn_epochs = 1000\nbatch_size = 128\n\nfor epoch in range(n_epochs):\n    # Generate random noise samples for contrastive divergence\n    noise_samples = torch.randn_like(train_data)\n\n    # Shuffle data\n    indices = torch.randperm(train_data.shape[0])\n\n    # Mini-batch training\n    for start_idx in range(0, train_data.shape[0], batch_size):\n        end_idx = min(start_idx + batch_size, train_data.shape[0])\n        batch_indices = indices[start_idx:end_idx]\n\n        data_batch = train_data[batch_indices]\n        noise_batch = noise_samples[batch_indices]\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Calculate energy for data and noise samples\n        data_energy = energy_fn(data_batch)\n        noise_energy = energy_fn(noise_batch)\n\n        # Contrastive divergence loss: make data energy lower, noise energy higher\n        loss = data_energy.mean() - noise_energy.mean()\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n    # Print progress\n    if (epoch + 1) % 100 == 0:\n        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}')\n\n# Visualize learned energy function\ndef visualize_energy_function(energy_fn, title=\"Learned Energy Function\"):\n    x = torch.linspace(-4, 4, 100)\n    y = torch.linspace(-4, 4, 100)\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    positions = torch.stack([X.flatten(), Y.flatten()], dim=1)\n\n    # Calculate energies\n    with torch.no_grad():\n        energies = energy_fn(positions).reshape(100, 100)\n\n    # Plot\n    plt.figure(figsize=(10, 8))\n    plt.contourf(X.numpy(), Y.numpy(), energies.numpy(), 50, cmap='viridis')\n    plt.colorbar(label='Energy')\n    plt.title(title)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.tight_layout()\n    plt.show()\n\n# Visualize the learned energy function\nvisualize_energy_function(energy_fn)\n\n# Sample from the learned energy function using Langevin dynamics\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\nsamples = sampler.sample_chain(\n    dim=2,\n    n_steps=1000,\n    n_samples=2000,\n    burn_in=200\n)\n\n# Visualize samples\nplt.figure(figsize=(10, 8))\nplt.scatter(samples[:, 0].numpy(), samples[:, 1].numpy(), s=1, alpha=0.5)\nplt.xlim(-4, 4)\nplt.ylim(-4, 4)\nplt.title('Samples from Learned Energy Function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"guides/custom_neural_networks/#example-convolutional-energy-function-for-images","title":"Example: Convolutional Energy Function for Images","text":"<p>For image data, convolutional architectures are more appropriate:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import EnergyFunction\n\nclass ConvolutionalEnergyFunction(EnergyFunction):\n    def __init__(self, channels=1, width=28, height=28):\n        super().__init__()\n\n        # Convolutional feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 14x14\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),  # 7x7\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),  # 4x4\n            nn.LeakyReLU(0.2),\n        )\n\n        # Calculate the size of the flattened features\n        feature_size = 128 * (width // 8) * (height // 8)\n\n        # Final energy output\n        self.energy_head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(feature_size, 128),\n            nn.LeakyReLU(0.2),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x):\n        # Ensure x is batched and has correct channel dimension\n        if x.ndim == 3:  # Single image with channels\n            x = x.unsqueeze(0)\n        elif x.ndim == 2:  # Single grayscale image\n            x = x.unsqueeze(0).unsqueeze(0)\n\n        # Extract features and compute energy\n        features = self.feature_extractor(x)\n        energy = self.energy_head(features).squeeze(-1)\n\n        return energy\n</code></pre>"},{"location":"guides/custom_neural_networks/#advanced-pattern-hybrid-energy-functions","title":"Advanced Pattern: Hybrid Energy Functions","text":"<p>You can combine analytical energy functions with neural networks for best of both worlds:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import EnergyFunction, GaussianEnergy\n\nclass HybridEnergyFunction(EnergyFunction):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super().__init__()\n\n        # Analytical component: Gaussian energy\n        self.analytical_component = GaussianEnergy(\n            mean=torch.zeros(input_dim),\n            cov=torch.eye(input_dim)\n        )\n\n        # Neural network component\n        self.neural_component = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Weight for combining components\n        self.alpha = nn.Parameter(torch.tensor(0.5))\n\n    def forward(self, x):\n        # Analytical energy\n        analytical_energy = self.analytical_component(x)\n\n        # Neural network energy\n        neural_energy = self.neural_component(x).squeeze(-1)\n\n        # Combine using learned weight\n        # Use sigmoid to keep alpha between 0 and 1\n        alpha = torch.sigmoid(self.alpha)\n        combined_energy = alpha * analytical_energy + (1 - alpha) * neural_energy\n\n        return combined_energy\n</code></pre>"},{"location":"guides/custom_neural_networks/#training-strategies","title":"Training Strategies","text":"<p>Training neural network energy functions requires special techniques:</p>"},{"location":"guides/custom_neural_networks/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>A common approach is contrastive divergence, which minimizes the energy of data samples while maximizing the energy of samples from the model:</p> <pre><code>def train_step_contrastive_divergence(energy_fn, optimizer, data_batch, sampler, n_sampling_steps=10):\n    # Zero gradients\n    optimizer.zero_grad()\n\n    # Data energy\n    data_energy = energy_fn(data_batch)\n\n    # Generate negative samples (model samples)\n    with torch.no_grad():\n        # Start from random noise\n        model_samples = torch.randn_like(data_batch)\n\n        # Run MCMC for a few steps\n        model_samples = sampler.sample_chain(\n            initial_points=model_samples,\n            n_steps=n_sampling_steps,\n            return_final=True\n        )\n\n    # Model energy\n    model_energy = energy_fn(model_samples)\n\n    # Loss: make data energy lower, model energy higher\n    loss = data_energy.mean() - model_energy.mean()\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"guides/custom_neural_networks/#score-matching","title":"Score Matching","text":"<p>Score matching is another approach that avoids the need for MCMC sampling:</p> <pre><code>def score_matching_loss(energy_fn, data_batch, noise_scale=0.01):\n    # Add noise to data\n    data_batch.requires_grad_(True)\n\n    # Compute energy\n    energy = energy_fn(data_batch)\n\n    # Compute gradients w.r.t. inputs\n    grad_energy = torch.autograd.grad(\n        outputs=energy.sum(),\n        inputs=data_batch,\n        create_graph=True,\n        retain_graph=True\n    )[0]\n\n    # Compute score matching loss\n    loss = 0.5 * (grad_energy ** 2).sum(dim=1).mean()\n\n    # Add regularization term\n    noise_data = data_batch + noise_scale * torch.randn_like(data_batch)\n    noise_energy = energy_fn(noise_data)\n    reg_loss = ((noise_energy - energy) ** 2).mean()\n\n    return loss + 0.1 * reg_loss\n</code></pre>"},{"location":"guides/custom_neural_networks/#tips-for-neural-network-energy-functions","title":"Tips for Neural Network Energy Functions","text":"<ol> <li>Start Simple: Begin with a simple architecture and gradually increase complexity</li> <li>Regularization: Use weight decay or spectral normalization to prevent extreme energy values</li> <li>Gradient Clipping: Apply gradient clipping during training to prevent instability</li> <li>Initialization: Careful initialization of weights can help convergence</li> <li>Monitoring: Track energy values during training to ensure they stay in a reasonable range</li> <li>Batch Normalization: Use with caution as it can affect the shape of the energy landscape</li> <li>Residual Connections: Can help with gradient flow in deeper networks</li> </ol>"},{"location":"guides/custom_neural_networks/#conclusion","title":"Conclusion","text":"<p>Neural network energy functions provide a powerful way to model complex distributions in energy-based models. By leveraging the flexibility of deep learning architectures, you can create expressive energy functions that capture intricate patterns in your data.</p> <p>Remember to carefully design your architecture, choose appropriate training methods, and monitor the behavior of your energy function during training and sampling. </p>"},{"location":"guides/energy_functions/","title":"Energy Functions","text":"<p>Energy functions are the core component of Energy-Based Models. In TorchEBM, energy functions define the probability distribution from which we sample and learn.</p>"},{"location":"guides/energy_functions/#built-in-energy-functions","title":"Built-in Energy Functions","text":"<p>TorchEBM provides several built-in energy functions for common use cases:</p>"},{"location":"guides/energy_functions/#gaussian-energy","title":"Gaussian Energy","text":"<p>The multivariate Gaussian energy function defines a normal distribution:</p> <pre><code>from torchebm.core import GaussianEnergy\nimport torch\n\n# Standard Gaussian\ngaussian = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n\n# Custom mean and covariance\ncustom_mean = torch.tensor([1.0, -1.0])\ncustom_cov = torch.tensor([[1.0, 0.5], [0.5, 2.0]])\ncustom_gaussian = GaussianEnergy(\n    mean=custom_mean,\n    cov=custom_cov\n)\n</code></pre>"},{"location":"guides/energy_functions/#double-well-energy","title":"Double Well Energy","text":"<p>The double well potential has two local minima separated by a barrier:</p> <pre><code>from torchebm.core import DoubleWellEnergy\n\n# Default barrier height = 2.0\ndouble_well = DoubleWellEnergy()\n\n# Custom barrier height\ncustom_double_well = DoubleWellEnergy(barrier_height=5.0)\n</code></pre>"},{"location":"guides/energy_functions/#rosenbrock-energy","title":"Rosenbrock Energy","text":"<p>The Rosenbrock function has a narrow, curved valley with a global minimum:</p> <pre><code>from torchebm.core import RosenbrockEnergy\n\n# Default parameters a=1.0, b=100.0\nrosenbrock = RosenbrockEnergy()\n\n# Custom parameters\ncustom_rosenbrock = RosenbrockEnergy(a=2.0, b=50.0)\n</code></pre>"},{"location":"guides/energy_functions/#rastrigin-energy","title":"Rastrigin Energy","text":"<p>The Rastrigin function has many local minima arranged in a regular pattern:</p> <pre><code>from torchebm.core import RastriginEnergy\n\nrastrigin = RastriginEnergy()\n</code></pre>"},{"location":"guides/energy_functions/#ackley-energy","title":"Ackley Energy","text":"<p>The Ackley function has many local minima with a single global minimum:</p> <pre><code>from torchebm.core import AckleyEnergy\n\nackley = AckleyEnergy()\n</code></pre>"},{"location":"guides/energy_functions/#using-energy-functions","title":"Using Energy Functions","text":"<p>Energy functions in TorchEBM implement these key methods:</p>"},{"location":"guides/energy_functions/#energy-calculation","title":"Energy Calculation","text":"<p>Calculate the energy of a batch of samples:</p> <pre><code># x shape: [batch_size, dimension]\nenergy_values = energy_function(x)  # returns [batch_size]\n</code></pre>"},{"location":"guides/energy_functions/#gradient-calculation","title":"Gradient Calculation","text":"<p>Calculate the gradient of the energy with respect to the input:</p> <pre><code># Requires grad enabled\nx.requires_grad_(True)\nenergy_values = energy_function(x)\n\n\n# Calculate gradients\ngradients = torch.autograd.grad(\n    energy_values.sum(), x, create_graph=True\n)[0]  # shape: [batch_size, dimension]\n</code></pre>"},{"location":"guides/energy_functions/#device-management","title":"Device Management","text":"<p>Energy functions can be moved between devices:</p> <pre><code># Move to GPU\nenergy_function = energy_function.to(\"cuda\")\n\n# Move to CPU\nenergy_function = energy_function.to(\"cpu\")\n</code></pre>"},{"location":"guides/energy_functions/#creating-custom-energy-functions","title":"Creating Custom Energy Functions","text":"<p>You can create custom energy functions by subclassing the <code>EnergyFunction</code> base class:</p> <pre><code>from torchebm.core import EnergyFunction\nimport torch\n\nclass MyCustomEnergy(EnergyFunction):\n    def __init__(self, param1, param2):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def forward(self, x):\n        # Implement your energy function here\n        # x shape: [batch_size, dimension]\n        # Return shape: [batch_size]\n        return torch.sum(self.param1 * x**2 + self.param2 * torch.sin(x), dim=-1)\n</code></pre>"},{"location":"guides/energy_functions/#neural-network-energy-functions","title":"Neural Network Energy Functions","text":"<p>For more complex energy functions, you can use neural networks:</p> <pre><code>import torch.nn as nn\nfrom torchebm.core import EnergyFunction\n\nclass NeuralNetworkEnergy(EnergyFunction):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x shape: [batch_size, input_dim]\n        return self.network(x).squeeze(-1)  # Return shape: [batch_size]\n</code></pre>"},{"location":"guides/energy_functions/#best-practices","title":"Best Practices","text":"<ol> <li>Numerical Stability: Avoid energy functions that can produce NaN or Inf values</li> <li>Scaling: Keep energy values within a reasonable range to avoid numerical issues</li> <li>Conditioning: Well-conditioned energy functions are easier to sample from</li> <li>Gradients: Ensure your energy function has well-behaved gradients</li> <li>Batching: Implement energy functions to efficiently handle batched inputs </li> </ol>"},{"location":"guides/loss_functions/","title":"Loss Functions","text":"<p>Training energy-based models involves estimating and minimizing the difference between the model distribution and the data distribution. TorchEBM provides various loss functions to accomplish this.</p>"},{"location":"guides/loss_functions/#contrastive-divergence-methods","title":"Contrastive Divergence Methods","text":"<p>Contrastive Divergence (CD) is a family of methods used to train energy-based models by comparing data samples with model samples.</p>"},{"location":"guides/loss_functions/#contrastive-divergence-cd-k","title":"Contrastive Divergence (CD-k)","text":"<p>CD-k uses k steps of MCMC to generate model samples:</p> <pre><code>from torchebm.losses import ContrastiveDivergenceLoss\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\nfrom torchebm.core import GaussianEnergy\nimport torch\n\n# Set up an energy model (could be a neural network)\nmodel = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n\n# Define a sampler for negative samples\nsampler = LangevinDynamics(\n    energy_function=model,\n    step_size=0.01\n)\n\n# Create CD loss\ncd_loss = ContrastiveDivergenceLoss(sampler, k=10)\n\n# During training:\ndata_samples = torch.randn(100, 2)  # Your real data\nloss = cd_loss(model, data_samples)\n</code></pre>"},{"location":"guides/loss_functions/#persistent-contrastive-divergence-pcd","title":"Persistent Contrastive Divergence (PCD)","text":"<p>PCD maintains a persistent chain of samples across training iterations:</p> <pre><code>from torchebm.losses import PersistentContrastiveDivergenceLoss\n\n# Create PCD loss\npcd_loss = PersistentContrastiveDivergenceLoss(\n    sampler,\n    n_persistent_chains=1000,\n    k=10\n)\n\n# During training:\nfor epoch in range(n_epochs):\n    data_samples = get_batch()  # Your data batch\n    loss = pcd_loss(model, data_samples)\n    # Optimizer step...\n</code></pre>"},{"location":"guides/loss_functions/#score-matching-techniques","title":"Score Matching Techniques","text":"<p>Score matching aims to match the gradient of the log-density rather than the density itself, avoiding the need to compute the partition function.</p>"},{"location":"guides/loss_functions/#standard-score-matching","title":"Standard Score Matching","text":"<pre><code>from torchebm.losses import ScoreMatchingLoss\n\n# Create score matching loss\nsm_loss = ScoreMatchingLoss()\n\n# During training:\ndata_samples = torch.randn(100, 2)  # Your real data\nloss = sm_loss(model, data_samples)\n</code></pre>"},{"location":"guides/loss_functions/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Denoising score matching adds noise to data samples and tries to predict the score of the noisy distribution:</p> <pre><code>from torchebm.losses import DenoisingScoreMatchingLoss\n\n# Create denoising score matching loss with noise scale sigma\ndsm_loss = DenoisingScoreMatchingLoss(sigma=0.1)\n\n# During training:\ndata_samples = torch.randn(100, 2)  # Your real data\nloss = dsm_loss(model, data_samples)\n</code></pre>"},{"location":"guides/loss_functions/#other-loss-functions","title":"Other Loss Functions","text":""},{"location":"guides/loss_functions/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<p>For models where the partition function can be computed:</p> <pre><code>from torchebm.losses import MaximumLikelihoodLoss\n\n# Only suitable for certain energy functions where Z is known\nmle_loss = MaximumLikelihoodLoss()\n\n# During training:\ndata_samples = torch.randn(100, 2)  # Your real data\nloss = mle_loss(model, data_samples)\n</code></pre>"},{"location":"guides/loss_functions/#noise-contrastive-estimation-nce","title":"Noise Contrastive Estimation (NCE)","text":"<p>NCE uses a noise distribution to avoid computing the partition function:</p> <pre><code>from torchebm.losses import NoiseContrastiveEstimationLoss\nimport torch.distributions as D\n\n# Define a noise distribution\nnoise_dist = D.Normal(0, 1)\n\n# Create NCE loss\nnce_loss = NoiseContrastiveEstimationLoss(\n    noise_distribution=noise_dist,\n    noise_samples_per_data=10\n)\n\n# During training:\ndata_samples = torch.randn(100, 2)  # Your real data\nloss = nce_loss(model, data_samples)\n</code></pre>"},{"location":"guides/loss_functions/#training-with-loss-functions","title":"Training with Loss Functions","text":"<p>Here's a general training loop for energy-based models:</p> <pre><code>import torch\nimport torch.optim as optim\nfrom torchebm.core import EnergyFunction\nimport torch.nn as nn\n\n# Define a neural network energy function\nclass NeuralNetEBM(EnergyFunction):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x).squeeze(-1)\n\n# Create model, loss, and optimizer\nmodel = NeuralNetEBM(input_dim=10, hidden_dim=64)\nsampler = LangevinDynamics(energy_function=model, step_size=0.01)\nloss_fn = ContrastiveDivergenceLoss(sampler, k=10)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(100):\n    # Get a batch of real data\n    real_data = get_data_batch()\n\n    # Compute loss\n    loss = loss_fn(model, real_data)\n\n    # Optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n</code></pre>"},{"location":"guides/loss_functions/#choosing-loss-functions","title":"Choosing Loss Functions","text":"<p>Different loss functions are suitable for different scenarios:</p> <ul> <li>Contrastive Divergence: Good general-purpose method, especially with complex energy landscapes</li> <li>Persistent CD: Better mixing properties than standard CD, but requires more memory</li> <li>Score Matching: Avoids sampling but can be numerically unstable in high dimensions</li> <li>Denoising Score Matching: More stable than standard score matching, good for high dimensions</li> <li>NCE: Works well with complex distributions where sampling is difficult</li> </ul>"},{"location":"guides/loss_functions/#loss-function-implementation-details","title":"Loss Function Implementation Details","text":"<p>Each loss function in TorchEBM follows a standard pattern:</p> <ol> <li>Compute energy of data samples</li> <li>Generate or obtain model samples</li> <li>Compute energy of model samples</li> <li>Calculate the loss based on these energies</li> <li>Return the loss value for backpropagation</li> </ol>"},{"location":"guides/loss_functions/#tips-for-stable-training","title":"Tips for Stable Training","text":"<ol> <li>Regularization: Add L2 regularization to prevent the energy from collapsing</li> <li>Gradient Clipping: Use gradient clipping to prevent unstable updates</li> <li>Learning Rate: Use a small learning rate, especially at the beginning</li> <li>Sampling Steps: Increase the number of sampling steps k for better negative samples</li> <li>Batch Size: Use larger batch sizes for more stable gradient estimates </li> </ol>"},{"location":"guides/parallel_sampling/","title":"Parallel Sampling","text":"<p>This guide explains how to efficiently sample from energy functions in parallel using TorchEBM.</p> <p>Under Construction</p> <p>This page is currently under development. Check back soon for the complete guide on parallel sampling techniques.</p>"},{"location":"guides/parallel_sampling/#overview","title":"Overview","text":"<p>Parallel sampling allows you to generate multiple samples simultaneously, leveraging modern hardware like GPUs for significant speedups.</p>"},{"location":"guides/parallel_sampling/#basic-usage","title":"Basic Usage","text":"<pre><code># Placeholder for code example\n</code></pre>"},{"location":"guides/parallel_sampling/#advanced-techniques","title":"Advanced Techniques","text":"<p>Coming soon! </p>"},{"location":"guides/samplers/","title":"Sampling Algorithms","text":"<p>Sampling from energy-based models is a core task in TorchEBM. This guide explains the different sampling algorithms available and how to use them effectively.</p>"},{"location":"guides/samplers/#overview-of-sampling","title":"Overview of Sampling","text":"<p>In energy-based models, we need to sample from the probability distribution defined by the energy function:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>Since the normalizing constant Z is typically intractable, we use Markov Chain Monte Carlo (MCMC) methods to generate samples without needing to compute Z.</p>"},{"location":"guides/samplers/#available-samplers","title":"Available Samplers","text":""},{"location":"guides/samplers/#langevin-dynamics","title":"Langevin Dynamics","text":"<p>Langevin Dynamics is a gradient-based MCMC method that updates samples using the energy gradient plus Gaussian noise:</p> <pre><code>from torchebm.samplers.langevin_dynamics import LangevinDynamics\nfrom torchebm.core import GaussianEnergy\nimport torch\n\n# Create an energy function\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(10),\n    cov=torch.eye(10)\n)\n\n# Create a Langevin dynamics sampler\nlangevin_sampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n\n# Generate samples\nsamples = langevin_sampler.sample_chain(\n    dim=10,\n    n_steps=1000,\n    n_samples=100,\n    return_trajectory=False\n)\n</code></pre>"},{"location":"guides/samplers/#parameters","title":"Parameters","text":"<ul> <li><code>energy_function</code>: The energy function to sample from</li> <li><code>step_size</code>: Step size for gradient updates (controls exploration vs. stability)</li> <li><code>noise_scale</code>: Scale of the noise (default is sqrt(2*step_size))</li> <li><code>device</code>: The device to perform sampling on (e.g., \"cuda\" or \"cpu\")</li> </ul>"},{"location":"guides/samplers/#hamiltonian-monte-carlo-hmc","title":"Hamiltonian Monte Carlo (HMC)","text":"<p>HMC uses Hamiltonian dynamics to make more efficient proposals, leading to better exploration of the distribution:</p> <pre><code>from torchebm.samplers.mcmc import HamiltonianMonteCarlo\nfrom torchebm.core import DoubleWellEnergy\nimport torch\n\n# Create an energy function\nenergy_fn = DoubleWellEnergy()\n\n# Create an HMC sampler\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n\n# Generate samples\nsamples = hmc_sampler.sample_chain(\n    dim=2,\n    n_steps=500,\n    n_samples=100,\n    return_trajectory=False\n)\n</code></pre>"},{"location":"guides/samplers/#parameters_1","title":"Parameters","text":"<ul> <li><code>energy_function</code>: The energy function to sample from</li> <li><code>step_size</code>: Step size for leapfrog integration</li> <li><code>n_leapfrog_steps</code>: Number of leapfrog steps per iteration</li> <li><code>device</code>: The device to perform sampling on</li> </ul>"},{"location":"guides/samplers/#advanced-sampling-usage","title":"Advanced Sampling Usage","text":""},{"location":"guides/samplers/#tracking-sampling-progress","title":"Tracking Sampling Progress","text":"<p>You can track diagnostics during sampling by setting <code>return_diagnostics=True</code>:</p> <pre><code>samples, diagnostics = sampler.sample_chain(\n    dim=10,\n    n_steps=1000,\n    n_samples=100,\n    return_trajectory=True,\n    return_diagnostics=True\n)\n\n# Diagnostics shape: [n_steps, n_diagnostics, n_samples, dim]\n# Includes: Mean, Variance, Energy, Acceptance rate (for HMC)\n</code></pre>"},{"location":"guides/samplers/#custom-initialization","title":"Custom Initialization","text":"<p>You can start the sampling chain from a specific point:</p> <pre><code># Custom initialization\nx_init = torch.randn(100, 10)  # [n_samples, dim]\nsamples = sampler.sample_chain(\n    x=x_init,\n    n_steps=1000,\n    return_trajectory=False\n)\n</code></pre>"},{"location":"guides/samplers/#burn-in-and-thinning","title":"Burn-in and Thinning","text":"<p>For better samples, you can implement burn-in and thinning:</p> <pre><code># Perform burn-in and thinning manually\nsamples, trajectory = sampler.sample_chain(\n    dim=10,\n    n_steps=2000,\n    n_samples=100,\n    return_trajectory=True\n)\n\n# Discard the first 1000 steps (burn-in)\n# Keep every 10th sample (thinning)\nthinned_samples = trajectory[:, 1000::10, :]\n</code></pre>"},{"location":"guides/samplers/#choosing-a-sampler","title":"Choosing a Sampler","text":"<ul> <li>Langevin Dynamics: Good for general-purpose sampling, especially in high dimensions</li> <li>Hamiltonian Monte Carlo: Better exploration of complex energy landscapes, but more expensive per step</li> </ul>"},{"location":"guides/samplers/#sampler-performance-tips","title":"Sampler Performance Tips","text":"<ol> <li>Adjust step size: Too large \u2192 unstable sampling; too small \u2192 slow mixing</li> <li>Use GPU acceleration: For large batches of samples or high dimensions</li> <li>Monitor acceptance rates: For HMC, aim for 60-90% acceptance rate</li> <li>Check sample quality: Correlation between successive samples should be low</li> <li>Burn-in: Discard initial samples before the chain reaches its stationary distribution</li> </ol>"},{"location":"guides/samplers/#implementing-custom-samplers","title":"Implementing Custom Samplers","text":"<p>You can create custom samplers by subclassing <code>BaseSampler</code>:</p> <pre><code>from torchebm.core import BaseSampler\nimport torch\n\nclass MyCustomSampler(BaseSampler):\n    def __init__(self, energy_function, param1, param2, device=\"cpu\"):\n        super().__init__(energy_function, device)\n        self.param1 = param1\n        self.param2 = param2\n\n    def step(self, x, step_idx=None):\n        # Implement your sampling step here\n        # x shape: [n_samples, dim]\n\n        # Example: simple random walk\n        noise = torch.randn_like(x) * self.param1\n        x_new = x + noise\n\n        # Return updated samples and any diagnostics\n        return x_new, {\"diagnostic1\": value1, \"diagnostic2\": value2}\n</code></pre>"},{"location":"guides/training/","title":"Training Energy-Based Models","text":"<p>This guide covers the fundamental techniques for training energy-based models (EBMs) using TorchEBM. We'll explore various training methods, loss functions, and optimization strategies to help you effectively train your models.</p>"},{"location":"guides/training/#overview","title":"Overview","text":"<p>Training energy-based models involves estimating the parameters of an energy function such that the corresponding probability distribution matches a target data distribution. Unlike in traditional supervised learning, this is often an unsupervised task where the goal is to learn the underlying structure of the data.</p> <p>The training process typically involves:</p> <ol> <li>Defining an energy function (parameterized by a neural network or analytical form)</li> <li>Choosing a training method and loss function</li> <li>Optimizing the energy function parameters</li> <li>Evaluating the model using sampling and visualization techniques</li> </ol>"},{"location":"guides/training/#methods-for-training-ebms","title":"Methods for Training EBMs","text":"<p>TorchEBM supports several methods for training EBMs, each with their own advantages and trade-offs:</p>"},{"location":"guides/training/#contrastive-divergence-cd","title":"Contrastive Divergence (CD)","text":"<p>Contrastive Divergence is one of the most widely used methods for training EBMs. It approximates the gradient of the log-likelihood by comparing data samples to model samples obtained through short MCMC runs.</p> <pre><code>import torch\nfrom torchebm.core import EnergyFunction, MLPEnergyFunction\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create energy function\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n# Create sampler for generating negative samples\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Optimizer\noptimizer = torch.optim.Adam(energy_fn.parameters(), lr=0.001)\n\n# Contrastive Divergence training step\ndef train_step_cd(data_batch, k_steps=10):\n    optimizer.zero_grad()\n\n    # Positive phase: compute energy of real data\n    pos_energy = energy_fn(data_batch)\n\n    # Negative phase: generate samples from current model\n    # Start from random noise\n    neg_samples = torch.randn_like(data_batch)\n\n    # Sample for k steps\n    neg_samples = sampler.sample_chain(\n        initial_points=neg_samples,\n        n_steps=k_steps,\n        return_final=True\n    )\n\n    # Compute energy of generated samples\n    neg_energy = energy_fn(neg_samples)\n\n    # Compute loss\n    # Minimize energy of real data, maximize energy of generated samples\n    loss = pos_energy.mean() - neg_energy.mean()\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    return loss.item(), pos_energy.mean().item(), neg_energy.mean().item()\n</code></pre>"},{"location":"guides/training/#persistent-contrastive-divergence-pcd","title":"Persistent Contrastive Divergence (PCD)","text":"<p>PCD improves on standard CD by maintaining a persistent chain of samples that are used across training iterations:</p> <pre><code># Initialize persistent samples\npersistent_samples = torch.randn(1000, 2)  # Shape: [n_persistent, data_dim]\n\n# PCD training step\ndef train_step_pcd(data_batch, k_steps=10):\n    global persistent_samples\n    optimizer.zero_grad()\n\n    # Positive phase: compute energy of real data\n    pos_energy = energy_fn(data_batch)\n\n    # Negative phase: continue sampling from persistent chains\n    # Start from existing persistent samples (select a random subset)\n    indices = torch.randperm(persistent_samples.shape[0])[:data_batch.shape[0]]\n    initial_samples = persistent_samples[indices].clone()\n\n    # Sample for k steps\n    neg_samples = sampler.sample_chain(\n        initial_points=initial_samples,\n        n_steps=k_steps,\n        return_final=True\n    )\n\n    # Update persistent samples\n    persistent_samples[indices] = neg_samples.detach()\n\n    # Compute energy of generated samples\n    neg_energy = energy_fn(neg_samples)\n\n    # Compute loss\n    loss = pos_energy.mean() - neg_energy.mean()\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    return loss.item(), pos_energy.mean().item(), neg_energy.mean().item()\n</code></pre>"},{"location":"guides/training/#score-matching","title":"Score Matching","text":"<p>Score Matching avoids the need for MCMC sampling altogether, focusing on matching the gradient of the log-density (score function) instead:</p> <pre><code>def train_step_score_matching(data_batch, noise_scale=0.01):\n    optimizer.zero_grad()\n\n    # Ensure data requires gradients\n    data_batch.requires_grad_(True)\n\n    # Compute energy\n    energy = energy_fn(data_batch)\n\n    # Compute gradients with respect to inputs\n    grad_energy = torch.autograd.grad(\n        outputs=energy.sum(),\n        inputs=data_batch,\n        create_graph=True\n    )[0]\n\n    # Compute score matching loss\n    loss = 0.5 * (grad_energy.pow(2)).sum(dim=1).mean()\n\n    # Add regularization (optional)\n    noise_data = data_batch + noise_scale * torch.randn_like(data_batch)\n    noise_energy = energy_fn(noise_data)\n    reg_loss = ((noise_energy - energy) ** 2).mean()\n\n    total_loss = loss + 0.1 * reg_loss\n\n    # Backpropagation\n    total_loss.backward()\n    optimizer.step()\n\n    return total_loss.item()\n</code></pre>"},{"location":"guides/training/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Denoising Score Matching is a variant that adds noise to data points and trains the model to predict the score of the noised data:</p> <pre><code>def train_step_denoising_score_matching(data_batch, sigma=0.1):\n    optimizer.zero_grad()\n\n    # Add noise to data\n    noise = sigma * torch.randn_like(data_batch)\n    noised_data = data_batch + noise\n    noised_data.requires_grad_(True)\n\n    # Compute energy of noised data\n    energy = energy_fn(noised_data)\n\n    # Compute gradients with respect to noised inputs\n    grad_energy = torch.autograd.grad(\n        outputs=energy.sum(),\n        inputs=noised_data,\n        create_graph=True\n    )[0]\n\n    # Ground truth score is -noise/sigma^2 for Gaussian noise\n    target_score = -noise / (sigma**2)\n\n    # Compute loss as MSE between predicted and target scores\n    loss = 0.5 * ((grad_energy - target_score) ** 2).sum(dim=1).mean()\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"guides/training/#full-training-loop-example","title":"Full Training Loop Example","text":"<p>Here's a complete example showing how to train an EBM using contrastive divergence:</p> <pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import EnergyFunction\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Define a neural network energy function\nclass MLPEnergyFunction(EnergyFunction):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Create a synthetic dataset (mixture of Gaussians)\ndef generate_data(n_samples=10000):\n    # Create a mixture of 5 Gaussians\n    centers = [\n        (0, 0),\n        (2, 2),\n        (-2, 2),\n        (2, -2),\n        (-2, -2)\n    ]\n    std = 0.3\n\n    # Equal number of samples per component\n    n_per_component = n_samples // len(centers)\n    data = []\n\n    for center in centers:\n        component_data = torch.randn(n_per_component, 2) * std\n        component_data[:, 0] += center[0]\n        component_data[:, 1] += center[1]\n        data.append(component_data)\n\n    # Combine all components\n    data = torch.cat(data, dim=0)\n\n    # Shuffle\n    indices = torch.randperm(data.shape[0])\n    data = data[indices]\n\n    return data\n\n# Generate dataset\ndataset = generate_data(10000)\n\n# Create energy function and sampler\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=128)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Optimizer\noptimizer = torch.optim.Adam(energy_fn.parameters(), lr=0.0001)\n\n# Training hyperparameters\nbatch_size = 128\nn_epochs = 200\nk_steps = 10  # Number of MCMC steps for negative samples\nlog_interval = 20\n\n# Persistent samples\npersistent_samples = torch.randn(1000, 2)\n\n# Training loop\nfor epoch in range(n_epochs):\n    # Shuffle dataset\n    indices = torch.randperm(dataset.shape[0])\n    dataset = dataset[indices]\n\n    total_loss = 0\n    n_batches = 0\n\n    for i in range(0, dataset.shape[0], batch_size):\n        # Get batch\n        batch_indices = indices[i:i+batch_size]\n        batch = dataset[batch_indices]\n\n        # Training step using PCD\n        optimizer.zero_grad()\n\n        # Positive phase\n        pos_energy = energy_fn(batch)\n\n        # Negative phase with persistent samples\n        pcd_indices = torch.randperm(persistent_samples.shape[0])[:batch_size]\n        neg_samples_init = persistent_samples[pcd_indices].clone()\n\n        neg_samples = sampler.sample_chain(\n            initial_points=neg_samples_init,\n            n_steps=k_steps,\n            return_final=True\n        )\n\n        # Update persistent samples\n        persistent_samples[pcd_indices] = neg_samples.detach()\n\n        # Compute energy of negative samples\n        neg_energy = energy_fn(neg_samples)\n\n        # Compute loss\n        loss = pos_energy.mean() - neg_energy.mean()\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        n_batches += 1\n\n    # Log progress\n    avg_loss = total_loss / n_batches\n\n    if (epoch + 1) % log_interval == 0:\n        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}')\n\n        # Visualize current energy function\n        with torch.no_grad():\n            # Create grid\n            x = torch.linspace(-4, 4, 100)\n            y = torch.linspace(-4, 4, 100)\n            X, Y = torch.meshgrid(x, y, indexing='ij')\n            grid_points = torch.stack([X.flatten(), Y.flatten()], dim=1)\n\n            # Compute energy values\n            energies = energy_fn(grid_points).reshape(100, 100)\n\n            # Visualize\n            plt.figure(figsize=(10, 8))\n            plt.contourf(X.numpy(), Y.numpy(), energies.numpy(), 50, cmap='viridis')\n\n            # Plot data points\n            plt.scatter(dataset[:500, 0], dataset[:500, 1], s=1, color='red', alpha=0.5)\n\n            # Plot samples from model\n            sampled = sampler.sample_chain(\n                dim=2,\n                n_samples=500,\n                n_steps=500,\n                burn_in=100\n            )\n            plt.scatter(sampled[:, 0], sampled[:, 1], s=1, color='white', alpha=0.5)\n\n            plt.title(f'Energy Landscape (Epoch {epoch+1})')\n            plt.xlim(-4, 4)\n            plt.ylim(-4, 4)\n            plt.colorbar(label='Energy')\n            plt.tight_layout()\n            plt.show()\n\n# Final evaluation\nprint(\"Training complete. Generating samples...\")\n\n# Generate final samples\nfinal_samples = sampler.sample_chain(\n    dim=2,\n    n_samples=5000,\n    n_steps=1000,\n    burn_in=200\n)\n\n# Visualize final distribution\nplt.figure(figsize=(12, 5))\n\n# Plot data distribution\nplt.subplot(1, 2, 1)\nplt.hist2d(dataset[:, 0].numpy(), dataset[:, 1].numpy(), bins=50, cmap='Blues')\nplt.title('Data Distribution')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.colorbar(label='Count')\n\n# Plot model distribution\nplt.subplot(1, 2, 2)\nplt.hist2d(final_samples[:, 0].numpy(), final_samples[:, 1].numpy(), bins=50, cmap='Reds')\nplt.title('Model Distribution')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.colorbar(label='Count')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"guides/training/#training-with-regularization","title":"Training with Regularization","text":"<p>Adding regularization can help stabilize training and prevent the energy function from assigning extremely low values to certain regions:</p> <pre><code>def train_step_with_regularization(data_batch, k_steps=10, l2_reg=0.001):\n    optimizer.zero_grad()\n\n    # Positive phase\n    pos_energy = energy_fn(data_batch)\n\n    # Negative phase\n    neg_samples = torch.randn_like(data_batch)\n    neg_samples = sampler.sample_chain(\n        initial_points=neg_samples,\n        n_steps=k_steps,\n        return_final=True\n    )\n    neg_energy = energy_fn(neg_samples)\n\n    # Contrastive divergence loss\n    cd_loss = pos_energy.mean() - neg_energy.mean()\n\n    # L2 regularization on parameters\n    l2_norm = sum(p.pow(2).sum() for p in energy_fn.parameters())\n\n    # Total loss\n    loss = cd_loss + l2_reg * l2_norm\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"guides/training/#training-with-gradient-clipping","title":"Training with Gradient Clipping","text":"<p>Gradient clipping can prevent explosive gradients during training:</p> <pre><code>def train_step_with_gradient_clipping(data_batch, k_steps=10, max_norm=1.0):\n    optimizer.zero_grad()\n\n    # Positive phase\n    pos_energy = energy_fn(data_batch)\n\n    # Negative phase\n    neg_samples = torch.randn_like(data_batch)\n    neg_samples = sampler.sample_chain(\n        initial_points=neg_samples,\n        n_steps=k_steps,\n        return_final=True\n    )\n    neg_energy = energy_fn(neg_samples)\n\n    # Contrastive divergence loss\n    loss = pos_energy.mean() - neg_energy.mean()\n\n    # Backpropagation\n    loss.backward()\n\n    # Gradient clipping\n    torch.nn.utils.clip_grad_norm_(energy_fn.parameters(), max_norm)\n\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"guides/training/#monitoring-training-progress","title":"Monitoring Training Progress","text":"<p>It's important to monitor various metrics during training to ensure the model is learning effectively:</p> <pre><code>def visualize_training_progress(energy_fn, data, sampler, epoch):\n    # Generate samples from current model\n    samples = sampler.sample_chain(\n        dim=data.shape[1],\n        n_samples=1000,\n        n_steps=500,\n        burn_in=100\n    )\n\n    # Compute energy statistics\n    with torch.no_grad():\n        data_energy = energy_fn(data[:1000]).mean().item()\n        sample_energy = energy_fn(samples).mean().item()\n\n    print(f\"Epoch {epoch}: Data Energy: {data_energy:.4f}, Sample Energy: {sample_energy:.4f}\")\n\n    # Create visualization\n    plt.figure(figsize=(15, 5))\n\n    # Plot data\n    plt.subplot(1, 3, 1)\n    plt.scatter(data[:1000, 0].numpy(), data[:1000, 1].numpy(), s=2, alpha=0.5)\n    plt.title('Data')\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n\n    # Plot samples\n    plt.subplot(1, 3, 2)\n    plt.scatter(samples[:, 0].numpy(), samples[:, 1].numpy(), s=2, alpha=0.5)\n    plt.title('Samples')\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n\n    # Plot energy landscape\n    plt.subplot(1, 3, 3)\n    x = torch.linspace(-4, 4, 100)\n    y = torch.linspace(-4, 4, 100)\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    grid_points = torch.stack([X.flatten(), Y.flatten()], dim=1)\n\n    with torch.no_grad():\n        energies = energy_fn(grid_points).reshape(100, 100)\n\n    plt.contourf(X.numpy(), Y.numpy(), energies.numpy(), 50, cmap='viridis')\n    plt.colorbar(label='Energy')\n    plt.title('Energy Landscape')\n\n    plt.tight_layout()\n    plt.savefig(f\"training_progress_epoch_{epoch}.png\")\n    plt.close()\n</code></pre>"},{"location":"guides/training/#debugging-tips","title":"Debugging Tips","text":"<p>Training EBMs can be challenging. Here are some tips for debugging:</p> <ol> <li>Start with simpler energy functions: Begin with analytical energy functions before moving to neural networks</li> <li>Monitor energy values: If energy values become extremely large or small, the model may be collapsing</li> <li>Visualize samples: Regularly visualize samples during training to check if they match the data distribution</li> <li>Adjust learning rate: Try different learning rates; EBMs often require smaller learning rates</li> <li>Increase MCMC steps: More MCMC steps for negative samples can improve training stability</li> <li>Add noise regularization: Adding small noise to data samples can help prevent overfitting</li> <li>Use gradient clipping: Clip gradients to prevent instability during training</li> <li>Try different initializations: Initial parameter values can significantly impact training dynamics</li> </ol>"},{"location":"guides/training/#training-on-different-data-types","title":"Training on Different Data Types","text":"<p>The training approach may vary depending on the type of data:</p>"},{"location":"guides/training/#images","title":"Images","text":"<p>For image data, convolutional architectures are recommended:</p> <pre><code>class ImageEBM(EnergyFunction):\n    def __init__(self, input_channels=1, image_size=28):\n        super().__init__()\n\n        self.conv_net = nn.Sequential(\n            nn.Conv2d(input_channels, 32, 3, 1, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(32, 64, 4, 2, 1),  # Downsampling\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, 2, 1),  # Downsampling\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, 2, 1),  # Downsampling\n            nn.LeakyReLU(0.2)\n        )\n\n        # Calculate final feature map size\n        feature_size = 256 * (image_size // 8) * (image_size // 8)\n\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(feature_size, 1)\n        )\n\n    def forward(self, x):\n        # Ensure proper dimensions for convolutional layers\n        if x.ndim == 3:\n            x = x.unsqueeze(1)  # Add channel dimension for grayscale\n\n        features = self.conv_net(x)\n        energy = self.fc(features).squeeze(-1)\n        return energy\n</code></pre>"},{"location":"guides/training/#sequential-data","title":"Sequential Data","text":"<p>For sequential data, recurrent or transformer architectures may be more appropriate:</p> <pre><code>class SequentialEBM(EnergyFunction):\n    def __init__(self, input_dim=1, hidden_dim=128, seq_len=50):\n        super().__init__()\n\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, input_dim]\n        lstm_out, _ = self.lstm(x)\n\n        # Use final hidden state\n        final_hidden = lstm_out[:, -1, :]\n\n        # Compute energy\n        energy = self.fc(final_hidden).squeeze(-1)\n        return energy\n</code></pre>"},{"location":"guides/training/#conclusion","title":"Conclusion","text":"<p>Training energy-based models is a challenging but rewarding process. By leveraging the techniques outlined in this guide, you can effectively train EBMs using TorchEBM for a variety of applications. Remember to monitor training progress, visualize results, and adjust your approach based on the specific characteristics of your data and modeling objectives.</p> <p>Whether you're using contrastive divergence, score matching, or other methods, the key is to ensure that your energy function accurately captures the underlying structure of your data distribution. With practice and experimentation, you can master the art of training energy-based models for complex tasks in unsupervised learning. </p>"},{"location":"guides/visualization/","title":"Visualization in TorchEBM","text":"<p>Data visualization is an essential tool for understanding, analyzing, and communicating the behavior of energy-based models. This guide covers various visualization techniques available in TorchEBM to help you gain insights into energy landscapes, sampling processes, and model performance.</p>"},{"location":"guides/visualization/#energy-landscape-visualization","title":"Energy Landscape Visualization","text":"<p>Visualizing energy landscapes is crucial for understanding the structure of the probability distribution you're working with. TorchEBM provides utilities to create both 2D and 3D visualizations of energy functions.</p>"},{"location":"guides/visualization/#basic-energy-landscape-visualization","title":"Basic Energy Landscape Visualization","text":"<p>Here's a simple example to visualize a 2D energy function:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create the energy function\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Create a grid for visualization\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Create 3D surface plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nsurf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('Energy')\nax.set_title('Double Well Energy Landscape')\nplt.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#advanced-energy-landscape-visualization","title":"Advanced Energy Landscape Visualization","text":"<p>For more advanced visualizations, you can combine contour plots with 3D surfaces:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom torchebm.core import RastriginEnergy\n\n# Create energy function\nenergy_fn = RastriginEnergy(a=10.0)\n\n# Create a grid\nresolution = 200\nx = np.linspace(-5, 5, resolution)\ny = np.linspace(-5, 5, resolution)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Apply log-scaling for better visualization\nZ_vis = np.log(Z - np.min(Z) + 1)\n\n# Create figure with two subplots\nfig = plt.figure(figsize=(16, 6))\n\n# 3D surface plot\nax1 = fig.add_subplot(121, projection='3d')\nsurf = ax1.plot_surface(X, Y, Z_vis, cmap=cm.viridis, linewidth=0, antialiased=True)\nax1.set_title('Rastrigin Energy Function (3D)')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_zlabel('Log Energy')\n\n# 2D contour plot\nax2 = fig.add_subplot(122)\ncontour = ax2.contourf(X, Y, Z_vis, 50, cmap=cm.viridis)\nax2.set_title('Rastrigin Energy Function (Contour)')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nfig.colorbar(contour, ax=ax2, label='Log Energy')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#sampling-trajectory-visualization","title":"Sampling Trajectory Visualization","text":"<p>Visualizing the trajectory of sampling algorithms can provide insights into their behavior and convergence properties.</p>"},{"location":"guides/visualization/#visualizing-langevin-dynamics-trajectories","title":"Visualizing Langevin Dynamics Trajectories","text":"<pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create energy function and sampler\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# We'll manually track the trajectory in a 2D space\ndim = 2\nn_steps = 1000\ninitial_point = torch.tensor([[-2.0, 0.0]], dtype=torch.float32)\n\n# Track the trajectory manually\ntrajectory = torch.zeros((1, n_steps, dim))\ncurrent_sample = initial_point\n\n# Run the sampling steps and store each position\nfor i in range(n_steps):\n    current_sample = sampler.langevin_step(current_sample, torch.randn_like(current_sample))\n    trajectory[:, i, :] = current_sample.clone().detach()\n\n# Prepare the background energy landscape\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Plot contour with trajectory\nplt.figure(figsize=(10, 8))\ncontour = plt.contourf(X, Y, Z, 50, cmap='viridis', alpha=0.7)\nplt.colorbar(label='Energy')\n\n# Extract trajectory coordinates\ntraj_x = trajectory[0, :, 0].numpy()\ntraj_y = trajectory[0, :, 1].numpy()\n\n# Plot trajectory with colormap based on step number\npoints = plt.scatter(\n    traj_x, traj_y, \n    c=np.arange(len(traj_x)),\n    cmap='plasma',\n    s=5,\n    alpha=0.7\n)\nplt.colorbar(points, label='Sampling Step')\n\n# Plot arrows to show direction of trajectory\nstep = 50  # Plot an arrow every 50 steps\nplt.quiver(\n    traj_x[:-1:step], traj_y[:-1:step],\n    traj_x[1::step] - traj_x[:-1:step], traj_y[1::step] - traj_y[:-1:step],\n    scale_units='xy', angles='xy', scale=1, color='red', alpha=0.7\n)\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Langevin Dynamics Sampling Trajectory on Double Well Potential')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"guides/visualization/#visualizing-multiple-chains","title":"Visualizing Multiple Chains","text":"<pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import RastriginEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create energy function and sampler\nenergy_fn = RastriginEnergy(a=10.0)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Parameters for sampling\ndim = 2\nn_steps = 1000\nnum_chains = 5\n\n# Generate random starting points\ninitial_points = torch.randn(num_chains, dim) * 3  \n\n# Track the trajectories manually\ntrajectories = torch.zeros((num_chains, n_steps, dim))\ncurrent_samples = initial_points.clone()\n\n# Run the sampling steps and store each position\nfor i in range(n_steps):\n    current_samples = sampler.langevin_step(current_samples, torch.randn_like(current_samples))\n    trajectories[:, i, :] = current_samples.clone().detach()\n\n# Create background contour\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Plot contour\nplt.figure(figsize=(12, 10))\ncontour = plt.contourf(X, Y, Z, 50, cmap='viridis', alpha=0.7)\nplt.colorbar(label='Energy')\n\n# Plot each trajectory with a different color\ncolors = ['red', 'blue', 'green', 'orange', 'purple']\nfor i in range(num_chains):\n    traj_x = trajectories[i, :, 0].numpy()\n    traj_y = trajectories[i, :, 1].numpy()\n\n    plt.plot(traj_x, traj_y, alpha=0.7, linewidth=1, c=colors[i], \n             label=f'Chain {i+1}')\n\n    # Mark start and end points\n    plt.scatter(traj_x[0], traj_y[0], c='black', s=50, marker='o')\n    plt.scatter(traj_x[-1], traj_y[-1], c=colors[i], s=100, marker='*')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Multiple Langevin Dynamics Sampling Chains on Rastrigin Potential')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"guides/visualization/#distribution-visualization","title":"Distribution Visualization","text":"<p>Visualizing the distribution of samples can help assess the quality of your sampling algorithm.</p>"},{"location":"guides/visualization/#comparing-generated-samples-with-ground-truth","title":"Comparing Generated Samples with Ground Truth","text":"<pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create a Gaussian energy function\nmean = torch.tensor([1.0, -1.0])\ncov = torch.tensor([[1.0, 0.5], [0.5, 1.0]])\nenergy_fn = GaussianEnergy(mean=mean, cov=cov)\n\n# Sample using Langevin dynamics\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Generate samples\nn_samples = 5000\ndim = 2\nn_steps = 1000\nburn_in = 200\n\n# Initialize random samples\nx = torch.randn(n_samples, dim)\n\n# Run sampling for burn-in period (discard these samples)\nfor i in range(burn_in):\n    x = sampler.langevin_step(x, torch.randn_like(x))\n\n# Run sampling for the desired number of steps\nfor i in range(n_steps):\n    x = sampler.langevin_step(x, torch.randn_like(x))\n\n# Final samples\nsamples = x\n\n# Convert to numpy for visualization\nsamples_np = samples.numpy()\nmean_np = mean.numpy()\ncov_np = cov.numpy()\n\n# Create a grid for the ground truth density\nx = np.linspace(-3, 5, 100)\ny = np.linspace(-5, 3, 100)\nX, Y = np.meshgrid(x, y)\npos = np.dstack((X, Y))\n\n# Calculate multivariate normal PDF\nrv = stats.multivariate_normal(mean_np, cov_np)\nZ = rv.pdf(pos)\n\n# Create figure with multiple plots\nfig = plt.figure(figsize=(15, 5))\n\n# Ground truth contour\nax1 = fig.add_subplot(131)\nax1.contourf(X, Y, Z, 50, cmap='Blues')\nax1.set_title('Ground Truth Density')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\n# Sample density (using kernel density estimation)\nax2 = fig.add_subplot(132)\nh = ax2.hist2d(samples_np[:, 0], samples_np[:, 1], bins=50, cmap='Reds', density=True)\nplt.colorbar(h[3], ax=ax2, label='Density')\nax2.set_title('Sampled Distribution')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\n\n# Scatter plot of samples\nax3 = fig.add_subplot(133)\nax3.scatter(samples_np[:, 0], samples_np[:, 1], alpha=0.5, s=3)\nax3.set_title('Sample Points')\nax3.set_xlabel('x')\nax3.set_ylabel('y')\nax3.set_xlim(ax2.get_xlim())\nax3.set_ylim(ax2.get_ylim())\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"guides/visualization/#energy-evolution-visualization","title":"Energy Evolution Visualization","text":"<p>Tracking how energy values evolve during sampling can help assess convergence.</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create energy function and sampler\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Parameters for sampling\ndim = 2\nn_steps = 1000\ninitial_point = torch.tensor([[-2.0, 0.0]], dtype=torch.float32)\n\n# Track the trajectory and energy manually\ntrajectory = torch.zeros((1, n_steps, dim))\nenergy_values = torch.zeros(n_steps)\ncurrent_sample = initial_point.clone()\n\n# Run the sampling steps and store each position and energy\nfor i in range(n_steps):\n    current_sample = sampler.langevin_step(current_sample, torch.randn_like(current_sample))\n    trajectory[:, i, :] = current_sample.clone().detach()\n    energy_values[i] = energy_fn(current_sample).item()\n\n# Convert to numpy for plotting\nenergy_values_np = energy_values.numpy()\n\n# Plot energy evolution\nplt.figure(figsize=(10, 6))\nplt.plot(energy_values_np)\nplt.xlabel('Step')\nplt.ylabel('Energy')\nplt.title('Energy Evolution During Langevin Dynamics Sampling')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#interactive-visualizations","title":"Interactive Visualizations","text":"<p>Since actual interactive visualizations can't be included in documentation, here's an example showing energy landscapes with different parameters:</p> <pre><code>import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create a grid for visualization\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Create barrier height values\nbarrier_heights = [0.5, 1.0, 2.0, 4.0]\n\n# Create a figure with multiple subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\n# Calculate energy landscapes for different barrier heights\nfor i, barrier_height in enumerate(barrier_heights):\n    # Create energy function with the specified barrier height\n    energy_fn = DoubleWellEnergy(barrier_height=barrier_height)\n\n    # Compute energy values\n    for j in range(X.shape[0]):\n        for k in range(X.shape[1]):\n            point = torch.tensor([X[j, k], Y[j, k]], dtype=torch.float32).unsqueeze(0)\n            Z[j, k] = energy_fn(point).item()\n\n    # Create contour plot\n    contour = axes[i].contourf(X, Y, Z, 50, cmap='viridis')\n    fig.colorbar(contour, ax=axes[i], label='Energy')\n    axes[i].set_xlabel('x')\n    axes[i].set_ylabel('y')\n    axes[i].set_title(f'Double Well Energy (Barrier Height = {barrier_height})')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#conclusion","title":"Conclusion","text":"<p>Effective visualization is key to understanding and debugging energy-based models. TorchEBM provides tools for visualizing energy landscapes, sampling trajectories, and model performance. These visualizations can help you gain insights into your models and improve their design and performance.</p> <p>Remember to adapt these examples to your specific needs - you might want to visualize higher-dimensional spaces using dimensionality reduction techniques, or create specialized plots for your particular application. </p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/research/","title":"Research","text":""},{"location":"blog/category/tutorials/","title":"Tutorials","text":""},{"location":"blog/category/examples/","title":"Examples","text":""}]}