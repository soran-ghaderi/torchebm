{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TorchEBM - Energy-Based Modeling in PyTorch","text":"PyTorch Toolkit for Generative Modeling <p>       A high-performance PyTorch library that makes Energy-Based Models accessible and efficient for researchers and practitioners alike.     </p> <p> </p> <p> TorchEBM provides components for \ud83d\udd2c sampling, \ud83e\udde0 inference, and \ud83d\udcca model training. </p> <p> Getting Started  Examples  API Reference  Development</p>"},{"location":"#what-is-torchebm","title":"What is TorchEBM?","text":"<p>Energy-Based Models (EBMs) offer a powerful and flexible framework for generative modeling by assigning an unnormalized probability (or \"energy\") to each data point. Lower energy corresponds to higher probability.</p> <p>TorchEBM simplifies working with EBMs in PyTorch. It provides a suite of tools designed for researchers and practitioners, enabling efficient implementation and exploration of:</p> <ul> <li>Defining complex energy functions: Easily create custom energy landscapes using PyTorch modules.</li> <li>Training: Loss functions and procedures suitable for EBM parameter estimation including score matching and contrastive divergence variants.</li> <li>Sampling: Algorithms to draw samples from the learned distribution \\( p(x) \\).</li> </ul>"},{"location":"#core-components","title":"Core Components","text":"<p>TorchEBM is structured around several key components:</p> <ul> <li> <p> Energy Functions</p> <p>Implement energy functions using <code>BaseEnergyFunction</code>. Includes predefined analytical functions (Gaussian, Double Well) and supports custom neural network architectures.</p> <p> Details</p> </li> <li> <p> Samplers</p> <p>MCMC samplers like Langevin Dynamics (<code>LangevinDynamics</code>), Hamiltonian Monte Carlo, and more are provided for generating samples from the energy distribution.</p> <p> Details</p> </li> <li> <p> Loss Functions</p> <p>Comprehensive loss functions for EBM training, including Contrastive Divergence, Score Matching, and Noise Contrastive Estimation.</p> <p> Details</p> </li> <li> <p> Datasets</p> <p>Helper functions to generate synthetic datasets (e.g., <code>make_gaussian_mixture</code>) useful for testing, debugging, and visualization purposes.</p> <p> Details</p> </li> <li> <p> Visualization</p> <p>Tools for visualizing energy landscapes, sampling processes, and training progression to better understand model behavior.</p> <p> Details</p> </li> <li> <p> Accelerated Computing</p> <p>CUDA implementations of key algorithms for dramatically faster sampling and training on GPU hardware.</p> <p> Details</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install the library using pip:</p> <pre><code>pip install torchebm\n</code></pre> <p>Here's a minimal example of defining an energy function and a sampler:</p> <ul> <li> <p>Create and Sample from Energy Models</p> <pre><code>import torch\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers import LangevinDynamics\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Define an (analytical) energy function -&gt; next example: trainable\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2), device=device)\n\n# Define a sampler\nsampler = LangevinDynamics(energy_function=energy_fn, step_size=0.01, device=device)\n\n# Generate samples\ninitial_points = torch.randn(500, 2, device=device)\nsamples = sampler.sample(x=initial_points, n_steps=100)\n\nprint(f\"Output batch_shape: {samples.shape}\")\n# Output batch_shape: torch.Size([500, 2])\n</code></pre> </li> </ul>"},{"location":"#training-and-visualization-example","title":"Training and Visualization Example","text":"<p>Training EBMs typically involves adjusting the energy function's parameters so that observed data points have lower energy than samples generated by the model. Contrastive Divergence (CD) is a common approach.</p> <p>Here's an example of setting up training using <code>ContrastiveDivergence</code> and <code>LangevinDynamics</code>:</p> <ul> <li> <p>Train an EBM</p> <pre><code>import torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom torchebm.losses import ContrastiveDivergence\nfrom torchebm.datasets import GaussianMixtureDataset\n\n# A trainable EBM\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n\n    def forward(self, x):\n        return self.net(x).squeeze(-1) # a scalar value\n\nenergy_fn = MLPEnergy(input_dim=2).to(device)\n\ncd_loss_fn = ContrastiveDivergence(\n    energy_function=energy_fn,\n    sampler=sampler, # from the previous example\n    k_steps=10 # MCMC steps for negative samples gen\n)\n\noptimizer = optim.Adam(energy_fn.parameters(), lr=0.001)\n\nmixture_dataset = GaussianMixtureDataset(n_samples=500, n_components=4, std=0.1, seed=123).get_data()\ndataloader = DataLoader(mixture_dataset, batch_size=32, shuffle=True)\n\n# Training Loop\nfor epoch in range(10):\n    epoch_loss = 0.0\n    for i, batch_data in enumerate(dataloader):\n        batch_data = batch_data.to(device)\n\n        optimizer.zero_grad()\n\n        loss, neg_samples = cd_loss(batch_data)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_loss = epoch_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.6f}\")\n</code></pre> </li> </ul> <p>Visualizing the learned energy landscape during training can be insightful. Below shows the evolution of an MLP-based energy function trained on a 2D Gaussian mixture:</p> <p>Training Progression (Gaussian Mixture Example)</p> Epoch 100 (Final)Epoch 30Epoch 20Epoch 10 <p></p> Learned landscape matching the target distribution. <p></p> <p>This visualization demonstrates how the model learns regions of low energy (high probability density, warmer colors) corresponding to the data distribution (white points), while assigning higher energy elsewhere. Red points are samples generated from the EBM at that training stage.</p> <p> Full Training Example</p> <p></p> <p></p> <p></p> Modes become more distinct.  This visualization demonstrates how the model learns regions of low energy (high probability density, warmer colors) corresponding to the data distribution (white points), while assigning higher energy elsewhere. Red points are samples generated from the EBM at that training stage.<p></p> <p> Full Training Example</p> <p></p> <p></p> <p></p> <p></p> <p></p> Energy landscape refinement.  This visualization demonstrates how the model learns regions of low energy (high probability density, warmer colors) corresponding to the data distribution (white points), while assigning higher energy elsewhere. Red points are samples generated from the EBM at that training stage.<p></p> <p> Full Training Example</p> <p></p> <p></p> <p></p> <p></p> <p></p> Early stage: Model starts identifying modes.  This visualization demonstrates how the model learns regions of low energy (high probability density, warmer colors) corresponding to the data distribution (white points), while assigning higher energy elsewhere. Red points are samples generated from the EBM at that training stage.<p></p> <p> Full Training Example</p> <p></p> <p></p> <p>Latest Release</p> <p>TorchEBM is currently in early development. Check our GitHub repository for the latest updates and features.</p>"},{"location":"#example-analytical-energy-landscapes","title":"Example Analytical Energy Landscapes","text":"<p>Toy Examples</p> <p>These are some TorchEBM's built-in toy analytical energy landscapes for functionality and performance testing purposes.</p> Gaussian EnergyDouble Well EnergyRastrigin EnergyRosenbrock Energy <p></p> <p></p> <p></p> Gaussian Energy<p></p> <p>\\(E(x) = \\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\) </p> <p></p> <pre><code>from torchebm.core import GaussianEnergy\nimport torch\n\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n</code></pre> <p></p> <p></p> <p></p> Double Well Energy<p></p> <p>\\(E(x) = h \\sum_{i=1}^n \\left[(x_i^2 - 1)^2\\right]\\) </p> <p></p> <pre><code>from torchebm.core import DoubleWellEnergy\n\nenergy_fn = DoubleWellEnergy(\n    barrier_height=2.0\n)\n</code></pre> <p></p> <p></p> <p></p> Rastrigin Energy<p></p> <p>\\(E(x) = an + \\sum_{i=1}^n \\left[ x_i^2 - a\\cos(2\\pi x_i) \\right]\\) </p> <p></p> <pre><code>from torchebm.core import RastriginEnergy\n\nenergy_fn = RastriginEnergy(\n    a=10.0\n)\n</code></pre> <p></p> <p></p> <p></p> Rosenbrock Energy<p></p> <p>\\(E(x) = \\sum_{i=1}^{n-1} \\left[ a(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\) </p> <p></p> <pre><code>from torchebm.core import RosenbrockEnergy\n\nenergy_fn = RosenbrockEnergy(\n    a=1.0, \n    b=100.0\n)\n</code></pre>"},{"location":"#community-contribution","title":"Community &amp; Contribution","text":"<p>TorchEBM is an open-source project developed with the research community in mind.</p> <ul> <li>Bug Reports &amp; Feature Requests: Please use the GitHub Issues.</li> <li>Contributing Code: We welcome contributions! Please see the Contributing Guidelines. Consider following the Commit Conventions.</li> <li>Show Support: If you find TorchEBM helpful for your work, consider starring the repository on GitHub! </li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>Please consider citing the TorchEBM repository if it contributes to your research:</p> <pre><code>@misc{torchebm_library_2025,\n  author       = {Ghaderi, Soran and Contributors},\n  title        = {TorchEBM: A PyTorch Library for Training Energy-Based Models},\n  year         = {2025},\n  url          = {https://github.com/soran-ghaderi/torchebm},\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>TorchEBM is available under the MIT License. See the LICENSE file for details.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-torchebm","title":"What is TorchEBM?","text":"<p>TorchEBM is a PyTorch-based library for Energy-Based Models (EBMs). It provides efficient implementations of sampling, inference, and learning algorithms for EBMs, with a focus on scalability and performance through CUDA acceleration.</p>"},{"location":"faq/#how-does-torchebm-differ-from-other-generative-modeling-libraries","title":"How does TorchEBM differ from other generative modeling libraries?","text":"<p>TorchEBM specifically focuses on energy-based models, which can model complex distributions without assuming a specific functional form. Unlike libraries for GANs or VAEs, TorchEBM emphasizes the energy function formulation and MCMC sampling techniques.</p>"},{"location":"faq/#what-can-i-use-torchebm-for","title":"What can I use TorchEBM for?","text":"<p>TorchEBM can be used for:</p> <ul> <li>Generative modeling</li> <li>Density estimation</li> <li>Unsupervised representation learning</li> <li>Outlier detection</li> <li>Exploration of complex energy landscapes</li> <li>Scientific simulation and statistical physics applications</li> </ul>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#what-are-the-system-requirements-for-torchebm","title":"What are the system requirements for TorchEBM?","text":"<p>TorchEBM requires:</p> <ul> <li>Python 3.8 or newer</li> <li>PyTorch 1.10.0 or newer</li> <li>CUDA (optional, but recommended for performance)</li> </ul>"},{"location":"faq/#does-torchebm-work-on-cpu-only-machines","title":"Does TorchEBM work on CPU-only machines?","text":"<p>Yes, TorchEBM works on CPU-only machines, though many operations will be significantly slower than on GPU.</p>"},{"location":"faq/#how-do-i-install-torchebm-with-cuda-support","title":"How do I install TorchEBM with CUDA support?","text":"<p>Make sure you have PyTorch with CUDA support installed first:</p> <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\npip install torchebm\n</code></pre>"},{"location":"faq/#technical-questions","title":"Technical Questions","text":""},{"location":"faq/#how-do-i-choose-the-right-energy-function-for-my-task","title":"How do I choose the right energy function for my task?","text":"<p>The choice of energy function depends on:</p> <ul> <li>The complexity of the distribution you want to model</li> <li>Domain knowledge about the structure of your data</li> <li>Computational constraints</li> </ul> <p>For complex data like images, neural network-based energy functions are typically used. For simpler problems, analytical energy functions may be sufficient.</p>"},{"location":"faq/#what-sampling-algorithm-should-i-use","title":"What sampling algorithm should I use?","text":"<p>Common considerations:</p> <ul> <li>Langevin Dynamics: Good for general-purpose sampling, especially in high dimensions</li> <li>Hamiltonian Monte Carlo: Better for complex energy landscapes, but more computationally expensive</li> <li>Metropolis-Hastings: Simple to implement, but may mix slowly in high dimensions</li> </ul>"},{"location":"faq/#how-do-i-diagnose-problems-with-sampling","title":"How do I diagnose problems with sampling?","text":"<p>Common issues and solutions:</p> <ul> <li>Poor mixing: Increase step size or try a different sampler</li> <li>Numerical instability: Decrease step size or check energy function implementation</li> <li>Slow convergence: Use more iterations or try a more efficient sampler</li> <li>Mode collapse: Check your energy function or use samplers with better exploration capabilities</li> </ul>"},{"location":"faq/#how-do-i-train-an-energy-based-model-on-my-own-data","title":"How do I train an energy-based model on my own data?","text":"<p>Basic steps:</p> <ol> <li>Define an energy function (e.g., a neural network)</li> <li>Choose a loss function (e.g., contrastive divergence)</li> <li>Set up a sampler for generating negative samples</li> <li>Train using gradient descent</li> <li>Evaluate the learned model</li> </ol> <p>See the training examples for more details.</p>"},{"location":"faq/#performance","title":"Performance","text":""},{"location":"faq/#how-can-i-speed-up-sampling","title":"How can I speed up sampling?","text":"<p>To improve sampling performance:</p> <ul> <li>Use GPU acceleration</li> <li>Reduce the dimensionality of your problem</li> <li>Parallelize sampling across multiple chains</li> <li>Optimize step sizes and other hyperparameters</li> <li>Use more efficient sampling algorithms for your specific energy landscape</li> </ul>"},{"location":"faq/#does-torchebm-support-distributed-training","title":"Does TorchEBM support distributed training?","text":"<p>Currently, TorchEBM focuses on single-machine GPU acceleration. Distributed training across multiple GPUs or machines is on our roadmap.</p>"},{"location":"faq/#how-does-torchebms-performance-compare-to-other-libraries","title":"How does TorchEBM's performance compare to other libraries?","text":"<p>TorchEBM is optimized for performance on GPU hardware, particularly for sampling operations. Our benchmarks show significant speedups compared to non-specialized implementations, especially for large-scale sampling tasks.</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute-to-torchebm","title":"How can I contribute to TorchEBM?","text":"<p>We welcome contributions! Check out:</p> <ul> <li>GitHub Issues for current tasks</li> <li>Contributing Guidelines for code style and contribution workflow</li> </ul>"},{"location":"faq/#i-found-a-bug-how-do-i-report-it","title":"I found a bug, how do I report it?","text":"<p>Please open an issue on our GitHub repository with:</p> <ul> <li>A clear description of the problem</li> <li>Steps to reproduce the issue</li> <li>Expected vs. actual behavior</li> <li>Version information (TorchEBM, PyTorch, Python, CUDA)</li> </ul>"},{"location":"faq/#can-i-add-my-own-sampler-or-energy-function-to-torchebm","title":"Can I add my own sampler or energy function to TorchEBM?","text":"<p>Absolutely! TorchEBM is designed to be extensible. See:</p> <ul> <li>Custom Energy Functions</li> <li>Implementing Custom Samplers</li> </ul>"},{"location":"faq/#future-development","title":"Future Development","text":""},{"location":"faq/#what-features-are-planned-for-future-releases","title":"What features are planned for future releases?","text":"<p>See our Roadmap for planned features, including:</p> <ul> <li>Additional samplers and energy functions</li> <li>More loss functions for training</li> <li>Improved visualization tools</li> <li>Advanced neural network architectures</li> <li>Better integration with the PyTorch ecosystem</li> </ul>"},{"location":"faq/#how-stable-is-the-torchebm-api","title":"How stable is the TorchEBM API?","text":"<p>TorchEBM is currently in early development, so the API may change between versions. We'll do our best to document breaking changes and provide migration guidance. </p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:hamiltonian","title":"hamiltonian","text":"<ul> <li>            Hamiltonian Mechanics          </li> </ul>"},{"location":"tags/#tag:langevin","title":"langevin","text":"<ul> <li>            Langevin Dynamics Sampling with TorchEBM          </li> </ul>"},{"location":"tags/#tag:sampling","title":"sampling","text":"<ul> <li>            Hamiltonian Mechanics          </li> <li>            Langevin Dynamics Sampling with TorchEBM          </li> </ul>"},{"location":"tags/#tag:tutorial","title":"tutorial","text":"<ul> <li>            Langevin Dynamics Sampling with TorchEBM          </li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#torchebm-api-reference","title":"TorchEBM API Reference","text":"<p>Welcome to the TorchEBM API reference documentation. This section provides detailed information about the classes and functions available in TorchEBM.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<p>TorchEBM is organized into several modules:</p> <ul> <li> <p> Core</p> <p>Base classes and core functionality for energy functions, samplers, and trainers.</p> <p> Core Module</p> </li> <li> <p> Samplers</p> <p>Sampling algorithms for energy-based models including Langevin Dynamics and MCMC.</p> <p> Samplers</p> </li> <li> <p> Losses</p> <p>BaseLoss functions for training energy-based models.</p> <p> Losses</p> </li> <li> <p> Utils</p> <p>Utility functions for working with energy-based models.</p> <p> Utils</p> </li> <li> <p>:material-gpu:{ .lg .middle } CUDA</p> <p>CUDA-accelerated implementations for faster computation.</p> <p> CUDA</p> </li> </ul>"},{"location":"api/#getting-started-with-the-api","title":"Getting Started with the API","text":"<p>If you're new to TorchEBM, we recommend starting with the following classes:</p> <ul> <li><code>BaseEnergyFunction</code>: Base class for all energy functions</li> <li><code>BaseSampler</code>: Base class for all sampling algorithms</li> <li><code>LangevinDynamics</code>: Implementation of Langevin dynamics sampling</li> </ul>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#energy-functions","title":"Energy Functions","text":"<p>TorchEBM provides various built-in energy functions:</p> Energy Function Description <code>GaussianEnergy</code> Multivariate Gaussian energy function <code>DoubleWellEnergy</code> Double well potential energy function <code>RastriginEnergy</code> Rastrigin function for testing optimization algorithms <code>RosenbrockEnergy</code> Rosenbrock function (banana function) <code>AckleyEnergy</code> Ackley function, a multimodal test function <code>HarmonicEnergy</code> Harmonic oscillator energy function"},{"location":"api/#samplers","title":"Samplers","text":"<p>Available sampling algorithms:</p> Sampler Description <code>LangevinDynamics</code> Langevin dynamics sampling algorithm <code>HamiltonianMonteCarlo</code> Hamiltonian Monte Carlo sampling"},{"location":"api/#baseloss-functions","title":"BaseLoss Functions","text":"<p>TorchEBM implements several loss functions for training EBMs:</p> BaseLoss Function Description <code>ContrastiveDivergence</code> Standard contrastive divergence (CD-k) <code>PersistentContrastiveDivergence</code> Persistent contrastive divergence <code>ParallelTemperingCD</code> Parallel tempering contrastive divergence"},{"location":"api/#module-details","title":"Module Details","text":"<p>For detailed information about each module, follow the links below:</p> <ul> <li>Core Module</li> <li>Samplers</li> <li>Losses</li> <li>Models</li> <li>Utils</li> <li>CUDA</li> </ul>"},{"location":"api/torchebm/","title":"Torchebm","text":""},{"location":"api/torchebm/#torchebm_1","title":"Torchebm","text":""},{"location":"api/torchebm/#contents","title":"Contents","text":""},{"location":"api/torchebm/#subpackages","title":"Subpackages","text":"<ul> <li>Core</li> <li>Cuda</li> <li>Datasets</li> <li>Losses</li> <li>Models</li> <li>Samplers</li> <li>Utils</li> </ul>"},{"location":"api/torchebm/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/#torchebm","title":"torchebm","text":"<p>TorchEBM: Energy-Based Modeling library for PyTorch, offering tools for sampling, inference, and learning in complex distributions.</p>"},{"location":"api/torchebm/core/","title":"Torchebm &gt; Core","text":""},{"location":"api/torchebm/core/#torchebm-core","title":"Torchebm &gt; Core","text":""},{"location":"api/torchebm/core/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/#modules","title":"Modules","text":"<ul> <li>Base_energy_function</li> <li>Base_evaluator</li> <li>Base_loss</li> <li>Base_metric</li> <li>Base_optimizer</li> <li>Base_sampler</li> <li>Base_scheduler</li> <li>Base_trainer</li> <li>Device_mixin</li> </ul>"},{"location":"api/torchebm/core/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/#torchebm.core","title":"torchebm.core","text":"<p>Core functionality for energy-based models, including energy functions, base sampler class, and training utilities.</p>"},{"location":"api/torchebm/core/base_energy_function/","title":"Torchebm &gt; Core &gt; Base_energy_function","text":""},{"location":"api/torchebm/core/base_energy_function/#torchebm-core-base_energy_function","title":"Torchebm &gt; Core &gt; Base_energy_function","text":""},{"location":"api/torchebm/core/base_energy_function/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_energy_function/#classes","title":"Classes","text":"<ul> <li><code>AckleyEnergy</code> - Energy function for the Ackley function.</li> <li><code>BaseEnergyFunction</code> - Abstract base class for energy functions (Potential Energy \\(E(x)\\)).</li> <li><code>DoubleWellEnergy</code> - Energy function for a double well potential. \\( E(x) = h \\sum_{i=1}^{n} (x_i^2 - b^2)^2 \\) where ...</li> <li><code>GaussianEnergy</code> - Energy function for a Gaussian distribution. (E(x) = \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x...</li> <li><code>HarmonicEnergy</code> - Energy function for a harmonic oscillator. \\(E(x) = \\frac{1}{2} k \\sum_{i=1}^{n} x_i^{2}\\).</li> <li><code>RastriginEnergy</code> - Energy function for the Rastrigin function.</li> <li><code>RosenbrockEnergy</code> - Energy function for the Rosenbrock function. (E(x) = \\sum_{i=1}^{n-1} \\left[ b(x_{i+1} - x_i<sup>2)</sup>...</li> </ul>"},{"location":"api/torchebm/core/base_energy_function/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_energy_function/#torchebm.core.base_energy_function","title":"torchebm.core.base_energy_function","text":""},{"location":"api/torchebm/core/base_energy_function/classes/AckleyEnergy/","title":"AckleyEnergy","text":""},{"location":"api/torchebm/core/base_energy_function/classes/AckleyEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseEnergyFunction</code></p> <p>Energy function for the Ackley function.</p> <p>The Ackley energy is defined as:</p> \\[ \\begin{aligned} E(x) &amp;= -a \\exp\\left(-b \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) \\\\ &amp;\\quad - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c x_i)\\right) + a + \\exp(1) \\end{aligned} \\] <p>This function has a global minimum at the origin surrounded by many local minima, creating a challenging optimization landscape that tests an algorithm's ability to escape local optima.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Ackley function.</p> <code>20.0</code> <code>b</code> <code>float</code> <p>Parameter <code>b</code> of the Ackley function.</p> <code>0.2</code> <code>c</code> <code>float</code> <p>Parameter <code>c</code> of the Ackley function.</p> <code>2 * pi</code> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>class AckleyEnergy(BaseEnergyFunction):\n    r\"\"\"\n    Energy function for the Ackley function.\n\n    The Ackley energy is defined as:\n\n    $$\n    \\begin{aligned}\n    E(x) &amp;= -a \\exp\\left(-b \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) \\\\\n    &amp;\\quad - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c x_i)\\right) + a + \\exp(1)\n    \\end{aligned}\n    $$\n\n    This function has a global minimum at the origin surrounded by many local minima,\n    creating a challenging optimization landscape that tests an algorithm's ability to\n    escape local optima.\n\n    Args:\n        a (float): Parameter `a` of the Ackley function.\n        b (float): Parameter `b` of the Ackley function.\n        c (float): Parameter `c` of the Ackley function.\n    \"\"\"\n\n    def __init__(\n        self, a: float = 20.0, b: float = 0.2, c: float = 2 * math.pi, *args, **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Computes the Ackley energy.\n\n        $$\n        \\begin{aligned}\n        E(x) &amp;= -a \\exp\\left(-b \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) \\\\\n        &amp;\\quad - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c x_i)\\right) + a + \\exp(1)\n        \\end{aligned}\n        $$\n        \"\"\"\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n\n        n = x.shape[-1]\n        sum1 = torch.sum(x**2, dim=-1)\n        sum2 = torch.sum(torch.cos(self.c * x), dim=-1)\n        term1 = -self.a * torch.exp(-self.b * torch.sqrt(sum1 / n))\n        term2 = -torch.exp(sum2 / n)\n        return term1 + term2 + self.a + math.e\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/AckleyEnergy/#torchebm.core.base_energy_function.AckleyEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/AckleyEnergy/#torchebm.core.base_energy_function.AckleyEnergy.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/AckleyEnergy/#torchebm.core.base_energy_function.AckleyEnergy.c","title":"c  <code>instance-attribute</code>","text":"<pre><code>c = c\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/AckleyEnergy/#torchebm.core.base_energy_function.AckleyEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Ackley energy.</p> \\[ \\begin{aligned} E(x) &amp;= -a \\exp\\left(-b \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) \\\\ &amp;\\quad - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c x_i)\\right) + a + \\exp(1) \\end{aligned} \\] Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the Ackley energy.\n\n    $$\n    \\begin{aligned}\n    E(x) &amp;= -a \\exp\\left(-b \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}\\right) \\\\\n    &amp;\\quad - \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\cos(c x_i)\\right) + a + \\exp(1)\n    \\end{aligned}\n    $$\n    \"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n\n    n = x.shape[-1]\n    sum1 = torch.sum(x**2, dim=-1)\n    sum2 = torch.sum(torch.cos(self.c * x), dim=-1)\n    term1 = -self.a * torch.exp(-self.b * torch.sqrt(sum1 / n))\n    term2 = -torch.exp(sum2 / n)\n    return term1 + term2 + self.a + math.e\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/BaseEnergyFunction/","title":"BaseEnergyFunction","text":""},{"location":"api/torchebm/core/base_energy_function/classes/BaseEnergyFunction/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>DeviceMixin</code>, <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for energy functions (Potential Energy \\(E(x)\\)).</p> <p>This class serves as a standard interface for defining energy functions used within the torchebm library. It is compatible with both pre-defined analytical functions (like Gaussian, DoubleWell) and trainable neural network models. It represents the potential energy \\(E(x)\\), often related to a probability distribution \\(p(x)\\) by \\(E(x) = -\\log p(x) +      ext{constant}\\).</p> <p>Core Requirements for Subclasses:</p> <ol> <li>Implement the <code>forward(x)</code> method to compute the scalar energy per sample.</li> <li>Optionally, override the <code>gradient(x)</code> method if an efficient analytical     gradient is available. Otherwise, the default implementation using     <code>torch.autograd</code> will be used.</li> </ol> <p>Inheriting from <code>torch.nn.Module</code> ensures that:</p> <ul> <li>Subclasses can contain trainable parameters (<code>nn.Parameter</code>).</li> <li>Standard PyTorch methods like <code>.to(device)</code>, <code>.parameters()</code>, <code>.state_dict()</code>,   and integration with <code>torch.optim</code> work as expected.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>device</code> <code>Union[str, device]</code> <p>Device for computations</p> required <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision for forward and gradient computation</p> <code>False</code> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>class BaseEnergyFunction(DeviceMixin, nn.Module, ABC):\n    \"\"\"\n    Abstract base class for energy functions (Potential Energy \\(E(x)\\)).\n\n    This class serves as a standard interface for defining energy functions used\n    within the torchebm library. It is compatible with both pre-defined analytical\n    functions (like Gaussian, DoubleWell) and trainable neural network models.\n    It represents the potential energy \\(E(x)\\), often related to a probability\n    distribution \\(p(x)\\) by \\(E(x) = -\\log p(x) + \\text{constant}\\).\n\n    Core Requirements for Subclasses:\n\n    1.  Implement the `forward(x)` method to compute the scalar energy per sample.\n    2.  Optionally, override the `gradient(x)` method if an efficient analytical\n        gradient is available. Otherwise, the default implementation using\n        `torch.autograd` will be used.\n\n    Inheriting from `torch.nn.Module` ensures that:\n\n    - Subclasses can contain trainable parameters (`nn.Parameter`).\n    - Standard PyTorch methods like `.to(device)`, `.parameters()`, `.state_dict()`,\n      and integration with `torch.optim` work as expected.\n\n    Args:\n        dtype (torch.dtype): Data type for computations\n        device (Union[str, torch.device]): Device for computations\n        use_mixed_precision (bool): Whether to use mixed precision for forward and gradient computation\n    \"\"\"\n\n    def __init__(\n        self,\n        dtype: torch.dtype = torch.float32,\n        # device: Optional[Union[str, torch.device]] = None,\n        use_mixed_precision: bool = False,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initializes the BaseEnergyFunction base class.\"\"\"\n        super().__init__(*args, **kwargs)\n        # if isinstance(device, str):\n        #     device = torch.device(device)\n\n        self.dtype = dtype\n        # self._device = device or (\n        #     torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        # )\n        self.use_mixed_precision = use_mixed_precision\n\n        if self.use_mixed_precision:\n            try:\n                from torch.cuda.amp import autocast\n\n                self.autocast_available = True\n            except ImportError:\n                warnings.warn(\n                    \"Mixed precision requested but torch.cuda.amp not available. \"\n                    \"Falling back to full precision. Requires PyTorch 1.6+.\",\n                    UserWarning,\n                )\n                self.use_mixed_precision = False\n                self.autocast_available = False\n        else:\n            self.autocast_available = False\n\n    # @property\n    # def device(self) -&gt; torch.device:\n    #     \"\"\"Returns the device associated with the module's parameters/buffers (if any).\"\"\"\n    #     try:\n    #         return next(self.parameters()).device\n    #     except StopIteration:\n    #         try:\n    #             return next(self.buffers()).device\n    #         except StopIteration:\n    #             return self._device\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the scalar energy value for each input sample.\n\n        Args:\n            x (torch.Tensor): Input tensor of batch_shape (batch_size, *input_dims).\n                              It's recommended that subclasses handle moving `x`\n                              to the correct device if necessary, although callers\n                              should ideally provide `x` on the correct device.\n\n        Returns:\n            torch.Tensor: Tensor of scalar energy values with batch_shape (batch_size,).\n                          Lower values typically indicate higher probability density.\n        \"\"\"\n        pass\n\n    def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Computes the gradient of the energy function with respect to the input \\(x\\) \\((\\nabla_x E(x))\\).\n\n        This default implementation uses automatic differentiation based on the\n        `forward` method. Subclasses should override this method if a more\n        efficient or numerically stable analytical gradient is available.\n\n        Args:\n            x (torch.Tensor): Input tensor of batch_shape (batch_size, *input_dims).\n\n        Returns:\n            torch.Tensor: Gradient tensor of the same batch_shape as x.\n        \"\"\"\n\n        original_dtype = x.dtype\n        device = x.device\n\n        if self.device and device != self.device:\n            x = x.to(self.device)\n            device = self.device\n\n        with torch.enable_grad():  # todo: consider removing conversion to fp32 and uncessary device change\n            x_for_grad = (\n                x.detach().to(dtype=torch.float32, device=device).requires_grad_(True)\n            )\n\n            if self.use_mixed_precision and self.autocast_available:\n                from torch.cuda.amp import autocast\n\n                with autocast():\n                    energy = self.forward(x_for_grad)\n            else:\n                energy = self.forward(x_for_grad)\n\n            if energy.shape != (x_for_grad.shape[0],):\n                raise ValueError(\n                    f\"BaseEnergyFunction forward() output expected batch_shape ({x_for_grad.shape[0]},), but got {energy.shape}.\"\n                )\n\n            if not energy.grad_fn:\n                raise RuntimeError(\n                    \"Cannot compute gradient: `forward` method did not use the input `x` (as float32) in a differentiable way.\"\n                )\n\n            gradient_float32 = torch.autograd.grad(\n                outputs=energy,\n                inputs=x_for_grad,\n                grad_outputs=torch.ones_like(energy, device=energy.device),\n                create_graph=False,  # false for standard grad computation\n                retain_graph=None,  # since create_graph=False, let PyTorch decide\n            )[0]\n\n        if gradient_float32 is None:  # for triple checking!\n            raise RuntimeError(\n                \"Gradient computation failed unexpectedly. Check the forward pass implementation.\"\n            )\n\n        gradient = gradient_float32.to(original_dtype)\n\n        return gradient.detach()\n\n    def __call__(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Alias for the forward method for standard PyTorch module usage.\"\"\"\n        if (x.device != self.device) or (x.dtype != self.dtype):\n            x = x.to(device=self.device, dtype=self.dtype)\n\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                return super().__call__(x, *args, **kwargs)\n        else:\n            return super().__call__(x, *args, **kwargs)\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/BaseEnergyFunction/#torchebm.core.base_energy_function.BaseEnergyFunction.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/BaseEnergyFunction/#torchebm.core.base_energy_function.BaseEnergyFunction.use_mixed_precision","title":"use_mixed_precision  <code>instance-attribute</code>","text":"<pre><code>use_mixed_precision = use_mixed_precision\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/BaseEnergyFunction/#torchebm.core.base_energy_function.BaseEnergyFunction.autocast_available","title":"autocast_available  <code>instance-attribute</code>","text":"<pre><code>autocast_available = True\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/BaseEnergyFunction/#torchebm.core.base_energy_function.BaseEnergyFunction.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the scalar energy value for each input sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of batch_shape (batch_size, *input_dims).               It's recommended that subclasses handle moving <code>x</code>               to the correct device if necessary, although callers               should ideally provide <code>x</code> on the correct device.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Tensor of scalar energy values with batch_shape (batch_size,).           Lower values typically indicate higher probability density.</p> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the scalar energy value for each input sample.\n\n    Args:\n        x (torch.Tensor): Input tensor of batch_shape (batch_size, *input_dims).\n                          It's recommended that subclasses handle moving `x`\n                          to the correct device if necessary, although callers\n                          should ideally provide `x` on the correct device.\n\n    Returns:\n        torch.Tensor: Tensor of scalar energy values with batch_shape (batch_size,).\n                      Lower values typically indicate higher probability density.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/BaseEnergyFunction/#torchebm.core.base_energy_function.BaseEnergyFunction.gradient","title":"gradient","text":"<pre><code>gradient(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the gradient of the energy function with respect to the input \\(x\\) \\((\\nabla_x E(x))\\).</p> <p>This default implementation uses automatic differentiation based on the <code>forward</code> method. Subclasses should override this method if a more efficient or numerically stable analytical gradient is available.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of batch_shape (batch_size, *input_dims).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Gradient tensor of the same batch_shape as x.</p> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the gradient of the energy function with respect to the input \\(x\\) \\((\\nabla_x E(x))\\).\n\n    This default implementation uses automatic differentiation based on the\n    `forward` method. Subclasses should override this method if a more\n    efficient or numerically stable analytical gradient is available.\n\n    Args:\n        x (torch.Tensor): Input tensor of batch_shape (batch_size, *input_dims).\n\n    Returns:\n        torch.Tensor: Gradient tensor of the same batch_shape as x.\n    \"\"\"\n\n    original_dtype = x.dtype\n    device = x.device\n\n    if self.device and device != self.device:\n        x = x.to(self.device)\n        device = self.device\n\n    with torch.enable_grad():  # todo: consider removing conversion to fp32 and uncessary device change\n        x_for_grad = (\n            x.detach().to(dtype=torch.float32, device=device).requires_grad_(True)\n        )\n\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                energy = self.forward(x_for_grad)\n        else:\n            energy = self.forward(x_for_grad)\n\n        if energy.shape != (x_for_grad.shape[0],):\n            raise ValueError(\n                f\"BaseEnergyFunction forward() output expected batch_shape ({x_for_grad.shape[0]},), but got {energy.shape}.\"\n            )\n\n        if not energy.grad_fn:\n            raise RuntimeError(\n                \"Cannot compute gradient: `forward` method did not use the input `x` (as float32) in a differentiable way.\"\n            )\n\n        gradient_float32 = torch.autograd.grad(\n            outputs=energy,\n            inputs=x_for_grad,\n            grad_outputs=torch.ones_like(energy, device=energy.device),\n            create_graph=False,  # false for standard grad computation\n            retain_graph=None,  # since create_graph=False, let PyTorch decide\n        )[0]\n\n    if gradient_float32 is None:  # for triple checking!\n        raise RuntimeError(\n            \"Gradient computation failed unexpectedly. Check the forward pass implementation.\"\n        )\n\n    gradient = gradient_float32.to(original_dtype)\n\n    return gradient.detach()\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/DoubleWellEnergy/","title":"DoubleWellEnergy","text":""},{"location":"api/torchebm/core/base_energy_function/classes/DoubleWellEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseEnergyFunction</code></p> <p>Energy function for a double well potential. \\( E(x) = h \\sum_{i=1}^{n} (x_i^2 - b^2)^2 \\) where h is the barrier height.</p> <p>This energy function creates a bimodal distribution with two modes at \\( x_i = \\pm b \\) (in each dimension), separated by a barrier of height h at \\(x_i = 0\\).</p> <p>Parameters:</p> Name Type Description Default <code>barrier_height</code> <code>float</code> <p>Height of the barrier between the wells.</p> <code>2.0</code> <code>b</code> <code>float</code> <p>Position of the wells (default is 1.0, wells at \u00b11).</p> <code>1.0</code> <p>Returns:</p> Type Description <p>torch.Tensor: Energy values for each input sample, with lower values indicating higher probability density.</p> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>class DoubleWellEnergy(BaseEnergyFunction):\n    r\"\"\"\n    Energy function for a double well potential. \\( E(x) = h \\sum_{i=1}^{n} (x_i^2 - b^2)^2 \\) where h is the barrier height.\n\n    This energy function creates a bimodal distribution with two modes at \\( x_i = \\pm b \\)\n    (in each dimension), separated by a barrier of height h at \\(x_i = 0\\).\n\n    Args:\n        barrier_height (float): Height of the barrier between the wells.\n        b (float): Position of the wells (default is 1.0, wells at \u00b11).\n\n    Returns:\n        torch.Tensor: Energy values for each input sample, with lower values indicating higher probability density.\n    \"\"\"\n\n    def __init__(self, barrier_height: float = 2.0, b: float = 1.0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.barrier_height = barrier_height\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Computes the double well energy: \\(h \\sum_{i=1}^{n} (x_i^2 - b^2)^2\\).\"\"\"\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n\n        return self.barrier_height * (x.pow(2) - self.b**2).pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/DoubleWellEnergy/#torchebm.core.base_energy_function.DoubleWellEnergy.barrier_height","title":"barrier_height  <code>instance-attribute</code>","text":"<pre><code>barrier_height = barrier_height\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/DoubleWellEnergy/#torchebm.core.base_energy_function.DoubleWellEnergy.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/DoubleWellEnergy/#torchebm.core.base_energy_function.DoubleWellEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the double well energy: \\(h \\sum_{i=1}^{n} (x_i^2 - b^2)^2\\).</p> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Computes the double well energy: \\(h \\sum_{i=1}^{n} (x_i^2 - b^2)^2\\).\"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n\n    return self.barrier_height * (x.pow(2) - self.b**2).pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/GaussianEnergy/","title":"GaussianEnergy","text":""},{"location":"api/torchebm/core/base_energy_function/classes/GaussianEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseEnergyFunction</code></p> <p>Energy function for a Gaussian distribution. \\(E(x) = \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\).</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Tensor</code> <p>Mean vector (\u03bc) of the Gaussian distribution.</p> required <code>cov</code> <code>Tensor</code> <p>Covariance matrix (\u03a3) of the Gaussian distribution.</p> required Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>class GaussianEnergy(BaseEnergyFunction):\n    r\"\"\"\n    Energy function for a Gaussian distribution. \\(E(x) = \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\).\n\n    Args:\n        mean (torch.Tensor): Mean vector (\u03bc) of the Gaussian distribution.\n        cov (torch.Tensor): Covariance matrix (\u03a3) of the Gaussian distribution.\n    \"\"\"\n\n    def __init__(self, mean: torch.Tensor, cov: torch.Tensor, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if mean.ndim != 1:\n            raise ValueError(\"Mean must be a 1D tensor.\")\n        if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n            raise ValueError(\"Covariance must be a 2D square matrix.\")\n        if mean.shape[0] != cov.shape[0]:\n            raise ValueError(\n                \"Mean vector dimension must match covariance matrix dimension.\"\n            )\n\n        self.register_buffer(\"mean\", mean.to(dtype=self.dtype, device=self.device))\n        try:\n            cov_inv = torch.inverse(cov)\n            self.register_buffer(\n                \"cov_inv\", cov_inv.to(dtype=self.dtype, device=self.device)\n            )\n        except RuntimeError as e:\n            raise ValueError(\n                f\"Failed to invert covariance matrix: {e}. Ensure it is invertible.\"\n            ) from e\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Computes the Gaussian energy: \\(E(x) = \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\).\"\"\"\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n        if x.ndim != 2 or x.shape[1] != self.mean.shape[0]:\n            raise ValueError(\n                f\"Input x expected batch_shape (batch_size, {self.mean.shape[0]}), but got {x.shape}\"\n            )\n\n        x = x.to(dtype=self.dtype, device=self.device)\n        # mean = self.mean.to(device=x.device)\n        cov_inv = self.cov_inv.to(dtype=self.dtype, device=x.device)\n\n        delta = (\n            x - self.mean\n        )  # avoid detaching or converting x to maintain grad tracking\n        # energy = 0.5 * torch.einsum(\"bi,ij,bj-&gt;b\", delta, cov_inv, delta)\n\n        if delta.shape[0] &gt; 1:\n            delta_expanded = delta.unsqueeze(-1)  # (batch_size, dim, 1)\n            cov_inv_expanded = cov_inv.unsqueeze(0).expand(\n                delta.shape[0], -1, -1\n            )  # (batch_size, dim, dim)\n\n            temp = torch.bmm(cov_inv_expanded, delta_expanded)  # (batch_size, dim, 1)\n            energy = 0.5 * torch.bmm(delta.unsqueeze(1), temp).squeeze(-1).squeeze(-1)\n        else:\n            energy = 0.5 * torch.sum(delta * torch.matmul(delta, cov_inv), dim=-1)\n\n        return energy\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/GaussianEnergy/#torchebm.core.base_energy_function.GaussianEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Gaussian energy: \\(E(x) = \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\).</p> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Computes the Gaussian energy: \\(E(x) = \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\).\"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n    if x.ndim != 2 or x.shape[1] != self.mean.shape[0]:\n        raise ValueError(\n            f\"Input x expected batch_shape (batch_size, {self.mean.shape[0]}), but got {x.shape}\"\n        )\n\n    x = x.to(dtype=self.dtype, device=self.device)\n    # mean = self.mean.to(device=x.device)\n    cov_inv = self.cov_inv.to(dtype=self.dtype, device=x.device)\n\n    delta = (\n        x - self.mean\n    )  # avoid detaching or converting x to maintain grad tracking\n    # energy = 0.5 * torch.einsum(\"bi,ij,bj-&gt;b\", delta, cov_inv, delta)\n\n    if delta.shape[0] &gt; 1:\n        delta_expanded = delta.unsqueeze(-1)  # (batch_size, dim, 1)\n        cov_inv_expanded = cov_inv.unsqueeze(0).expand(\n            delta.shape[0], -1, -1\n        )  # (batch_size, dim, dim)\n\n        temp = torch.bmm(cov_inv_expanded, delta_expanded)  # (batch_size, dim, 1)\n        energy = 0.5 * torch.bmm(delta.unsqueeze(1), temp).squeeze(-1).squeeze(-1)\n    else:\n        energy = 0.5 * torch.sum(delta * torch.matmul(delta, cov_inv), dim=-1)\n\n    return energy\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/HarmonicEnergy/","title":"HarmonicEnergy","text":""},{"location":"api/torchebm/core/base_energy_function/classes/HarmonicEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseEnergyFunction</code></p> <p>Energy function for a harmonic oscillator. \\(E(x) = \\frac{1}{2} k \\sum_{i=1}^{n} x_i^{2}\\).</p> <p>This energy function represents a quadratic potential centered at the origin, equivalent to a Gaussian distribution with zero mean and variance proportional to \\(\\frac{1}{k}\\).</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>float</code> <p>Spring constant.</p> <code>1.0</code> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>class HarmonicEnergy(BaseEnergyFunction):\n    r\"\"\"\n    Energy function for a harmonic oscillator. \\(E(x) = \\frac{1}{2} k \\sum_{i=1}^{n} x_i^{2}\\).\n\n    This energy function represents a quadratic potential centered at the origin,\n    equivalent to a Gaussian distribution with zero mean and variance proportional to \\(\\frac{1}{k}\\).\n\n    Args:\n        k (float): Spring constant.\n    \"\"\"\n\n    def __init__(self, k: float = 1.0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.k = k\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Computes the harmonic oscillator energy: \\(\\frac{1}{2} k \\sum_{i=1}^{n} x_i^{2}\\).\"\"\"\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n\n        return 0.5 * self.k * x.pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/HarmonicEnergy/#torchebm.core.base_energy_function.HarmonicEnergy.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k = k\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/HarmonicEnergy/#torchebm.core.base_energy_function.HarmonicEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the harmonic oscillator energy: \\(\\frac{1}{2} k \\sum_{i=1}^{n} x_i^{2}\\).</p> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Computes the harmonic oscillator energy: \\(\\frac{1}{2} k \\sum_{i=1}^{n} x_i^{2}\\).\"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n\n    return 0.5 * self.k * x.pow(2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/RastriginEnergy/","title":"RastriginEnergy","text":""},{"location":"api/torchebm/core/base_energy_function/classes/RastriginEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseEnergyFunction</code></p> <p>Energy function for the Rastrigin function.</p> <p>The Rastrigin energy is defined as:</p> \\[E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]\\] <p>This function is characterized by a large number of local minima arranged in a regular lattice, with a global minimum at the origin. It's a classic test for optimization algorithms due to its highly multimodal nature.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Rastrigin function.</p> <code>10.0</code> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>class RastriginEnergy(BaseEnergyFunction):\n    r\"\"\"\n    Energy function for the Rastrigin function.\n\n    The Rastrigin energy is defined as:\n\n    $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n\n    This function is characterized by a large number of local minima arranged in a\n    regular lattice, with a global minimum at the origin. It's a classic test for\n    optimization algorithms due to its highly multimodal nature.\n\n    Args:\n        a (float): Parameter `a` of the Rastrigin function.\n    \"\"\"\n\n    def __init__(self, a: float = 10.0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.a = a\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Computes the Rastrigin energy.\n\n        $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n        \"\"\"\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n\n        n = x.shape[-1]\n        return self.a * n + torch.sum(\n            x**2 - self.a * torch.cos(2 * math.pi * x), dim=-1\n        )\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/RastriginEnergy/#torchebm.core.base_energy_function.RastriginEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/RastriginEnergy/#torchebm.core.base_energy_function.RastriginEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Rastrigin energy.</p> \\[E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]\\] Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the Rastrigin energy.\n\n    $$E(x) = an + \\sum_{i=1}^{n} [x_i^2 - a \\cos(2\\pi x_i)]$$\n    \"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n\n    n = x.shape[-1]\n    return self.a * n + torch.sum(\n        x**2 - self.a * torch.cos(2 * math.pi * x), dim=-1\n    )\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/RosenbrockEnergy/","title":"RosenbrockEnergy","text":""},{"location":"api/torchebm/core/base_energy_function/classes/RosenbrockEnergy/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseEnergyFunction</code></p> <p>Energy function for the Rosenbrock function. \\(E(x) = \\sum_{i=1}^{n-1} \\left[ b(x_{i+1} - x_i^2)^2 + (a - x_i)^2 \\right]\\).</p> <p>This energy function creates a challenging valley-shaped distribution with the global minimum at \\((a, a^2, a^2, \\ldots, a^2)\\). It's commonly used as a benchmark for optimization algorithms due to its curved, narrow valley which is difficult to traverse.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Parameter <code>a</code> of the Rosenbrock function.</p> <code>1.0</code> <code>b</code> <code>float</code> <p>Parameter <code>b</code> of the Rosenbrock function.</p> <code>100.0</code> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>class RosenbrockEnergy(BaseEnergyFunction):\n    r\"\"\"\n    Energy function for the Rosenbrock function. \\(E(x) = \\sum_{i=1}^{n-1} \\left[ b(x_{i+1} - x_i^2)^2 + (a - x_i)^2 \\right]\\).\n\n    This energy function creates a challenging valley-shaped distribution with the\n    global minimum at \\((a, a^2, a^2, \\ldots, a^2)\\). It's commonly used as a benchmark for optimization algorithms\n    due to its curved, narrow valley which is difficult to traverse.\n\n    Args:\n        a (float): Parameter `a` of the Rosenbrock function.\n        b (float): Parameter `b` of the Rosenbrock function.\n    \"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 100.0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Computes the Rosenbrock energy: \\(\\sum_{i=1}^{n-1} \\left[ b(x_{i+1} - x_i^2)^2 + (a - x_i)^2 \\right]\\).\"\"\"\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n        if x.shape[-1] &lt; 2:\n            raise ValueError(\n                f\"Rosenbrock energy function requires at least 2 dimensions, got {x.shape[-1]}\"\n            )\n\n        # return (self.a - x[..., 0]) ** 2 + self.b * (x[..., 1] - x[..., 0] ** 2) ** 2\n        # return sum(\n        #     self.b * (x[..., i + 1] - x[..., i] ** 2) ** 2 + (self.a - x[i]) ** 2\n        #     for i in range(len(x) - 1)\n        # )\n\n        x_i = x[:, :-1]\n        x_ip1 = x[:, 1:]\n        term1 = (self.a - x_i).pow(2)\n        term2 = self.b * (x_ip1 - x_i.pow(2)).pow(2)\n        return (term1 + term2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/RosenbrockEnergy/#torchebm.core.base_energy_function.RosenbrockEnergy.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/RosenbrockEnergy/#torchebm.core.base_energy_function.RosenbrockEnergy.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/torchebm/core/base_energy_function/classes/RosenbrockEnergy/#torchebm.core.base_energy_function.RosenbrockEnergy.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes the Rosenbrock energy: \\(\\sum_{i=1}^{n-1} \\left[ b(x_{i+1} - x_i^2)^2 + (a - x_i)^2 \\right]\\).</p> Source code in <code>torchebm/core/base_energy_function.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Computes the Rosenbrock energy: \\(\\sum_{i=1}^{n-1} \\left[ b(x_{i+1} - x_i^2)^2 + (a - x_i)^2 \\right]\\).\"\"\"\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n    if x.shape[-1] &lt; 2:\n        raise ValueError(\n            f\"Rosenbrock energy function requires at least 2 dimensions, got {x.shape[-1]}\"\n        )\n\n    # return (self.a - x[..., 0]) ** 2 + self.b * (x[..., 1] - x[..., 0] ** 2) ** 2\n    # return sum(\n    #     self.b * (x[..., i + 1] - x[..., i] ** 2) ** 2 + (self.a - x[i]) ** 2\n    #     for i in range(len(x) - 1)\n    # )\n\n    x_i = x[:, :-1]\n    x_ip1 = x[:, 1:]\n    term1 = (self.a - x_i).pow(2)\n    term2 = self.b * (x_ip1 - x_i.pow(2)).pow(2)\n    return (term1 + term2).sum(dim=-1)\n</code></pre>"},{"location":"api/torchebm/core/base_evaluator/","title":"Base_evaluator","text":""},{"location":"api/torchebm/core/base_evaluator/#torchebm-core-base_evaluator","title":"Torchebm &gt; Core &gt; Base_evaluator","text":""},{"location":"api/torchebm/core/base_evaluator/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_evaluator/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_evaluator/#torchebm.core.base_evaluator","title":"torchebm.core.base_evaluator","text":""},{"location":"api/torchebm/core/base_loss/","title":"Torchebm &gt; Core &gt; Base_loss","text":""},{"location":"api/torchebm/core/base_loss/#torchebm-core-base_loss","title":"Torchebm &gt; Core &gt; Base_loss","text":""},{"location":"api/torchebm/core/base_loss/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_loss/#classes","title":"Classes","text":"<ul> <li><code>BaseContrastiveDivergence</code> - Abstract base class for Contrastive Divergence (CD) based loss functions.</li> <li><code>BaseLoss</code> - Abstract base class for loss functions used in energy-based models.</li> <li><code>BaseScoreMatching</code> - Abstract base class for Score Matching based loss functions.</li> </ul>"},{"location":"api/torchebm/core/base_loss/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_loss/#torchebm.core.base_loss","title":"torchebm.core.base_loss","text":"<p>Base Loss Classes for Energy-Based Models</p> <p>This module provides abstract base classes for defining loss functions for training energy-based models (EBMs). It includes the general BaseLoss class for arbitrary loss functions and the more specialized BaseContrastiveDivergence for contrastive divergence based training methods.</p> <p>Loss functions in TorchEBM are designed to work with energy functions and samplers to define the training objective for energy-based models.</p>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/","title":"BaseContrastiveDivergence","text":""},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseLoss</code></p> <p>Abstract base class for Contrastive Divergence (CD) based loss functions.</p> <p>Contrastive Divergence is a family of methods for training energy-based models that approximate the gradient of the log-likelihood by comparing the energy between real data samples (positive phase) and model samples (negative phase) generated through MCMC sampling.</p> <p>This class provides the common structure for CD variants, including standard CD, Persistent CD (PCD), and others.</p> <p>Methods:</p> Name Description <code>- __call__</code> <p>Calls the forward method of the loss function.</p> <code>- initialize_buffer</code> <p>Initializes the replay buffer with random noise.</p> <code>- get_negative_samples</code> <p>Generates negative samples using the replay buffer strategy.</p> <code>- update_buffer</code> <p>Updates the replay buffer with new samples.</p> <code>- forward</code> <p>Computes CD loss given real data samples.</p> <code>- compute_loss</code> <p>Computes the contrastive divergence loss from positive and negative samples.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>The energy function being trained</p> required <code>sampler</code> <code>BaseSampler</code> <p>MCMC sampler for generating negative samples</p> required <code>k_steps</code> <code>int</code> <p>Number of MCMC steps to perform for each update</p> <code>1</code> <code>persistent</code> <code>bool</code> <p>Whether to use replay buffer (PCD)</p> <code>False</code> <code>buffer_size</code> <code>int</code> <p>Size of the buffer for storing replay buffer</p> <code>100</code> <code>new_sample_ratio</code> <code>float</code> <p>Ratio of new samples (default 5%). Adds noise to a fraction of buffer samples for exploration</p> <code>0.0</code> <code>init_steps</code> <code>int</code> <p>Number of steps to run when initializing new chain elements</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for computations</p> <code>None</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training (requires PyTorch 1.6+)</p> <code>False</code> <code>*args</code> <p>Additional positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>class BaseContrastiveDivergence(BaseLoss):\n    \"\"\"\n    Abstract base class for Contrastive Divergence (CD) based loss functions.\n\n    Contrastive Divergence is a family of methods for training energy-based models that\n    approximate the gradient of the log-likelihood by comparing the energy between real\n    data samples (positive phase) and model samples (negative phase) generated through\n    MCMC sampling.\n\n    This class provides the common structure for CD variants, including standard CD,\n    Persistent CD (PCD), and others.\n\n    Methods:\n        - __call__: Calls the forward method of the loss function.\n        - initialize_buffer: Initializes the replay buffer with random noise.\n        - get_negative_samples: Generates negative samples using the replay buffer strategy.\n        - update_buffer: Updates the replay buffer with new samples.\n        - forward: Computes CD loss given real data samples.\n        - compute_loss: Computes the contrastive divergence loss from positive and negative samples.\n\n    Args:\n        energy_function: The energy function being trained\n        sampler: MCMC sampler for generating negative samples\n        k_steps: Number of MCMC steps to perform for each update\n        persistent: Whether to use replay buffer (PCD)\n        buffer_size: Size of the buffer for storing replay buffer\n        new_sample_ratio: Ratio of new samples (default 5%). Adds noise to a fraction of buffer samples for exploration\n        init_steps: Number of steps to run when initializing new chain elements\n        dtype: Data type for computations\n        device: Device for computations\n        use_mixed_precision: Whether to use mixed precision training (requires PyTorch 1.6+)\n        *args: Additional positional arguments\n        **kwargs: Additional keyword arguments\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        sampler: BaseSampler,\n        k_steps: int = 1,\n        persistent: bool = False,\n        buffer_size: int = 100,\n        new_sample_ratio: float = 0.0,\n        init_steps: int = 0,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n        use_mixed_precision: bool = False,\n        clip_value: Optional[float] = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            dtype=dtype,\n            device=device,\n            use_mixed_precision=use_mixed_precision,\n            clip_value=clip_value,\n            *args,\n            **kwargs,\n        )\n        self.energy_function = energy_function\n        self.sampler = sampler\n        self.k_steps = k_steps\n        self.persistent = persistent\n        self.buffer_size = buffer_size\n        self.new_sample_ratio = new_sample_ratio\n        self.init_steps = init_steps\n\n        self.energy_function = self.energy_function.to(device=self.device)\n        if hasattr(self.sampler, \"to\") and callable(getattr(self.sampler, \"to\")):\n            self.sampler = self.sampler.to(device=self.device)\n\n        self.register_buffer(\"replay_buffer\", None)\n        self.register_buffer(\n            \"buffer_ptr\", torch.tensor(0, dtype=torch.long, device=self.device)\n        )\n        self.buffer_initialized = False\n\n    def initialize_buffer(\n        self,\n        data_shape_no_batch: Tuple[int, ...],\n        buffer_chunk_size: int = 1024,\n        init_noise_scale: float = 0.01,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Initialize the replay buffer with random noise.\n\n        For persistent CD variants, this method initializes the replay buffer\n        with random noise. This is typically called the first time the loss\n        is computed or when the batch size changes.\n\n        Args:\n            data_shape_no_batch: Shape of the data excluding batch dimension.\n            buffer_chunk_size: Size of the chunks to process during initialization.\n            init_noise_scale: Scale of the initial noise.\n\n        Returns:\n            The initialized buffer.\n        \"\"\"\n        if not self.persistent or self.buffer_initialized:\n            return\n\n        if self.buffer_size &lt;= 0:\n            raise ValueError(\n                f\"Replay buffer size must be positive, got {self.buffer_size}\"\n            )\n\n        buffer_shape = (\n            self.buffer_size,\n        ) + data_shape_no_batch  # shape: [buffer_size, *data_shape]\n        print(f\"Initializing replay buffer with shape {buffer_shape}...\")\n\n        self.replay_buffer = (\n            torch.randn(buffer_shape, dtype=self.dtype, device=self.device)\n            * init_noise_scale\n        )\n\n        if self.init_steps &gt; 0:\n            print(f\"Running {self.init_steps} MCMC steps to populate buffer...\")\n            with torch.no_grad():\n                chunk_size = min(self.buffer_size, buffer_chunk_size)\n                for i in range(0, self.buffer_size, chunk_size):\n                    end = min(i + chunk_size, self.buffer_size)\n                    current_chunk = self.replay_buffer[i:end].clone()\n                    try:\n                        if self.use_mixed_precision and self.autocast_available:\n                            from torch.cuda.amp import autocast\n\n                            with autocast():\n                                updated_chunk = self.sampler.sample(\n                                    x=current_chunk, n_steps=self.init_steps\n                                ).detach()\n                        else:\n                            updated_chunk = self.sampler.sample(\n                                x=current_chunk, n_steps=self.init_steps\n                            ).detach()\n\n                        if updated_chunk.shape == current_chunk.shape:\n                            self.replay_buffer[i:end] = updated_chunk\n                        else:\n                            warnings.warn(\n                                f\"Sampler output shape mismatch during buffer init. Expected {current_chunk.shape}, got {updated_chunk.shape}. Skipping update for chunk {i}-{end}.\"\n                            )\n                    except Exception as e:\n                        warnings.warn(\n                            f\"Error during buffer initialization sampling for chunk {i}-{end}: {e}. Keeping noise for this chunk.\"\n                        )\n\n        self.buffer_ptr.zero_()\n        self.buffer_initialized = True\n        print(f\"Replay buffer initialized.\")\n\n        return self.replay_buffer\n\n    def get_start_points(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Gets the starting points for the MCMC sampler.\n\n        Handles both persistent (PCD) and non-persistent (CD-k) modes.\n        Initializes the buffer for PCD on the first call if needed.\n\n        Args:\n            x (torch.Tensor): The input data batch. Used directly for non-persistent CD\n                              and for shape inference/initialization trigger for PCD.\n\n        Returns:\n            torch.Tensor: The tensor of starting points for the sampler.\n        \"\"\"\n        x = x.to(device=self.device, dtype=self.dtype)\n\n        batch_size = x.shape[0]\n        data_shape_no_batch = x.shape[1:]\n\n        if self.persistent:\n            if not self.buffer_initialized:\n                self.initialize_buffer(data_shape_no_batch)\n                if not self.buffer_initialized:\n                    raise RuntimeError(\"Buffer initialization failed.\")\n\n            if self.buffer_size &lt; batch_size:\n                warnings.warn(\n                    f\"Buffer size ({self.buffer_size}) is smaller than batch size ({batch_size}). Sampling with replacement.\",\n                    UserWarning,\n                )\n                indices = torch.randint(\n                    0, self.buffer_size, (batch_size,), device=self.device\n                )\n            else:\n                # stratified sampling for better buffer coverage\n                stride = self.buffer_size // batch_size\n                base_indices = torch.arange(0, batch_size, device=self.device) * stride\n                offset = torch.randint(0, stride, (batch_size,), device=self.device)\n                indices = (base_indices + offset) % self.buffer_size\n\n            start_points = self.replay_buffer[indices].detach().clone()\n\n            # add some noise for exploration\n            if self.new_sample_ratio &gt; 0.0:\n                n_new = max(1, int(batch_size * self.new_sample_ratio))\n                noise_indices = torch.randperm(batch_size, device=self.device)[:n_new]\n                noise_scale = 0.01\n                start_points[noise_indices] = (\n                    start_points[noise_indices]\n                    + torch.randn_like(\n                        start_points[noise_indices],\n                        device=self.device,\n                        dtype=self.dtype,\n                    )\n                    * noise_scale\n                )\n        else:\n            # standard CD-k uses data as starting points\n            start_points = x.detach().clone()\n\n        return start_points\n\n    def get_negative_samples(self, x, batch_size, data_shape) -&gt; torch.Tensor:\n        \"\"\"Get negative samples using the replay buffer strategy.\n\n        Args:\n            batch_size: Number of samples to generate.\n            data_shape: Shape of the data samples (excluding batch size).\n\n        Returns:\n            torch.Tensor: Negative samples generated from the replay buffer.\n\n        \"\"\"\n        if not self.persistent or not self.buffer_initialized:\n            # For non-persistent CD, just return random noise\n            return torch.randn(\n                (batch_size,) + data_shape, dtype=self.dtype, device=self.device\n            )\n\n        n_new = max(1, int(batch_size * self.new_sample_ratio))\n        n_old = batch_size - n_new\n\n        all_samples = torch.empty(\n            (batch_size,) + data_shape, dtype=self.dtype, device=self.device\n        )\n\n        # new random samples\n        if n_new &gt; 0:\n            all_samples[:n_new] = torch.randn(\n                (n_new,) + data_shape, dtype=self.dtype, device=self.device\n            )\n\n        # samples from buffer\n        if n_old &gt; 0:\n\n            indices = torch.randint(0, self.buffer_size, (n_old,), device=self.device)\n            all_samples[n_new:] = self.replay_buffer[indices]\n\n        return all_samples\n\n    def update_buffer(self, samples: torch.Tensor) -&gt; None:\n        \"\"\"Update the replay buffer with new samples using FIFO strategy.\n\n        Args:\n            samples: New samples to add to the buffer.\n        \"\"\"\n        if not self.persistent or not self.buffer_initialized:\n            return\n\n        # Ensure samples are on the correct device and dtype\n        samples = samples.to(device=self.device, dtype=self.dtype).detach()\n\n        batch_size = samples.shape[0]\n\n        # FIFO strategy\n        ptr = int(self.buffer_ptr.item())\n\n        if batch_size &gt;= self.buffer_size:\n            # batch larger than buffer, use latest samples\n            self.replay_buffer[:] = samples[-self.buffer_size :].detach()\n            self.buffer_ptr[...] = 0\n        else:\n            # handle buffer wraparound\n            end_ptr = (ptr + batch_size) % self.buffer_size\n\n            if end_ptr &gt; ptr:\n                self.replay_buffer[ptr:end_ptr] = samples.detach()\n            else:\n                # wraparound case - split update\n                first_part = self.buffer_size - ptr\n                self.replay_buffer[ptr:] = samples[:first_part].detach()\n                self.replay_buffer[:end_ptr] = samples[first_part:].detach()\n\n            self.buffer_ptr[...] = end_ptr\n\n    @abstractmethod\n    def forward(\n        self, x: torch.Tensor, *args, **kwargs\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Compute CD loss given real data samples.\n\n        This method should implement the specifics of the contrastive divergence\n        variant, typically:\n        1. Generate negative samples using the MCMC sampler\n        2. Compute energies for real and negative samples\n        3. Calculate the contrastive loss\n\n        Args:\n            x: Real data samples (positive samples).\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]:\n                - loss: The contrastive divergence loss\n                - pred_x: Generated negative samples\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_loss(\n        self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the contrastive divergence loss from positive and negative samples.\n\n        This method defines how the loss is calculated given real samples (positive phase)\n        and samples from the model (negative phase). Typical implementations compute\n        the difference between mean energies of positive and negative samples.\n\n        Args:\n            x: Real data samples (positive samples).\n            pred_x: Generated negative samples.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            torch.Tensor: The contrastive divergence loss\n        \"\"\"\n        pass\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the loss function.\"\"\"\n        return f\"{self.__class__.__name__}(energy_function={self.energy_function}, sampler={self.sampler})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of the loss function.\"\"\"\n        return self.__repr__()\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = sampler\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.k_steps","title":"k_steps  <code>instance-attribute</code>","text":"<pre><code>k_steps = k_steps\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.persistent","title":"persistent  <code>instance-attribute</code>","text":"<pre><code>persistent = persistent\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.buffer_size","title":"buffer_size  <code>instance-attribute</code>","text":"<pre><code>buffer_size = buffer_size\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.new_sample_ratio","title":"new_sample_ratio  <code>instance-attribute</code>","text":"<pre><code>new_sample_ratio = new_sample_ratio\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.init_steps","title":"init_steps  <code>instance-attribute</code>","text":"<pre><code>init_steps = init_steps\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = to(device=device)\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.buffer_initialized","title":"buffer_initialized  <code>instance-attribute</code>","text":"<pre><code>buffer_initialized = False\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.initialize_buffer","title":"initialize_buffer","text":"<pre><code>initialize_buffer(data_shape_no_batch: Tuple[int, ...], buffer_chunk_size: int = 1024, init_noise_scale: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Initialize the replay buffer with random noise.</p> <p>For persistent CD variants, this method initializes the replay buffer with random noise. This is typically called the first time the loss is computed or when the batch size changes.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape_no_batch</code> <code>Tuple[int, ...]</code> <p>Shape of the data excluding batch dimension.</p> required <code>buffer_chunk_size</code> <code>int</code> <p>Size of the chunks to process during initialization.</p> <code>1024</code> <code>init_noise_scale</code> <code>float</code> <p>Scale of the initial noise.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The initialized buffer.</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>def initialize_buffer(\n    self,\n    data_shape_no_batch: Tuple[int, ...],\n    buffer_chunk_size: int = 1024,\n    init_noise_scale: float = 0.01,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Initialize the replay buffer with random noise.\n\n    For persistent CD variants, this method initializes the replay buffer\n    with random noise. This is typically called the first time the loss\n    is computed or when the batch size changes.\n\n    Args:\n        data_shape_no_batch: Shape of the data excluding batch dimension.\n        buffer_chunk_size: Size of the chunks to process during initialization.\n        init_noise_scale: Scale of the initial noise.\n\n    Returns:\n        The initialized buffer.\n    \"\"\"\n    if not self.persistent or self.buffer_initialized:\n        return\n\n    if self.buffer_size &lt;= 0:\n        raise ValueError(\n            f\"Replay buffer size must be positive, got {self.buffer_size}\"\n        )\n\n    buffer_shape = (\n        self.buffer_size,\n    ) + data_shape_no_batch  # shape: [buffer_size, *data_shape]\n    print(f\"Initializing replay buffer with shape {buffer_shape}...\")\n\n    self.replay_buffer = (\n        torch.randn(buffer_shape, dtype=self.dtype, device=self.device)\n        * init_noise_scale\n    )\n\n    if self.init_steps &gt; 0:\n        print(f\"Running {self.init_steps} MCMC steps to populate buffer...\")\n        with torch.no_grad():\n            chunk_size = min(self.buffer_size, buffer_chunk_size)\n            for i in range(0, self.buffer_size, chunk_size):\n                end = min(i + chunk_size, self.buffer_size)\n                current_chunk = self.replay_buffer[i:end].clone()\n                try:\n                    if self.use_mixed_precision and self.autocast_available:\n                        from torch.cuda.amp import autocast\n\n                        with autocast():\n                            updated_chunk = self.sampler.sample(\n                                x=current_chunk, n_steps=self.init_steps\n                            ).detach()\n                    else:\n                        updated_chunk = self.sampler.sample(\n                            x=current_chunk, n_steps=self.init_steps\n                        ).detach()\n\n                    if updated_chunk.shape == current_chunk.shape:\n                        self.replay_buffer[i:end] = updated_chunk\n                    else:\n                        warnings.warn(\n                            f\"Sampler output shape mismatch during buffer init. Expected {current_chunk.shape}, got {updated_chunk.shape}. Skipping update for chunk {i}-{end}.\"\n                        )\n                except Exception as e:\n                    warnings.warn(\n                        f\"Error during buffer initialization sampling for chunk {i}-{end}: {e}. Keeping noise for this chunk.\"\n                    )\n\n    self.buffer_ptr.zero_()\n    self.buffer_initialized = True\n    print(f\"Replay buffer initialized.\")\n\n    return self.replay_buffer\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.get_start_points","title":"get_start_points","text":"<pre><code>get_start_points(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Gets the starting points for the MCMC sampler.</p> <p>Handles both persistent (PCD) and non-persistent (CD-k) modes. Initializes the buffer for PCD on the first call if needed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data batch. Used directly for non-persistent CD               and for shape inference/initialization trigger for PCD.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The tensor of starting points for the sampler.</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>def get_start_points(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Gets the starting points for the MCMC sampler.\n\n    Handles both persistent (PCD) and non-persistent (CD-k) modes.\n    Initializes the buffer for PCD on the first call if needed.\n\n    Args:\n        x (torch.Tensor): The input data batch. Used directly for non-persistent CD\n                          and for shape inference/initialization trigger for PCD.\n\n    Returns:\n        torch.Tensor: The tensor of starting points for the sampler.\n    \"\"\"\n    x = x.to(device=self.device, dtype=self.dtype)\n\n    batch_size = x.shape[0]\n    data_shape_no_batch = x.shape[1:]\n\n    if self.persistent:\n        if not self.buffer_initialized:\n            self.initialize_buffer(data_shape_no_batch)\n            if not self.buffer_initialized:\n                raise RuntimeError(\"Buffer initialization failed.\")\n\n        if self.buffer_size &lt; batch_size:\n            warnings.warn(\n                f\"Buffer size ({self.buffer_size}) is smaller than batch size ({batch_size}). Sampling with replacement.\",\n                UserWarning,\n            )\n            indices = torch.randint(\n                0, self.buffer_size, (batch_size,), device=self.device\n            )\n        else:\n            # stratified sampling for better buffer coverage\n            stride = self.buffer_size // batch_size\n            base_indices = torch.arange(0, batch_size, device=self.device) * stride\n            offset = torch.randint(0, stride, (batch_size,), device=self.device)\n            indices = (base_indices + offset) % self.buffer_size\n\n        start_points = self.replay_buffer[indices].detach().clone()\n\n        # add some noise for exploration\n        if self.new_sample_ratio &gt; 0.0:\n            n_new = max(1, int(batch_size * self.new_sample_ratio))\n            noise_indices = torch.randperm(batch_size, device=self.device)[:n_new]\n            noise_scale = 0.01\n            start_points[noise_indices] = (\n                start_points[noise_indices]\n                + torch.randn_like(\n                    start_points[noise_indices],\n                    device=self.device,\n                    dtype=self.dtype,\n                )\n                * noise_scale\n            )\n    else:\n        # standard CD-k uses data as starting points\n        start_points = x.detach().clone()\n\n    return start_points\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.get_negative_samples","title":"get_negative_samples","text":"<pre><code>get_negative_samples(x, batch_size, data_shape) -&gt; torch.Tensor\n</code></pre> <p>Get negative samples using the replay buffer strategy.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <p>Number of samples to generate.</p> required <code>data_shape</code> <p>Shape of the data samples (excluding batch size).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Negative samples generated from the replay buffer.</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>def get_negative_samples(self, x, batch_size, data_shape) -&gt; torch.Tensor:\n    \"\"\"Get negative samples using the replay buffer strategy.\n\n    Args:\n        batch_size: Number of samples to generate.\n        data_shape: Shape of the data samples (excluding batch size).\n\n    Returns:\n        torch.Tensor: Negative samples generated from the replay buffer.\n\n    \"\"\"\n    if not self.persistent or not self.buffer_initialized:\n        # For non-persistent CD, just return random noise\n        return torch.randn(\n            (batch_size,) + data_shape, dtype=self.dtype, device=self.device\n        )\n\n    n_new = max(1, int(batch_size * self.new_sample_ratio))\n    n_old = batch_size - n_new\n\n    all_samples = torch.empty(\n        (batch_size,) + data_shape, dtype=self.dtype, device=self.device\n    )\n\n    # new random samples\n    if n_new &gt; 0:\n        all_samples[:n_new] = torch.randn(\n            (n_new,) + data_shape, dtype=self.dtype, device=self.device\n        )\n\n    # samples from buffer\n    if n_old &gt; 0:\n\n        indices = torch.randint(0, self.buffer_size, (n_old,), device=self.device)\n        all_samples[n_new:] = self.replay_buffer[indices]\n\n    return all_samples\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.update_buffer","title":"update_buffer","text":"<pre><code>update_buffer(samples: Tensor) -&gt; None\n</code></pre> <p>Update the replay buffer with new samples using FIFO strategy.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Tensor</code> <p>New samples to add to the buffer.</p> required Source code in <code>torchebm/core/base_loss.py</code> <pre><code>def update_buffer(self, samples: torch.Tensor) -&gt; None:\n    \"\"\"Update the replay buffer with new samples using FIFO strategy.\n\n    Args:\n        samples: New samples to add to the buffer.\n    \"\"\"\n    if not self.persistent or not self.buffer_initialized:\n        return\n\n    # Ensure samples are on the correct device and dtype\n    samples = samples.to(device=self.device, dtype=self.dtype).detach()\n\n    batch_size = samples.shape[0]\n\n    # FIFO strategy\n    ptr = int(self.buffer_ptr.item())\n\n    if batch_size &gt;= self.buffer_size:\n        # batch larger than buffer, use latest samples\n        self.replay_buffer[:] = samples[-self.buffer_size :].detach()\n        self.buffer_ptr[...] = 0\n    else:\n        # handle buffer wraparound\n        end_ptr = (ptr + batch_size) % self.buffer_size\n\n        if end_ptr &gt; ptr:\n            self.replay_buffer[ptr:end_ptr] = samples.detach()\n        else:\n            # wraparound case - split update\n            first_part = self.buffer_size - ptr\n            self.replay_buffer[ptr:] = samples[:first_part].detach()\n            self.replay_buffer[:end_ptr] = samples[first_part:].detach()\n\n        self.buffer_ptr[...] = end_ptr\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Compute CD loss given real data samples.</p> <p>This method should implement the specifics of the contrastive divergence variant, typically: 1. Generate negative samples using the MCMC sampler 2. Compute energies for real and negative samples 3. Calculate the contrastive loss</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Real data samples (positive samples).</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: - loss: The contrastive divergence loss - pred_x: Generated negative samples</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>@abstractmethod\ndef forward(\n    self, x: torch.Tensor, *args, **kwargs\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute CD loss given real data samples.\n\n    This method should implement the specifics of the contrastive divergence\n    variant, typically:\n    1. Generate negative samples using the MCMC sampler\n    2. Compute energies for real and negative samples\n    3. Calculate the contrastive loss\n\n    Args:\n        x: Real data samples (positive samples).\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]:\n            - loss: The contrastive divergence loss\n            - pred_x: Generated negative samples\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseContrastiveDivergence/#torchebm.core.base_loss.BaseContrastiveDivergence.compute_loss","title":"compute_loss  <code>abstractmethod</code>","text":"<pre><code>compute_loss(x: Tensor, pred_x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the contrastive divergence loss from positive and negative samples.</p> <p>This method defines how the loss is calculated given real samples (positive phase) and samples from the model (negative phase). Typical implementations compute the difference between mean energies of positive and negative samples.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Real data samples (positive samples).</p> required <code>pred_x</code> <code>Tensor</code> <p>Generated negative samples.</p> required <code>*args</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The contrastive divergence loss</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>@abstractmethod\ndef compute_loss(\n    self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the contrastive divergence loss from positive and negative samples.\n\n    This method defines how the loss is calculated given real samples (positive phase)\n    and samples from the model (negative phase). Typical implementations compute\n    the difference between mean energies of positive and negative samples.\n\n    Args:\n        x: Real data samples (positive samples).\n        pred_x: Generated negative samples.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        torch.Tensor: The contrastive divergence loss\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseLoss/","title":"BaseLoss","text":""},{"location":"api/torchebm/core/base_loss/classes/BaseLoss/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>DeviceMixin</code>, <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for loss functions used in energy-based models.</p> <p>This class builds on torch.nn.Module to allow loss functions to be part of PyTorch's computational graph and have trainable parameters if needed. It serves as the foundation for all loss functions in TorchEBM.</p> <p>Inheriting from torch.nn.Module ensures compatibility with PyTorch's training infrastructure, including device placement, parameter management, and gradient computation.</p> <p>Subclasses must implement the forward method to define the loss computation.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for computations</p> <code>None</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training (requires PyTorch 1.6+)</p> <code>False</code> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>class BaseLoss(DeviceMixin, nn.Module, ABC):\n    \"\"\"\n    Abstract base class for loss functions used in energy-based models.\n\n    This class builds on torch.nn.Module to allow loss functions to be part of PyTorch's\n    computational graph and have trainable parameters if needed. It serves as the foundation\n    for all loss functions in TorchEBM.\n\n    Inheriting from torch.nn.Module ensures compatibility with PyTorch's training\n    infrastructure, including device placement, parameter management, and gradient\n    computation.\n\n    Subclasses must implement the forward method to define the loss computation.\n\n    Args:\n        dtype: Data type for computations\n        device: Device for computations\n        use_mixed_precision: Whether to use mixed precision training (requires PyTorch 1.6+)\n    \"\"\"\n\n    def __init__(\n        self,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n        use_mixed_precision: bool = False,\n        clip_value: Optional[float] = None,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize the base loss class.\"\"\"\n        super().__init__(device=device, *args, **kwargs)\n\n        # if isinstance(device, str):\n        #     device = torch.device(device)\n        self.dtype = dtype\n        self.clip_value = clip_value\n        self.use_mixed_precision = use_mixed_precision\n\n        if self.use_mixed_precision:\n            try:\n                from torch.cuda.amp import autocast\n\n                self.autocast_available = True\n            except ImportError:\n                warnings.warn(\n                    \"Mixed precision training requested but torch.cuda.amp not available. \"\n                    \"Falling back to full precision. Requires PyTorch 1.6+.\",\n                    UserWarning,\n                )\n                self.use_mixed_precision = False\n                self.autocast_available = False\n        else:\n            self.autocast_available = False\n\n    # @property\n    # def device(self) -&gt; torch.device:\n    #     \"\"\"Returns the device associated with the module's parameters/buffers (if any).\"\"\"\n    #     try:\n    #         return next(self.parameters()).device\n    #     except StopIteration:\n    #         try:\n    #             return next(self.buffers()).device\n    #         except StopIteration:\n    #             return self._device\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the loss value given input data.\n\n        Args:\n            x: Input data tensor, typically real samples from the target distribution.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            torch.Tensor: The computed scalar loss value.\n        \"\"\"\n        pass\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the loss function.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of the loss function.\"\"\"\n        return self.__repr__()\n\n    def __call__(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"\n        Call the forward method of the loss function.\n\n        Args:\n            x: Input data tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            torch.Tensor: The computed loss value.\n        \"\"\"\n        x = x.to(device=self.device, dtype=self.dtype)\n\n        if (\n            hasattr(self, \"use_mixed_precision\")\n            and self.use_mixed_precision\n            and self.autocast_available\n        ):\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                loss = self.forward(x, *args, **kwargs)\n        else:\n            loss = self.forward(x, *args, **kwargs)\n\n        if self.clip_value:\n            loss = torch.clamp(loss, -self.clip_value, self.clip_value)\n        return loss\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseLoss/#torchebm.core.base_loss.BaseLoss.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseLoss/#torchebm.core.base_loss.BaseLoss.clip_value","title":"clip_value  <code>instance-attribute</code>","text":"<pre><code>clip_value = clip_value\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseLoss/#torchebm.core.base_loss.BaseLoss.use_mixed_precision","title":"use_mixed_precision  <code>instance-attribute</code>","text":"<pre><code>use_mixed_precision = use_mixed_precision\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseLoss/#torchebm.core.base_loss.BaseLoss.autocast_available","title":"autocast_available  <code>instance-attribute</code>","text":"<pre><code>autocast_available = True\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseLoss/#torchebm.core.base_loss.BaseLoss.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the loss value given input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor, typically real samples from the target distribution.</p> required <code>*args</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed scalar loss value.</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the loss value given input data.\n\n    Args:\n        x: Input data tensor, typically real samples from the target distribution.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        torch.Tensor: The computed scalar loss value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/","title":"BaseScoreMatching","text":""},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseLoss</code></p> <p>Abstract base class for Score Matching based loss functions.</p> <p>Score Matching is a family of methods for training energy-based models that avoid the sampling problem by directly matching the score function (gradient of log density). This class provides the common structure for different score matching variants including original score matching (Hyv\u00e4rinen), denoising score matching, sliced score matching, etc.</p> <p>Methods:</p> Name Description <code>- forward</code> <p>Computes the score matching loss given data samples</p> <code>- compute_score</code> <p>Computes score function (gradient of energy w.r.t. input)</p> <code>- compute_loss</code> <p>Computes the specific score matching loss variant</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>The energy function being trained</p> required <code>noise_scale</code> <code>float</code> <p>Scale of noise for perturbation (used in denoising variants)</p> <code>0.01</code> <code>regularization_strength</code> <code>float</code> <p>Coefficient for regularization terms</p> <code>0.0</code> <code>use_autograd</code> <code>bool</code> <p>Whether to use PyTorch autograd for computing derivatives</p> <code>True</code> <code>hutchinson_samples</code> <code>int</code> <p>Number of random samples for Hutchinson's trick in stochastic variants</p> <code>1</code> <code>custom_regularization</code> <code>Optional[Callable]</code> <p>Optional function for custom regularization (signature: f(loss, x, energy_fn) -&gt; loss)</p> <code>None</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training (requires PyTorch 1.6+)</p> <code>False</code> <code>dtype</code> <p>Data type for computations</p> required <code>device</code> <p>Device for computations</p> required <code>*args</code> <p>Additional positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>class BaseScoreMatching(BaseLoss):\n    \"\"\"\n    Abstract base class for Score Matching based loss functions.\n\n    Score Matching is a family of methods for training energy-based models that\n    avoid the sampling problem by directly matching the score function (gradient of log density).\n    This class provides the common structure for different score matching variants including\n    original score matching (Hyv\u00e4rinen), denoising score matching, sliced score matching, etc.\n\n    Methods:\n        - forward: Computes the score matching loss given data samples\n        - compute_score: Computes score function (gradient of energy w.r.t. input)\n        - compute_loss: Computes the specific score matching loss variant\n\n    Args:\n        energy_function: The energy function being trained\n        noise_scale: Scale of noise for perturbation (used in denoising variants)\n        regularization_strength: Coefficient for regularization terms\n        use_autograd: Whether to use PyTorch autograd for computing derivatives\n        hutchinson_samples: Number of random samples for Hutchinson's trick in stochastic variants\n        custom_regularization: Optional function for custom regularization (signature: f(loss, x, energy_fn) -&gt; loss)\n        use_mixed_precision: Whether to use mixed precision training (requires PyTorch 1.6+)\n        dtype: Data type for computations\n        device: Device for computations\n        *args: Additional positional arguments\n        **kwargs: Additional keyword arguments\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        noise_scale: float = 0.01,\n        regularization_strength: float = 0.0,\n        use_autograd: bool = True,\n        hutchinson_samples: int = 1,\n        custom_regularization: Optional[Callable] = None,\n        use_mixed_precision: bool = False,\n        clip_value: Optional[float] = None,\n        # dtype: torch.dtype = torch.float32,\n        # device: Optional[Union[str, torch.device]] = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            use_mixed_precision=use_mixed_precision,\n            clip_value=clip_value,\n            *args,\n            **kwargs,  # dtype=dtype, device=device,\n        )\n        self.energy_function = energy_function.to(device=self.device)\n        self.noise_scale = noise_scale\n        self.regularization_strength = regularization_strength\n        self.use_autograd = use_autograd\n        self.hutchinson_samples = hutchinson_samples\n        self.custom_regularization = custom_regularization\n        self.use_mixed_precision = use_mixed_precision\n\n        self.energy_function = self.energy_function.to(device=self.device)\n\n        if self.use_mixed_precision:\n            try:\n                from torch.cuda.amp import autocast\n\n                self.autocast_available = True\n            except ImportError:\n                warnings.warn(\n                    \"Mixed precision training requested but torch.cuda.amp not available. \"\n                    \"Falling back to full precision. Requires PyTorch 1.6+.\",\n                    UserWarning,\n                )\n                self.use_mixed_precision = False\n                self.autocast_available = False\n        else:\n            self.autocast_available = False\n\n    def compute_score(\n        self, x: torch.Tensor, noise: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the score function (gradient of energy function w.r.t. input).\n\n        Args:\n            x: Input data tensor\n            noise: Optional noise tensor for perturbed variants\n\n        Returns:\n            torch.Tensor: The score function evaluated at x\n        \"\"\"\n\n        x = x.to(device=self.device, dtype=self.dtype)\n\n        if noise is not None:\n            noise = noise.to(device=self.device, dtype=self.dtype)\n            x_perturbed = x + noise\n        else:\n            x_perturbed = x\n\n        if not x_perturbed.requires_grad:\n            x_perturbed.requires_grad_(True)\n\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                energy = self.energy_function(x_perturbed)\n        else:\n            energy = self.energy_function(x_perturbed)\n\n        if self.use_autograd:\n            score = torch.autograd.grad(energy.sum(), x_perturbed, create_graph=True)[0]\n        else:\n            raise NotImplementedError(\n                \"Custom gradient computation must be implemented in subclasses\"\n            )\n\n        return score\n\n    def perturb_data(\n        self, x: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:  # todo: add more noise types\n        \"\"\"\n        Perturb the input data with Gaussian noise for denoising variants.\n\n        Args:\n            x: Input data tensor\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Perturbed data and noise tensor\n        \"\"\"\n\n        x = x.to(device=self.device, dtype=self.dtype)\n        noise = (\n            torch.randn_like(x, device=self.device, dtype=self.dtype) * self.noise_scale\n        )\n        x_perturbed = x + noise\n        return x_perturbed, noise\n\n    def __call__(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"\n        Call the forward method of the loss function.\n\n        Args:\n            x: Input data tensor\n            *args: Additional positional arguments\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            torch.Tensor: The computed loss\n        \"\"\"\n\n        x = x.to(device=self.device, dtype=self.dtype)\n        return self.forward(x, *args, **kwargs)\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the score matching loss given input data.\n\n        Args:\n            x: Input data tensor\n            *args: Additional positional arguments\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            torch.Tensor: The computed score matching loss\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the specific score matching loss variant.\n\n        Args:\n            x: Input data tensor\n            *args: Additional positional arguments\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            torch.Tensor: The specific score matching loss\n        \"\"\"\n        pass\n\n    def add_regularization(\n        self,\n        loss: torch.Tensor,\n        x: torch.Tensor,\n        custom_reg_fn: Optional[Callable] = None,\n        reg_strength: Optional[float] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Add regularization terms to the loss.\n\n        Args:\n            loss: Current loss value\n            x: Input tensor\n            custom_reg_fn: Optional custom regularization function with signature f(x, energy_fn) -&gt; tensor\n            reg_strength: Optional regularization strength (overrides self.regularization_strength)\n\n        Returns:\n            regularized_loss: Loss with regularization\n        \"\"\"\n        strength = (\n            reg_strength if reg_strength is not None else self.regularization_strength\n        )\n\n        if strength &lt;= 0:\n            return loss\n\n        if custom_reg_fn is not None:\n            reg_term = custom_reg_fn(x, self.energy_function)\n\n        elif self.custom_regularization is not None:\n            reg_term = self.custom_regularization(x, self.energy_function)\n        # default: L2 norm of score\n        else:\n            score = self.compute_score(x)\n            reg_term = score.pow(2).sum(dim=list(range(1, len(x.shape)))).mean()\n\n        return loss + strength * reg_term\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the loss function.\"\"\"\n        return f\"{self.__class__.__name__}(energy_function={self.energy_function})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of the loss function.\"\"\"\n        return self.__repr__()\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.noise_scale","title":"noise_scale  <code>instance-attribute</code>","text":"<pre><code>noise_scale = noise_scale\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.regularization_strength","title":"regularization_strength  <code>instance-attribute</code>","text":"<pre><code>regularization_strength = regularization_strength\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.use_autograd","title":"use_autograd  <code>instance-attribute</code>","text":"<pre><code>use_autograd = use_autograd\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.hutchinson_samples","title":"hutchinson_samples  <code>instance-attribute</code>","text":"<pre><code>hutchinson_samples = hutchinson_samples\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.custom_regularization","title":"custom_regularization  <code>instance-attribute</code>","text":"<pre><code>custom_regularization = custom_regularization\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.use_mixed_precision","title":"use_mixed_precision  <code>instance-attribute</code>","text":"<pre><code>use_mixed_precision = use_mixed_precision\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = to(device=device)\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.autocast_available","title":"autocast_available  <code>instance-attribute</code>","text":"<pre><code>autocast_available = True\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.compute_score","title":"compute_score","text":"<pre><code>compute_score(x: Tensor, noise: Optional[Tensor] = None) -&gt; torch.Tensor\n</code></pre> <p>Compute the score function (gradient of energy function w.r.t. input).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor</p> required <code>noise</code> <code>Optional[Tensor]</code> <p>Optional noise tensor for perturbed variants</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The score function evaluated at x</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>def compute_score(\n    self, x: torch.Tensor, noise: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the score function (gradient of energy function w.r.t. input).\n\n    Args:\n        x: Input data tensor\n        noise: Optional noise tensor for perturbed variants\n\n    Returns:\n        torch.Tensor: The score function evaluated at x\n    \"\"\"\n\n    x = x.to(device=self.device, dtype=self.dtype)\n\n    if noise is not None:\n        noise = noise.to(device=self.device, dtype=self.dtype)\n        x_perturbed = x + noise\n    else:\n        x_perturbed = x\n\n    if not x_perturbed.requires_grad:\n        x_perturbed.requires_grad_(True)\n\n    if self.use_mixed_precision and self.autocast_available:\n        from torch.cuda.amp import autocast\n\n        with autocast():\n            energy = self.energy_function(x_perturbed)\n    else:\n        energy = self.energy_function(x_perturbed)\n\n    if self.use_autograd:\n        score = torch.autograd.grad(energy.sum(), x_perturbed, create_graph=True)[0]\n    else:\n        raise NotImplementedError(\n            \"Custom gradient computation must be implemented in subclasses\"\n        )\n\n    return score\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.perturb_data","title":"perturb_data","text":"<pre><code>perturb_data(x: Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Perturb the input data with Gaussian noise for denoising variants.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: Perturbed data and noise tensor</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>def perturb_data(\n    self, x: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:  # todo: add more noise types\n    \"\"\"\n    Perturb the input data with Gaussian noise for denoising variants.\n\n    Args:\n        x: Input data tensor\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Perturbed data and noise tensor\n    \"\"\"\n\n    x = x.to(device=self.device, dtype=self.dtype)\n    noise = (\n        torch.randn_like(x, device=self.device, dtype=self.dtype) * self.noise_scale\n    )\n    x_perturbed = x + noise\n    return x_perturbed, noise\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the score matching loss given input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor</p> required <code>*args</code> <p>Additional positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed score matching loss</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the score matching loss given input data.\n\n    Args:\n        x: Input data tensor\n        *args: Additional positional arguments\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        torch.Tensor: The computed score matching loss\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.compute_loss","title":"compute_loss  <code>abstractmethod</code>","text":"<pre><code>compute_loss(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the specific score matching loss variant.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor</p> required <code>*args</code> <p>Additional positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The specific score matching loss</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>@abstractmethod\ndef compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the specific score matching loss variant.\n\n    Args:\n        x: Input data tensor\n        *args: Additional positional arguments\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        torch.Tensor: The specific score matching loss\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_loss/classes/BaseScoreMatching/#torchebm.core.base_loss.BaseScoreMatching.add_regularization","title":"add_regularization","text":"<pre><code>add_regularization(loss: Tensor, x: Tensor, custom_reg_fn: Optional[Callable] = None, reg_strength: Optional[float] = None) -&gt; torch.Tensor\n</code></pre> <p>Add regularization terms to the loss.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <p>Current loss value</p> required <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <code>custom_reg_fn</code> <code>Optional[Callable]</code> <p>Optional custom regularization function with signature f(x, energy_fn) -&gt; tensor</p> <code>None</code> <code>reg_strength</code> <code>Optional[float]</code> <p>Optional regularization strength (overrides self.regularization_strength)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>regularized_loss</code> <code>Tensor</code> <p>Loss with regularization</p> Source code in <code>torchebm/core/base_loss.py</code> <pre><code>def add_regularization(\n    self,\n    loss: torch.Tensor,\n    x: torch.Tensor,\n    custom_reg_fn: Optional[Callable] = None,\n    reg_strength: Optional[float] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Add regularization terms to the loss.\n\n    Args:\n        loss: Current loss value\n        x: Input tensor\n        custom_reg_fn: Optional custom regularization function with signature f(x, energy_fn) -&gt; tensor\n        reg_strength: Optional regularization strength (overrides self.regularization_strength)\n\n    Returns:\n        regularized_loss: Loss with regularization\n    \"\"\"\n    strength = (\n        reg_strength if reg_strength is not None else self.regularization_strength\n    )\n\n    if strength &lt;= 0:\n        return loss\n\n    if custom_reg_fn is not None:\n        reg_term = custom_reg_fn(x, self.energy_function)\n\n    elif self.custom_regularization is not None:\n        reg_term = self.custom_regularization(x, self.energy_function)\n    # default: L2 norm of score\n    else:\n        score = self.compute_score(x)\n        reg_term = score.pow(2).sum(dim=list(range(1, len(x.shape)))).mean()\n\n    return loss + strength * reg_term\n</code></pre>"},{"location":"api/torchebm/core/base_metric/","title":"Torchebm &gt; Core &gt; Base_metric","text":""},{"location":"api/torchebm/core/base_metric/#torchebm-core-base_metric","title":"Torchebm &gt; Core &gt; Base_metric","text":""},{"location":"api/torchebm/core/base_metric/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_metric/#classes","title":"Classes","text":"<ul> <li><code>BaseMetric</code> - Abstract base class for all evaluation metrics in torchebm.</li> <li><code>EnergySampleMetric</code> - Base class for metrics that evaluate energy functions and samples.</li> <li><code>SampleQualityMetric</code> - Base class for metrics that evaluate the quality of samples.</li> </ul>"},{"location":"api/torchebm/core/base_metric/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_metric/#torchebm.core.base_metric","title":"torchebm.core.base_metric","text":""},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/","title":"BaseMetric","text":""},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all evaluation metrics in torchebm.</p> <p>This class defines the standard interface for implementing evaluation metrics for energy-based models. All metrics should inherit from this class and implement the required methods.</p> Source code in <code>torchebm/core/base_metric.py</code> <pre><code>class BaseMetric(ABC):\n    \"\"\"\n    Abstract base class for all evaluation metrics in torchebm.\n\n    This class defines the standard interface for implementing evaluation metrics\n    for energy-based models. All metrics should inherit from this class and implement\n    the required methods.\n\n    Attributes:\n        name (str): The name of the metric\n        lower_is_better (bool): Whether lower values indicate better performance\n    \"\"\"\n\n    def __init__(self, name: str, lower_is_better: bool = True):\n        \"\"\"\n        Initialize the base metric.\n\n        Args:\n            name (str): Name of the metric\n            lower_is_better (bool): Whether lower values indicate better performance\n        \"\"\"\n        self.name = name\n        self.lower_is_better = lower_is_better\n        self._device = None\n\n    @property\n    def device(self) -&gt; Optional[torch.device]:\n        \"\"\"Returns the device associated with the metric.\"\"\"\n        return self._device\n\n    def to(self, device: Union[str, torch.device]) -&gt; \"BaseMetric\":\n        \"\"\"\n        Move the metric to the specified device.\n\n        Args:\n            device: The device to move the metric to\n\n        Returns:\n            self: The metric instance moved to the device\n        \"\"\"\n        if isinstance(device, str):\n            device = torch.device(device)\n        self._device = device\n        return self\n\n    @abstractmethod\n    def __call__(self, *args, **kwargs) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute the metric value.\n\n        Returns:\n            Dict[str, torch.Tensor]: Dictionary containing the computed metric values\n        \"\"\"\n        pass\n\n    def compute(self, *args, **kwargs) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Alias for __call__ to match sklearn and other libraries' conventions.\n\n        Returns:\n            Dict[str, torch.Tensor]: Dictionary containing the computed metric values\n        \"\"\"\n        return self(*args, **kwargs)\n</code></pre>"},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/#torchebm.core.base_metric.BaseMetric.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/#torchebm.core.base_metric.BaseMetric.lower_is_better","title":"lower_is_better  <code>instance-attribute</code>","text":"<pre><code>lower_is_better = lower_is_better\n</code></pre>"},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/#torchebm.core.base_metric.BaseMetric._device","title":"_device  <code>instance-attribute</code>","text":"<pre><code>_device = None\n</code></pre>"},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/#torchebm.core.base_metric.BaseMetric.device","title":"device  <code>property</code>","text":"<pre><code>device: Optional[device]\n</code></pre> <p>Returns the device associated with the metric.</p>"},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/#torchebm.core.base_metric.BaseMetric.to","title":"to","text":"<pre><code>to(device: Union[str, device]) -&gt; BaseMetric\n</code></pre> <p>Move the metric to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, device]</code> <p>The device to move the metric to</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>BaseMetric</code> <p>The metric instance moved to the device</p> Source code in <code>torchebm/core/base_metric.py</code> <pre><code>def to(self, device: Union[str, torch.device]) -&gt; \"BaseMetric\":\n    \"\"\"\n    Move the metric to the specified device.\n\n    Args:\n        device: The device to move the metric to\n\n    Returns:\n        self: The metric instance moved to the device\n    \"\"\"\n    if isinstance(device, str):\n        device = torch.device(device)\n    self._device = device\n    return self\n</code></pre>"},{"location":"api/torchebm/core/base_metric/classes/BaseMetric/#torchebm.core.base_metric.BaseMetric.compute","title":"compute","text":"<pre><code>compute(*args, **kwargs) -&gt; Dict[str, torch.Tensor]\n</code></pre> <p>Alias for call to match sklearn and other libraries' conventions.</p> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, torch.Tensor]: Dictionary containing the computed metric values</p> Source code in <code>torchebm/core/base_metric.py</code> <pre><code>def compute(self, *args, **kwargs) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Alias for __call__ to match sklearn and other libraries' conventions.\n\n    Returns:\n        Dict[str, torch.Tensor]: Dictionary containing the computed metric values\n    \"\"\"\n    return self(*args, **kwargs)\n</code></pre>"},{"location":"api/torchebm/core/base_metric/classes/EnergySampleMetric/","title":"EnergySampleMetric","text":""},{"location":"api/torchebm/core/base_metric/classes/EnergySampleMetric/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Base class for metrics that evaluate energy functions and samples.</p> <p>This class extends BaseMetric for metrics that specifically work with energy-based models by evaluating energy functions and sample quality.</p> Source code in <code>torchebm/core/base_metric.py</code> <pre><code>class EnergySampleMetric(BaseMetric):\n    \"\"\"\n    Base class for metrics that evaluate energy functions and samples.\n\n    This class extends BaseMetric for metrics that specifically work with\n    energy-based models by evaluating energy functions and sample quality.\n    \"\"\"\n\n    def __init__(self, name: str, lower_is_better: bool = True):\n        \"\"\"\n        Initialize the energy sample metric.\n\n        Args:\n            name (str): Name of the metric\n            lower_is_better (bool): Whether lower values indicate better performance\n        \"\"\"\n        super().__init__(name=name, lower_is_better=lower_is_better)\n\n    @abstractmethod\n    def __call__(\n        self, energy_fn: torch.nn.Module, samples: torch.Tensor, *args, **kwargs\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute the metric value for energy functions and samples.\n\n        Args:\n            energy_fn: The energy function to evaluate\n            samples: Tensor of samples, batch_shape (n_samples, *dim)\n\n        Returns:\n            Dict[str, torch.Tensor]: Dictionary containing the computed metric values\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/torchebm/core/base_metric/classes/SampleQualityMetric/","title":"SampleQualityMetric","text":""},{"location":"api/torchebm/core/base_metric/classes/SampleQualityMetric/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Base class for metrics that evaluate the quality of samples.</p> <p>This class extends BaseMetric for metrics that specifically evaluate sample quality by comparing with reference data.</p> Source code in <code>torchebm/core/base_metric.py</code> <pre><code>class SampleQualityMetric(BaseMetric):\n    \"\"\"\n    Base class for metrics that evaluate the quality of samples.\n\n    This class extends BaseMetric for metrics that specifically evaluate\n    sample quality by comparing with reference data.\n    \"\"\"\n\n    def __init__(self, name: str, lower_is_better: bool = True):\n        \"\"\"\n        Initialize the sample quality metric.\n\n        Args:\n            name (str): Name of the metric\n            lower_is_better (bool): Whether lower values indicate better performance\n        \"\"\"\n        super().__init__(name=name, lower_is_better=lower_is_better)\n\n    @abstractmethod\n    def __call__(\n        self, samples: torch.Tensor, reference: torch.Tensor, *args, **kwargs\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute the metric value for samples against reference data.\n\n        Args:\n            samples: Tensor of samples to evaluate, batch_shape (n_samples, *dim)\n            reference: Tensor of reference data, batch_shape (n_reference, *dim)\n\n        Returns:\n            Dict[str, torch.Tensor]: Dictionary containing the computed metric values\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/torchebm/core/base_optimizer/","title":"Torchebm &gt; Core &gt; Base_optimizer","text":""},{"location":"api/torchebm/core/base_optimizer/#torchebm-core-base_optimizer","title":"Torchebm &gt; Core &gt; Base_optimizer","text":""},{"location":"api/torchebm/core/base_optimizer/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_optimizer/#classes","title":"Classes","text":"<ul> <li><code>Optimizer</code> - No description available.</li> </ul>"},{"location":"api/torchebm/core/base_optimizer/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_optimizer/#torchebm.core.base_optimizer","title":"torchebm.core.base_optimizer","text":""},{"location":"api/torchebm/core/base_optimizer/classes/Optimizer/","title":"Optimizer","text":""},{"location":"api/torchebm/core/base_optimizer/classes/Optimizer/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Module</code></p> Source code in <code>torchebm/core/base_optimizer.py</code> <pre><code>class Optimizer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def step(self):\n        pass\n\n    @abstractmethod\n    def zero_grad(self):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state_dict):\n        pass\n\n    @abstractmethod\n    def to(self, device):\n        pass\n</code></pre>"},{"location":"api/torchebm/core/base_optimizer/classes/Optimizer/#torchebm.core.base_optimizer.Optimizer.step","title":"step  <code>abstractmethod</code>","text":"<pre><code>step()\n</code></pre> Source code in <code>torchebm/core/base_optimizer.py</code> <pre><code>@abstractmethod\ndef step(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_optimizer/classes/Optimizer/#torchebm.core.base_optimizer.Optimizer.zero_grad","title":"zero_grad  <code>abstractmethod</code>","text":"<pre><code>zero_grad()\n</code></pre> Source code in <code>torchebm/core/base_optimizer.py</code> <pre><code>@abstractmethod\ndef zero_grad(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_optimizer/classes/Optimizer/#torchebm.core.base_optimizer.Optimizer.state_dict","title":"state_dict  <code>abstractmethod</code>","text":"<pre><code>state_dict()\n</code></pre> Source code in <code>torchebm/core/base_optimizer.py</code> <pre><code>@abstractmethod\ndef state_dict(self):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_optimizer/classes/Optimizer/#torchebm.core.base_optimizer.Optimizer.load_state_dict","title":"load_state_dict  <code>abstractmethod</code>","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> Source code in <code>torchebm/core/base_optimizer.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_optimizer/classes/Optimizer/#torchebm.core.base_optimizer.Optimizer.to","title":"to  <code>abstractmethod</code>","text":"<pre><code>to(device)\n</code></pre> Source code in <code>torchebm/core/base_optimizer.py</code> <pre><code>@abstractmethod\ndef to(self, device):\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/","title":"Torchebm &gt; Core &gt; Base_sampler","text":""},{"location":"api/torchebm/core/base_sampler/#torchebm-core-base_sampler","title":"Torchebm &gt; Core &gt; Base_sampler","text":""},{"location":"api/torchebm/core/base_sampler/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_sampler/#classes","title":"Classes","text":"<ul> <li><code>BaseSampler</code> - Base class for samplers.</li> </ul>"},{"location":"api/torchebm/core/base_sampler/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_sampler/#torchebm.core.base_sampler","title":"torchebm.core.base_sampler","text":""},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/","title":"BaseSampler","text":""},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>DeviceMixin</code>, <code>ABC</code></p> <p>Base class for samplers.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to sample from.</p> required <code>dtype</code> <code>dtype</code> <p>Data type to use for the computations.</p> <code>float32</code> <code>device</code> <code>Union[str, device]</code> <p>Device to run the computations on (e.g., \"cpu\" or \"cuda\").</p> <code>None</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision for sampling operations.</p> <code>False</code> <p>Methods:</p> Name Description <code>sample</code> <p>Run the sampling process.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics dictionary.</p> <code>to</code> <p>Move sampler to specified device.</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>class BaseSampler(DeviceMixin, ABC):\n    \"\"\"\n    Base class for samplers.\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to sample from.\n        dtype (torch.dtype): Data type to use for the computations.\n        device (Union[str, torch.device]): Device to run the computations on (e.g., \"cpu\" or \"cuda\").\n        use_mixed_precision (bool): Whether to use mixed precision for sampling operations.\n\n    Methods:\n        sample(x, dim, k_steps, n_samples, thin, return_trajectory, return_diagnostics): Run the sampling process.\n        sample_chain(dim, k_steps, n_samples, thin, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(): Initialize the diagnostics dictionary.\n        to(device): Move sampler to specified device.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n        use_mixed_precision: bool = False,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(device=device, dtype=dtype, *args, **kwargs)\n        self.energy_function = energy_function\n        self.dtype = dtype\n        # if isinstance(device, str):\n        #     device = torch.device(device)\n        # self.device = device or torch.device(\n        #     \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        # )\n        self.use_mixed_precision = use_mixed_precision\n\n        # Check if mixed precision is available\n        if self.use_mixed_precision:\n            try:\n                from torch.cuda.amp import autocast\n\n                self.autocast_available = True\n                # Ensure device is CUDA for mixed precision\n                if not self.device.type.startswith(\"cuda\"):\n                    warnings.warn(\n                        f\"Mixed precision requested but device is {self.device}. \"\n                        f\"Mixed precision requires CUDA. Falling back to full precision.\",\n                        UserWarning,\n                    )\n                    self.use_mixed_precision = False\n                    self.autocast_available = False\n            except ImportError:\n                warnings.warn(\n                    \"Mixed precision requested but torch.cuda.amp not available. \"\n                    \"Falling back to full precision. Requires PyTorch 1.6+.\",\n                    UserWarning,\n                )\n                self.use_mixed_precision = False\n                self.autocast_available = False\n        else:\n            self.autocast_available = False\n\n        self.schedulers: Dict[str, BaseScheduler] = {}\n\n        # Align child components using the mixin helper\n        self.energy_function = DeviceMixin.safe_to(\n            self.energy_function, device=self.device, dtype=self.dtype\n        )\n\n        # Ensure the energy function has matching precision settings\n        if hasattr(self.energy_function, \"use_mixed_precision\"):\n            self.energy_function.use_mixed_precision = self.use_mixed_precision\n\n    @abstractmethod\n    def sample(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        \"\"\"\n        Run the sampling process.\n\n        Args:\n            x: Initial state to start the sampling from.\n            dim: Dimension of the state space.\n            k_steps: Number of steps to take between samples.\n            n_samples: Number of samples to generate.\n            thin: Thinning factor (not supported yet).\n            return_trajectory: Whether to return the trajectory of the samples.\n            return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n        Returns:\n            torch.Tensor: Samples from the sampler.\n            List[dict]: Diagnostics of the sampling process.\n        \"\"\"\n        raise NotImplementedError\n\n    def register_scheduler(self, name: str, scheduler: BaseScheduler) -&gt; None:\n        \"\"\"\n        Register a parameter scheduler.\n\n        Args:\n            name: Name of the parameter to schedule\n            scheduler: Scheduler instance to use\n        \"\"\"\n        self.schedulers[name] = scheduler\n\n    def get_schedulers(self) -&gt; Dict[str, BaseScheduler]:\n        \"\"\"\n        Get all registered schedulers.\n\n        Returns:\n            Dictionary mapping parameter names to their schedulers\n        \"\"\"\n        return self.schedulers\n\n    def get_scheduled_value(self, name: str) -&gt; float:\n        \"\"\"\n        Get current value for a scheduled parameter.\n\n        Args:\n            name: Name of the scheduled parameter\n\n        Returns:\n            Current value of the parameter\n\n        Raises:\n            KeyError: If no scheduler exists for the parameter\n        \"\"\"\n        if name not in self.schedulers:\n            raise KeyError(f\"No scheduler registered for parameter '{name}'\")\n        return self.schedulers[name].get_value()\n\n    def step_schedulers(self) -&gt; Dict[str, float]:\n        \"\"\"\n        Advance all schedulers by one step.\n\n        Returns:\n            Dictionary mapping parameter names to their updated values\n        \"\"\"\n        return {name: scheduler.step() for name, scheduler in self.schedulers.items()}\n\n    def reset_schedulers(self) -&gt; None:\n        \"\"\"Reset all schedulers to their initial state.\"\"\"\n        for scheduler in self.schedulers.values():\n            scheduler.reset()\n\n    # @abstractmethod\n    def _setup_diagnostics(self) -&gt; dict:\n        \"\"\"\n        Initialize the diagnostics dictionary.\n\n            .. deprecated:: 1.0\n               This method is deprecated and will be removed in a future version.\n        \"\"\"\n        return {\n            \"energies\": torch.empty(0, device=self.device, dtype=self.dtype),\n            \"acceptance_rate\": torch.tensor(0.0, device=self.device, dtype=self.dtype),\n        }\n        # raise NotImplementedError\n\n    # def to(\n    #     self, device: Union[str, torch.device], dtype: Optional[torch.dtype] = None\n    # ) -&gt; \"BaseSampler\":\n    #     \"\"\"\n    #     Move sampler to the specified device and optionally change its dtype.\n    #\n    #     Args:\n    #         device: Target device for computations\n    #         dtype: Optional data type to convert to\n    #\n    #     Returns:\n    #         The sampler instance moved to the specified device/dtype\n    #     \"\"\"\n    #     if isinstance(device, str):\n    #         device = torch.device(device)\n    #\n    #     self.device = device\n    #\n    #     if dtype is not None:\n    #         self.dtype = dtype\n    #\n    #     # Update mixed precision availability if device changed\n    #     if self.use_mixed_precision and not self.device.type.startswith(\"cuda\"):\n    #         warnings.warn(\n    #             f\"Mixed precision active but moving to {self.device}. \"\n    #             f\"Mixed precision requires CUDA. Disabling mixed precision.\",\n    #             UserWarning,\n    #         )\n    #         self.use_mixed_precision = False\n    #\n    #     # Move energy function if it has a to method\n    #     if hasattr(self.energy_function, \"to\") and callable(\n    #         getattr(self.energy_function, \"to\")\n    #     ):\n    #         self.energy_function = self.energy_function.to(\n    #             device=self.device, dtype=self.dtype\n    #         )\n    #\n    #     return self\n\n    def apply_mixed_precision(self, func):\n        \"\"\"\n        Decorator to apply mixed precision context to a method.\n\n        Args:\n            func: Function to wrap with mixed precision\n\n        Returns:\n            Wrapped function with mixed precision support\n        \"\"\"\n\n        def wrapper(*args, **kwargs):\n            if self.use_mixed_precision and self.autocast_available:\n                from torch.cuda.amp import autocast\n\n                with autocast():\n                    return func(*args, **kwargs)\n            else:\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    def to(self, *args, **kwargs):\n        \"\"\"Move sampler and its children; update mixin state and propagate.\"\"\"\n        # Let DeviceMixin update internal state and parent class handle movement\n        result = super().to(*args, **kwargs)\n        # After move, make sure energy_function follows\n        self.energy_function = DeviceMixin.safe_to(\n            self.energy_function, device=self.device, dtype=self.dtype\n        )\n        return result\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.use_mixed_precision","title":"use_mixed_precision  <code>instance-attribute</code>","text":"<pre><code>use_mixed_precision = use_mixed_precision\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.autocast_available","title":"autocast_available  <code>instance-attribute</code>","text":"<pre><code>autocast_available = True\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.schedulers","title":"schedulers  <code>instance-attribute</code>","text":"<pre><code>schedulers: Dict[str, BaseScheduler] = {}\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = safe_to(energy_function, device=device, dtype=dtype)\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(x: Optional[Tensor] = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> <p>Run the sampling process.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Initial state to start the sampling from.</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of the state space.</p> <code>10</code> <code>k_steps</code> <p>Number of steps to take between samples.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>1</code> <code>thin</code> <code>int</code> <p>Thinning factor (not supported yet).</p> <code>1</code> <code>return_trajectory</code> <code>bool</code> <p>Whether to return the trajectory of the samples.</p> <code>False</code> <code>return_diagnostics</code> <code>bool</code> <p>Whether to return the diagnostics of the sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>torch.Tensor: Samples from the sampler.</p> <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>List[dict]: Diagnostics of the sampling process.</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    thin: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    \"\"\"\n    Run the sampling process.\n\n    Args:\n        x: Initial state to start the sampling from.\n        dim: Dimension of the state space.\n        k_steps: Number of steps to take between samples.\n        n_samples: Number of samples to generate.\n        thin: Thinning factor (not supported yet).\n        return_trajectory: Whether to return the trajectory of the samples.\n        return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n    Returns:\n        torch.Tensor: Samples from the sampler.\n        List[dict]: Diagnostics of the sampling process.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.register_scheduler","title":"register_scheduler","text":"<pre><code>register_scheduler(name: str, scheduler: BaseScheduler) -&gt; None\n</code></pre> <p>Register a parameter scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the parameter to schedule</p> required <code>scheduler</code> <code>BaseScheduler</code> <p>Scheduler instance to use</p> required Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def register_scheduler(self, name: str, scheduler: BaseScheduler) -&gt; None:\n    \"\"\"\n    Register a parameter scheduler.\n\n    Args:\n        name: Name of the parameter to schedule\n        scheduler: Scheduler instance to use\n    \"\"\"\n    self.schedulers[name] = scheduler\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.get_schedulers","title":"get_schedulers","text":"<pre><code>get_schedulers() -&gt; Dict[str, BaseScheduler]\n</code></pre> <p>Get all registered schedulers.</p> <p>Returns:</p> Type Description <code>Dict[str, BaseScheduler]</code> <p>Dictionary mapping parameter names to their schedulers</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def get_schedulers(self) -&gt; Dict[str, BaseScheduler]:\n    \"\"\"\n    Get all registered schedulers.\n\n    Returns:\n        Dictionary mapping parameter names to their schedulers\n    \"\"\"\n    return self.schedulers\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.get_scheduled_value","title":"get_scheduled_value","text":"<pre><code>get_scheduled_value(name: str) -&gt; float\n</code></pre> <p>Get current value for a scheduled parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the scheduled parameter</p> required <p>Returns:</p> Type Description <code>float</code> <p>Current value of the parameter</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no scheduler exists for the parameter</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def get_scheduled_value(self, name: str) -&gt; float:\n    \"\"\"\n    Get current value for a scheduled parameter.\n\n    Args:\n        name: Name of the scheduled parameter\n\n    Returns:\n        Current value of the parameter\n\n    Raises:\n        KeyError: If no scheduler exists for the parameter\n    \"\"\"\n    if name not in self.schedulers:\n        raise KeyError(f\"No scheduler registered for parameter '{name}'\")\n    return self.schedulers[name].get_value()\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.step_schedulers","title":"step_schedulers","text":"<pre><code>step_schedulers() -&gt; Dict[str, float]\n</code></pre> <p>Advance all schedulers by one step.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary mapping parameter names to their updated values</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def step_schedulers(self) -&gt; Dict[str, float]:\n    \"\"\"\n    Advance all schedulers by one step.\n\n    Returns:\n        Dictionary mapping parameter names to their updated values\n    \"\"\"\n    return {name: scheduler.step() for name, scheduler in self.schedulers.items()}\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.reset_schedulers","title":"reset_schedulers","text":"<pre><code>reset_schedulers() -&gt; None\n</code></pre> <p>Reset all schedulers to their initial state.</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def reset_schedulers(self) -&gt; None:\n    \"\"\"Reset all schedulers to their initial state.\"\"\"\n    for scheduler in self.schedulers.values():\n        scheduler.reset()\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler._setup_diagnostics","title":"_setup_diagnostics","text":"<pre><code>_setup_diagnostics() -&gt; dict\n</code></pre> <p>Initialize the diagnostics dictionary.</p> <pre><code>.. deprecated:: 1.0\n   This method is deprecated and will be removed in a future version.\n</code></pre> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def _setup_diagnostics(self) -&gt; dict:\n    \"\"\"\n    Initialize the diagnostics dictionary.\n\n        .. deprecated:: 1.0\n           This method is deprecated and will be removed in a future version.\n    \"\"\"\n    return {\n        \"energies\": torch.empty(0, device=self.device, dtype=self.dtype),\n        \"acceptance_rate\": torch.tensor(0.0, device=self.device, dtype=self.dtype),\n    }\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.apply_mixed_precision","title":"apply_mixed_precision","text":"<pre><code>apply_mixed_precision(func)\n</code></pre> <p>Decorator to apply mixed precision context to a method.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>Function to wrap with mixed precision</p> required <p>Returns:</p> Type Description <p>Wrapped function with mixed precision support</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def apply_mixed_precision(self, func):\n    \"\"\"\n    Decorator to apply mixed precision context to a method.\n\n    Args:\n        func: Function to wrap with mixed precision\n\n    Returns:\n        Wrapped function with mixed precision support\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                return func(*args, **kwargs)\n        else:\n            return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/torchebm/core/base_sampler/classes/BaseSampler/#torchebm.core.base_sampler.BaseSampler.to","title":"to","text":"<pre><code>to(*args, **kwargs)\n</code></pre> <p>Move sampler and its children; update mixin state and propagate.</p> Source code in <code>torchebm/core/base_sampler.py</code> <pre><code>def to(self, *args, **kwargs):\n    \"\"\"Move sampler and its children; update mixin state and propagate.\"\"\"\n    # Let DeviceMixin update internal state and parent class handle movement\n    result = super().to(*args, **kwargs)\n    # After move, make sure energy_function follows\n    self.energy_function = DeviceMixin.safe_to(\n        self.energy_function, device=self.device, dtype=self.dtype\n    )\n    return result\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/","title":"Torchebm &gt; Core &gt; Base_scheduler","text":""},{"location":"api/torchebm/core/base_scheduler/#torchebm-core-base_scheduler","title":"Torchebm &gt; Core &gt; Base_scheduler","text":""},{"location":"api/torchebm/core/base_scheduler/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_scheduler/#classes","title":"Classes","text":"<ul> <li><code>BaseScheduler</code> - Abstract base class for parameter schedulers.</li> <li><code>ConstantScheduler</code> - Scheduler that maintains a constant parameter value.</li> <li><code>CosineScheduler</code> - Scheduler with cosine annealing.</li> <li><code>ExponentialDecayScheduler</code> - Scheduler with exponential decay.</li> <li><code>LinearScheduler</code> - Scheduler with linear interpolation between start and end values.</li> <li><code>MultiStepScheduler</code> - Scheduler that reduces the parameter value at specific milestone steps.</li> <li><code>WarmupScheduler</code> - Scheduler that combines linear warmup with another scheduler.</li> </ul>"},{"location":"api/torchebm/core/base_scheduler/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_scheduler/#torchebm.core.base_scheduler","title":"torchebm.core.base_scheduler","text":"<p>Parameter schedulers for MCMC samplers and optimization algorithms.</p> <p>This module provides a comprehensive set of parameter schedulers that can be used to dynamically adjust parameters during training or sampling processes. Schedulers are particularly useful for controlling step sizes, noise scales, learning rates, and other hyperparameters that benefit from adaptive adjustment over time.</p> <p>The schedulers implement various mathematical schedules including exponential decay, linear annealing, cosine annealing, multi-step schedules, and warmup strategies. All schedulers inherit from the <code>BaseScheduler</code> abstract base class, ensuring a consistent interface across different scheduling strategies.</p> <p>Mathematical Foundation</p> <p>Parameter scheduling involves updating a parameter value \\(v(t)\\) at each time step \\(t\\) according to a predefined schedule. Common patterns include:</p> <ul> <li>Exponential decay: \\(v(t) = v_0 \\times \\gamma^t\\)</li> <li>Linear annealing: \\(v(t) = v_0 + (v_{end} - v_0) \\times t/T\\)</li> <li>Cosine annealing: \\(v(t) = v_{end} + (v_0 - v_{end}) \\times (1 + \\cos(\\pi t/T))/2\\)</li> </ul> <p>where \\(v_0\\) is the initial value, \\(v_{end}\\) is the final value, \\(T\\) is the total number of steps, and \\(\\gamma\\) is the decay rate.</p> <p>Basic Usage</p> <p>Basic usage with different scheduler types:</p> <pre><code>import torch\nfrom torchebm.core import ExponentialDecayScheduler, CosineScheduler\n\n# Exponential decay for step size\nstep_scheduler = ExponentialDecayScheduler(\n    start_value=0.1, decay_rate=0.99, min_value=0.001\n)\n\n# Cosine annealing for noise scale\nnoise_scheduler = CosineScheduler(\n    start_value=1.0, end_value=0.01, n_steps=1000\n)\n\n# Use in training loop\nfor epoch in range(100):\n    current_step_size = step_scheduler.step()\n    current_noise = noise_scheduler.step()\n    # Use current_step_size and current_noise in your algorithm\n</code></pre> <p>Integration with Samplers</p> <pre><code>from torchebm.samplers import LangevinDynamics\nfrom torchebm.core import LinearScheduler\n\n# Create scheduler for adaptive step size\nstep_scheduler = LinearScheduler(\n    start_value=0.1, end_value=0.001, n_steps=500\n)\n\n# Use with Langevin sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=step_scheduler,\n    noise_scale=0.1\n)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/","title":"BaseScheduler","text":""},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for parameter schedulers.</p> <p>This class provides the foundation for all parameter scheduling strategies in TorchEBM. Schedulers are used to dynamically adjust parameters such as step sizes, noise scales, learning rates, and other hyperparameters during training or sampling processes.</p> <p>The scheduler maintains an internal step counter and computes parameter values based on the current step. Subclasses must implement the <code>_compute_value</code> method to define the specific scheduling strategy.</p> <p>Mathematical Foundation</p> <p>A scheduler defines a function \\(f: \\mathbb{N} \\to \\mathbb{R}\\) that maps step numbers to parameter values:</p> \\[v(t) = f(t)\\] <p>where \\(t\\) is the current step count and \\(v(t)\\) is the parameter value at step \\(t\\).</p> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>Initial parameter value at step 0.</p> required <p>Creating a Custom Scheduler</p> <pre><code>class CustomScheduler(BaseScheduler):\n    def __init__(self, start_value: float, factor: float):\n        super().__init__(start_value)\n        self.factor = factor\n\n    def _compute_value(self) -&gt; float:\n        return self.start_value * (self.factor ** self.step_count)\n\nscheduler = CustomScheduler(start_value=1.0, factor=0.9)\nfor i in range(5):\n    value = scheduler.step()\n    print(f\"Step {i+1}: {value:.4f}\")\n</code></pre> <p>State Management</p> <pre><code>scheduler = ExponentialDecayScheduler(start_value=0.1, decay_rate=0.95)\n# Take some steps\nfor _ in range(10):\n    scheduler.step()\n\n# Save state\nstate = scheduler.state_dict()\n\n# Reset and restore\nscheduler.reset()\nscheduler.load_state_dict(state)\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>class BaseScheduler(ABC):\n    r\"\"\"\n    Abstract base class for parameter schedulers.\n\n    This class provides the foundation for all parameter scheduling strategies in TorchEBM.\n    Schedulers are used to dynamically adjust parameters such as step sizes, noise scales,\n    learning rates, and other hyperparameters during training or sampling processes.\n\n    The scheduler maintains an internal step counter and computes parameter values based\n    on the current step. Subclasses must implement the `_compute_value` method to define\n    the specific scheduling strategy.\n\n    !!! info \"Mathematical Foundation\"\n        A scheduler defines a function \\(f: \\mathbb{N} \\to \\mathbb{R}\\) that maps step numbers to parameter values:\n\n        $$v(t) = f(t)$$\n\n        where \\(t\\) is the current step count and \\(v(t)\\) is the parameter value at step \\(t\\).\n\n    Args:\n        start_value (float): Initial parameter value at step 0.\n\n    Attributes:\n        start_value (float): The initial parameter value.\n        current_value (float): The current parameter value.\n        step_count (int): Number of steps taken since initialization or last reset.\n\n    !!! example \"Creating a Custom Scheduler\"\n        ```python\n        class CustomScheduler(BaseScheduler):\n            def __init__(self, start_value: float, factor: float):\n                super().__init__(start_value)\n                self.factor = factor\n\n            def _compute_value(self) -&gt; float:\n                return self.start_value * (self.factor ** self.step_count)\n\n        scheduler = CustomScheduler(start_value=1.0, factor=0.9)\n        for i in range(5):\n            value = scheduler.step()\n            print(f\"Step {i+1}: {value:.4f}\")\n        ```\n\n    !!! tip \"State Management\"\n        ```python\n        scheduler = ExponentialDecayScheduler(start_value=0.1, decay_rate=0.95)\n        # Take some steps\n        for _ in range(10):\n            scheduler.step()\n\n        # Save state\n        state = scheduler.state_dict()\n\n        # Reset and restore\n        scheduler.reset()\n        scheduler.load_state_dict(state)\n        ```\n    \"\"\"\n\n    def __init__(self, start_value: float):\n        r\"\"\"\n        Initialize the base scheduler.\n\n        Args:\n            start_value (float): Initial parameter value. Must be a finite number.\n\n        Raises:\n            TypeError: If start_value is not a float or int.\n        \"\"\"\n        if not isinstance(start_value, (float, int)):\n            raise TypeError(\n                f\"{type(self).__name__} received an invalid start_value of type \"\n                f\"{type(start_value).__name__}. Expected float or int.\"\n            )\n\n        self.start_value = float(start_value)\n        self.current_value = self.start_value\n        self.step_count = 0\n\n    @abstractmethod\n    def _compute_value(self) -&gt; float:\n        r\"\"\"\n        Compute the parameter value for the current step count.\n\n        This method must be implemented by subclasses to define the specific\n        scheduling strategy. It should return the parameter value based on\n        the current `self.step_count`.\n\n        Returns:\n            float: The computed parameter value for the current step.\n\n        !!! warning \"Implementation Note\"\n            This method is called internally by `step()` after incrementing\n            the step counter. Subclasses should not call this method directly.\n        \"\"\"\n        pass\n\n    def step(self) -&gt; float:\n        r\"\"\"\n        Advance the scheduler by one step and return the new parameter value.\n\n        This method increments the internal step counter and computes the new\n        parameter value using the scheduler's strategy. The computed value\n        becomes the new current value.\n\n        Returns:\n            float: The new parameter value after stepping.\n\n        !!! example \"Basic Usage\"\n            ```python\n            scheduler = ExponentialDecayScheduler(start_value=1.0, decay_rate=0.9)\n            print(f\"Initial: {scheduler.get_value()}\")  # 1.0\n            print(f\"Step 1: {scheduler.step()}\")        # 0.9\n            print(f\"Step 2: {scheduler.step()}\")        # 0.81\n            ```\n        \"\"\"\n        self.step_count += 1\n        self.current_value = self._compute_value()\n        return self.current_value\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Reset the scheduler to its initial state.\n\n        This method resets both the step counter and current value to their\n        initial states, effectively restarting the scheduling process.\n\n        !!! example \"Reset Example\"\n            ```python\n            scheduler = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=10)\n            for _ in range(5):\n                scheduler.step()\n            print(f\"Before reset: step={scheduler.step_count}, value={scheduler.current_value}\")\n            scheduler.reset()\n            print(f\"After reset: step={scheduler.step_count}, value={scheduler.current_value}\")\n            ```\n        \"\"\"\n        self.current_value = self.start_value\n        self.step_count = 0\n\n    def get_value(self) -&gt; float:\n        r\"\"\"\n        Get the current parameter value without advancing the scheduler.\n\n        This method returns the current parameter value without modifying\n        the scheduler's internal state. Use this when you need to query\n        the current value without stepping.\n\n        Returns:\n            float: The current parameter value.\n\n        !!! example \"Query Current Value\"\n            ```python\n            scheduler = ConstantScheduler(start_value=0.5)\n            print(scheduler.get_value())  # 0.5\n            scheduler.step()\n            print(scheduler.get_value())  # 0.5 (still constant)\n            ```\n        \"\"\"\n        return self.current_value\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        r\"\"\"\n        Return the state of the scheduler as a dictionary.\n\n        This method returns a dictionary containing all the scheduler's internal\n        state, which can be used to save and restore the scheduler's state.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing the scheduler's state.\n\n        !!! example \"State Management\"\n            ```python\n            scheduler = CosineScheduler(start_value=1.0, end_value=0.0, n_steps=100)\n            for _ in range(50):\n                scheduler.step()\n            state = scheduler.state_dict()\n            print(state['step_count'])  # 50\n            ```\n        \"\"\"\n        return {key: value for key, value in self.__dict__.items()}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        r\"\"\"\n        Load the scheduler's state from a dictionary.\n\n        This method restores the scheduler's internal state from a dictionary\n        previously created by `state_dict()`. This is useful for resuming\n        training or sampling from a checkpoint.\n\n        Args:\n            state_dict (Dict[str, Any]): Dictionary containing the scheduler state.\n                Should be an object returned from a call to `state_dict()`.\n\n        !!! example \"State Restoration\"\n            ```python\n            scheduler1 = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=100)\n            for _ in range(25):\n                scheduler1.step()\n            state = scheduler1.state_dict()\n\n            scheduler2 = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=100)\n            scheduler2.load_state_dict(state)\n            print(scheduler2.step_count)  # 25\n            ```\n        \"\"\"\n        self.__dict__.update(state_dict)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.start_value","title":"start_value  <code>instance-attribute</code>","text":"<pre><code>start_value = float(start_value)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.current_value","title":"current_value  <code>instance-attribute</code>","text":"<pre><code>current_value = start_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.step_count","title":"step_count  <code>instance-attribute</code>","text":"<pre><code>step_count = 0\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler._compute_value","title":"_compute_value  <code>abstractmethod</code>","text":"<pre><code>_compute_value() -&gt; float\n</code></pre> <p>Compute the parameter value for the current step count.</p> <p>This method must be implemented by subclasses to define the specific scheduling strategy. It should return the parameter value based on the current <code>self.step_count</code>.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed parameter value for the current step.</p> <p>Implementation Note</p> <p>This method is called internally by <code>step()</code> after incrementing the step counter. Subclasses should not call this method directly.</p> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>@abstractmethod\ndef _compute_value(self) -&gt; float:\n    r\"\"\"\n    Compute the parameter value for the current step count.\n\n    This method must be implemented by subclasses to define the specific\n    scheduling strategy. It should return the parameter value based on\n    the current `self.step_count`.\n\n    Returns:\n        float: The computed parameter value for the current step.\n\n    !!! warning \"Implementation Note\"\n        This method is called internally by `step()` after incrementing\n        the step counter. Subclasses should not call this method directly.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.step","title":"step","text":"<pre><code>step() -&gt; float\n</code></pre> <p>Advance the scheduler by one step and return the new parameter value.</p> <p>This method increments the internal step counter and computes the new parameter value using the scheduler's strategy. The computed value becomes the new current value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The new parameter value after stepping.</p> <p>Basic Usage</p> <pre><code>scheduler = ExponentialDecayScheduler(start_value=1.0, decay_rate=0.9)\nprint(f\"Initial: {scheduler.get_value()}\")  # 1.0\nprint(f\"Step 1: {scheduler.step()}\")        # 0.9\nprint(f\"Step 2: {scheduler.step()}\")        # 0.81\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def step(self) -&gt; float:\n    r\"\"\"\n    Advance the scheduler by one step and return the new parameter value.\n\n    This method increments the internal step counter and computes the new\n    parameter value using the scheduler's strategy. The computed value\n    becomes the new current value.\n\n    Returns:\n        float: The new parameter value after stepping.\n\n    !!! example \"Basic Usage\"\n        ```python\n        scheduler = ExponentialDecayScheduler(start_value=1.0, decay_rate=0.9)\n        print(f\"Initial: {scheduler.get_value()}\")  # 1.0\n        print(f\"Step 1: {scheduler.step()}\")        # 0.9\n        print(f\"Step 2: {scheduler.step()}\")        # 0.81\n        ```\n    \"\"\"\n    self.step_count += 1\n    self.current_value = self._compute_value()\n    return self.current_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the scheduler to its initial state.</p> <p>This method resets both the step counter and current value to their initial states, effectively restarting the scheduling process.</p> <p>Reset Example</p> <pre><code>scheduler = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=10)\nfor _ in range(5):\n    scheduler.step()\nprint(f\"Before reset: step={scheduler.step_count}, value={scheduler.current_value}\")\nscheduler.reset()\nprint(f\"After reset: step={scheduler.step_count}, value={scheduler.current_value}\")\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Reset the scheduler to its initial state.\n\n    This method resets both the step counter and current value to their\n    initial states, effectively restarting the scheduling process.\n\n    !!! example \"Reset Example\"\n        ```python\n        scheduler = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=10)\n        for _ in range(5):\n            scheduler.step()\n        print(f\"Before reset: step={scheduler.step_count}, value={scheduler.current_value}\")\n        scheduler.reset()\n        print(f\"After reset: step={scheduler.step_count}, value={scheduler.current_value}\")\n        ```\n    \"\"\"\n    self.current_value = self.start_value\n    self.step_count = 0\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.get_value","title":"get_value","text":"<pre><code>get_value() -&gt; float\n</code></pre> <p>Get the current parameter value without advancing the scheduler.</p> <p>This method returns the current parameter value without modifying the scheduler's internal state. Use this when you need to query the current value without stepping.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The current parameter value.</p> <p>Query Current Value</p> <pre><code>scheduler = ConstantScheduler(start_value=0.5)\nprint(scheduler.get_value())  # 0.5\nscheduler.step()\nprint(scheduler.get_value())  # 0.5 (still constant)\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def get_value(self) -&gt; float:\n    r\"\"\"\n    Get the current parameter value without advancing the scheduler.\n\n    This method returns the current parameter value without modifying\n    the scheduler's internal state. Use this when you need to query\n    the current value without stepping.\n\n    Returns:\n        float: The current parameter value.\n\n    !!! example \"Query Current Value\"\n        ```python\n        scheduler = ConstantScheduler(start_value=0.5)\n        print(scheduler.get_value())  # 0.5\n        scheduler.step()\n        print(scheduler.get_value())  # 0.5 (still constant)\n        ```\n    \"\"\"\n    return self.current_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Return the state of the scheduler as a dictionary.</p> <p>This method returns a dictionary containing all the scheduler's internal state, which can be used to save and restore the scheduler's state.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing the scheduler's state.</p> <p>State Management</p> <pre><code>scheduler = CosineScheduler(start_value=1.0, end_value=0.0, n_steps=100)\nfor _ in range(50):\n    scheduler.step()\nstate = scheduler.state_dict()\nprint(state['step_count'])  # 50\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\n    r\"\"\"\n    Return the state of the scheduler as a dictionary.\n\n    This method returns a dictionary containing all the scheduler's internal\n    state, which can be used to save and restore the scheduler's state.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing the scheduler's state.\n\n    !!! example \"State Management\"\n        ```python\n        scheduler = CosineScheduler(start_value=1.0, end_value=0.0, n_steps=100)\n        for _ in range(50):\n            scheduler.step()\n        state = scheduler.state_dict()\n        print(state['step_count'])  # 50\n        ```\n    \"\"\"\n    return {key: value for key, value in self.__dict__.items()}\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/BaseScheduler/#torchebm.core.base_scheduler.BaseScheduler.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>Load the scheduler's state from a dictionary.</p> <p>This method restores the scheduler's internal state from a dictionary previously created by <code>state_dict()</code>. This is useful for resuming training or sampling from a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing the scheduler state. Should be an object returned from a call to <code>state_dict()</code>.</p> required <p>State Restoration</p> <pre><code>scheduler1 = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=100)\nfor _ in range(25):\n    scheduler1.step()\nstate = scheduler1.state_dict()\n\nscheduler2 = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=100)\nscheduler2.load_state_dict(state)\nprint(scheduler2.step_count)  # 25\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    r\"\"\"\n    Load the scheduler's state from a dictionary.\n\n    This method restores the scheduler's internal state from a dictionary\n    previously created by `state_dict()`. This is useful for resuming\n    training or sampling from a checkpoint.\n\n    Args:\n        state_dict (Dict[str, Any]): Dictionary containing the scheduler state.\n            Should be an object returned from a call to `state_dict()`.\n\n    !!! example \"State Restoration\"\n        ```python\n        scheduler1 = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=100)\n        for _ in range(25):\n            scheduler1.step()\n        state = scheduler1.state_dict()\n\n        scheduler2 = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=100)\n        scheduler2.load_state_dict(state)\n        print(scheduler2.step_count)  # 25\n        ```\n    \"\"\"\n    self.__dict__.update(state_dict)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/ConstantScheduler/","title":"ConstantScheduler","text":""},{"location":"api/torchebm/core/base_scheduler/classes/ConstantScheduler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>Scheduler that maintains a constant parameter value.</p> <p>This scheduler returns the same value at every step, effectively providing no scheduling. It's useful as a baseline or when you want to disable scheduling for certain parameters while keeping the scheduler interface.</p> <p>Mathematical Formula</p> \\[v(t) = v_0 \\text{ for all } t \\geq 0\\] <p>where \\(v_0\\) is the start_value.</p> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>The constant value to maintain.</p> required <p>Basic Usage</p> <pre><code>scheduler = ConstantScheduler(start_value=0.01)\nfor i in range(5):\n    value = scheduler.step()\n    print(f\"Step {i+1}: {value}\")  # Always prints 0.01\n</code></pre> <p>Using with Samplers</p> <pre><code>from torchebm.samplers import LangevinDynamics\nconstant_step = ConstantScheduler(start_value=0.05)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=constant_step,\n    noise_scale=0.1\n)\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>class ConstantScheduler(BaseScheduler):\n    r\"\"\"\n    Scheduler that maintains a constant parameter value.\n\n    This scheduler returns the same value at every step, effectively providing\n    no scheduling. It's useful as a baseline or when you want to disable\n    scheduling for certain parameters while keeping the scheduler interface.\n\n    !!! info \"Mathematical Formula\"\n        $$v(t) = v_0 \\text{ for all } t \\geq 0$$\n\n        where \\(v_0\\) is the start_value.\n\n    Args:\n        start_value (float): The constant value to maintain.\n\n    !!! example \"Basic Usage\"\n        ```python\n        scheduler = ConstantScheduler(start_value=0.01)\n        for i in range(5):\n            value = scheduler.step()\n            print(f\"Step {i+1}: {value}\")  # Always prints 0.01\n        ```\n\n    !!! tip \"Using with Samplers\"\n        ```python\n        from torchebm.samplers import LangevinDynamics\n        constant_step = ConstantScheduler(start_value=0.05)\n        sampler = LangevinDynamics(\n            energy_function=energy_fn,\n            step_size=constant_step,\n            noise_scale=0.1\n        )\n        ```\n    \"\"\"\n\n    def _compute_value(self) -&gt; float:\n        r\"\"\"\n        Return the constant value.\n\n        Returns:\n            float: The constant start_value.\n        \"\"\"\n        return self.start_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/ConstantScheduler/#torchebm.core.base_scheduler.ConstantScheduler._compute_value","title":"_compute_value","text":"<pre><code>_compute_value() -&gt; float\n</code></pre> <p>Return the constant value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The constant start_value.</p> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def _compute_value(self) -&gt; float:\n    r\"\"\"\n    Return the constant value.\n\n    Returns:\n        float: The constant start_value.\n    \"\"\"\n    return self.start_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/CosineScheduler/","title":"CosineScheduler","text":""},{"location":"api/torchebm/core/base_scheduler/classes/CosineScheduler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>Scheduler with cosine annealing.</p> <p>This scheduler implements cosine annealing, which provides a smooth transition from the start value to the end value following a cosine curve. Cosine annealing is popular in deep learning as it provides fast initial decay followed by slower decay, which can help with convergence.</p> <p>Mathematical Formula</p> \\[v(t) = \\begin{cases} v_{end} + (v_0 - v_{end}) \\times \\frac{1 + \\cos(\\pi t/T)}{2}, &amp; \\text{if } t &lt; T \\\\ v_{end}, &amp; \\text{if } t \\geq T \\end{cases}\\] <p>where:</p> <ul> <li>\\(v_0\\) is the start_value</li> <li>\\(v_{end}\\) is the end_value  </li> <li>\\(T\\) is n_steps</li> <li>\\(t\\) is the current step count</li> </ul> <p>Cosine Curve Properties</p> <p>The cosine function creates a smooth S-shaped curve that starts with rapid decay and gradually slows down as it approaches the end value.</p> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>Starting parameter value.</p> required <code>end_value</code> <code>float</code> <p>Target parameter value.</p> required <code>n_steps</code> <code>int</code> <p>Number of steps to reach the final value.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_steps is not positive.</p> <p>Step Size Annealing</p> <pre><code>scheduler = CosineScheduler(start_value=0.1, end_value=0.001, n_steps=100)\nvalues = []\nfor i in range(10):\n    value = scheduler.step()\n    values.append(value)\n    if i &lt; 3:  # Show first few values\n        print(f\"Step {i+1}: {value:.6f}\")\n# Shows smooth decay: 0.099951, 0.099606, 0.098866, ...\n</code></pre> <p>Learning Rate Scheduling</p> <pre><code>lr_scheduler = CosineScheduler(\n    start_value=0.01, end_value=0.0001, n_steps=1000\n)\n# In training loop\nfor epoch in range(1000):\n    lr = lr_scheduler.step()\n    # Update optimizer learning rate\n</code></pre> <p>Noise Scale Annealing</p> <pre><code>noise_scheduler = CosineScheduler(\n    start_value=1.0, end_value=0.01, n_steps=500\n)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=noise_scheduler\n)\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>class CosineScheduler(BaseScheduler):\n    r\"\"\"\n    Scheduler with cosine annealing.\n\n    This scheduler implements cosine annealing, which provides a smooth transition\n    from the start value to the end value following a cosine curve. Cosine annealing\n    is popular in deep learning as it provides fast initial decay followed by\n    slower decay, which can help with convergence.\n\n    !!! info \"Mathematical Formula\"\n        $$v(t) = \\begin{cases}\n        v_{end} + (v_0 - v_{end}) \\times \\frac{1 + \\cos(\\pi t/T)}{2}, &amp; \\text{if } t &lt; T \\\\\n        v_{end}, &amp; \\text{if } t \\geq T\n        \\end{cases}$$\n\n        where:\n\n        - \\(v_0\\) is the start_value\n        - \\(v_{end}\\) is the end_value  \n        - \\(T\\) is n_steps\n        - \\(t\\) is the current step count\n\n    !!! note \"Cosine Curve Properties\"\n        The cosine function creates a smooth S-shaped curve that starts with rapid\n        decay and gradually slows down as it approaches the end value.\n\n    Args:\n        start_value (float): Starting parameter value.\n        end_value (float): Target parameter value.\n        n_steps (int): Number of steps to reach the final value.\n\n    Raises:\n        ValueError: If n_steps is not positive.\n\n    !!! example \"Step Size Annealing\"\n        ```python\n        scheduler = CosineScheduler(start_value=0.1, end_value=0.001, n_steps=100)\n        values = []\n        for i in range(10):\n            value = scheduler.step()\n            values.append(value)\n            if i &lt; 3:  # Show first few values\n                print(f\"Step {i+1}: {value:.6f}\")\n        # Shows smooth decay: 0.099951, 0.099606, 0.098866, ...\n        ```\n\n    !!! tip \"Learning Rate Scheduling\"\n        ```python\n        lr_scheduler = CosineScheduler(\n            start_value=0.01, end_value=0.0001, n_steps=1000\n        )\n        # In training loop\n        for epoch in range(1000):\n            lr = lr_scheduler.step()\n            # Update optimizer learning rate\n        ```\n\n    !!! example \"Noise Scale Annealing\"\n        ```python\n        noise_scheduler = CosineScheduler(\n            start_value=1.0, end_value=0.01, n_steps=500\n        )\n        sampler = LangevinDynamics(\n            energy_function=energy_fn,\n            step_size=0.01,\n            noise_scale=noise_scheduler\n        )\n        ```\n    \"\"\"\n\n    def __init__(self, start_value: float, end_value: float, n_steps: int):\n        r\"\"\"\n        Initialize the cosine scheduler.\n\n        Args:\n            start_value (float): Starting parameter value.\n            end_value (float): Target parameter value.\n            n_steps (int): Number of steps to reach the final value.\n\n        Raises:\n            ValueError: If n_steps is not positive.\n        \"\"\"\n        super().__init__(start_value)\n        if n_steps &lt;= 0:\n            raise ValueError(f\"n_steps must be a positive integer, got {n_steps}\")\n\n        self.end_value = end_value\n        self.n_steps = n_steps\n\n    def _compute_value(self) -&gt; float:\n        r\"\"\"\n        Compute the cosine annealed value.\n\n        Returns:\n            float: The annealed value following cosine schedule.\n        \"\"\"\n        if self.step_count &gt;= self.n_steps:\n            return self.end_value\n        else:\n            # Cosine schedule from start_value to end_value\n            progress = self.step_count / self.n_steps\n            cosine_factor = 0.5 * (1 + math.cos(math.pi * progress))\n            return self.end_value + (self.start_value - self.end_value) * cosine_factor\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/CosineScheduler/#torchebm.core.base_scheduler.CosineScheduler.end_value","title":"end_value  <code>instance-attribute</code>","text":"<pre><code>end_value = end_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/CosineScheduler/#torchebm.core.base_scheduler.CosineScheduler.n_steps","title":"n_steps  <code>instance-attribute</code>","text":"<pre><code>n_steps = n_steps\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/CosineScheduler/#torchebm.core.base_scheduler.CosineScheduler._compute_value","title":"_compute_value","text":"<pre><code>_compute_value() -&gt; float\n</code></pre> <p>Compute the cosine annealed value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The annealed value following cosine schedule.</p> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def _compute_value(self) -&gt; float:\n    r\"\"\"\n    Compute the cosine annealed value.\n\n    Returns:\n        float: The annealed value following cosine schedule.\n    \"\"\"\n    if self.step_count &gt;= self.n_steps:\n        return self.end_value\n    else:\n        # Cosine schedule from start_value to end_value\n        progress = self.step_count / self.n_steps\n        cosine_factor = 0.5 * (1 + math.cos(math.pi * progress))\n        return self.end_value + (self.start_value - self.end_value) * cosine_factor\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/ExponentialDecayScheduler/","title":"ExponentialDecayScheduler","text":""},{"location":"api/torchebm/core/base_scheduler/classes/ExponentialDecayScheduler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>Scheduler with exponential decay.</p> <p>This scheduler implements exponential decay of the parameter value according to: \\(v(t) = \\max(v_{min}, v_0  imes \\gamma^t)\\)</p> <p>Exponential decay is commonly used for step sizes in optimization and sampling algorithms, as it provides rapid initial decay that slows down over time, allowing for both exploration and convergence.</p> <p>Mathematical Formula</p> \\[v(t) = \\max(v_{min}, v_0      imes \\gamma^t)\\] <p>where:</p> <ul> <li>\\(v_0\\) is the start_value</li> <li>\\(\\gamma\\) is the decay_rate \\((0 &lt; \\gamma \\leq 1)\\)</li> <li>\\(t\\) is the step count</li> <li>\\(v_{min}\\) is the min_value (lower bound)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>Initial parameter value.</p> required <code>decay_rate</code> <code>float</code> <p>Decay factor applied at each step. Must be in (0, 1].</p> required <code>min_value</code> <code>float</code> <p>Minimum value to clamp the result. Defaults to 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If decay_rate is not in (0, 1] or min_value is negative.</p> <p>Basic Exponential Decay</p> <pre><code>scheduler = ExponentialDecayScheduler(\n    start_value=1.0, decay_rate=0.9, min_value=0.01\n)\nfor i in range(5):\n    value = scheduler.step()\n    print(f\"Step {i+1}: {value:.4f}\")\n# Output: 0.9000, 0.8100, 0.7290, 0.6561, 0.5905\n</code></pre> <p>Training Loop Integration</p> <pre><code>step_scheduler = ExponentialDecayScheduler(\n    start_value=0.1, decay_rate=0.995, min_value=0.001\n)\n# In training loop\nfor epoch in range(1000):\n    current_step_size = step_scheduler.step()\n    # Use current_step_size in your algorithm\n</code></pre> <p>Decay Rate Selection</p> <ul> <li>Aggressive decay: Use smaller decay_rate (e.g., 0.5)</li> <li>Gentle decay: Use larger decay_rate (e.g., 0.99)</li> </ul> <pre><code># Aggressive decay\naggressive = ExponentialDecayScheduler(start_value=1.0, decay_rate=0.5)\n# Gentle decay\ngentle = ExponentialDecayScheduler(start_value=1.0, decay_rate=0.99)\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>class ExponentialDecayScheduler(BaseScheduler):\n    \"\"\"\n    Scheduler with exponential decay.\n\n    This scheduler implements exponential decay of the parameter value according to:\n    \\(v(t) = \\max(v_{min}, v_0 \\times \\gamma^t)\\)\n\n    Exponential decay is commonly used for step sizes in optimization and sampling\n    algorithms, as it provides rapid initial decay that slows down over time,\n    allowing for both exploration and convergence.\n\n    !!! info \"Mathematical Formula\"\n        $$v(t) = \\max(v_{min}, v_0 \\times \\gamma^t)$$\n\n        where:\n\n        - \\(v_0\\) is the start_value\n        - \\(\\gamma\\) is the decay_rate \\((0 &lt; \\gamma \\leq 1)\\)\n        - \\(t\\) is the step count\n        - \\(v_{min}\\) is the min_value (lower bound)\n\n    Args:\n        start_value (float): Initial parameter value.\n        decay_rate (float): Decay factor applied at each step. Must be in (0, 1].\n        min_value (float, optional): Minimum value to clamp the result. Defaults to 0.0.\n\n    Raises:\n        ValueError: If decay_rate is not in (0, 1] or min_value is negative.\n\n    !!! example \"Basic Exponential Decay\"\n        ```python\n        scheduler = ExponentialDecayScheduler(\n            start_value=1.0, decay_rate=0.9, min_value=0.01\n        )\n        for i in range(5):\n            value = scheduler.step()\n            print(f\"Step {i+1}: {value:.4f}\")\n        # Output: 0.9000, 0.8100, 0.7290, 0.6561, 0.5905\n        ```\n\n    !!! tip \"Training Loop Integration\"\n        ```python\n        step_scheduler = ExponentialDecayScheduler(\n            start_value=0.1, decay_rate=0.995, min_value=0.001\n        )\n        # In training loop\n        for epoch in range(1000):\n            current_step_size = step_scheduler.step()\n            # Use current_step_size in your algorithm\n        ```\n\n    !!! note \"Decay Rate Selection\"\n        - **Aggressive decay**: Use smaller decay_rate (e.g., 0.5)\n        - **Gentle decay**: Use larger decay_rate (e.g., 0.99)\n\n        ```python\n        # Aggressive decay\n        aggressive = ExponentialDecayScheduler(start_value=1.0, decay_rate=0.5)\n        # Gentle decay\n        gentle = ExponentialDecayScheduler(start_value=1.0, decay_rate=0.99)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        start_value: float,\n        decay_rate: float,\n        min_value: float = 0.0,\n    ):\n        r\"\"\"\n        Initialize the exponential decay scheduler.\n\n        Args:\n            start_value (float): Initial parameter value.\n            decay_rate (float): Decay factor applied at each step. Must be in (0, 1].\n            min_value (float, optional): Minimum value to clamp the result. Defaults to 0.0.\n\n        Raises:\n            ValueError: If decay_rate is not in (0, 1] or min_value is negative.\n        \"\"\"\n        super().__init__(start_value)\n        if not 0.0 &lt; decay_rate &lt;= 1.0:\n            raise ValueError(f\"decay_rate must be in (0, 1], got {decay_rate}\")\n        if min_value &lt; 0:\n            raise ValueError(f\"min_value must be non-negative, got {min_value}\")\n        self.decay_rate: float = decay_rate\n        self.min_value: float = min_value\n\n    def _compute_value(self) -&gt; float:\n        r\"\"\"\n        Compute the exponentially decayed value.\n\n        Returns:\n            float: The decayed value, clamped to min_value.\n        \"\"\"\n        val = self.start_value * (self.decay_rate**self.step_count)\n        return max(self.min_value, val)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/ExponentialDecayScheduler/#torchebm.core.base_scheduler.ExponentialDecayScheduler.decay_rate","title":"decay_rate  <code>instance-attribute</code>","text":"<pre><code>decay_rate: float = decay_rate\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/ExponentialDecayScheduler/#torchebm.core.base_scheduler.ExponentialDecayScheduler.min_value","title":"min_value  <code>instance-attribute</code>","text":"<pre><code>min_value: float = min_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/ExponentialDecayScheduler/#torchebm.core.base_scheduler.ExponentialDecayScheduler._compute_value","title":"_compute_value","text":"<pre><code>_compute_value() -&gt; float\n</code></pre> <p>Compute the exponentially decayed value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The decayed value, clamped to min_value.</p> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def _compute_value(self) -&gt; float:\n    r\"\"\"\n    Compute the exponentially decayed value.\n\n    Returns:\n        float: The decayed value, clamped to min_value.\n    \"\"\"\n    val = self.start_value * (self.decay_rate**self.step_count)\n    return max(self.min_value, val)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/LinearScheduler/","title":"LinearScheduler","text":""},{"location":"api/torchebm/core/base_scheduler/classes/LinearScheduler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>Scheduler with linear interpolation between start and end values.</p> <p>This scheduler linearly interpolates between a start value and an end value over a specified number of steps. After reaching the end value, it remains constant. Linear scheduling is useful when you want predictable, uniform changes in parameter values.</p> <p>Mathematical Formula</p> \\[v(t) = \\begin{cases} v_0 + (v_{end} - v_0) \\times \\frac{t}{T}, &amp; \\text{if } t &lt; T \\\\ v_{end}, &amp; \\text{if } t \\geq T \\end{cases}\\] <p>where:</p> <ul> <li>\\(v_0\\) is the start_value</li> <li>\\(v_{end}\\) is the end_value</li> <li>\\(T\\) is n_steps</li> <li>\\(t\\) is the current step count</li> </ul> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>Starting parameter value.</p> required <code>end_value</code> <code>float</code> <p>Target parameter value.</p> required <code>n_steps</code> <code>int</code> <p>Number of steps to reach the final value.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_steps is not positive.</p> <p>Linear Decay</p> <pre><code>scheduler = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=5)\nfor i in range(7):  # Go beyond n_steps to see clamping\n    value = scheduler.step()\n    print(f\"Step {i+1}: {value:.2f}\")\n# Output: 0.80, 0.60, 0.40, 0.20, 0.00, 0.00, 0.00\n</code></pre> <p>Warmup Strategy</p> <pre><code>warmup_scheduler = LinearScheduler(\n    start_value=0.0, end_value=0.1, n_steps=100\n)\n# Use for learning rate warmup\nfor epoch in range(100):\n    lr = warmup_scheduler.step()\n    # Set learning rate in optimizer\n</code></pre> <p>MCMC Integration</p> <pre><code>step_scheduler = LinearScheduler(\n    start_value=0.1, end_value=0.001, n_steps=1000\n)\n# Use in MCMC sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=step_scheduler\n)\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>class LinearScheduler(BaseScheduler):\n    r\"\"\"\n    Scheduler with linear interpolation between start and end values.\n\n    This scheduler linearly interpolates between a start value and an end value\n    over a specified number of steps. After reaching the end value, it remains\n    constant. Linear scheduling is useful when you want predictable, uniform\n    changes in parameter values.\n\n    !!! info \"Mathematical Formula\"\n        $$v(t) = \\begin{cases}\n        v_0 + (v_{end} - v_0) \\times \\frac{t}{T}, &amp; \\text{if } t &lt; T \\\\\n        v_{end}, &amp; \\text{if } t \\geq T\n        \\end{cases}$$\n\n        where:\n\n        - \\(v_0\\) is the start_value\n        - \\(v_{end}\\) is the end_value\n        - \\(T\\) is n_steps\n        - \\(t\\) is the current step count\n\n    Args:\n        start_value (float): Starting parameter value.\n        end_value (float): Target parameter value.\n        n_steps (int): Number of steps to reach the final value.\n\n    Raises:\n        ValueError: If n_steps is not positive.\n\n    !!! example \"Linear Decay\"\n        ```python\n        scheduler = LinearScheduler(start_value=1.0, end_value=0.0, n_steps=5)\n        for i in range(7):  # Go beyond n_steps to see clamping\n            value = scheduler.step()\n            print(f\"Step {i+1}: {value:.2f}\")\n        # Output: 0.80, 0.60, 0.40, 0.20, 0.00, 0.00, 0.00\n        ```\n\n    !!! tip \"Warmup Strategy\"\n        ```python\n        warmup_scheduler = LinearScheduler(\n            start_value=0.0, end_value=0.1, n_steps=100\n        )\n        # Use for learning rate warmup\n        for epoch in range(100):\n            lr = warmup_scheduler.step()\n            # Set learning rate in optimizer\n        ```\n\n    !!! example \"MCMC Integration\"\n        ```python\n        step_scheduler = LinearScheduler(\n            start_value=0.1, end_value=0.001, n_steps=1000\n        )\n        # Use in MCMC sampler\n        sampler = LangevinDynamics(\n            energy_function=energy_fn,\n            step_size=step_scheduler\n        )\n        ```\n    \"\"\"\n\n    def __init__(self, start_value: float, end_value: float, n_steps: int):\n        r\"\"\"\n        Initialize the linear scheduler.\n\n        Args:\n            start_value (float): Starting parameter value.\n            end_value (float): Target parameter value.\n            n_steps (int): Number of steps to reach the final value.\n\n        Raises:\n            ValueError: If n_steps is not positive.\n        \"\"\"\n        super().__init__(start_value)\n        if n_steps &lt;= 0:\n            raise ValueError(f\"n_steps must be positive, got {n_steps}\")\n\n        self.end_value = end_value\n        self.n_steps = n_steps\n        self.step_size: float = (end_value - start_value) / n_steps\n\n    def _compute_value(self) -&gt; float:\n        r\"\"\"\n        Compute the linearly interpolated value.\n\n        Returns:\n            float: The interpolated value, clamped to end_value after n_steps.\n        \"\"\"\n        if self.step_count &gt;= self.n_steps:\n            return self.end_value\n        else:\n            return self.start_value + self.step_size * self.step_count\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/LinearScheduler/#torchebm.core.base_scheduler.LinearScheduler.end_value","title":"end_value  <code>instance-attribute</code>","text":"<pre><code>end_value = end_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/LinearScheduler/#torchebm.core.base_scheduler.LinearScheduler.n_steps","title":"n_steps  <code>instance-attribute</code>","text":"<pre><code>n_steps = n_steps\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/LinearScheduler/#torchebm.core.base_scheduler.LinearScheduler.step_size","title":"step_size  <code>instance-attribute</code>","text":"<pre><code>step_size: float = (end_value - start_value) / n_steps\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/LinearScheduler/#torchebm.core.base_scheduler.LinearScheduler._compute_value","title":"_compute_value","text":"<pre><code>_compute_value() -&gt; float\n</code></pre> <p>Compute the linearly interpolated value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The interpolated value, clamped to end_value after n_steps.</p> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def _compute_value(self) -&gt; float:\n    r\"\"\"\n    Compute the linearly interpolated value.\n\n    Returns:\n        float: The interpolated value, clamped to end_value after n_steps.\n    \"\"\"\n    if self.step_count &gt;= self.n_steps:\n        return self.end_value\n    else:\n        return self.start_value + self.step_size * self.step_count\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/MultiStepScheduler/","title":"MultiStepScheduler","text":""},{"location":"api/torchebm/core/base_scheduler/classes/MultiStepScheduler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>Scheduler that reduces the parameter value at specific milestone steps.</p> <p>This scheduler maintains the current value until reaching predefined milestone steps, at which point it multiplies the value by a decay factor (gamma). This creates a step-wise decay pattern commonly used in learning rate scheduling.</p> <p>Mathematical Formula</p> \\[v(t) = v_0 \\times \\gamma^k\\] <p>where:</p> <ul> <li>\\(v_0\\) is the start_value</li> <li>\\(\\gamma\\) is the gamma decay factor</li> <li>\\(k\\) is the number of milestones that have been reached by step \\(t\\)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>Initial parameter value.</p> required <code>milestones</code> <code>List[int]</code> <p>List of step numbers at which to apply decay. Must be positive and strictly increasing.</p> required <code>gamma</code> <code>float</code> <p>Multiplicative factor for decay. Defaults to 0.1.</p> <code>0.1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If milestones are not positive or not strictly increasing.</p> <p>Step-wise Learning Rate Decay</p> <pre><code>scheduler = MultiStepScheduler(\n    start_value=0.1,\n    milestones=[30, 60, 90],\n    gamma=0.1\n)\n# Simulate training steps\nfor step in [0, 29, 30, 31, 59, 60, 61, 89, 90, 91]:\n    if step &gt; 0:\n        scheduler.step_count = step\n        value = scheduler._compute_value()\n    else:\n        value = scheduler.get_value()\n    print(f\"Step {step}: {value:.4f}\")\n# Output shows: 0.1 until step 30, then 0.01, then 0.001 at step 60, etc.\n</code></pre> <p>Different Decay Strategies</p> <pre><code># Gentle decay\ngentle_scheduler = MultiStepScheduler(\n    start_value=1.0, milestones=[100, 200], gamma=0.5\n)\n\n# Aggressive decay\naggressive_scheduler = MultiStepScheduler(\n    start_value=1.0, milestones=[50, 100], gamma=0.01\n)\n</code></pre> <p>Training Loop Integration</p> <pre><code>step_scheduler = MultiStepScheduler(\n    start_value=0.01,\n    milestones=[500, 1000, 1500],\n    gamma=0.2\n)\n# In training loop\nfor epoch in range(2000):\n    current_step_size = step_scheduler.step()\n    # Use current_step_size in your algorithm\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>class MultiStepScheduler(BaseScheduler):\n    r\"\"\"\n    Scheduler that reduces the parameter value at specific milestone steps.\n\n    This scheduler maintains the current value until reaching predefined milestone\n    steps, at which point it multiplies the value by a decay factor (gamma).\n    This creates a step-wise decay pattern commonly used in learning rate scheduling.\n\n    !!! info \"Mathematical Formula\"\n        $$v(t) = v_0 \\times \\gamma^k$$\n\n        where:\n\n        - \\(v_0\\) is the start_value\n        - \\(\\gamma\\) is the gamma decay factor\n        - \\(k\\) is the number of milestones that have been reached by step \\(t\\)\n\n    Args:\n        start_value (float): Initial parameter value.\n        milestones (List[int]): List of step numbers at which to apply decay.\n            Must be positive and strictly increasing.\n        gamma (float, optional): Multiplicative factor for decay. Defaults to 0.1.\n\n    Raises:\n        ValueError: If milestones are not positive or not strictly increasing.\n\n    !!! example \"Step-wise Learning Rate Decay\"\n        ```python\n        scheduler = MultiStepScheduler(\n            start_value=0.1,\n            milestones=[30, 60, 90],\n            gamma=0.1\n        )\n        # Simulate training steps\n        for step in [0, 29, 30, 31, 59, 60, 61, 89, 90, 91]:\n            if step &gt; 0:\n                scheduler.step_count = step\n                value = scheduler._compute_value()\n            else:\n                value = scheduler.get_value()\n            print(f\"Step {step}: {value:.4f}\")\n        # Output shows: 0.1 until step 30, then 0.01, then 0.001 at step 60, etc.\n        ```\n\n    !!! tip \"Different Decay Strategies\"\n        ```python\n        # Gentle decay\n        gentle_scheduler = MultiStepScheduler(\n            start_value=1.0, milestones=[100, 200], gamma=0.5\n        )\n\n        # Aggressive decay\n        aggressive_scheduler = MultiStepScheduler(\n            start_value=1.0, milestones=[50, 100], gamma=0.01\n        )\n        ```\n\n    !!! example \"Training Loop Integration\"\n        ```python\n        step_scheduler = MultiStepScheduler(\n            start_value=0.01,\n            milestones=[500, 1000, 1500],\n            gamma=0.2\n        )\n        # In training loop\n        for epoch in range(2000):\n            current_step_size = step_scheduler.step()\n            # Use current_step_size in your algorithm\n        ```\n    \"\"\"\n\n    def __init__(self, start_value: float, milestones: List[int], gamma: float = 0.1):\n        r\"\"\"\n        Initialize the multi-step scheduler.\n\n        Args:\n            start_value (float): Initial parameter value.\n            milestones (List[int]): List of step numbers at which to apply decay.\n                Must be positive and strictly increasing.\n            gamma (float, optional): Multiplicative factor for decay. Defaults to 0.1.\n\n        Raises:\n            ValueError: If milestones are not positive or not strictly increasing.\n        \"\"\"\n        super().__init__(start_value)\n        if not all(m &gt; 0 for m in milestones):\n            raise ValueError(\"Milestone steps must be positive integers.\")\n        if not all(\n            milestones[i] &lt; milestones[i + 1] for i in range(len(milestones) - 1)\n        ):\n            raise ValueError(\"Milestones must be strictly increasing.\")\n        self.milestones = sorted(milestones)\n        self.gamma = gamma\n\n    def _compute_value(self) -&gt; float:\n        r\"\"\"\n        Compute the value based on reached milestones.\n\n        Returns:\n            float: The parameter value after applying decay for reached milestones.\n        \"\"\"\n        power = sum(1 for m in self.milestones if self.step_count &gt;= m)\n        return self.start_value * (self.gamma**power)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/MultiStepScheduler/#torchebm.core.base_scheduler.MultiStepScheduler.milestones","title":"milestones  <code>instance-attribute</code>","text":"<pre><code>milestones = sorted(milestones)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/MultiStepScheduler/#torchebm.core.base_scheduler.MultiStepScheduler.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/MultiStepScheduler/#torchebm.core.base_scheduler.MultiStepScheduler._compute_value","title":"_compute_value","text":"<pre><code>_compute_value() -&gt; float\n</code></pre> <p>Compute the value based on reached milestones.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The parameter value after applying decay for reached milestones.</p> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def _compute_value(self) -&gt; float:\n    r\"\"\"\n    Compute the value based on reached milestones.\n\n    Returns:\n        float: The parameter value after applying decay for reached milestones.\n    \"\"\"\n    power = sum(1 for m in self.milestones if self.step_count &gt;= m)\n    return self.start_value * (self.gamma**power)\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/WarmupScheduler/","title":"WarmupScheduler","text":""},{"location":"api/torchebm/core/base_scheduler/classes/WarmupScheduler/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>Scheduler that combines linear warmup with another scheduler.</p> <p>This scheduler implements a two-phase approach: first, it linearly increases the parameter value from a small initial value to the target value over a warmup period, then it follows the schedule defined by the main scheduler. Warmup is commonly used in deep learning to stabilize training in the initial phases.</p> <p>Mathematical Formula</p> \\[v(t) = \begin{cases} v_{init} + (v_{target} - v_{init})      imes \frac{t}{T_{warmup}}, &amp;     ext{if } t &lt; T_{warmup} \\         ext{main\\_scheduler}(t - T_{warmup}), &amp;         ext{if } t \\geq T_{warmup} \\end{cases}\\] <p>where:</p> <ul> <li>\\(v_{init} = v_{target}       imes    ext{warmup\\_init\\_factor}\\)</li> <li>\\(v_{target}\\) is the main scheduler's start_value</li> <li>\\(T_{warmup}\\) is warmup_steps</li> <li>\\(t\\) is the current step count</li> </ul> <p>Parameters:</p> Name Type Description Default <code>main_scheduler</code> <code>BaseScheduler</code> <p>The scheduler to use after warmup.</p> required <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps.</p> required <code>warmup_init_factor</code> <code>float</code> <p>Factor to determine initial warmup value. Defaults to 0.01.</p> <code>0.01</code> <p>Learning Rate Warmup + Cosine Annealing</p> <pre><code>main_scheduler = CosineScheduler(\n    start_value=0.1, end_value=0.001, n_steps=1000\n)\nwarmup_scheduler = WarmupScheduler(\n    main_scheduler=main_scheduler,\n    warmup_steps=100,\n    warmup_init_factor=0.01\n)\n\n# First 100 steps: linear warmup from 0.001 to 0.1\n# Next 1000 steps: cosine annealing from 0.1 to 0.001\nfor i in range(10):\n    value = warmup_scheduler.step()\n    print(f\"Warmup step {i+1}: {value:.6f}\")\n</code></pre> <p>MCMC Sampling with Warmup</p> <pre><code>decay_scheduler = ExponentialDecayScheduler(\n    start_value=0.05, decay_rate=0.999, min_value=0.001\n)\nstep_scheduler = WarmupScheduler(\n    main_scheduler=decay_scheduler,\n    warmup_steps=50,\n    warmup_init_factor=0.1\n)\n\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=step_scheduler\n)\n</code></pre> <p>Noise Scale Warmup</p> <pre><code>linear_scheduler = LinearScheduler(\n    start_value=1.0, end_value=0.01, n_steps=500\n)\nnoise_scheduler = WarmupScheduler(\n    main_scheduler=linear_scheduler,\n    warmup_steps=25,\n    warmup_init_factor=0.05\n)\n</code></pre> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>class WarmupScheduler(BaseScheduler):\n    \"\"\"\n    Scheduler that combines linear warmup with another scheduler.\n\n    This scheduler implements a two-phase approach: first, it linearly increases\n    the parameter value from a small initial value to the target value over a\n    warmup period, then it follows the schedule defined by the main scheduler.\n    Warmup is commonly used in deep learning to stabilize training in the\n    initial phases.\n\n    !!! info \"Mathematical Formula\"\n        $$v(t) = \\begin{cases}\n        v_{init} + (v_{target} - v_{init}) \\times \\frac{t}{T_{warmup}}, &amp; \\text{if } t &lt; T_{warmup} \\\\\n        \\text{main\\_scheduler}(t - T_{warmup}), &amp; \\text{if } t \\geq T_{warmup}\n        \\end{cases}$$\n\n        where:\n\n        - \\(v_{init} = v_{target} \\times \\text{warmup\\_init\\_factor}\\)\n        - \\(v_{target}\\) is the main scheduler's start_value\n        - \\(T_{warmup}\\) is warmup_steps\n        - \\(t\\) is the current step count\n\n    Args:\n        main_scheduler (BaseScheduler): The scheduler to use after warmup.\n        warmup_steps (int): Number of warmup steps.\n        warmup_init_factor (float, optional): Factor to determine initial warmup value.\n            Defaults to 0.01.\n\n    !!! example \"Learning Rate Warmup + Cosine Annealing\"\n        ```python\n        main_scheduler = CosineScheduler(\n            start_value=0.1, end_value=0.001, n_steps=1000\n        )\n        warmup_scheduler = WarmupScheduler(\n            main_scheduler=main_scheduler,\n            warmup_steps=100,\n            warmup_init_factor=0.01\n        )\n\n        # First 100 steps: linear warmup from 0.001 to 0.1\n        # Next 1000 steps: cosine annealing from 0.1 to 0.001\n        for i in range(10):\n            value = warmup_scheduler.step()\n            print(f\"Warmup step {i+1}: {value:.6f}\")\n        ```\n\n    !!! tip \"MCMC Sampling with Warmup\"\n        ```python\n        decay_scheduler = ExponentialDecayScheduler(\n            start_value=0.05, decay_rate=0.999, min_value=0.001\n        )\n        step_scheduler = WarmupScheduler(\n            main_scheduler=decay_scheduler,\n            warmup_steps=50,\n            warmup_init_factor=0.1\n        )\n\n        sampler = LangevinDynamics(\n            energy_function=energy_fn,\n            step_size=step_scheduler\n        )\n        ```\n\n    !!! example \"Noise Scale Warmup\"\n        ```python\n        linear_scheduler = LinearScheduler(\n            start_value=1.0, end_value=0.01, n_steps=500\n        )\n        noise_scheduler = WarmupScheduler(\n            main_scheduler=linear_scheduler,\n            warmup_steps=25,\n            warmup_init_factor=0.05\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        main_scheduler: BaseScheduler,\n        warmup_steps: int,\n        warmup_init_factor: float = 0.01,\n    ):\n        r\"\"\"\n        Initialize the warmup scheduler.\n\n        Args:\n            main_scheduler (BaseScheduler): The scheduler to use after warmup.\n            warmup_steps (int): Number of warmup steps.\n            warmup_init_factor (float, optional): Factor to determine initial warmup value.\n                The initial value will be main_scheduler.start_value * warmup_init_factor.\n                Defaults to 0.01.\n        \"\"\"\n        # Initialize based on the main scheduler's initial value\n        super().__init__(main_scheduler.start_value * warmup_init_factor)\n        self.main_scheduler = main_scheduler\n        self.warmup_steps = warmup_steps\n        self.warmup_init_factor = warmup_init_factor\n        self.target_value = main_scheduler.start_value  # Store the target after warmup\n\n        # Reset main scheduler as warmup controls the initial phase\n        self.main_scheduler.reset()\n\n    def _compute_value(self) -&gt; float:\n        \"\"\"\n        Compute the value based on warmup phase or main scheduler.\n\n        Returns:\n            float: The parameter value from warmup or main scheduler.\n        \"\"\"\n        if self.step_count &lt; self.warmup_steps:\n            # Linear warmup phase\n            progress = self.step_count / self.warmup_steps\n            return self.start_value + progress * (self.target_value - self.start_value)\n        else:\n            # Main scheduler phase\n            # We need its value based on steps *after* warmup\n            main_scheduler_step = self.step_count - self.warmup_steps\n            # Temporarily set main scheduler state, get value, restore state\n            original_step = self.main_scheduler.step_count\n            original_value = self.main_scheduler.current_value\n            self.main_scheduler.step_count = main_scheduler_step\n            computed_main_value = self.main_scheduler._compute_value()\n            # Restore state\n            self.main_scheduler.step_count = original_step\n            self.main_scheduler.current_value = original_value\n            return computed_main_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/WarmupScheduler/#torchebm.core.base_scheduler.WarmupScheduler.main_scheduler","title":"main_scheduler  <code>instance-attribute</code>","text":"<pre><code>main_scheduler = main_scheduler\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/WarmupScheduler/#torchebm.core.base_scheduler.WarmupScheduler.warmup_steps","title":"warmup_steps  <code>instance-attribute</code>","text":"<pre><code>warmup_steps = warmup_steps\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/WarmupScheduler/#torchebm.core.base_scheduler.WarmupScheduler.warmup_init_factor","title":"warmup_init_factor  <code>instance-attribute</code>","text":"<pre><code>warmup_init_factor = warmup_init_factor\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/WarmupScheduler/#torchebm.core.base_scheduler.WarmupScheduler.target_value","title":"target_value  <code>instance-attribute</code>","text":"<pre><code>target_value = start_value\n</code></pre>"},{"location":"api/torchebm/core/base_scheduler/classes/WarmupScheduler/#torchebm.core.base_scheduler.WarmupScheduler._compute_value","title":"_compute_value","text":"<pre><code>_compute_value() -&gt; float\n</code></pre> <p>Compute the value based on warmup phase or main scheduler.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The parameter value from warmup or main scheduler.</p> Source code in <code>torchebm/core/base_scheduler.py</code> <pre><code>def _compute_value(self) -&gt; float:\n    \"\"\"\n    Compute the value based on warmup phase or main scheduler.\n\n    Returns:\n        float: The parameter value from warmup or main scheduler.\n    \"\"\"\n    if self.step_count &lt; self.warmup_steps:\n        # Linear warmup phase\n        progress = self.step_count / self.warmup_steps\n        return self.start_value + progress * (self.target_value - self.start_value)\n    else:\n        # Main scheduler phase\n        # We need its value based on steps *after* warmup\n        main_scheduler_step = self.step_count - self.warmup_steps\n        # Temporarily set main scheduler state, get value, restore state\n        original_step = self.main_scheduler.step_count\n        original_value = self.main_scheduler.current_value\n        self.main_scheduler.step_count = main_scheduler_step\n        computed_main_value = self.main_scheduler._compute_value()\n        # Restore state\n        self.main_scheduler.step_count = original_step\n        self.main_scheduler.current_value = original_value\n        return computed_main_value\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/","title":"Torchebm &gt; Core &gt; Base_trainer","text":""},{"location":"api/torchebm/core/base_trainer/#torchebm-core-base_trainer","title":"Torchebm &gt; Core &gt; Base_trainer","text":""},{"location":"api/torchebm/core/base_trainer/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/base_trainer/#classes","title":"Classes","text":"<ul> <li><code>BaseTrainer</code> - Base class for training energy-based models.</li> <li><code>ContrastiveDivergenceTrainer</code> - Specialized trainer for contrastive divergence training of EBMs.</li> </ul>"},{"location":"api/torchebm/core/base_trainer/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/base_trainer/#torchebm.core.base_trainer","title":"torchebm.core.base_trainer","text":""},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/","title":"BaseTrainer","text":""},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#methods-and-attributes","title":"Methods and Attributes","text":"<p>Base class for training energy-based models.</p> <p>This class provides a generic interface for training EBMs, supporting various training methods and mixed precision training.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to train</p> required <code>optimizer</code> <code>Optimizer</code> <p>PyTorch optimizer to use</p> required <code>loss_fn</code> <code>BaseLoss</code> <p>Loss function for training</p> required <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to run training on</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training</p> <code>False</code> <code>callbacks</code> <code>Optional[List[Callable]]</code> <p>List of callback functions for training events</p> <code>None</code> <p>Methods:</p> Name Description <code>train_step</code> <p>Perform a single training step</p> <code>train_epoch</code> <p>Train for a full epoch</p> <code>train</code> <p>Train for multiple epochs</p> <code>validate</code> <p>Validate the model</p> <code>save_checkpoint</code> <p>Save model checkpoint</p> <code>load_checkpoint</code> <p>Load model from checkpoint</p> Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>class BaseTrainer:\n    \"\"\"\n    Base class for training energy-based models.\n\n    This class provides a generic interface for training EBMs, supporting various\n    training methods and mixed precision training.\n\n    Args:\n        energy_function: Energy function to train\n        optimizer: PyTorch optimizer to use\n        loss_fn: Loss function for training\n        device: Device to run training on\n        dtype: Data type for computations\n        use_mixed_precision: Whether to use mixed precision training\n        callbacks: List of callback functions for training events\n\n    Methods:\n        train_step: Perform a single training step\n        train_epoch: Train for a full epoch\n        train: Train for multiple epochs\n        validate: Validate the model\n        save_checkpoint: Save model checkpoint\n        load_checkpoint: Load model from checkpoint\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        optimizer: torch.optim.Optimizer,\n        loss_fn: BaseLoss,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        use_mixed_precision: bool = False,\n        callbacks: Optional[List[Callable]] = None,\n    ):\n        self.energy_function = energy_function\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n\n        # Set up device\n        if isinstance(device, str):\n            device = torch.device(device)\n        self.device = device or torch.device(\n            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n\n        # Set up dtype and mixed precision\n        self.dtype = dtype\n        self.use_mixed_precision = use_mixed_precision\n\n        # Initialize callbacks\n        self.callbacks = callbacks or []\n\n        # Configure mixed precision\n        if self.use_mixed_precision:\n            try:\n                from torch.cuda.amp import autocast, GradScaler\n\n                self.autocast_available = True\n                self.grad_scaler = GradScaler()\n\n                # Ensure device is CUDA for mixed precision\n                if not self.device.type.startswith(\"cuda\"):\n                    warnings.warn(\n                        f\"Mixed precision requested but device is {self.device}. \"\n                        f\"Mixed precision requires CUDA. Falling back to full precision.\",\n                        UserWarning,\n                    )\n                    self.use_mixed_precision = False\n                    self.autocast_available = False\n            except ImportError:\n                warnings.warn(\n                    \"Mixed precision requested but torch.cuda.amp not available. \"\n                    \"Falling back to full precision. Requires PyTorch 1.6+.\",\n                    UserWarning,\n                )\n                self.use_mixed_precision = False\n                self.autocast_available = False\n        else:\n            self.autocast_available = False\n\n        # Move model and loss function to appropriate device/dtype\n        self.energy_function = self.energy_function.to(\n            device=self.device, dtype=self.dtype\n        )\n\n        # Propagate mixed precision settings to components\n        if hasattr(self.loss_fn, \"use_mixed_precision\"):\n            self.loss_fn.use_mixed_precision = self.use_mixed_precision\n        if hasattr(self.energy_function, \"use_mixed_precision\"):\n            self.energy_function.use_mixed_precision = self.use_mixed_precision\n\n        # Move loss function to appropriate device\n        if hasattr(self.loss_fn, \"to\"):\n            self.loss_fn = self.loss_fn.to(device=self.device, dtype=self.dtype)\n\n        # Create metrics dictionary for tracking\n        self.metrics: Dict[str, Any] = {\"loss\": []}\n\n    def train_step(self, batch: torch.Tensor) -&gt; Dict[str, Any]:\n        \"\"\"\n        Perform a single training step.\n\n        Args:\n            batch: Batch of training data\n\n        Returns:\n            Dictionary containing metrics from this step\n        \"\"\"\n        # Ensure batch is on the correct device and dtype\n        batch = batch.to(device=self.device, dtype=self.dtype)\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Forward pass with mixed precision if enabled\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                loss = self.loss_fn(batch)\n\n            # Backward pass with gradient scaling\n            self.grad_scaler.scale(loss).backward()\n            self.grad_scaler.step(self.optimizer)\n            self.grad_scaler.update()\n        else:\n            # Standard training step\n            loss = self.loss_fn(batch)\n            loss.backward()\n            self.optimizer.step()\n\n        # Return metrics\n        return {\"loss\": loss.item()}\n\n    def train_epoch(self, dataloader: DataLoader) -&gt; Dict[str, float]:\n        \"\"\"\n        Train for one epoch.\n\n        Args:\n            dataloader: DataLoader containing training data\n\n        Returns:\n            Dictionary with average metrics for the epoch\n        \"\"\"\n        # Set model to training mode\n        self.energy_function.train()\n\n        # Initialize metrics for this epoch\n        epoch_metrics: Dict[str, List[float]] = {\"loss\": []}\n\n        # Iterate through batches\n        for batch in dataloader:\n            # Call any batch start callbacks\n            for callback in self.callbacks:\n                if hasattr(callback, \"on_batch_start\"):\n                    callback.on_batch_start(self, batch)\n\n            # Perform training step\n            step_metrics = self.train_step(batch)\n\n            # Update epoch metrics\n            for key, value in step_metrics.items():\n                if key not in epoch_metrics:\n                    epoch_metrics[key] = []\n                epoch_metrics[key].append(value)\n\n            # Call any batch end callbacks\n            for callback in self.callbacks:\n                if hasattr(callback, \"on_batch_end\"):\n                    callback.on_batch_end(self, batch, step_metrics)\n\n        # Calculate average metrics\n        avg_metrics = {\n            key: sum(values) / len(values) for key, values in epoch_metrics.items()\n        }\n\n        return avg_metrics\n\n    def train(\n        self,\n        dataloader: DataLoader,\n        num_epochs: int,\n        validate_fn: Optional[Callable] = None,\n    ) -&gt; Dict[str, List[float]]:\n        \"\"\"\n        Train the model for multiple epochs.\n\n        Args:\n            dataloader: DataLoader containing training data\n            num_epochs: Number of epochs to train for\n            validate_fn: Optional function for validation after each epoch\n\n        Returns:\n            Dictionary with metrics over all epochs\n        \"\"\"\n        # Initialize training history\n        history: Dict[str, List[float]] = {\"loss\": []}\n\n        # Call any training start callbacks\n        for callback in self.callbacks:\n            if hasattr(callback, \"on_train_start\"):\n                callback.on_train_start(self)\n\n        # Train for specified number of epochs\n        for epoch in range(num_epochs):\n            # Call any epoch start callbacks\n            for callback in self.callbacks:\n                if hasattr(callback, \"on_epoch_start\"):\n                    callback.on_epoch_start(self, epoch)\n\n            # Train for one epoch\n            epoch_metrics = self.train_epoch(dataloader)\n\n            # Update training history\n            for key, value in epoch_metrics.items():\n                if key not in history:\n                    history[key] = []\n                history[key].append(value)\n\n            # Print progress\n            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_metrics['loss']:.6f}\")\n\n            # Validate if function provided\n            if validate_fn is not None:\n                val_metrics = validate_fn(self.energy_function)\n                print(f\"Validation: {val_metrics}\")\n\n                # Update validation metrics in history\n                for key, value in val_metrics.items():\n                    val_key = f\"val_{key}\"\n                    if val_key not in history:\n                        history[val_key] = []\n                    history[val_key].append(value)\n\n            # Call any epoch end callbacks\n            for callback in self.callbacks:\n                if hasattr(callback, \"on_epoch_end\"):\n                    callback.on_epoch_end(self, epoch, epoch_metrics)\n\n        # Call any training end callbacks\n        for callback in self.callbacks:\n            if hasattr(callback, \"on_train_end\"):\n                callback.on_train_end(self, history)\n\n        return history\n\n    def save_checkpoint(self, path: str) -&gt; None:\n        \"\"\"\n        Save a checkpoint of the current training state.\n\n        Args:\n            path: Path to save the checkpoint to\n        \"\"\"\n        checkpoint = {\n            \"energy_function_state_dict\": self.energy_function.state_dict(),\n            \"optimizer_state_dict\": self.optimizer.state_dict(),\n            \"metrics\": self.metrics,\n        }\n\n        if self.use_mixed_precision and hasattr(self, \"grad_scaler\"):\n            checkpoint[\"grad_scaler_state_dict\"] = self.grad_scaler.state_dict()\n\n        torch.save(checkpoint, path)\n\n    def load_checkpoint(self, path: str) -&gt; None:\n        \"\"\"\n        Load a checkpoint to resume training.\n\n        Args:\n            path: Path to the checkpoint file\n        \"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n\n        self.energy_function.load_state_dict(checkpoint[\"energy_function_state_dict\"])\n        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n        if \"metrics\" in checkpoint:\n            self.metrics = checkpoint[\"metrics\"]\n\n        if (\n            self.use_mixed_precision\n            and \"grad_scaler_state_dict\" in checkpoint\n            and hasattr(self, \"grad_scaler\")\n        ):\n            self.grad_scaler.load_state_dict(checkpoint[\"grad_scaler_state_dict\"])\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = optimizer\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.loss_fn","title":"loss_fn  <code>instance-attribute</code>","text":"<pre><code>loss_fn = loss_fn\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device or device('cuda' if is_available() else 'cpu')\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.use_mixed_precision","title":"use_mixed_precision  <code>instance-attribute</code>","text":"<pre><code>use_mixed_precision = use_mixed_precision\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.callbacks","title":"callbacks  <code>instance-attribute</code>","text":"<pre><code>callbacks = callbacks or []\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.autocast_available","title":"autocast_available  <code>instance-attribute</code>","text":"<pre><code>autocast_available = True\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.grad_scaler","title":"grad_scaler  <code>instance-attribute</code>","text":"<pre><code>grad_scaler = GradScaler()\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = to(device=device, dtype=dtype)\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics: Dict[str, Any] = {'loss': []}\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.train_step","title":"train_step","text":"<pre><code>train_step(batch: Tensor) -&gt; Dict[str, Any]\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of training data</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing metrics from this step</p> Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>def train_step(self, batch: torch.Tensor) -&gt; Dict[str, Any]:\n    \"\"\"\n    Perform a single training step.\n\n    Args:\n        batch: Batch of training data\n\n    Returns:\n        Dictionary containing metrics from this step\n    \"\"\"\n    # Ensure batch is on the correct device and dtype\n    batch = batch.to(device=self.device, dtype=self.dtype)\n\n    # Zero gradients\n    self.optimizer.zero_grad()\n\n    # Forward pass with mixed precision if enabled\n    if self.use_mixed_precision and self.autocast_available:\n        from torch.cuda.amp import autocast\n\n        with autocast():\n            loss = self.loss_fn(batch)\n\n        # Backward pass with gradient scaling\n        self.grad_scaler.scale(loss).backward()\n        self.grad_scaler.step(self.optimizer)\n        self.grad_scaler.update()\n    else:\n        # Standard training step\n        loss = self.loss_fn(batch)\n        loss.backward()\n        self.optimizer.step()\n\n    # Return metrics\n    return {\"loss\": loss.item()}\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.train_epoch","title":"train_epoch","text":"<pre><code>train_epoch(dataloader: DataLoader) -&gt; Dict[str, float]\n</code></pre> <p>Train for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing training data</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with average metrics for the epoch</p> Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>def train_epoch(self, dataloader: DataLoader) -&gt; Dict[str, float]:\n    \"\"\"\n    Train for one epoch.\n\n    Args:\n        dataloader: DataLoader containing training data\n\n    Returns:\n        Dictionary with average metrics for the epoch\n    \"\"\"\n    # Set model to training mode\n    self.energy_function.train()\n\n    # Initialize metrics for this epoch\n    epoch_metrics: Dict[str, List[float]] = {\"loss\": []}\n\n    # Iterate through batches\n    for batch in dataloader:\n        # Call any batch start callbacks\n        for callback in self.callbacks:\n            if hasattr(callback, \"on_batch_start\"):\n                callback.on_batch_start(self, batch)\n\n        # Perform training step\n        step_metrics = self.train_step(batch)\n\n        # Update epoch metrics\n        for key, value in step_metrics.items():\n            if key not in epoch_metrics:\n                epoch_metrics[key] = []\n            epoch_metrics[key].append(value)\n\n        # Call any batch end callbacks\n        for callback in self.callbacks:\n            if hasattr(callback, \"on_batch_end\"):\n                callback.on_batch_end(self, batch, step_metrics)\n\n    # Calculate average metrics\n    avg_metrics = {\n        key: sum(values) / len(values) for key, values in epoch_metrics.items()\n    }\n\n    return avg_metrics\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.train","title":"train","text":"<pre><code>train(dataloader: DataLoader, num_epochs: int, validate_fn: Optional[Callable] = None) -&gt; Dict[str, List[float]]\n</code></pre> <p>Train the model for multiple epochs.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing training data</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train for</p> required <code>validate_fn</code> <code>Optional[Callable]</code> <p>Optional function for validation after each epoch</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[float]]</code> <p>Dictionary with metrics over all epochs</p> Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>def train(\n    self,\n    dataloader: DataLoader,\n    num_epochs: int,\n    validate_fn: Optional[Callable] = None,\n) -&gt; Dict[str, List[float]]:\n    \"\"\"\n    Train the model for multiple epochs.\n\n    Args:\n        dataloader: DataLoader containing training data\n        num_epochs: Number of epochs to train for\n        validate_fn: Optional function for validation after each epoch\n\n    Returns:\n        Dictionary with metrics over all epochs\n    \"\"\"\n    # Initialize training history\n    history: Dict[str, List[float]] = {\"loss\": []}\n\n    # Call any training start callbacks\n    for callback in self.callbacks:\n        if hasattr(callback, \"on_train_start\"):\n            callback.on_train_start(self)\n\n    # Train for specified number of epochs\n    for epoch in range(num_epochs):\n        # Call any epoch start callbacks\n        for callback in self.callbacks:\n            if hasattr(callback, \"on_epoch_start\"):\n                callback.on_epoch_start(self, epoch)\n\n        # Train for one epoch\n        epoch_metrics = self.train_epoch(dataloader)\n\n        # Update training history\n        for key, value in epoch_metrics.items():\n            if key not in history:\n                history[key] = []\n            history[key].append(value)\n\n        # Print progress\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_metrics['loss']:.6f}\")\n\n        # Validate if function provided\n        if validate_fn is not None:\n            val_metrics = validate_fn(self.energy_function)\n            print(f\"Validation: {val_metrics}\")\n\n            # Update validation metrics in history\n            for key, value in val_metrics.items():\n                val_key = f\"val_{key}\"\n                if val_key not in history:\n                    history[val_key] = []\n                history[val_key].append(value)\n\n        # Call any epoch end callbacks\n        for callback in self.callbacks:\n            if hasattr(callback, \"on_epoch_end\"):\n                callback.on_epoch_end(self, epoch, epoch_metrics)\n\n    # Call any training end callbacks\n    for callback in self.callbacks:\n        if hasattr(callback, \"on_train_end\"):\n            callback.on_train_end(self, history)\n\n    return history\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(path: str) -&gt; None\n</code></pre> <p>Save a checkpoint of the current training state.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the checkpoint to</p> required Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>def save_checkpoint(self, path: str) -&gt; None:\n    \"\"\"\n    Save a checkpoint of the current training state.\n\n    Args:\n        path: Path to save the checkpoint to\n    \"\"\"\n    checkpoint = {\n        \"energy_function_state_dict\": self.energy_function.state_dict(),\n        \"optimizer_state_dict\": self.optimizer.state_dict(),\n        \"metrics\": self.metrics,\n    }\n\n    if self.use_mixed_precision and hasattr(self, \"grad_scaler\"):\n        checkpoint[\"grad_scaler_state_dict\"] = self.grad_scaler.state_dict()\n\n    torch.save(checkpoint, path)\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/BaseTrainer/#torchebm.core.base_trainer.BaseTrainer.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(path: str) -&gt; None\n</code></pre> <p>Load a checkpoint to resume training.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the checkpoint file</p> required Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>def load_checkpoint(self, path: str) -&gt; None:\n    \"\"\"\n    Load a checkpoint to resume training.\n\n    Args:\n        path: Path to the checkpoint file\n    \"\"\"\n    checkpoint = torch.load(path, map_location=self.device)\n\n    self.energy_function.load_state_dict(checkpoint[\"energy_function_state_dict\"])\n    self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n    if \"metrics\" in checkpoint:\n        self.metrics = checkpoint[\"metrics\"]\n\n    if (\n        self.use_mixed_precision\n        and \"grad_scaler_state_dict\" in checkpoint\n        and hasattr(self, \"grad_scaler\")\n    ):\n        self.grad_scaler.load_state_dict(checkpoint[\"grad_scaler_state_dict\"])\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/ContrastiveDivergenceTrainer/","title":"ContrastiveDivergenceTrainer","text":""},{"location":"api/torchebm/core/base_trainer/classes/ContrastiveDivergenceTrainer/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Specialized trainer for contrastive divergence training of EBMs.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to train</p> required <code>sampler</code> <code>BaseSampler</code> <p>MCMC sampler for generating negative samples</p> required <code>optimizer</code> <code>Optional[Optimizer]</code> <p>PyTorch optimizer</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>Learning rate (if optimizer not provided)</p> <code>0.01</code> <code>k_steps</code> <code>int</code> <p>Number of MCMC steps for generating samples</p> <code>10</code> <code>persistent</code> <code>bool</code> <p>Whether to use persistent contrastive divergence (PCD)</p> <code>False</code> <code>buffer_size</code> <code>int</code> <p>Replay buffer size for PCD</p> <code>1000</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to run training on</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training</p> <code>False</code> Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>class ContrastiveDivergenceTrainer(BaseTrainer):\n    \"\"\"\n    Specialized trainer for contrastive divergence training of EBMs.\n\n    Args:\n        energy_function: Energy function to train\n        sampler: MCMC sampler for generating negative samples\n        optimizer: PyTorch optimizer\n        learning_rate: Learning rate (if optimizer not provided)\n        k_steps: Number of MCMC steps for generating samples\n        persistent: Whether to use persistent contrastive divergence (PCD)\n        buffer_size: Replay buffer size for PCD\n        device: Device to run training on\n        dtype: Data type for computations\n        use_mixed_precision: Whether to use mixed precision training\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        sampler: BaseSampler,\n        optimizer: Optional[torch.optim.Optimizer] = None,\n        learning_rate: float = 0.01,\n        k_steps: int = 10,\n        persistent: bool = False,\n        buffer_size: int = 1000,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        use_mixed_precision: bool = False,\n    ):\n        # Create optimizer if not provided\n        if optimizer is None:\n            optimizer = torch.optim.Adam(energy_function.parameters(), lr=learning_rate)\n\n        # Import here to avoid circular import\n        from torchebm.losses.contrastive_divergence import ContrastiveDivergence\n\n        # Create loss function\n        loss_fn = ContrastiveDivergence(\n            energy_function=energy_function,\n            sampler=sampler,\n            k_steps=k_steps,\n            persistent=persistent,\n            buffer_size=buffer_size,\n            dtype=dtype,\n            device=device,\n            use_mixed_precision=use_mixed_precision,\n        )\n\n        # Initialize base trainer\n        super().__init__(\n            energy_function=energy_function,\n            optimizer=optimizer,\n            loss_fn=loss_fn,\n            device=device,\n            dtype=dtype,\n            use_mixed_precision=use_mixed_precision,\n        )\n\n        self.sampler = sampler\n\n    def train_step(self, batch: torch.Tensor) -&gt; Dict[str, Any]:\n        \"\"\"\n        Perform a single contrastive divergence training step.\n\n        Args:\n            batch: Batch of real data samples\n\n        Returns:\n            Dictionary containing metrics from this step\n        \"\"\"\n        # Ensure batch is on the correct device and dtype\n        batch = batch.to(device=self.device, dtype=self.dtype)\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Forward pass with mixed precision if enabled\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                # ContrastiveDivergence returns (loss, neg_samples)\n                loss, neg_samples = self.loss_fn(batch)\n\n            # Backward pass with gradient scaling\n            self.grad_scaler.scale(loss).backward()\n            self.grad_scaler.step(self.optimizer)\n            self.grad_scaler.update()\n        else:\n            # Standard training step\n            loss, neg_samples = self.loss_fn(batch)\n            loss.backward()\n            self.optimizer.step()\n\n        # Return metrics\n        return {\n            \"loss\": loss.item(),\n            \"pos_energy\": self.energy_function(batch).mean().item(),\n            \"neg_energy\": self.energy_function(neg_samples).mean().item(),\n        }\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.base_trainer.ContrastiveDivergenceTrainer.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = sampler\n</code></pre>"},{"location":"api/torchebm/core/base_trainer/classes/ContrastiveDivergenceTrainer/#torchebm.core.base_trainer.ContrastiveDivergenceTrainer.train_step","title":"train_step","text":"<pre><code>train_step(batch: Tensor) -&gt; Dict[str, Any]\n</code></pre> <p>Perform a single contrastive divergence training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of real data samples</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing metrics from this step</p> Source code in <code>torchebm/core/base_trainer.py</code> <pre><code>def train_step(self, batch: torch.Tensor) -&gt; Dict[str, Any]:\n    \"\"\"\n    Perform a single contrastive divergence training step.\n\n    Args:\n        batch: Batch of real data samples\n\n    Returns:\n        Dictionary containing metrics from this step\n    \"\"\"\n    # Ensure batch is on the correct device and dtype\n    batch = batch.to(device=self.device, dtype=self.dtype)\n\n    # Zero gradients\n    self.optimizer.zero_grad()\n\n    # Forward pass with mixed precision if enabled\n    if self.use_mixed_precision and self.autocast_available:\n        from torch.cuda.amp import autocast\n\n        with autocast():\n            # ContrastiveDivergence returns (loss, neg_samples)\n            loss, neg_samples = self.loss_fn(batch)\n\n        # Backward pass with gradient scaling\n        self.grad_scaler.scale(loss).backward()\n        self.grad_scaler.step(self.optimizer)\n        self.grad_scaler.update()\n    else:\n        # Standard training step\n        loss, neg_samples = self.loss_fn(batch)\n        loss.backward()\n        self.optimizer.step()\n\n    # Return metrics\n    return {\n        \"loss\": loss.item(),\n        \"pos_energy\": self.energy_function(batch).mean().item(),\n        \"neg_energy\": self.energy_function(neg_samples).mean().item(),\n    }\n</code></pre>"},{"location":"api/torchebm/core/device_mixin/","title":"Torchebm &gt; Core &gt; Device_mixin","text":""},{"location":"api/torchebm/core/device_mixin/#torchebm-core-device_mixin","title":"Torchebm &gt; Core &gt; Device_mixin","text":""},{"location":"api/torchebm/core/device_mixin/#contents","title":"Contents","text":""},{"location":"api/torchebm/core/device_mixin/#classes","title":"Classes","text":"<ul> <li><code>DeviceMixin</code> - Consistent device management across all modules.</li> </ul>"},{"location":"api/torchebm/core/device_mixin/#functions","title":"Functions","text":"<ul> <li><code>normalize_device()</code> - Normalize the device for consistent usage across the library.</li> </ul>"},{"location":"api/torchebm/core/device_mixin/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/core/device_mixin/#torchebm.core.device_mixin","title":"torchebm.core.device_mixin","text":"<p>Author: Soran Ghaderi Email: soran.gdr.cs@gmail.com This module handles the device management or TorchEBM modules</p>"},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/","title":"DeviceMixin","text":""},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/#methods-and-attributes","title":"Methods and Attributes","text":"<p>Consistent device management across all modules.</p> <p>This should be inherited by all classes.</p> Source code in <code>torchebm/core/device_mixin.py</code> <pre><code>class DeviceMixin:\n    \"\"\"Consistent device management across all modules.\n\n    This should be inherited by all classes.\n    \"\"\"\n\n    def __init__(self, device: Union[str, torch.device, None] = None, dtype: Optional[torch.dtype] = None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._device = normalize_device(device)\n        self._dtype: Optional[torch.dtype] = dtype\n\n    @property\n    def device(self) -&gt; torch.device:\n        if self._device is not None:\n            return normalize_device(self._device)\n        if self._device is None:\n            if hasattr(self, \"parameters\") and callable(getattr(self, \"parameters\")):\n                try:\n                    param_device = next(self.parameters()).device\n                    return normalize_device(param_device)\n                except StopIteration:\n                    pass\n\n            if hasattr(self, \"buffers\") and callable(getattr(self, \"buffers\")):\n                try:\n                    buffer_device = next(self.buffers()).device\n                    return normalize_device(buffer_device)\n                except StopIteration:\n                    pass\n\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    @property\n    def dtype(self) -&gt; torch.dtype:\n        if self._dtype is not None:\n            return self._dtype\n        # Try infer from parameters/buffers if available\n        if hasattr(self, \"parameters\") and callable(getattr(self, \"parameters\")):\n            try:\n                param_dtype = next(self.parameters()).dtype\n                return param_dtype\n            except StopIteration:\n                pass\n        if hasattr(self, \"buffers\") and callable(getattr(self, \"buffers\")):\n            try:\n                buffer_dtype = next(self.buffers()).dtype\n                return buffer_dtype\n            except StopIteration:\n                pass\n        return torch.float32\n\n    @dtype.setter\n    def dtype(self, value: torch.dtype):\n        self._dtype = value\n\n    def to(self, *args, **kwargs):\n        \"\"\"Override to() to update internal device tracking.\"\"\"\n        # Call parent's to() if it exists (e.g., nn.Module); otherwise, operate in-place\n        parent_to = getattr(super(), \"to\", None)\n        result = self\n        if callable(parent_to):\n            result = parent_to(*args, **kwargs)\n\n        # Update internal device tracking based on provided args/kwargs\n        target_device = None\n        target_dtype = None\n        if args and isinstance(args[0], (str, torch.device)):\n            target_device = normalize_device(args[0])\n        elif args and isinstance(args[0], torch.dtype):\n            target_dtype = args[0]\n        if \"device\" in kwargs:\n            target_device = normalize_device(kwargs[\"device\"])\n        if \"dtype\" in kwargs and isinstance(kwargs[\"dtype\"], torch.dtype):\n            target_dtype = kwargs[\"dtype\"]\n        if target_device is not None:\n            self._device = target_device\n        if target_dtype is not None:\n            self._dtype = target_dtype\n\n        return result\n\n    @staticmethod\n    def safe_to(obj, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None):\n        \"\"\"\n        Safely move an object supporting .to(...) to device/dtype, handling different signatures.\n        \"\"\"\n        if not hasattr(obj, \"to\") or not callable(getattr(obj, \"to\")):\n            return obj\n        try:\n            if device is not None or dtype is not None:\n                return obj.to(device=device, dtype=dtype)\n            return obj\n        except TypeError:\n            # Fallbacks for custom signatures\n            if device is not None:\n                try:\n                    return obj.to(device)\n                except Exception:\n                    pass\n            if dtype is not None:\n                try:\n                    return obj.to(dtype)\n                except Exception:\n                    pass\n            return obj\n</code></pre>"},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/#torchebm.core.device_mixin.DeviceMixin._device","title":"_device  <code>instance-attribute</code>","text":"<pre><code>_device = normalize_device(device)\n</code></pre>"},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/#torchebm.core.device_mixin.DeviceMixin._dtype","title":"_dtype  <code>instance-attribute</code>","text":"<pre><code>_dtype: Optional[dtype] = dtype\n</code></pre>"},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/#torchebm.core.device_mixin.DeviceMixin.device","title":"device  <code>property</code>","text":"<pre><code>device: device\n</code></pre>"},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/#torchebm.core.device_mixin.DeviceMixin.dtype","title":"dtype  <code>property</code> <code>writable</code>","text":"<pre><code>dtype: dtype\n</code></pre>"},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/#torchebm.core.device_mixin.DeviceMixin.to","title":"to","text":"<pre><code>to(*args, **kwargs)\n</code></pre> <p>Override to() to update internal device tracking.</p> Source code in <code>torchebm/core/device_mixin.py</code> <pre><code>def to(self, *args, **kwargs):\n    \"\"\"Override to() to update internal device tracking.\"\"\"\n    # Call parent's to() if it exists (e.g., nn.Module); otherwise, operate in-place\n    parent_to = getattr(super(), \"to\", None)\n    result = self\n    if callable(parent_to):\n        result = parent_to(*args, **kwargs)\n\n    # Update internal device tracking based on provided args/kwargs\n    target_device = None\n    target_dtype = None\n    if args and isinstance(args[0], (str, torch.device)):\n        target_device = normalize_device(args[0])\n    elif args and isinstance(args[0], torch.dtype):\n        target_dtype = args[0]\n    if \"device\" in kwargs:\n        target_device = normalize_device(kwargs[\"device\"])\n    if \"dtype\" in kwargs and isinstance(kwargs[\"dtype\"], torch.dtype):\n        target_dtype = kwargs[\"dtype\"]\n    if target_device is not None:\n        self._device = target_device\n    if target_dtype is not None:\n        self._dtype = target_dtype\n\n    return result\n</code></pre>"},{"location":"api/torchebm/core/device_mixin/classes/DeviceMixin/#torchebm.core.device_mixin.DeviceMixin.safe_to","title":"safe_to  <code>staticmethod</code>","text":"<pre><code>safe_to(obj, device: Optional[device] = None, dtype: Optional[dtype] = None)\n</code></pre> <p>Safely move an object supporting .to(...) to device/dtype, handling different signatures.</p> Source code in <code>torchebm/core/device_mixin.py</code> <pre><code>@staticmethod\ndef safe_to(obj, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None):\n    \"\"\"\n    Safely move an object supporting .to(...) to device/dtype, handling different signatures.\n    \"\"\"\n    if not hasattr(obj, \"to\") or not callable(getattr(obj, \"to\")):\n        return obj\n    try:\n        if device is not None or dtype is not None:\n            return obj.to(device=device, dtype=dtype)\n        return obj\n    except TypeError:\n        # Fallbacks for custom signatures\n        if device is not None:\n            try:\n                return obj.to(device)\n            except Exception:\n                pass\n        if dtype is not None:\n            try:\n                return obj.to(dtype)\n            except Exception:\n                pass\n        return obj\n</code></pre>"},{"location":"api/torchebm/cuda/","title":"Cuda","text":""},{"location":"api/torchebm/cuda/#torchebm-cuda","title":"Torchebm &gt; Cuda","text":""},{"location":"api/torchebm/cuda/#contents","title":"Contents","text":""},{"location":"api/torchebm/cuda/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/cuda/#torchebm.cuda","title":"torchebm.cuda","text":"<p>CUDA-accelerated implementations of key operations for improved performance.</p>"},{"location":"api/torchebm/datasets/","title":"Torchebm &gt; Datasets","text":""},{"location":"api/torchebm/datasets/#torchebm-datasets","title":"Torchebm &gt; Datasets","text":""},{"location":"api/torchebm/datasets/#contents","title":"Contents","text":""},{"location":"api/torchebm/datasets/#modules","title":"Modules","text":"<ul> <li>Generators</li> </ul>"},{"location":"api/torchebm/datasets/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/datasets/#torchebm.datasets","title":"torchebm.datasets","text":""},{"location":"api/torchebm/datasets/generators/","title":"Torchebm &gt; Datasets &gt; Generators","text":""},{"location":"api/torchebm/datasets/generators/#torchebm-datasets-generators","title":"Torchebm &gt; Datasets &gt; Generators","text":""},{"location":"api/torchebm/datasets/generators/#contents","title":"Contents","text":""},{"location":"api/torchebm/datasets/generators/#classes","title":"Classes","text":"<ul> <li><code>BaseSyntheticDataset</code> - Abstract Base Class for generating 2D synthetic datasets.</li> <li><code>CheckerboardDataset</code> - Generates points in a 2D checkerboard pattern using rejection sampling.</li> <li><code>CircleDataset</code> - Generates points sampled uniformly on a circle with noise.</li> <li><code>EightGaussiansDataset</code> - Generates samples from the specific '8 Gaussians' mixture.</li> <li><code>GaussianMixtureDataset</code> - Generates samples from a 2D Gaussian mixture arranged uniformly in a circle.</li> <li><code>GridDataset</code> - Generates points on a 2D grid within [-range_limit, range_limit].</li> <li><code>PinwheelDataset</code> - Generates the pinwheel dataset with curved blades.</li> <li><code>SwissRollDataset</code> - Generates a 2D Swiss roll dataset.</li> <li><code>TwoMoonsDataset</code> - Generates the 'two moons' dataset.</li> </ul>"},{"location":"api/torchebm/datasets/generators/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/datasets/generators/#torchebm.datasets.generators","title":"torchebm.datasets.generators","text":"<p>Dataset Generators Module.</p> <p>This module provides a collection of classes for generating synthetic datasets commonly used in testing and evaluating energy-based models. These generators create various 2D distributions with different characteristics, making them ideal for visualization and demonstration purposes. They are implemented as PyTorch Datasets for easy integration with DataLoaders.</p> <p>Key Features</p> <ul> <li>Diverse collection of 2D synthetic distributions via classes.</li> <li>Configurable sample sizes, noise levels, and distribution parameters.</li> <li>Direct compatibility with <code>torch.utils.data.Dataset</code> and <code>DataLoader</code>.</li> <li>Device and dtype support for tensor outputs.</li> <li>Reproducibility through random seeds.</li> <li>Visualization support for generated datasets.</li> </ul>"},{"location":"api/torchebm/datasets/generators/#torchebm.datasets.generators--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>BaseSyntheticDataset</code> <p>Abstract base class for synthetic dataset generators.</p> <code>GaussianMixtureDataset</code> <p>Generates samples from a 2D Gaussian mixture arranged in a circle.</p> <code>EightGaussiansDataset</code> <p>Generates samples from a specific 8-component Gaussian mixture.</p> <code>TwoMoonsDataset</code> <p>Generates the classic \"two moons\" dataset.</p> <code>SwissRollDataset</code> <p>Generates a 2D Swiss roll dataset.</p> <code>CircleDataset</code> <p>Generates points sampled uniformly on a circle with noise.</p> <code>CheckerboardDataset</code> <p>Generates points in a 2D checkerboard pattern.</p> <code>PinwheelDataset</code> <p>Generates the pinwheel dataset with specified number of \"blades\".</p> <code>GridDataset</code> <p>Generates points on a regular 2D grid with optional noise.</p>"},{"location":"api/torchebm/datasets/generators/#torchebm.datasets.generators--usage-example","title":"Usage Example","text":"<p>Generating and Using Datasets</p> <pre><code>from torchebm.datasets.generators import TwoMoonsDataset, GaussianMixtureDataset\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport torch\n\n# Instantiate dataset objects\nmoons_dataset = TwoMoonsDataset(n_samples=1000, noise=0.05, seed=42)\nmixture_dataset = GaussianMixtureDataset(n_samples=500, n_components=4, std=0.1, seed=123)\n\n# Access the full dataset tensor\nmoons_data = moons_dataset.get_data()\nmixture_data = mixture_dataset.get_data()\nprint(f\"Two Moons data batch_shape: {moons_data.batch_shape}\")\nprint(f\"Mixture data batch_shape: {mixture_data.batch_shape}\")\n\n# Use with DataLoader\ndataloader = DataLoader(moons_dataset, batch_size=32, shuffle=True)\nfirst_batch = next(iter(dataloader))\nprint(f\"First batch batch_shape: {first_batch.batch_shape}\")\n\n# Visualize the datasets\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(moons_data[:, 0], moons_data[:, 1], s=5)\nplt.title(\"Two Moons\")\nplt.subplot(1, 2, 2)\nplt.scatter(mixture_data[:, 0], mixture_data[:, 1], s=5)\nplt.title(\"Gaussian Mixture\")\nplt.show()\n</code></pre>"},{"location":"api/torchebm/datasets/generators/#torchebm.datasets.generators--mathematical-background","title":"Mathematical Background","text":"<p>Distribution Characteristics</p> <p>Each dataset generator creates points from a different probability distribution:</p> <ul> <li>Gaussian Mixtures: Weighted combinations of Gaussian distributions, often arranged   in specific patterns like circles.</li> <li>Two Moons: Two interlocking half-circles with added noise, creating a challenging   bimodal distribution that's not linearly separable.</li> <li>Checkerboard: Alternating high and low density regions in a grid pattern, testing   an EBM's ability to capture multiple modes in a regular structure.</li> <li>Swiss Roll: A 2D manifold with spiral structure, testing the model's ability to   learn curved manifolds.</li> </ul> <p>Choosing a Dataset</p> <ul> <li>For testing basic density estimation: use <code>GaussianMixtureDataset</code></li> <li>For evaluating mode-seeking behavior: use <code>EightGaussiansDataset</code> or <code>CheckerboardDataset</code></li> <li>For testing separation of entangled distributions: use <code>TwoMoonsDataset</code></li> <li>For manifold learning: use <code>SwissRollDataset</code> or <code>CircleDataset</code></li> </ul>"},{"location":"api/torchebm/datasets/generators/#torchebm.datasets.generators--implementation-details","title":"Implementation Details","text":"<p>Device Handling</p> <p>The <code>device</code> parameter determines where the generated dataset tensor resides. Set this appropriately (e.g., 'cuda') for efficient GPU usage.</p> <p>Random Number Generation</p> <p>The generators use PyTorch and NumPy random functions. For reproducible results, provide a <code>seed</code> argument during instantiation.</p>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/","title":"BaseSyntheticDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>Dataset</code>, <code>ABC</code></p> <p>Abstract Base Class for generating 2D synthetic datasets.</p> <p>Provides common functionality for handling sample size, device, dtype, seeding, and PyTorch Dataset integration. Subclasses must implement the <code>_generate_data</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Total number of samples to generate.</p> required <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place the tensor on.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the output tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. If None, generation will be non-deterministic.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class BaseSyntheticDataset(Dataset, ABC):\n    \"\"\"\n    Abstract Base Class for generating 2D synthetic datasets.\n\n    Provides common functionality for handling sample size, device, dtype,\n    seeding, and PyTorch Dataset integration. Subclasses must implement\n    the `_generate_data` method.\n\n    Args:\n        n_samples (int): Total number of samples to generate.\n        device (Optional[Union[str, torch.device]]): Device to place the tensor on.\n        dtype (torch.dtype): Data type for the output tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility. If None, generation\n            will be non-deterministic.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        if n_samples &lt;= 0:\n            raise ValueError(\"n_samples must be positive\")\n\n        self.n_samples = n_samples\n        self.device = device\n        self.dtype = dtype\n        self.seed = seed\n        self.data: Optional[torch.Tensor] = None  # Data will be stored here\n        self._generate()  # Generate data upon initialization\n\n    def _seed_generators(self):\n        \"\"\"Sets the random seeds for numpy and torch if a seed is provided.\"\"\"\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n            # If using CUDA, also seed the CUDA generator\n            if torch.cuda.is_available() and (\n                isinstance(self.device, torch.device)\n                and self.device.type == \"cuda\"\n                or self.device == \"cuda\"\n            ):\n                torch.cuda.manual_seed_all(self.seed)  # Seed all GPUs\n\n    @abstractmethod\n    def _generate_data(self) -&gt; torch.Tensor:\n        \"\"\"\n        Core data generation logic to be implemented by subclasses.\n\n        This method should perform the actual sampling based on the dataset's\n        specific parameters and return the generated data as a NumPy array\n        or directly as a PyTorch tensor (without final device/dtype conversion,\n        as that's handled by the base class).\n        \"\"\"\n        pass\n\n    def _generate(self):\n        \"\"\"Internal method to handle seeding and call the generation logic.\"\"\"\n        self._seed_generators()\n        # Generate data using the subclass implementation\n        generated_output = self._generate_data()\n\n        # Ensure it's a tensor and on the correct device/dtype\n        if isinstance(generated_output, np.ndarray):\n            self.data = _to_tensor(\n                generated_output, dtype=self.dtype, device=self.device\n            )\n        elif isinstance(generated_output, torch.Tensor):\n            self.data = generated_output.to(dtype=self.dtype, device=self.device)\n        else:\n            raise TypeError(\n                f\"_generate_data must return a NumPy array or PyTorch Tensor, got {type(generated_output)}\"\n            )\n\n        # Verify batch_shape\n        if self.data.shape[0] != self.n_samples:\n            warnings.warn(\n                f\"Generated data has {self.data.shape[0]} samples, but {self.n_samples} were requested. Check generation logic.\",\n                RuntimeWarning,\n            )\n            # Optional: adjust self.n_samples or raise error depending on desired strictness\n            # self.n_samples = self.data.batch_shape[0]\n\n    def regenerate(self, seed: Optional[int] = None):\n        \"\"\"\n        Re-generates the dataset, optionally with a new seed.\n\n        Args:\n            seed (Optional[int]): New random seed. If None, uses the original seed\n                                  (if provided) or remains non-deterministic.\n        \"\"\"\n        if seed is not None:\n            self.seed = seed  # Update the seed if a new one is provided\n        self._generate()\n\n    def get_data(self) -&gt; torch.Tensor:\n        \"\"\"\n        Returns the entire generated dataset as a single tensor.\n\n        Returns:\n            torch.Tensor: The generated data tensor.\n        \"\"\"\n        if self.data is None:\n            # Should not happen if _generate() is called in __init__\n            self._generate()\n        return self.data\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of samples in the dataset.\"\"\"\n        return self.n_samples\n\n    def __getitem__(self, idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Returns the sample at the specified index.\n\n        Args:\n            idx (int): The index of the sample to retrieve.\n\n        Returns:\n            torch.Tensor: The data sample at the given index.\n        \"\"\"\n        if self.data is None:\n            self._generate()  # Ensure data exists\n\n        if not 0 &lt;= idx &lt; self.n_samples:\n            raise IndexError(\n                f\"Index {idx} out of bounds for dataset with size {self.n_samples}\"\n            )\n        return self.data[idx]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the dataset object.\"\"\"\n        params = [f\"n_samples={self.n_samples}\"]\n        # Add specific params from subclasses if desired, e.g. by inspecting self.__dict__\n        # Or define __repr__ in subclasses\n        return f\"{self.__class__.__name__}({', '.join(params)}, device={self.device}, dtype={self.dtype})\"\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset.n_samples","title":"n_samples  <code>instance-attribute</code>","text":"<pre><code>n_samples = n_samples\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed = seed\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: Optional[Tensor] = None\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset._seed_generators","title":"_seed_generators","text":"<pre><code>_seed_generators()\n</code></pre> <p>Sets the random seeds for numpy and torch if a seed is provided.</p> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _seed_generators(self):\n    \"\"\"Sets the random seeds for numpy and torch if a seed is provided.\"\"\"\n    if self.seed is not None:\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n        # If using CUDA, also seed the CUDA generator\n        if torch.cuda.is_available() and (\n            isinstance(self.device, torch.device)\n            and self.device.type == \"cuda\"\n            or self.device == \"cuda\"\n        ):\n            torch.cuda.manual_seed_all(self.seed)  # Seed all GPUs\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset._generate_data","title":"_generate_data  <code>abstractmethod</code>","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> <p>Core data generation logic to be implemented by subclasses.</p> <p>This method should perform the actual sampling based on the dataset's specific parameters and return the generated data as a NumPy array or directly as a PyTorch tensor (without final device/dtype conversion, as that's handled by the base class).</p> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>@abstractmethod\ndef _generate_data(self) -&gt; torch.Tensor:\n    \"\"\"\n    Core data generation logic to be implemented by subclasses.\n\n    This method should perform the actual sampling based on the dataset's\n    specific parameters and return the generated data as a NumPy array\n    or directly as a PyTorch tensor (without final device/dtype conversion,\n    as that's handled by the base class).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset._generate","title":"_generate","text":"<pre><code>_generate()\n</code></pre> <p>Internal method to handle seeding and call the generation logic.</p> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate(self):\n    \"\"\"Internal method to handle seeding and call the generation logic.\"\"\"\n    self._seed_generators()\n    # Generate data using the subclass implementation\n    generated_output = self._generate_data()\n\n    # Ensure it's a tensor and on the correct device/dtype\n    if isinstance(generated_output, np.ndarray):\n        self.data = _to_tensor(\n            generated_output, dtype=self.dtype, device=self.device\n        )\n    elif isinstance(generated_output, torch.Tensor):\n        self.data = generated_output.to(dtype=self.dtype, device=self.device)\n    else:\n        raise TypeError(\n            f\"_generate_data must return a NumPy array or PyTorch Tensor, got {type(generated_output)}\"\n        )\n\n    # Verify batch_shape\n    if self.data.shape[0] != self.n_samples:\n        warnings.warn(\n            f\"Generated data has {self.data.shape[0]} samples, but {self.n_samples} were requested. Check generation logic.\",\n            RuntimeWarning,\n        )\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset.regenerate","title":"regenerate","text":"<pre><code>regenerate(seed: Optional[int] = None)\n</code></pre> <p>Re-generates the dataset, optionally with a new seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>Optional[int]</code> <p>New random seed. If None, uses the original seed                   (if provided) or remains non-deterministic.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def regenerate(self, seed: Optional[int] = None):\n    \"\"\"\n    Re-generates the dataset, optionally with a new seed.\n\n    Args:\n        seed (Optional[int]): New random seed. If None, uses the original seed\n                              (if provided) or remains non-deterministic.\n    \"\"\"\n    if seed is not None:\n        self.seed = seed  # Update the seed if a new one is provided\n    self._generate()\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/BaseSyntheticDataset/#torchebm.datasets.generators.BaseSyntheticDataset.get_data","title":"get_data","text":"<pre><code>get_data() -&gt; torch.Tensor\n</code></pre> <p>Returns the entire generated dataset as a single tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The generated data tensor.</p> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def get_data(self) -&gt; torch.Tensor:\n    \"\"\"\n    Returns the entire generated dataset as a single tensor.\n\n    Returns:\n        torch.Tensor: The generated data tensor.\n    \"\"\"\n    if self.data is None:\n        # Should not happen if _generate() is called in __init__\n        self._generate()\n    return self.data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CheckerboardDataset/","title":"CheckerboardDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/CheckerboardDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates points in a 2D checkerboard pattern using rejection sampling.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Target number of samples. Default: 2000.</p> <code>2000</code> <code>range_limit</code> <code>float</code> <p>Defines the square region [-lim, lim] x [-lim, lim]. Default: 4.0.</p> <code>4.0</code> <code>noise</code> <code>float</code> <p>Small Gaussian noise added to points. Default: 0.01.</p> <code>0.01</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class CheckerboardDataset(BaseSyntheticDataset):\n    \"\"\"\n    Generates points in a 2D checkerboard pattern using rejection sampling.\n\n    Args:\n        n_samples (int): Target number of samples. Default: 2000.\n        range_limit (float): Defines the square region [-lim, lim] x [-lim, lim]. Default: 4.0.\n        noise (float): Small Gaussian noise added to points. Default: 0.01.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int = 2000,\n        range_limit: float = 4.0,\n        noise: float = 0.01,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        self.range_limit = range_limit\n        self.noise = noise\n        super().__init__(n_samples=n_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; torch.Tensor:\n        # Logic from make_checkerboard\n        collected_samples = []\n        target = self.n_samples\n        # Estimate batch size needed (density is ~0.5)\n        batch_size = max(1000, int(target * 2.5))  # Generate more than needed per batch\n\n        while len(collected_samples) &lt; target:\n            x = np.random.uniform(-self.range_limit, self.range_limit, size=batch_size)\n            y = np.random.uniform(-self.range_limit, self.range_limit, size=batch_size)\n\n            keep = (np.floor(x) + np.floor(y)) % 2 != 0\n            valid_points = np.vstack((x[keep], y[keep])).T.astype(np.float32)\n\n            needed = target - len(collected_samples)\n            collected_samples.extend(valid_points[:needed])  # Add only needed points\n\n        X = np.array(\n            collected_samples[:target], dtype=np.float32\n        )  # Ensure exact n_samples\n        tensor_data = torch.from_numpy(X)\n        tensor_data += torch.randn_like(tensor_data) * self.noise\n\n        return tensor_data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CheckerboardDataset/#torchebm.datasets.generators.CheckerboardDataset.range_limit","title":"range_limit  <code>instance-attribute</code>","text":"<pre><code>range_limit = range_limit\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CheckerboardDataset/#torchebm.datasets.generators.CheckerboardDataset.noise","title":"noise  <code>instance-attribute</code>","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CheckerboardDataset/#torchebm.datasets.generators.CheckerboardDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; torch.Tensor:\n    # Logic from make_checkerboard\n    collected_samples = []\n    target = self.n_samples\n    # Estimate batch size needed (density is ~0.5)\n    batch_size = max(1000, int(target * 2.5))  # Generate more than needed per batch\n\n    while len(collected_samples) &lt; target:\n        x = np.random.uniform(-self.range_limit, self.range_limit, size=batch_size)\n        y = np.random.uniform(-self.range_limit, self.range_limit, size=batch_size)\n\n        keep = (np.floor(x) + np.floor(y)) % 2 != 0\n        valid_points = np.vstack((x[keep], y[keep])).T.astype(np.float32)\n\n        needed = target - len(collected_samples)\n        collected_samples.extend(valid_points[:needed])  # Add only needed points\n\n    X = np.array(\n        collected_samples[:target], dtype=np.float32\n    )  # Ensure exact n_samples\n    tensor_data = torch.from_numpy(X)\n    tensor_data += torch.randn_like(tensor_data) * self.noise\n\n    return tensor_data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CircleDataset/","title":"CircleDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/CircleDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates points sampled uniformly on a circle with noise.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples. Default: 2000.</p> <code>2000</code> <code>noise</code> <code>float</code> <p>Standard deviation of Gaussian noise added. Default: 0.05.</p> <code>0.05</code> <code>radius</code> <code>float</code> <p>Radius of the circle. Default: 1.0.</p> <code>1.0</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class CircleDataset(BaseSyntheticDataset):\n    \"\"\"\n    Generates points sampled uniformly on a circle with noise.\n\n    Args:\n        n_samples (int): Number of samples. Default: 2000.\n        noise (float): Standard deviation of Gaussian noise added. Default: 0.05.\n        radius (float): Radius of the circle. Default: 1.0.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int = 2000,\n        noise: float = 0.05,\n        radius: float = 1.0,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        self.noise = noise\n        self.radius = radius\n        super().__init__(n_samples=n_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; torch.Tensor:\n        # Logic from make_circle\n        angles = 2 * np.pi * np.random.rand(self.n_samples)\n        x = self.radius * np.cos(angles)\n        y = self.radius * np.sin(angles)\n        X = np.vstack((x, y)).T.astype(np.float32)\n\n        tensor_data = torch.from_numpy(X)\n        tensor_data += torch.randn_like(tensor_data) * self.noise\n\n        return tensor_data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CircleDataset/#torchebm.datasets.generators.CircleDataset.noise","title":"noise  <code>instance-attribute</code>","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CircleDataset/#torchebm.datasets.generators.CircleDataset.radius","title":"radius  <code>instance-attribute</code>","text":"<pre><code>radius = radius\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/CircleDataset/#torchebm.datasets.generators.CircleDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; torch.Tensor:\n    # Logic from make_circle\n    angles = 2 * np.pi * np.random.rand(self.n_samples)\n    x = self.radius * np.cos(angles)\n    y = self.radius * np.sin(angles)\n    X = np.vstack((x, y)).T.astype(np.float32)\n\n    tensor_data = torch.from_numpy(X)\n    tensor_data += torch.randn_like(tensor_data) * self.noise\n\n    return tensor_data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/EightGaussiansDataset/","title":"EightGaussiansDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/EightGaussiansDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates samples from the specific '8 Gaussians' mixture.</p> <p>This creates a specific arrangement of 8 Gaussian modes commonly used in the energy-based modeling literature.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Total number of samples. Default: 2000.</p> <code>2000</code> <code>std</code> <code>float</code> <p>Standard deviation of each component. Default: 0.02.</p> <code>0.02</code> <code>scale</code> <code>float</code> <p>Scaling factor for the centers (often 2). Default: 2.0.</p> <code>2.0</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class EightGaussiansDataset(BaseSyntheticDataset):\n    \"\"\"\n    Generates samples from the specific '8 Gaussians' mixture.\n\n    This creates a specific arrangement of 8 Gaussian modes commonly used in the\n    energy-based modeling literature.\n\n    Args:\n        n_samples (int): Total number of samples. Default: 2000.\n        std (float): Standard deviation of each component. Default: 0.02.\n        scale (float): Scaling factor for the centers (often 2). Default: 2.0.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int = 2000,\n        std: float = 0.02,\n        scale: float = 2.0,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        self.std = std\n        self.scale = scale\n        # Define the specific 8 centers\n        centers_np = (\n            np.array(\n                [\n                    (1, 0),\n                    (-1, 0),\n                    (0, 1),\n                    (0, -1),\n                    (1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n                    (1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n                    (-1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n                    (-1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n                ],\n                dtype=np.float32,\n            )\n            * self.scale\n        )\n        self.centers_torch = torch.from_numpy(centers_np)\n        self.n_components = 8\n        super().__init__(n_samples=n_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; torch.Tensor:\n        # Similar logic to GaussianMixtureDataset but with fixed centers\n        centers_dev = self.centers_torch.to(device=self.device, dtype=self.dtype)\n\n        data = torch.empty(self.n_samples, 2, device=self.device, dtype=self.dtype)\n        samples_per_component = self.n_samples // self.n_components\n        remainder = self.n_samples % self.n_components\n\n        current_idx = 0\n        for i in range(self.n_components):\n            num = samples_per_component + (1 if i &lt; remainder else 0)\n            if num == 0:\n                continue\n            end_idx = current_idx + num\n            noise = torch.randn(num, 2, device=self.device, dtype=self.dtype) * self.std\n            data[current_idx:end_idx] = centers_dev[i] + noise\n            current_idx = end_idx\n\n        data = data[torch.randperm(self.n_samples, device=self.device)]\n        return data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/EightGaussiansDataset/#torchebm.datasets.generators.EightGaussiansDataset.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std = std\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/EightGaussiansDataset/#torchebm.datasets.generators.EightGaussiansDataset.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/EightGaussiansDataset/#torchebm.datasets.generators.EightGaussiansDataset.centers_torch","title":"centers_torch  <code>instance-attribute</code>","text":"<pre><code>centers_torch = from_numpy(centers_np)\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/EightGaussiansDataset/#torchebm.datasets.generators.EightGaussiansDataset.n_components","title":"n_components  <code>instance-attribute</code>","text":"<pre><code>n_components = 8\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/EightGaussiansDataset/#torchebm.datasets.generators.EightGaussiansDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; torch.Tensor:\n    # Similar logic to GaussianMixtureDataset but with fixed centers\n    centers_dev = self.centers_torch.to(device=self.device, dtype=self.dtype)\n\n    data = torch.empty(self.n_samples, 2, device=self.device, dtype=self.dtype)\n    samples_per_component = self.n_samples // self.n_components\n    remainder = self.n_samples % self.n_components\n\n    current_idx = 0\n    for i in range(self.n_components):\n        num = samples_per_component + (1 if i &lt; remainder else 0)\n        if num == 0:\n            continue\n        end_idx = current_idx + num\n        noise = torch.randn(num, 2, device=self.device, dtype=self.dtype) * self.std\n        data[current_idx:end_idx] = centers_dev[i] + noise\n        current_idx = end_idx\n\n    data = data[torch.randperm(self.n_samples, device=self.device)]\n    return data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GaussianMixtureDataset/","title":"GaussianMixtureDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/GaussianMixtureDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates samples from a 2D Gaussian mixture arranged uniformly in a circle.</p> <p>Creates a mixture of Gaussian distributions with centers equally spaced on a circle. This distribution is useful for testing mode-seeking behavior in energy-based models.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Total number of samples to generate.</p> <code>2000</code> <code>n_components</code> <code>int</code> <p>Number of Gaussian components (modes). Default: 8.</p> <code>8</code> <code>std</code> <code>float</code> <p>Standard deviation of each Gaussian component. Default: 0.05.</p> <code>0.05</code> <code>radius</code> <code>float</code> <p>Radius of the circle on which the centers lie. Default: 1.0.</p> <code>1.0</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class GaussianMixtureDataset(BaseSyntheticDataset):\n    \"\"\"\n    Generates samples from a 2D Gaussian mixture arranged uniformly in a circle.\n\n    Creates a mixture of Gaussian distributions with centers equally spaced on a circle.\n    This distribution is useful for testing mode-seeking behavior in energy-based models.\n\n    Args:\n        n_samples (int): Total number of samples to generate.\n        n_components (int): Number of Gaussian components (modes). Default: 8.\n        std (float): Standard deviation of each Gaussian component. Default: 0.05.\n        radius (float): Radius of the circle on which the centers lie. Default: 1.0.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int = 2000,\n        n_components: int = 8,\n        std: float = 0.05,\n        radius: float = 1.0,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        if n_components &lt;= 0:\n            raise ValueError(\"n_components must be positive\")\n        if std &lt; 0:\n            raise ValueError(\"std must be non-negative\")\n        self.n_components = n_components\n        self.std = std\n        self.radius = radius\n        super().__init__(n_samples=n_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; torch.Tensor:\n        # Logic from make_gaussian_mixture\n        thetas = np.linspace(0, 2 * np.pi, self.n_components, endpoint=False)\n        centers = np.array(\n            [(self.radius * np.cos(t), self.radius * np.sin(t)) for t in thetas],\n            dtype=np.float32,\n        )\n        # Use torch directly for efficiency and device handling\n        centers_torch = torch.from_numpy(centers)  # Keep on CPU for indexing efficiency\n\n        data = torch.empty(self.n_samples, 2, device=self.device, dtype=self.dtype)\n        samples_per_component = self.n_samples // self.n_components\n        remainder = self.n_samples % self.n_components\n\n        current_idx = 0\n        for i in range(self.n_components):\n            num = samples_per_component + (1 if i &lt; remainder else 0)\n            if num == 0:\n                continue\n            end_idx = current_idx + num\n            # Generate noise directly on target device if possible\n            noise = torch.randn(num, 2, device=self.device, dtype=self.dtype) * self.std\n            component_center = centers_torch[i].to(device=self.device, dtype=self.dtype)\n            data[current_idx:end_idx] = component_center + noise\n            current_idx = end_idx\n\n        # Shuffle the data to mix components\n        data = data[\n            torch.randperm(self.n_samples, device=self.device)\n        ]  # Use device-aware permutation\n        return data  # Return tensor directly\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GaussianMixtureDataset/#torchebm.datasets.generators.GaussianMixtureDataset.n_components","title":"n_components  <code>instance-attribute</code>","text":"<pre><code>n_components = n_components\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GaussianMixtureDataset/#torchebm.datasets.generators.GaussianMixtureDataset.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std = std\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GaussianMixtureDataset/#torchebm.datasets.generators.GaussianMixtureDataset.radius","title":"radius  <code>instance-attribute</code>","text":"<pre><code>radius = radius\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GaussianMixtureDataset/#torchebm.datasets.generators.GaussianMixtureDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; torch.Tensor:\n    # Logic from make_gaussian_mixture\n    thetas = np.linspace(0, 2 * np.pi, self.n_components, endpoint=False)\n    centers = np.array(\n        [(self.radius * np.cos(t), self.radius * np.sin(t)) for t in thetas],\n        dtype=np.float32,\n    )\n    # Use torch directly for efficiency and device handling\n    centers_torch = torch.from_numpy(centers)  # Keep on CPU for indexing efficiency\n\n    data = torch.empty(self.n_samples, 2, device=self.device, dtype=self.dtype)\n    samples_per_component = self.n_samples // self.n_components\n    remainder = self.n_samples % self.n_components\n\n    current_idx = 0\n    for i in range(self.n_components):\n        num = samples_per_component + (1 if i &lt; remainder else 0)\n        if num == 0:\n            continue\n        end_idx = current_idx + num\n        # Generate noise directly on target device if possible\n        noise = torch.randn(num, 2, device=self.device, dtype=self.dtype) * self.std\n        component_center = centers_torch[i].to(device=self.device, dtype=self.dtype)\n        data[current_idx:end_idx] = component_center + noise\n        current_idx = end_idx\n\n    # Shuffle the data to mix components\n    data = data[\n        torch.randperm(self.n_samples, device=self.device)\n    ]  # Use device-aware permutation\n    return data  # Return tensor directly\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GridDataset/","title":"GridDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/GridDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates points on a 2D grid within [-range_limit, range_limit].</p> The total number of samples will be n_samples_per_dim ** 2. <p>The <code>n_samples</code> parameter in the base class will be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples_per_dim</code> <code>int</code> <p>Number of points along each dimension. Default: 10.</p> <code>10</code> <code>range_limit</code> <code>float</code> <p>Defines the square region [-lim, lim] x [-lim, lim]. Default: 1.0.</p> <code>1.0</code> <code>noise</code> <code>float</code> <p>Standard deviation of Gaussian noise added. Default: 0.01.</p> <code>0.01</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed (primarily affects noise).</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class GridDataset(BaseSyntheticDataset):\n    \"\"\"Generates points on a 2D grid within [-range_limit, range_limit].\n\n    Note: The total number of samples will be n_samples_per_dim ** 2.\n          The `n_samples` parameter in the base class will be overridden.\n\n    Args:\n        n_samples_per_dim (int): Number of points along each dimension. Default: 10.\n        range_limit (float): Defines the square region [-lim, lim] x [-lim, lim]. Default: 1.0.\n        noise (float): Standard deviation of Gaussian noise added. Default: 0.01.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed (primarily affects noise).\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples_per_dim: int = 10,\n        range_limit: float = 1.0,\n        noise: float = 0.01,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,  # Seed mainly affects noise here\n    ):\n        if n_samples_per_dim &lt;= 0:\n            raise ValueError(\"n_samples_per_dim must be positive\")\n        self.n_samples_per_dim = n_samples_per_dim\n        self.range_limit = range_limit\n        self.noise = noise\n        # Override n_samples for the base class\n        total_samples = n_samples_per_dim * n_samples_per_dim\n        super().__init__(n_samples=total_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; torch.Tensor:\n        # Create a more uniform grid spacing\n        x_coords = torch.linspace(\n            -self.range_limit, self.range_limit, self.n_samples_per_dim\n        )\n        y_coords = torch.linspace(\n            -self.range_limit, self.range_limit, self.n_samples_per_dim\n        )\n\n        # Create the grid points\n        grid_x, grid_y = torch.meshgrid(x_coords, y_coords, indexing=\"ij\")\n\n        # Stack the coordinates to form the 2D points\n        points = torch.stack([grid_x.flatten(), grid_y.flatten()], dim=1)\n\n        # Apply noise if specified\n        if self.noise &gt; 0:\n            # Set the random seed if provided\n            if hasattr(self, \"rng\"):\n                # Use the RNG from the base class\n                noise = torch.tensor(\n                    self.rng.normal(0, self.noise, size=points.shape),\n                    dtype=points.dtype,\n                    device=points.device,\n                )\n            else:\n                # Fall back to torch's random generator\n                noise = torch.randn_like(points) * self.noise\n\n            points = points + noise\n\n        return points\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GridDataset/#torchebm.datasets.generators.GridDataset.n_samples_per_dim","title":"n_samples_per_dim  <code>instance-attribute</code>","text":"<pre><code>n_samples_per_dim = n_samples_per_dim\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GridDataset/#torchebm.datasets.generators.GridDataset.range_limit","title":"range_limit  <code>instance-attribute</code>","text":"<pre><code>range_limit = range_limit\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GridDataset/#torchebm.datasets.generators.GridDataset.noise","title":"noise  <code>instance-attribute</code>","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/GridDataset/#torchebm.datasets.generators.GridDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; torch.Tensor:\n    # Create a more uniform grid spacing\n    x_coords = torch.linspace(\n        -self.range_limit, self.range_limit, self.n_samples_per_dim\n    )\n    y_coords = torch.linspace(\n        -self.range_limit, self.range_limit, self.n_samples_per_dim\n    )\n\n    # Create the grid points\n    grid_x, grid_y = torch.meshgrid(x_coords, y_coords, indexing=\"ij\")\n\n    # Stack the coordinates to form the 2D points\n    points = torch.stack([grid_x.flatten(), grid_y.flatten()], dim=1)\n\n    # Apply noise if specified\n    if self.noise &gt; 0:\n        # Set the random seed if provided\n        if hasattr(self, \"rng\"):\n            # Use the RNG from the base class\n            noise = torch.tensor(\n                self.rng.normal(0, self.noise, size=points.shape),\n                dtype=points.dtype,\n                device=points.device,\n            )\n        else:\n            # Fall back to torch's random generator\n            noise = torch.randn_like(points) * self.noise\n\n        points = points + noise\n\n    return points\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/","title":"PinwheelDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates the pinwheel dataset with curved blades.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Total number of samples. Default: 2000.</p> <code>2000</code> <code>n_classes</code> <code>int</code> <p>Number of 'blades'. Default: 5.</p> <code>5</code> <code>noise</code> <code>float</code> <p>Std dev of final additive Cartesian noise. Default: 0.05.</p> <code>0.05</code> <code>radial_scale</code> <code>float</code> <p>Controls max radius/length of blades. Default: 2.0.</p> <code>2.0</code> <code>angular_scale</code> <code>float</code> <p>Controls std dev of angle noise (thickness). Default: 0.1.</p> <code>0.1</code> <code>spiral_scale</code> <code>float</code> <p>Controls spiral tightness. Default: 5.0.</p> <code>5.0</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class PinwheelDataset(BaseSyntheticDataset):\n    \"\"\"\n    Generates the pinwheel dataset with curved blades.\n\n    Args:\n        n_samples (int): Total number of samples. Default: 2000.\n        n_classes (int): Number of 'blades'. Default: 5.\n        noise (float): Std dev of final additive Cartesian noise. Default: 0.05.\n        radial_scale (float): Controls max radius/length of blades. Default: 2.0.\n        angular_scale (float): Controls std dev of angle noise (thickness). Default: 0.1.\n        spiral_scale (float): Controls spiral tightness. Default: 5.0.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int = 2000,\n        n_classes: int = 5,\n        noise: float = 0.05,\n        radial_scale: float = 2.0,\n        angular_scale: float = 0.1,\n        spiral_scale: float = 5.0,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        if n_classes &lt;= 0:\n            raise ValueError(\"n_classes must be positive\")\n        self.n_classes = n_classes\n        self.noise = noise\n        self.radial_scale = radial_scale\n        self.angular_scale = angular_scale\n        self.spiral_scale = spiral_scale\n        super().__init__(n_samples=n_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; torch.Tensor:\n        # Logic from make_pinwheel\n        all_points_np = []\n        samples_per_class = self.n_samples // self.n_classes\n        remainder = self.n_samples % self.n_classes\n\n        for class_idx in range(self.n_classes):\n            n_class_samples = samples_per_class + (1 if class_idx &lt; remainder else 0)\n            if n_class_samples == 0:\n                continue\n\n            t = np.sqrt(np.random.rand(n_class_samples))  # Radial density control\n            radii = t * self.radial_scale\n            base_angle = class_idx * (2 * np.pi / self.n_classes)\n            spiral_angle = self.spiral_scale * t\n            angle_noise = np.random.randn(n_class_samples) * self.angular_scale\n            thetas = base_angle + spiral_angle + angle_noise\n\n            x = radii * np.cos(thetas)\n            y = radii * np.sin(thetas)\n            all_points_np.append(np.stack([x, y], axis=1))\n\n        data_np = np.concatenate(all_points_np, axis=0).astype(np.float32)\n        np.random.shuffle(data_np)  # Shuffle before converting to tensor\n\n        tensor_data = torch.from_numpy(data_np)\n\n        if self.noise &gt; 0:\n            tensor_data += torch.randn_like(tensor_data) * self.noise\n\n        return tensor_data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/#torchebm.datasets.generators.PinwheelDataset.n_classes","title":"n_classes  <code>instance-attribute</code>","text":"<pre><code>n_classes = n_classes\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/#torchebm.datasets.generators.PinwheelDataset.noise","title":"noise  <code>instance-attribute</code>","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/#torchebm.datasets.generators.PinwheelDataset.radial_scale","title":"radial_scale  <code>instance-attribute</code>","text":"<pre><code>radial_scale = radial_scale\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/#torchebm.datasets.generators.PinwheelDataset.angular_scale","title":"angular_scale  <code>instance-attribute</code>","text":"<pre><code>angular_scale = angular_scale\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/#torchebm.datasets.generators.PinwheelDataset.spiral_scale","title":"spiral_scale  <code>instance-attribute</code>","text":"<pre><code>spiral_scale = spiral_scale\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/PinwheelDataset/#torchebm.datasets.generators.PinwheelDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; torch.Tensor:\n    # Logic from make_pinwheel\n    all_points_np = []\n    samples_per_class = self.n_samples // self.n_classes\n    remainder = self.n_samples % self.n_classes\n\n    for class_idx in range(self.n_classes):\n        n_class_samples = samples_per_class + (1 if class_idx &lt; remainder else 0)\n        if n_class_samples == 0:\n            continue\n\n        t = np.sqrt(np.random.rand(n_class_samples))  # Radial density control\n        radii = t * self.radial_scale\n        base_angle = class_idx * (2 * np.pi / self.n_classes)\n        spiral_angle = self.spiral_scale * t\n        angle_noise = np.random.randn(n_class_samples) * self.angular_scale\n        thetas = base_angle + spiral_angle + angle_noise\n\n        x = radii * np.cos(thetas)\n        y = radii * np.sin(thetas)\n        all_points_np.append(np.stack([x, y], axis=1))\n\n    data_np = np.concatenate(all_points_np, axis=0).astype(np.float32)\n    np.random.shuffle(data_np)  # Shuffle before converting to tensor\n\n    tensor_data = torch.from_numpy(data_np)\n\n    if self.noise &gt; 0:\n        tensor_data += torch.randn_like(tensor_data) * self.noise\n\n    return tensor_data\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/SwissRollDataset/","title":"SwissRollDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/SwissRollDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates a 2D Swiss roll dataset.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples. Default: 2000.</p> <code>2000</code> <code>noise</code> <code>float</code> <p>Standard deviation of Gaussian noise added. Default: 0.05.</p> <code>0.05</code> <code>arclength</code> <code>float</code> <p>Controls how many rolls (pi*arclength). Default: 3.0.</p> <code>3.0</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class SwissRollDataset(BaseSyntheticDataset):\n    \"\"\"\n    Generates a 2D Swiss roll dataset.\n\n    Args:\n        n_samples (int): Number of samples. Default: 2000.\n        noise (float): Standard deviation of Gaussian noise added. Default: 0.05.\n        arclength (float): Controls how many rolls (pi*arclength). Default: 3.0.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int = 2000,\n        noise: float = 0.05,\n        arclength: float = 3.0,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        self.noise = noise\n        self.arclength = arclength\n        super().__init__(n_samples=n_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; torch.Tensor:\n        # Logic from make_swiss_roll\n        t = self.arclength * np.pi * (1 + 2 * np.random.rand(self.n_samples))\n        x = t * np.cos(t)\n        y = t * np.sin(t)\n        X = np.vstack((x, y)).T.astype(np.float32)\n\n        tensor_data = torch.from_numpy(X)  # CPU tensor initially\n        tensor_data += torch.randn_like(tensor_data) * self.noise\n\n        # Center and scale slightly (optional, can be done outside)\n        tensor_data = (tensor_data - tensor_data.mean(dim=0)) / (\n            tensor_data.std(dim=0).mean()\n            * 2.0  # Be careful with division by zero if std is ~0\n        )\n\n        return tensor_data  # Return tensor, base class handles device/dtype\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/SwissRollDataset/#torchebm.datasets.generators.SwissRollDataset.noise","title":"noise  <code>instance-attribute</code>","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/SwissRollDataset/#torchebm.datasets.generators.SwissRollDataset.arclength","title":"arclength  <code>instance-attribute</code>","text":"<pre><code>arclength = arclength\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/SwissRollDataset/#torchebm.datasets.generators.SwissRollDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; torch.Tensor:\n    # Logic from make_swiss_roll\n    t = self.arclength * np.pi * (1 + 2 * np.random.rand(self.n_samples))\n    x = t * np.cos(t)\n    y = t * np.sin(t)\n    X = np.vstack((x, y)).T.astype(np.float32)\n\n    tensor_data = torch.from_numpy(X)  # CPU tensor initially\n    tensor_data += torch.randn_like(tensor_data) * self.noise\n\n    # Center and scale slightly (optional, can be done outside)\n    tensor_data = (tensor_data - tensor_data.mean(dim=0)) / (\n        tensor_data.std(dim=0).mean()\n        * 2.0  # Be careful with division by zero if std is ~0\n    )\n\n    return tensor_data  # Return tensor, base class handles device/dtype\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/TwoMoonsDataset/","title":"TwoMoonsDataset","text":""},{"location":"api/torchebm/datasets/generators/classes/TwoMoonsDataset/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSyntheticDataset</code></p> <p>Generates the 'two moons' dataset.</p> <p>Creates two interleaving half-circles with added Gaussian noise.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Total number of samples. Default: 2000.</p> <code>2000</code> <code>noise</code> <code>float</code> <p>Standard deviation of Gaussian noise added. Default: 0.05.</p> <code>0.05</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensor. Default: torch.float32.</p> <code>float32</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>class TwoMoonsDataset(BaseSyntheticDataset):\n    \"\"\"\n    Generates the 'two moons' dataset.\n\n    Creates two interleaving half-circles with added Gaussian noise.\n\n    Args:\n        n_samples (int): Total number of samples. Default: 2000.\n        noise (float): Standard deviation of Gaussian noise added. Default: 0.05.\n        device (Optional[Union[str, torch.device]]): Device for the tensor.\n        dtype (torch.dtype): Data type for the tensor. Default: torch.float32.\n        seed (Optional[int]): Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: int = 2000,\n        noise: float = 0.05,\n        device: Optional[Union[str, torch.device]] = None,\n        dtype: torch.dtype = torch.float32,\n        seed: Optional[int] = None,\n    ):\n        self.noise = noise\n        super().__init__(n_samples=n_samples, device=device, dtype=dtype, seed=seed)\n\n    def _generate_data(self) -&gt; np.ndarray:\n        # Logic from make_two_moons (using numpy initially is fine here)\n        n_samples_out = self.n_samples // 2\n        n_samples_in = self.n_samples - n_samples_out\n\n        outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples_out))\n        outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples_out))\n        inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, n_samples_in))\n        inner_circ_y = 1 - np.sin(np.linspace(0, np.pi, n_samples_in)) - 0.5\n\n        X = np.vstack(\n            [\n                np.append(outer_circ_x, inner_circ_x),\n                np.append(outer_circ_y, inner_circ_y),\n            ]\n        ).T.astype(np.float32)\n\n        # Add noise using torch AFTER converting base batch_shape to tensor\n        tensor_data = torch.from_numpy(X)  # Keep on CPU initially for noise addition\n        noise_val = torch.randn_like(tensor_data) * self.noise\n        tensor_data += noise_val\n\n        # Base class __init__ will handle final _to_tensor conversion for device/dtype\n        # Alternatively, add noise directly on the target device:\n        # tensor_data = torch.from_numpy(X).to(device=self.device, dtype=self.dtype)\n        # tensor_data += torch.randn_like(tensor_data) * self.noise\n        # return tensor_data # Return tensor directly if handled here\n\n        return tensor_data  # Return tensor, base class handles device/dtype\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/TwoMoonsDataset/#torchebm.datasets.generators.TwoMoonsDataset.noise","title":"noise  <code>instance-attribute</code>","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"api/torchebm/datasets/generators/classes/TwoMoonsDataset/#torchebm.datasets.generators.TwoMoonsDataset._generate_data","title":"_generate_data","text":"<pre><code>_generate_data() -&gt; np.ndarray\n</code></pre> Source code in <code>torchebm/datasets/generators.py</code> <pre><code>def _generate_data(self) -&gt; np.ndarray:\n    # Logic from make_two_moons (using numpy initially is fine here)\n    n_samples_out = self.n_samples // 2\n    n_samples_in = self.n_samples - n_samples_out\n\n    outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples_out))\n    outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples_out))\n    inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, n_samples_in))\n    inner_circ_y = 1 - np.sin(np.linspace(0, np.pi, n_samples_in)) - 0.5\n\n    X = np.vstack(\n        [\n            np.append(outer_circ_x, inner_circ_x),\n            np.append(outer_circ_y, inner_circ_y),\n        ]\n    ).T.astype(np.float32)\n\n    # Add noise using torch AFTER converting base batch_shape to tensor\n    tensor_data = torch.from_numpy(X)  # Keep on CPU initially for noise addition\n    noise_val = torch.randn_like(tensor_data) * self.noise\n    tensor_data += noise_val\n\n    # Base class __init__ will handle final _to_tensor conversion for device/dtype\n    # Alternatively, add noise directly on the target device:\n    # tensor_data = torch.from_numpy(X).to(device=self.device, dtype=self.dtype)\n    # tensor_data += torch.randn_like(tensor_data) * self.noise\n    # return tensor_data # Return tensor directly if handled here\n\n    return tensor_data  # Return tensor, base class handles device/dtype\n</code></pre>"},{"location":"api/torchebm/losses/","title":"Torchebm &gt; Losses","text":""},{"location":"api/torchebm/losses/#torchebm-losses","title":"Torchebm &gt; Losses","text":""},{"location":"api/torchebm/losses/#contents","title":"Contents","text":""},{"location":"api/torchebm/losses/#modules","title":"Modules","text":"<ul> <li>Contrastive_divergence</li> <li>Score_matching</li> </ul>"},{"location":"api/torchebm/losses/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/losses/#torchebm.losses","title":"torchebm.losses","text":"<p>BaseLoss functions for training energy-based models, including contrastive divergence variants.</p>"},{"location":"api/torchebm/losses/contrastive_divergence/","title":"Torchebm &gt; Losses &gt; Contrastive_divergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm-losses-contrastive_divergence","title":"Torchebm &gt; Losses &gt; Contrastive_divergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#contents","title":"Contents","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#classes","title":"Classes","text":"<ul> <li><code>ContrastiveDivergence</code> - Implementation of the standard Contrastive Divergence (CD-k) algorithm.</li> <li><code>ParallelTemperingCD</code> - No description available.</li> <li><code>PersistentContrastiveDivergence</code> - No description available.</li> </ul>"},{"location":"api/torchebm/losses/contrastive_divergence/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence","title":"torchebm.losses.contrastive_divergence","text":"<p>Contrastive Divergence Loss Module.</p> <p>This module provides implementations of Contrastive Divergence (CD) and its variants for training energy-based models (EBMs). Contrastive Divergence is a computationally efficient approximation to the maximum likelihood estimation that avoids the need  for complete MCMC sampling from the model distribution.</p> <p>Key Features</p> <ul> <li>Standard Contrastive Divergence (CD-k)</li> <li>Persistent Contrastive Divergence (PCD)</li> <li>Parallel Tempering Contrastive Divergence (PTCD)</li> <li>Support for different MCMC samplers</li> </ul>"},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>ContrastiveDivergence</code> <p>Standard CD-k implementation.</p> <code>PersistentContrastiveDivergence</code> <p>Implementation with persistent Markov chains.</p> <code>ParallelTemperingCD</code> <p>Implementation with parallel chains at different temperatures.</p>"},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence--usage-example","title":"Usage Example","text":"<p>Basic ContrastiveDivergence Usage</p> <pre><code>from torchebm.losses import ContrastiveDivergence\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.energy_functions import MLPEnergyFunction\nimport torch\n\n# Define the energy function\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n# Set up the sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.1,\n    noise_scale=0.01\n)\n\n# Create the CD loss\ncd_loss = ContrastiveDivergence(\n    energy_function=energy_fn,\n    sampler=sampler,\n    k_steps=10,\n    persistent=False\n)\n\n# In the training loop:\ndata_batch = torch.randn(32, 2)  # Real data samples\nloss, negative_samples = cd_loss(data_batch)\nloss.backward()\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Contrastive Divergence Principles</p> <p>Contrastive Divergence approximates the gradient of the log-likelihood:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) + \\mathbb{E}_{p_\\theta(x')} [\\nabla_\\theta E_\\theta(x')] \\] <p>by replacing the expectation under the model distribution with samples obtained after \\( k \\) steps of MCMC starting from the data:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) \\approx -\\nabla_\\theta E_\\theta(x) + \\nabla_\\theta E_\\theta(x_k) \\] <p>where \\( x_k \\) is obtained after running \\( k \\) steps of MCMC starting from \\( x \\).</p> <p>Why Contrastive Divergence Works</p> <ul> <li>Computational Efficiency: Requires only a few MCMC steps rather than running chains to convergence.</li> <li>Stability: Starting chains from data points ensures the negative samples are in high-density regions.</li> <li>Effective Learning: Despite theoretical limitations, works well in practice for many energy-based models.</li> </ul>"},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence--variants","title":"Variants","text":"<p>Persistent Contrastive Divergence (PCD)</p> <p>PCD improves upon standard CD by maintaining a persistent set of Markov chains between parameter updates. Instead of  restarting chains from the data, it continues chains from the previous iterations:</p> <ol> <li>Initialize a set of persistent chains (often with random noise)</li> <li>For each training batch:    a. Update the persistent chains with k steps of MCMC    b. Use these updated chains for the negative samples    c. Keep the updated state for the next batch</li> </ol> <p>PCD can explore the energy landscape more thoroughly, especially for complex distributions.</p> <p>Parallel Tempering CD</p> <p>Parallel Tempering CD uses multiple chains at different temperatures to improve exploration:</p> <ol> <li>Maintain chains at different temperatures \\( T_1 &lt; T_2 &lt; ... &lt; T_n \\)</li> <li>For each chain, perform MCMC steps using the energy function \\( E(x)/T_i \\)</li> <li>Occasionally swap states between adjacent temperature chains</li> <li>Use samples from the chain with \\( T_1 = 1 \\) as negative samples</li> </ol> <p>This helps overcome energy barriers and explore multimodal distributions.</p>"},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence--practical-considerations","title":"Practical Considerations","text":"<p>Tuning Parameters</p> <ul> <li>k_steps: More steps improves quality of negative samples but increases computational cost.</li> <li>persistent: Setting to True enables PCD, which often improves learning for complex distributions.</li> <li>sampler parameters: The quality of CD depends heavily on the underlying MCMC sampler parameters.</li> </ul> <p>How to Diagnose Issues?</p> <p>Watch for these signs of problematic training:</p> <ul> <li>Exploding or vanishing gradients</li> <li>Increasing loss values over time</li> <li>Negative samples that don't resemble the data distribution</li> <li>Energy function collapsing (assigning same energy to all points)</li> </ul> <p>Common Pitfalls</p> <ul> <li>Too Few MCMC Steps: Can lead to biased gradients and poor convergence</li> <li>Improper Initialization: For PCD, poor initial chain states may hinder learning</li> <li>Unbalanced Energy: If negative samples have much higher energy than positive samples, learning may be ineffective</li> </ul>"},{"location":"api/torchebm/losses/contrastive_divergence/#torchebm.losses.contrastive_divergence--useful-insights","title":"Useful Insights","text":"<p>Why CD May Outperform MLE</p> <p>In some cases, CD might actually lead to better models than exact maximum likelihood:</p> <ul> <li>Prevents overfitting to noise in the data</li> <li>Focuses the model capacity on distinguishing data from nearby non-data regions</li> <li>May result in more useful representations for downstream tasks</li> </ul> Further Reading <ul> <li>Hinton, G. E. (2002). \"Training products of experts by minimizing contrastive divergence.\"</li> <li>Tieleman, T. (2008). \"Training restricted Boltzmann machines using approximations to the likelihood gradient.\"</li> <li>Desjardins, G., et al. (2010). \"Tempered Markov chain Monte Carlo for training of restricted Boltzmann machines.\"</li> </ul>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/","title":"ContrastiveDivergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseContrastiveDivergence</code></p> <p>Implementation of the standard Contrastive Divergence (CD-k) algorithm.</p> <p>Contrastive Divergence approximates the gradient of the log-likelihood by comparing the energy between real data samples and samples generated after k steps of MCMC initialized from the data samples (or from noise in persistent mode).</p> <p>The CD loss is defined as:</p> \\[\\mathcal{L}_{CD} = \\mathbb{E}_{p_{data}}[E_\\theta(x)] - \\mathbb{E}_{p_k}[E_\\theta(x')]\\] <p>where:</p> <ul> <li>\\(E_\\theta(x)\\) is the energy function with parameters \\(\\theta\\)</li> <li>\\(p_{data}\\) is the data distribution</li> <li>\\(p_k\\) is the distribution after \\(k\\) steps of MCMC</li> </ul> <p>Algorithm Overview</p> <ol> <li> <p>For non-persistent CD:    a. Start MCMC chains from real data samples    b. Run MCMC for k steps to generate negative samples    c. Compute gradient comparing real and negative samples</p> </li> <li> <p>For persistent CD:    a. Maintain a set of persistent chains between updates    b. Continue chains from previous state for k steps    c. Update the persistent state for next iteration</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to train</p> required <code>sampler</code> <code>BaseSampler</code> <p>MCMC sampler for generating negative samples</p> required <code>k_steps</code> <code>int</code> <p>Number of MCMC steps (k in CD-k)</p> <code>10</code> <code>persistent</code> <code>bool</code> <p>Whether to use persistent Contrastive Divergence</p> <code>False</code> <code>buffer_size</code> <code>int</code> <p>Size of buffer for PCD. Defaults to 10000.</p> <code>10000</code> <code>init_steps</code> <code>int</code> <p>Number of initial MCMC steps to warm up buffer. Defaults to 100.</p> <code>100</code> <code>new_sample_ratio</code> <code>float</code> <p>Fraction of new random samples to introduce. Defaults to 0.05.</p> <code>0.05</code> <code>energy_reg_weight</code> <code>float</code> <p>Weight for energy regularization. Defaults to 0.001.</p> <code>0.001</code> <code>use_temperature_annealing</code> <code>bool</code> <p>Whether to use temperature annealing for sampler. Defaults to False.</p> <code>False</code> <code>min_temp</code> <code>float</code> <p>Minimum temperature for annealing. Defaults to 0.01.</p> <code>0.01</code> <code>max_temp</code> <code>float</code> <p>Maximum temperature for annealing. Defaults to 2.0.</p> <code>2.0</code> <code>temp_decay</code> <code>float</code> <p>Decay rate for temperature annealing. Defaults to 0.999.</p> <code>0.999</code> <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>device</code> <code>device</code> <p>Device to run computations on</p> <code>device('cpu')</code> <p>Basic Usage</p> <pre><code># Setup energy function, sampler and CD loss\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\nsampler = LangevinDynamics(energy_fn, step_size=0.1)\ncd_loss = ContrastiveDivergence(\n    energy_function=energy_fn,\n    sampler=sampler,\n    k_steps=10,\n    persistent=False\n)\n\n# In training loop\noptimizer = torch.optim.Adam(energy_fn.parameters(), lr=0.001)\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss, _ = cd_loss(batch)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Persistent vs Standard CD</p> <ul> <li>Standard CD (<code>persistent=False</code>) is more stable but can struggle with complex distributions</li> <li>Persistent CD (<code>persistent=True</code>) can explore better but may require careful initialization</li> </ul> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ContrastiveDivergence(BaseContrastiveDivergence):\n    r\"\"\"\n    Implementation of the standard Contrastive Divergence (CD-k) algorithm.\n\n    Contrastive Divergence approximates the gradient of the log-likelihood by comparing\n    the energy between real data samples and samples generated after k steps of MCMC\n    initialized from the data samples (or from noise in persistent mode).\n\n    The CD loss is defined as:\n\n    $$\\mathcal{L}_{CD} = \\mathbb{E}_{p_{data}}[E_\\theta(x)] - \\mathbb{E}_{p_k}[E_\\theta(x')]$$\n\n    where:\n\n    - $E_\\theta(x)$ is the energy function with parameters $\\theta$\n    - $p_{data}$ is the data distribution\n    - $p_k$ is the distribution after $k$ steps of MCMC\n\n    !!! note \"Algorithm Overview\"\n        1. For non-persistent CD:\n           a. Start MCMC chains from real data samples\n           b. Run MCMC for k steps to generate negative samples\n           c. Compute gradient comparing real and negative samples\n\n        2. For persistent CD:\n           a. Maintain a set of persistent chains between updates\n           b. Continue chains from previous state for k steps\n           c. Update the persistent state for next iteration\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to train\n        sampler (BaseSampler): MCMC sampler for generating negative samples\n        k_steps (int): Number of MCMC steps (k in CD-k)\n        persistent (bool): Whether to use persistent Contrastive Divergence\n        buffer_size (int, optional): Size of buffer for PCD. Defaults to 10000.\n        init_steps (int, optional): Number of initial MCMC steps to warm up buffer. Defaults to 100.\n        new_sample_ratio (float, optional): Fraction of new random samples to introduce. Defaults to 0.05.\n        energy_reg_weight (float, optional): Weight for energy regularization. Defaults to 0.001.\n        use_temperature_annealing (bool, optional): Whether to use temperature annealing for sampler. Defaults to False.\n        min_temp (float, optional): Minimum temperature for annealing. Defaults to 0.01.\n        max_temp (float, optional): Maximum temperature for annealing. Defaults to 2.0.\n        temp_decay (float, optional): Decay rate for temperature annealing. Defaults to 0.999.\n        dtype (torch.dtype): Data type for computations\n        device (torch.device): Device to run computations on\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Setup energy function, sampler and CD loss\n        energy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n        sampler = LangevinDynamics(energy_fn, step_size=0.1)\n        cd_loss = ContrastiveDivergence(\n            energy_function=energy_fn,\n            sampler=sampler,\n            k_steps=10,\n            persistent=False\n        )\n\n        # In training loop\n        optimizer = torch.optim.Adam(energy_fn.parameters(), lr=0.001)\n\n        for batch in dataloader:\n            optimizer.zero_grad()\n            loss, _ = cd_loss(batch)\n            loss.backward()\n            optimizer.step()\n        ```\n\n    !!! tip \"Persistent vs Standard CD\"\n        - Standard CD (`persistent=False`) is more stable but can struggle with complex distributions\n        - Persistent CD (`persistent=True`) can explore better but may require careful initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function,\n        sampler,\n        k_steps=10,\n        persistent=False,\n        buffer_size=10000,\n        init_steps=100,\n        new_sample_ratio=0.05,\n        energy_reg_weight=0.001,\n        use_temperature_annealing=False,\n        min_temp=0.01,\n        max_temp=2.0,\n        temp_decay=0.999,\n        dtype=torch.float32,\n        device=torch.device(\"cpu\"),\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            energy_function=energy_function,\n            sampler=sampler,\n            k_steps=k_steps,\n            persistent=persistent,\n            buffer_size=buffer_size,\n            new_sample_ratio=new_sample_ratio,\n            init_steps=init_steps,\n            dtype=dtype,\n            device=device,\n            *args,\n            **kwargs,\n        )\n        # Additional parameters for improved stability\n        self.energy_reg_weight = energy_reg_weight\n        self.use_temperature_annealing = use_temperature_annealing\n        self.min_temp = min_temp\n        self.max_temp = max_temp\n        self.temp_decay = temp_decay\n        self.current_temp = max_temp\n\n        # Register temperature as buffer for persistence\n        self.register_buffer(\n            \"temperature\", torch.tensor(max_temp, dtype=self.dtype, device=self.device)\n        )\n\n    def forward(\n        self, x: torch.Tensor, *args, **kwargs\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Compute the Contrastive Divergence loss and generate negative samples.\n\n        This method implements the energy_functions CD algorithm by:\n\n        1. Initializing MCMC chains (either from data or persistent state)\n        2. Running the sampler for k_steps to generate negative samples\n        3. Computing the CD loss using the energy difference\n\n        Args:\n            x (torch.Tensor): Batch of real data samples (positive samples)\n            *args: Additional positional arguments\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]:\n                - loss: The CD loss value (scalar)\n                - pred_samples: Generated negative samples\n\n        !!! note \"Shape Information\"\n            - Input `x`: (batch_size, feature_dimensions)\n            - Output loss: scalar\n            - Output pred_samples: (batch_size, feature_dimensions)\n        \"\"\"\n\n        batch_size = x.shape[0]\n        data_shape = x.shape[1:]\n\n        # Update temperature if annealing is enabled\n        if self.use_temperature_annealing and self.training:\n            self.current_temp = max(self.min_temp, self.current_temp * self.temp_decay)\n            self.temperature[...] = self.current_temp  # Use ellipsis instead of index\n\n            # If sampler has a temperature parameter, update it\n            if hasattr(self.sampler, \"temperature\"):\n                self.sampler.temperature = self.current_temp\n            elif hasattr(self.sampler, \"noise_scale\"):\n                # For samplers like Langevin, adjust noise scale based on temperature\n                original_noise = getattr(self.sampler, \"_original_noise_scale\", None)\n                if original_noise is None:\n                    setattr(\n                        self.sampler, \"_original_noise_scale\", self.sampler.noise_scale\n                    )\n                    original_noise = self.sampler.noise_scale\n\n                self.sampler.noise_scale = original_noise * math.sqrt(self.current_temp)\n\n        # Get starting points for chains (either from buffer or data)\n        start_points = self.get_start_points(x)\n\n        # Run MCMC chains to get negative samples\n        pred_samples = self.sampler.sample(\n            x=start_points,\n            n_steps=self.k_steps,\n        )\n\n        # Update persistent buffer if using PCD\n        if self.persistent:\n            with torch.no_grad():\n                self.update_buffer(pred_samples.detach())\n\n        # Add energy regularization to kwargs for compute_loss\n        kwargs[\"energy_reg_weight\"] = kwargs.get(\n            \"energy_reg_weight\", self.energy_reg_weight\n        )\n\n        # Compute contrastive divergence loss\n        loss = self.compute_loss(x, pred_samples, *args, **kwargs)\n\n        return loss, pred_samples\n\n    def compute_loss(\n        self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the Contrastive Divergence loss given positive and negative samples.\n\n        The CD loss is defined as the difference between the average energy of positive samples\n        (from the data distribution) and the average energy of negative samples (from the model).\n\n        Args:\n            x (torch.Tensor): Real data samples (positive samples)\n            pred_x (torch.Tensor): Generated negative samples\n            *args: Additional positional arguments\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            Scalar loss value\n\n        !!! warning \"Gradient Direction\"\n            Note that this implementation returns `E(x) - E(x')`, so during optimization\n            we *minimize* this value. This is different from some formulations that\n            maximize `E(x') - E(x)`.\n        \"\"\"\n        # Ensure inputs are on the correct device and dtype\n        x = x.to(self.device, self.dtype)\n        pred_x = pred_x.to(self.device, self.dtype)\n\n        # Compute energy of real and generated samples\n        with torch.set_grad_enabled(True):\n            # Add small noise to real data for stability (optional)\n            if kwargs.get(\"add_noise_to_real\", False):\n                noise_scale = kwargs.get(\"noise_scale\", 1e-4)\n                x_noisy = x + noise_scale * torch.randn_like(x)\n                x_energy = self.energy_function(x_noisy)\n            else:\n                x_energy = self.energy_function(x)\n\n            pred_x_energy = self.energy_function(pred_x)\n\n        # Compute mean energies with improved numerical stability\n        mean_x_energy = torch.mean(x_energy)\n        mean_pred_energy = torch.mean(pred_x_energy)\n\n        # Basic contrastive divergence loss: E[data] - E[model]\n        loss = mean_x_energy - mean_pred_energy\n\n        # Optional: Regularization to prevent energies from becoming too large\n        # This helps with stability especially in the early phases of training\n        energy_reg_weight = kwargs.get(\"energy_reg_weight\", 0.001)\n        if energy_reg_weight &gt; 0:\n            energy_reg = energy_reg_weight * (\n                torch.mean(x_energy**2) + torch.mean(pred_x_energy**2)\n            )\n            loss = loss + energy_reg\n\n        # Prevent extremely large gradients with a safety check\n        if torch.isnan(loss) or torch.isinf(loss):\n            warnings.warn(\n                f\"NaN or Inf detected in CD loss. x_energy: {mean_x_energy}, pred_energy: {mean_pred_energy}\",\n                RuntimeWarning,\n            )\n            # Return a small positive constant instead of NaN/Inf to prevent training collapse\n            return torch.tensor(0.1, device=self.device, dtype=self.dtype)\n\n        return loss\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.energy_reg_weight","title":"energy_reg_weight  <code>instance-attribute</code>","text":"<pre><code>energy_reg_weight = energy_reg_weight\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.use_temperature_annealing","title":"use_temperature_annealing  <code>instance-attribute</code>","text":"<pre><code>use_temperature_annealing = use_temperature_annealing\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.min_temp","title":"min_temp  <code>instance-attribute</code>","text":"<pre><code>min_temp = min_temp\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.max_temp","title":"max_temp  <code>instance-attribute</code>","text":"<pre><code>max_temp = max_temp\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.temp_decay","title":"temp_decay  <code>instance-attribute</code>","text":"<pre><code>temp_decay = temp_decay\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.current_temp","title":"current_temp  <code>instance-attribute</code>","text":"<pre><code>current_temp = max_temp\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.forward","title":"forward","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Compute the Contrastive Divergence loss and generate negative samples.</p> <p>This method implements the energy_functions CD algorithm by:</p> <ol> <li>Initializing MCMC chains (either from data or persistent state)</li> <li>Running the sampler for k_steps to generate negative samples</li> <li>Computing the CD loss using the energy difference</li> </ol> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Batch of real data samples (positive samples)</p> required <code>*args</code> <p>Additional positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: - loss: The CD loss value (scalar) - pred_samples: Generated negative samples</p> <p>Shape Information</p> <ul> <li>Input <code>x</code>: (batch_size, feature_dimensions)</li> <li>Output loss: scalar</li> <li>Output pred_samples: (batch_size, feature_dimensions)</li> </ul> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, *args, **kwargs\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute the Contrastive Divergence loss and generate negative samples.\n\n    This method implements the energy_functions CD algorithm by:\n\n    1. Initializing MCMC chains (either from data or persistent state)\n    2. Running the sampler for k_steps to generate negative samples\n    3. Computing the CD loss using the energy difference\n\n    Args:\n        x (torch.Tensor): Batch of real data samples (positive samples)\n        *args: Additional positional arguments\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]:\n            - loss: The CD loss value (scalar)\n            - pred_samples: Generated negative samples\n\n    !!! note \"Shape Information\"\n        - Input `x`: (batch_size, feature_dimensions)\n        - Output loss: scalar\n        - Output pred_samples: (batch_size, feature_dimensions)\n    \"\"\"\n\n    batch_size = x.shape[0]\n    data_shape = x.shape[1:]\n\n    # Update temperature if annealing is enabled\n    if self.use_temperature_annealing and self.training:\n        self.current_temp = max(self.min_temp, self.current_temp * self.temp_decay)\n        self.temperature[...] = self.current_temp  # Use ellipsis instead of index\n\n        # If sampler has a temperature parameter, update it\n        if hasattr(self.sampler, \"temperature\"):\n            self.sampler.temperature = self.current_temp\n        elif hasattr(self.sampler, \"noise_scale\"):\n            # For samplers like Langevin, adjust noise scale based on temperature\n            original_noise = getattr(self.sampler, \"_original_noise_scale\", None)\n            if original_noise is None:\n                setattr(\n                    self.sampler, \"_original_noise_scale\", self.sampler.noise_scale\n                )\n                original_noise = self.sampler.noise_scale\n\n            self.sampler.noise_scale = original_noise * math.sqrt(self.current_temp)\n\n    # Get starting points for chains (either from buffer or data)\n    start_points = self.get_start_points(x)\n\n    # Run MCMC chains to get negative samples\n    pred_samples = self.sampler.sample(\n        x=start_points,\n        n_steps=self.k_steps,\n    )\n\n    # Update persistent buffer if using PCD\n    if self.persistent:\n        with torch.no_grad():\n            self.update_buffer(pred_samples.detach())\n\n    # Add energy regularization to kwargs for compute_loss\n    kwargs[\"energy_reg_weight\"] = kwargs.get(\n        \"energy_reg_weight\", self.energy_reg_weight\n    )\n\n    # Compute contrastive divergence loss\n    loss = self.compute_loss(x, pred_samples, *args, **kwargs)\n\n    return loss, pred_samples\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ContrastiveDivergence/#torchebm.losses.contrastive_divergence.ContrastiveDivergence.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(x: Tensor, pred_x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the Contrastive Divergence loss given positive and negative samples.</p> <p>The CD loss is defined as the difference between the average energy of positive samples (from the data distribution) and the average energy of negative samples (from the model).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Real data samples (positive samples)</p> required <code>pred_x</code> <code>Tensor</code> <p>Generated negative samples</p> required <code>*args</code> <p>Additional positional arguments</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss value</p> <p>Gradient Direction</p> <p>Note that this implementation returns <code>E(x) - E(x')</code>, so during optimization we minimize this value. This is different from some formulations that maximize <code>E(x') - E(x)</code>.</p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>def compute_loss(\n    self, x: torch.Tensor, pred_x: torch.Tensor, *args, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the Contrastive Divergence loss given positive and negative samples.\n\n    The CD loss is defined as the difference between the average energy of positive samples\n    (from the data distribution) and the average energy of negative samples (from the model).\n\n    Args:\n        x (torch.Tensor): Real data samples (positive samples)\n        pred_x (torch.Tensor): Generated negative samples\n        *args: Additional positional arguments\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        Scalar loss value\n\n    !!! warning \"Gradient Direction\"\n        Note that this implementation returns `E(x) - E(x')`, so during optimization\n        we *minimize* this value. This is different from some formulations that\n        maximize `E(x') - E(x)`.\n    \"\"\"\n    # Ensure inputs are on the correct device and dtype\n    x = x.to(self.device, self.dtype)\n    pred_x = pred_x.to(self.device, self.dtype)\n\n    # Compute energy of real and generated samples\n    with torch.set_grad_enabled(True):\n        # Add small noise to real data for stability (optional)\n        if kwargs.get(\"add_noise_to_real\", False):\n            noise_scale = kwargs.get(\"noise_scale\", 1e-4)\n            x_noisy = x + noise_scale * torch.randn_like(x)\n            x_energy = self.energy_function(x_noisy)\n        else:\n            x_energy = self.energy_function(x)\n\n        pred_x_energy = self.energy_function(pred_x)\n\n    # Compute mean energies with improved numerical stability\n    mean_x_energy = torch.mean(x_energy)\n    mean_pred_energy = torch.mean(pred_x_energy)\n\n    # Basic contrastive divergence loss: E[data] - E[model]\n    loss = mean_x_energy - mean_pred_energy\n\n    # Optional: Regularization to prevent energies from becoming too large\n    # This helps with stability especially in the early phases of training\n    energy_reg_weight = kwargs.get(\"energy_reg_weight\", 0.001)\n    if energy_reg_weight &gt; 0:\n        energy_reg = energy_reg_weight * (\n            torch.mean(x_energy**2) + torch.mean(pred_x_energy**2)\n        )\n        loss = loss + energy_reg\n\n    # Prevent extremely large gradients with a safety check\n    if torch.isnan(loss) or torch.isinf(loss):\n        warnings.warn(\n            f\"NaN or Inf detected in CD loss. x_energy: {mean_x_energy}, pred_energy: {mean_pred_energy}\",\n            RuntimeWarning,\n        )\n        # Return a small positive constant instead of NaN/Inf to prevent training collapse\n        return torch.tensor(0.1, device=self.device, dtype=self.dtype)\n\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/","title":"ParallelTemperingCD","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseContrastiveDivergence</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class ParallelTemperingCD(BaseContrastiveDivergence):\n    def __init__(self, temps=[1.0, 0.5], k=5):\n        super().__init__(k)\n        self.temps = temps  # List of temperatures\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/ParallelTemperingCD/#torchebm.losses.contrastive_divergence.ParallelTemperingCD.temps","title":"temps  <code>instance-attribute</code>","text":"<pre><code>temps = temps\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/","title":"PersistentContrastiveDivergence","text":""},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseContrastiveDivergence</code></p> Source code in <code>torchebm/losses/contrastive_divergence.py</code> <pre><code>class PersistentContrastiveDivergence(BaseContrastiveDivergence):\n    def __init__(self, buffer_size=100):\n        super().__init__(k_steps=1)\n        self.buffer = None  # Persistent chain state\n        self.buffer_size = buffer_size\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#torchebm.losses.contrastive_divergence.PersistentContrastiveDivergence.buffer","title":"buffer  <code>instance-attribute</code>","text":"<pre><code>buffer = None\n</code></pre>"},{"location":"api/torchebm/losses/contrastive_divergence/classes/PersistentContrastiveDivergence/#torchebm.losses.contrastive_divergence.PersistentContrastiveDivergence.buffer_size","title":"buffer_size  <code>instance-attribute</code>","text":"<pre><code>buffer_size = buffer_size\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/","title":"Torchebm &gt; Losses &gt; Score_matching","text":""},{"location":"api/torchebm/losses/score_matching/#torchebm-losses-score_matching","title":"Torchebm &gt; Losses &gt; Score_matching","text":""},{"location":"api/torchebm/losses/score_matching/#contents","title":"Contents","text":""},{"location":"api/torchebm/losses/score_matching/#classes","title":"Classes","text":"<ul> <li><code>DenoisingScoreMatching</code> - Implementation of Denoising Score Matching (DSM) by Vincent (2011).</li> <li><code>ScoreMatching</code> - Implementation of the original Score Matching method by Hyv\u00e4rinen (2005).</li> <li><code>SlicedScoreMatching</code> - Implementation of Sliced Score Matching (SSM) by Song et al. (2019).</li> </ul>"},{"location":"api/torchebm/losses/score_matching/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching","title":"torchebm.losses.score_matching","text":"<p>Score Matching Loss Module.</p> <p>This module provides implementations of various Score Matching techniques for training energy-based models (EBMs). Score Matching offers a powerful alternative to Contrastive Divergence by directly estimating the score function without requiring MCMC sampling, making it more computationally efficient and stable in many cases.</p> <p>Key Features</p> <ul> <li>Original Score Matching (Hyv\u00e4rinen, 2005)</li> <li>Denoising Score Matching (Vincent, 2011)</li> <li>Sliced Score Matching (Song et al., 2019)</li> <li>Support for different Hessian computation methods</li> <li>Mixed precision training support</li> </ul>"},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>ScoreMatching</code> <p>Original score matching with exact or approximate Hessian computation</p> <code>DenoisingScoreMatching</code> <p>Denoising variant that avoids Hessian computation</p> <code>SlicedScoreMatching</code> <p>Efficient variant using random projections</p>"},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching--usage-example","title":"Usage Example","text":"<p>Basic Score Matching Usage</p> <pre><code>from torchebm.losses import ScoreMatching\nfrom torchebm.energy_functions import MLPEnergyFunction\nimport torch\n\n# Define the energy function\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n# Create the score matching loss\nsm_loss = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",  # More efficient for high dimensions\n    hutchinson_samples=5\n)\n\n# In the training loop:\ndata_batch = torch.randn(32, 2)  # Real data samples\nloss = sm_loss(data_batch)\nloss.backward()\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Score Matching Principles</p> <p>Score Matching minimizes the expected squared distance between the model's score and the data's score:</p> \\[ J(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\| \\nabla_x E_\\theta(x) \\|^2 \\right] - \\mathbb{E}_{p_{\\text{data}}} \\left[ \\text{tr}(\\nabla_x^2 E_\\theta(x)) \\right] \\] <p>where: - \\( E_\\theta(x) \\) is the energy function with parameters \\( \\theta \\) - \\( \\nabla_x E_\\theta(x) \\) is the score function (gradient of energy w.r.t. input) - \\( \\nabla_x^2 E_\\theta(x) \\) is the Hessian of the energy function</p> <p>Why Score Matching Works</p> <ul> <li>No MCMC Required: Directly estimates the score function without sampling</li> <li>Computational Efficiency: Avoids the need for expensive MCMC chains</li> <li>Stability: More stable training dynamics compared to CD</li> <li>Theoretical Guarantees: Consistent estimator under mild conditions</li> </ul>"},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching--variants","title":"Variants","text":"<p>Denoising Score Matching (DSM)</p> <p>DSM avoids computing the Hessian trace by working with noise-perturbed data:</p> \\[ J_{\\text{DSM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{\\varepsilon}  \\left[ \\left\\| \\nabla_x E_\\theta(x + \\varepsilon) + \\frac{\\varepsilon}{\\sigma^2} \\right\\|^2 \\right] \\] <p>where \\( \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\) is the added noise.</p> <p>Noise Scale Selection</p> <ul> <li>Smaller \\( \\sigma \\): Better for fine details but may be unstable</li> <li>Larger \\( \\sigma \\): More stable but may lose fine structure</li> <li>Annealing \\( \\sigma \\) during training can help</li> </ul> <p>Sliced Score Matching (SSM)</p> <p>SSM uses random projections to estimate the score matching objective:</p> \\[ J_{\\text{SSM}}(\\theta) = \\mathbb{E}_{p_{\\text{data}}(x)} \\left[ v^T \\nabla_x \\log p_\\theta(x) v + \\frac{1}{2} \\left( v^T \\nabla_x \\log p_\\theta(x) \\right)^2 \\right] + \\text{const.} \\] <p>where \\( v \\) is a random projection vector.</p> <p>Projection Types</p> <ul> <li>Rademacher: Values in \\(\\{-1, +1\\}\\), often lower variance</li> <li>Gaussian: Standard normal distribution, more general</li> <li>Sphere: Uniformly sampled unit vectors on the hypersphere, preserving direction norms</li> </ul>"},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching--practical-considerations","title":"Practical Considerations","text":"<p>Hessian Computation Methods</p> <ul> <li>exact: Computes full Hessian diagonal, accurate but expensive</li> <li>hutchinson: Uses random projections, efficient for high dimensions</li> <li>approx: Uses finite differences, numerically stable</li> </ul> <p>How to Choose the Right Method?</p> <p>Consider these factors when selecting a score matching variant:</p> <ul> <li>Data Dimension: For high dimensions, prefer SSM or DSM</li> <li>Computational Resources: Exact Hessian requires more memory</li> <li>Training Stability: DSM often more stable than original SM</li> <li>Accuracy Requirements: Exact method most accurate but slowest</li> </ul> <p>Common Pitfalls</p> <ul> <li>Numerical Instability: Hessian computation can be unstable</li> <li>Gradient Explosion: Score terms can grow very large</li> <li>Memory Usage: Exact Hessian requires \\( O(d^2) \\) memory</li> <li>Noise Scale: Poor choice of \\( \\sigma \\) in DSM can hurt performance</li> </ul>"},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching--useful-insights","title":"Useful Insights","text":"<p>When to Use Score Matching</p> <p>Score Matching is particularly effective when:</p> <ul> <li>Training high-dimensional energy-based models</li> <li>Working with continuous data distributions</li> <li>Computational efficiency is important</li> <li>MCMC sampling is unstable or expensive</li> </ul> Further Reading <ul> <li>Hyv\u00e4rinen, A. (2005). \"Estimation of non-normalized statistical models by score matching.\"</li> <li>Vincent, P. (2011). \"A connection between score matching and denoising autoencoders.\"</li> <li>Song, Y., et al. (2019). \"Sliced score matching: A scalable approach to density and score estimation.\"</li> </ul>"},{"location":"api/torchebm/losses/score_matching/#torchebm.losses.score_matching--other-examples","title":"Other Examples","text":"<p>Denoising Score Matching with Annealing</p> <pre><code>from torchebm.losses import DenoisingScoreMatching\nfrom torchebm.core import LinearScheduler\n\n# Create noise scale scheduler\nnoise_scheduler = LinearScheduler(\n    start_value=0.1,\n    end_value=0.01,\n    n_steps=1000\n)\n\n# Create DSM loss with dynamic noise scale\ndsm_loss = DenoisingScoreMatching(\n    energy_function=energy_fn,\n    noise_scale=noise_scheduler\n)\n</code></pre> <p>Sliced Score Matching with Multiple Projections</p> <pre><code>from torchebm.losses import SlicedScoreMatching\n\n# Create SSM loss with multiple projections\nssm_loss = SlicedScoreMatching(\n    energy_function=energy_fn,\n    n_projections=10,\n    projection_type=\"rademacher\"\n)\n</code></pre> <p>Mixed Precision Training</p> <pre><code># Enable mixed precision for better performance\nsm_loss = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",\n    use_mixed_precision=True\n)\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/DenoisingScoreMatching/","title":"DenoisingScoreMatching","text":""},{"location":"api/torchebm/losses/score_matching/classes/DenoisingScoreMatching/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScoreMatching</code></p> <p>Implementation of Denoising Score Matching (DSM) by Vincent (2011).</p> <p>DSM is a variant of score matching that avoids computing the trace of the Hessian by instead matching the score function to the score of noise-perturbed data. This makes it more computationally efficient and numerically stable than the original score matching.</p> <p>Key Advantages</p> <ul> <li>No Hessian computation required</li> <li>More stable than original score matching</li> <li>Computationally efficient</li> <li>Works well with high-dimensional data</li> </ul>"},{"location":"api/torchebm/losses/score_matching/classes/DenoisingScoreMatching/#torchebm.losses.score_matching.DenoisingScoreMatching--mathematical-formulation","title":"Mathematical Formulation","text":"<p>For data \\( x \\), we add noise \\( \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\) to get perturbed data \\( \\tilde{x} = x + \\varepsilon \\). The DSM objective is:</p> \\[ J_{\\text{DSM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{\\varepsilon} \\left[ \\left\\| \\nabla_{\\tilde{x}} E_\\theta(\\tilde{x}) + \\frac{\\varepsilon}{\\sigma^2} \\right\\|^2 \\right] \\] <p>The key insight is that the score of the noise-perturbed data distribution can be related to the original data distribution and the noise model:</p> \\[ \\nabla_{\\tilde{x}} \\log p(\\tilde{x}) \\approx \\frac{x - \\tilde{x}}{\\sigma^2} = -\\frac{\\varepsilon}{\\sigma^2} \\] <p>This allows us to train the model without computing Hessians, using only first-order gradients.</p> <p>Noise Scale Selection</p> <p>The choice of noise scale \\( \\sigma \\) is crucial: - Small \\( \\sigma \\): Better for fine details but may be unstable - Large \\( \\sigma \\): More stable but may lose fine structure - Annealing \\( \\sigma \\) during training can help balance these trade-offs</p>"},{"location":"api/torchebm/losses/score_matching/classes/DenoisingScoreMatching/#torchebm.losses.score_matching.DenoisingScoreMatching--practical-considerations","title":"Practical Considerations","text":"<ul> <li>The noise scale \\( \\sigma \\) is a critical hyperparameter that affects the training dynamics</li> <li>Smaller noise scales focus on fine details of the data distribution</li> <li>Larger noise scales help with stability but may lose some detailed structure</li> <li>Annealing the noise scale during training can sometimes improve results</li> </ul> <p>Common Issues</p> <ul> <li>Too small noise scale can lead to numerical instability</li> <li>Too large noise scale can cause loss of fine details</li> <li>Noise scale should be tuned based on data characteristics</li> </ul> <p>Basic Usage</p> <pre><code># Create energy function\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n# Initialize DSM with default noise scale\ndsm_loss = DenoisingScoreMatching(\n    energy_function=energy_fn,\n    noise_scale=0.01\n)\n\n# Training loop\noptimizer = torch.optim.Adam(energy_fn.parameters(), lr=1e-3)\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = dsm_loss(batch)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Advanced Configuration</p> <pre><code># With noise scale annealing\nfrom torchebm.core import LinearScheduler\n\nnoise_scheduler = LinearScheduler(\n    start_value=0.1,\n    end_value=0.01,\n    n_steps=1000\n)\n\ndsm_loss = DenoisingScoreMatching(\n    energy_function=energy_fn,\n    noise_scale=noise_scheduler\n)\n\n# With mixed precision training\ndsm_loss = DenoisingScoreMatching(\n    energy_function=energy_fn,\n    noise_scale=0.01,\n    use_mixed_precision=True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to train</p> required <code>noise_scale</code> <code>float</code> <p>Scale of Gaussian noise for data perturbation</p> <code>0.01</code> <code>regularization_strength</code> <code>float</code> <p>Coefficient for regularization terms</p> <code>0.0</code> <code>custom_regularization</code> <code>Optional[Callable]</code> <p>Optional function for custom regularization</p> <code>None</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training</p> <code>False</code> <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for computations</p> <code>None</code> References <p>Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7), 1661-1674.</p> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>class DenoisingScoreMatching(BaseScoreMatching):\n    r\"\"\"\n    Implementation of Denoising Score Matching (DSM) by Vincent (2011).\n\n    DSM is a variant of score matching that avoids computing the trace of the Hessian\n    by instead matching the score function to the score of noise-perturbed data. This makes\n    it more computationally efficient and numerically stable than the original score matching.\n\n    !!! success \"Key Advantages\"\n        - No Hessian computation required\n        - More stable than original score matching\n        - Computationally efficient\n        - Works well with high-dimensional data\n\n    ## Mathematical Formulation\n\n    For data \\( x \\), we add noise \\( \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\) to get\n    perturbed data \\( \\tilde{x} = x + \\varepsilon \\). The DSM objective is:\n\n    \\[\n    J_{\\text{DSM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{\\varepsilon}\n    \\left[ \\left\\| \\nabla_{\\tilde{x}} E_\\theta(\\tilde{x}) + \\frac{\\varepsilon}{\\sigma^2} \\right\\|^2 \\right]\n    \\]\n\n    The key insight is that the score of the noise-perturbed data distribution can be related to\n    the original data distribution and the noise model:\n\n    \\[\n    \\nabla_{\\tilde{x}} \\log p(\\tilde{x}) \\approx \\frac{x - \\tilde{x}}{\\sigma^2} = -\\frac{\\varepsilon}{\\sigma^2}\n    \\]\n\n    This allows us to train the model without computing Hessians, using only first-order gradients.\n\n    !!! tip \"Noise Scale Selection\"\n        The choice of noise scale \\( \\sigma \\) is crucial:\n        - Small \\( \\sigma \\): Better for fine details but may be unstable\n        - Large \\( \\sigma \\): More stable but may lose fine structure\n        - Annealing \\( \\sigma \\) during training can help balance these trade-offs\n\n    ## Practical Considerations\n\n    - The noise scale \\( \\sigma \\) is a critical hyperparameter that affects the training dynamics\n    - Smaller noise scales focus on fine details of the data distribution\n    - Larger noise scales help with stability but may lose some detailed structure\n    - Annealing the noise scale during training can sometimes improve results\n\n    !!! warning \"Common Issues\"\n        - Too small noise scale can lead to numerical instability\n        - Too large noise scale can cause loss of fine details\n        - Noise scale should be tuned based on data characteristics\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Create energy function\n        energy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n        # Initialize DSM with default noise scale\n        dsm_loss = DenoisingScoreMatching(\n            energy_function=energy_fn,\n            noise_scale=0.01\n        )\n\n        # Training loop\n        optimizer = torch.optim.Adam(energy_fn.parameters(), lr=1e-3)\n\n        for batch in dataloader:\n            optimizer.zero_grad()\n            loss = dsm_loss(batch)\n            loss.backward()\n            optimizer.step()\n        ```\n\n    !!! example \"Advanced Configuration\"\n        ```python\n        # With noise scale annealing\n        from torchebm.core import LinearScheduler\n\n        noise_scheduler = LinearScheduler(\n            start_value=0.1,\n            end_value=0.01,\n            n_steps=1000\n        )\n\n        dsm_loss = DenoisingScoreMatching(\n            energy_function=energy_fn,\n            noise_scale=noise_scheduler\n        )\n\n        # With mixed precision training\n        dsm_loss = DenoisingScoreMatching(\n            energy_function=energy_fn,\n            noise_scale=0.01,\n            use_mixed_precision=True\n        )\n        ```\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to train\n        noise_scale (float): Scale of Gaussian noise for data perturbation\n        regularization_strength (float): Coefficient for regularization terms\n        custom_regularization (Optional[Callable]): Optional function for custom regularization\n        use_mixed_precision (bool): Whether to use mixed precision training\n        dtype (torch.dtype): Data type for computations\n        device (Optional[Union[str, torch.device]]): Device for computations\n\n    References:\n        Vincent, P. (2011). A connection between score matching and denoising autoencoders.\n        Neural Computation, 23(7), 1661-1674.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        noise_scale: float = 0.01,\n        regularization_strength: float = 0.0,\n        custom_regularization: Optional[Callable] = None,\n        use_mixed_precision: bool = False,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            energy_function=energy_function,\n            noise_scale=noise_scale,\n            regularization_strength=regularization_strength,\n            use_autograd=True,\n            custom_regularization=custom_regularization,\n            use_mixed_precision=use_mixed_precision,\n            dtype=dtype,\n            device=device,\n            *args,\n            **kwargs,\n        )\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the denoising score matching loss for a batch of data.\n\n        This method first computes the denoising score matching loss using perturbed data,\n        then adds regularization if needed.\n\n        !!! note\n            The input tensor is automatically converted to the device and dtype specified\n            during initialization.\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n            *args: Additional positional arguments passed to compute_loss\n            **kwargs: Additional keyword arguments passed to compute_loss\n\n        Returns:\n            torch.Tensor: The denoising score matching loss (scalar)\n\n        Examples:\n            &gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n            &gt;&gt;&gt; loss_fn = DenoisingScoreMatching(\n            ...     energy_fn,\n            ...     noise_scale=0.01  # Controls the noise level added to data\n            ... )\n            &gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n            &gt;&gt;&gt; loss = loss_fn(x)  # Compute the DSM loss\n            &gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n        \"\"\"\n        if (x.device != self.device) or (x.dtype != self.dtype):\n            x = x.to(device=self.device, dtype=self.dtype)\n\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                loss = self.compute_loss(x, *args, **kwargs)\n        else:\n            loss = self.compute_loss(x, *args, **kwargs)\n\n        if self.regularization_strength &gt; 0 or self.custom_regularization is not None:\n            loss = self.add_regularization(loss, x)\n\n        return loss\n\n    def compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the denoising score matching loss.\n\n        DSM adds noise \\( \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\) to the data and\n        trains the score network to predict \\( -\\varepsilon/\\sigma^2 \\):\n\n        \\[\n        \\mathcal{L}_{\\text{DSM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{\\varepsilon}\n        \\left[ \\left\\| \\nabla_{\\tilde{x}} E_\\theta(\\tilde{x}) + \\frac{\\varepsilon}{\\sigma^2} \\right\\|^2 \\right]\n        \\]\n\n        where \\( \\tilde{x} = x + \\varepsilon \\) is the perturbed data point.\n\n        !!! note\n            The noise scale \\( \\sigma \\) is a critical hyperparameter that affects the learning dynamics.\n\n        !!! tip\n            - Smaller noise scales focus on fine details of the data distribution\n            - Larger noise scales help with stability but may lose some detailed structure\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n            *args: Additional arguments (not used)\n            **kwargs: Additional keyword arguments (not used)\n\n        Returns:\n            torch.Tensor: The denoising score matching loss (scalar)\n\n        Examples:\n            &gt;&gt;&gt; # Creating loss functions with different noise scales:\n            &gt;&gt;&gt; # Small noise for capturing fine details\n            &gt;&gt;&gt; fine_dsm = DenoisingScoreMatching(energy_fn, noise_scale=0.01)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Larger noise for stability\n            &gt;&gt;&gt; stable_dsm = DenoisingScoreMatching(energy_fn, noise_scale=0.1)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Computing loss\n            &gt;&gt;&gt; x = torch.randn(32, 2)  # 32 samples of 2D data\n            &gt;&gt;&gt; loss = fine_dsm(x)\n        \"\"\"\n        x_perturbed, noise = self.perturb_data(x)\n\n        score = self.compute_score(x_perturbed)\n\n        target_score = -noise / (self.noise_scale**2)\n\n        loss = (\n            0.5\n            * torch.sum(\n                (score - target_score) ** 2, dim=list(range(1, len(x.shape)))\n            ).mean()\n        )\n\n        return loss\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/DenoisingScoreMatching/#torchebm.losses.score_matching.DenoisingScoreMatching.forward","title":"forward","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the denoising score matching loss for a batch of data.</p> <p>This method first computes the denoising score matching loss using perturbed data, then adds regularization if needed.</p> <p>Note</p> <p>The input tensor is automatically converted to the device and dtype specified during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <code>*args</code> <p>Additional positional arguments passed to compute_loss</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments passed to compute_loss</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The denoising score matching loss (scalar)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n&gt;&gt;&gt; loss_fn = DenoisingScoreMatching(\n...     energy_fn,\n...     noise_scale=0.01  # Controls the noise level added to data\n... )\n&gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n&gt;&gt;&gt; loss = loss_fn(x)  # Compute the DSM loss\n&gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n</code></pre> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the denoising score matching loss for a batch of data.\n\n    This method first computes the denoising score matching loss using perturbed data,\n    then adds regularization if needed.\n\n    !!! note\n        The input tensor is automatically converted to the device and dtype specified\n        during initialization.\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n        *args: Additional positional arguments passed to compute_loss\n        **kwargs: Additional keyword arguments passed to compute_loss\n\n    Returns:\n        torch.Tensor: The denoising score matching loss (scalar)\n\n    Examples:\n        &gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n        &gt;&gt;&gt; loss_fn = DenoisingScoreMatching(\n        ...     energy_fn,\n        ...     noise_scale=0.01  # Controls the noise level added to data\n        ... )\n        &gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n        &gt;&gt;&gt; loss = loss_fn(x)  # Compute the DSM loss\n        &gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n    \"\"\"\n    if (x.device != self.device) or (x.dtype != self.dtype):\n        x = x.to(device=self.device, dtype=self.dtype)\n\n    if self.use_mixed_precision and self.autocast_available:\n        from torch.cuda.amp import autocast\n\n        with autocast():\n            loss = self.compute_loss(x, *args, **kwargs)\n    else:\n        loss = self.compute_loss(x, *args, **kwargs)\n\n    if self.regularization_strength &gt; 0 or self.custom_regularization is not None:\n        loss = self.add_regularization(loss, x)\n\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/DenoisingScoreMatching/#torchebm.losses.score_matching.DenoisingScoreMatching.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the denoising score matching loss.</p> <p>DSM adds noise \\( \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\) to the data and trains the score network to predict \\( -\\varepsilon/\\sigma^2 \\):</p> \\[ \\mathcal{L}_{\\text{DSM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{\\varepsilon} \\left[ \\left\\| \\nabla_{\\tilde{x}} E_\\theta(\\tilde{x}) + \\frac{\\varepsilon}{\\sigma^2} \\right\\|^2 \\right] \\] <p>where \\( \\tilde{x} = x + \\varepsilon \\) is the perturbed data point.</p> <p>Note</p> <p>The noise scale \\( \\sigma \\) is a critical hyperparameter that affects the learning dynamics.</p> <p>Tip</p> <ul> <li>Smaller noise scales focus on fine details of the data distribution</li> <li>Larger noise scales help with stability but may lose some detailed structure</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <code>*args</code> <p>Additional arguments (not used)</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments (not used)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The denoising score matching loss (scalar)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Creating loss functions with different noise scales:\n&gt;&gt;&gt; # Small noise for capturing fine details\n&gt;&gt;&gt; fine_dsm = DenoisingScoreMatching(energy_fn, noise_scale=0.01)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Larger noise for stability\n&gt;&gt;&gt; stable_dsm = DenoisingScoreMatching(energy_fn, noise_scale=0.1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Computing loss\n&gt;&gt;&gt; x = torch.randn(32, 2)  # 32 samples of 2D data\n&gt;&gt;&gt; loss = fine_dsm(x)\n</code></pre> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the denoising score matching loss.\n\n    DSM adds noise \\( \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\) to the data and\n    trains the score network to predict \\( -\\varepsilon/\\sigma^2 \\):\n\n    \\[\n    \\mathcal{L}_{\\text{DSM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{\\varepsilon}\n    \\left[ \\left\\| \\nabla_{\\tilde{x}} E_\\theta(\\tilde{x}) + \\frac{\\varepsilon}{\\sigma^2} \\right\\|^2 \\right]\n    \\]\n\n    where \\( \\tilde{x} = x + \\varepsilon \\) is the perturbed data point.\n\n    !!! note\n        The noise scale \\( \\sigma \\) is a critical hyperparameter that affects the learning dynamics.\n\n    !!! tip\n        - Smaller noise scales focus on fine details of the data distribution\n        - Larger noise scales help with stability but may lose some detailed structure\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n        *args: Additional arguments (not used)\n        **kwargs: Additional keyword arguments (not used)\n\n    Returns:\n        torch.Tensor: The denoising score matching loss (scalar)\n\n    Examples:\n        &gt;&gt;&gt; # Creating loss functions with different noise scales:\n        &gt;&gt;&gt; # Small noise for capturing fine details\n        &gt;&gt;&gt; fine_dsm = DenoisingScoreMatching(energy_fn, noise_scale=0.01)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Larger noise for stability\n        &gt;&gt;&gt; stable_dsm = DenoisingScoreMatching(energy_fn, noise_scale=0.1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Computing loss\n        &gt;&gt;&gt; x = torch.randn(32, 2)  # 32 samples of 2D data\n        &gt;&gt;&gt; loss = fine_dsm(x)\n    \"\"\"\n    x_perturbed, noise = self.perturb_data(x)\n\n    score = self.compute_score(x_perturbed)\n\n    target_score = -noise / (self.noise_scale**2)\n\n    loss = (\n        0.5\n        * torch.sum(\n            (score - target_score) ** 2, dim=list(range(1, len(x.shape)))\n        ).mean()\n    )\n\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/","title":"ScoreMatching","text":""},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScoreMatching</code></p> <p>Implementation of the original Score Matching method by Hyv\u00e4rinen (2005).</p> <p>Score Matching trains energy-based models by making the gradient of the model's energy function (score) match the gradient of the data's log density. This method avoids the need for MCMC sampling that is typically required in contrastive divergence.</p> <p>Key Advantages</p> <ul> <li>No MCMC sampling required</li> <li>Direct estimation of score function</li> <li>More stable training dynamics</li> <li>Consistent estimator under mild conditions</li> </ul>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching--mathematical-formulation","title":"Mathematical Formulation","text":"<p>The score matching objective minimizes:</p> \\[ J(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\| \\nabla_x E_\\theta(x) \\|_2^2 \\right] - \\mathbb{E}_{p_{\\text{data}}} \\left[ \\operatorname{tr}(\\nabla_x^2 E_\\theta(x)) \\right] \\] <p>which is equivalent (up to an additive constant independent of \\(\\theta\\)) to the log-density formulation:</p> \\[ J(\\theta) = \\mathbb{E}_{p_{\\text{data}}} \\left[     \\operatorname{tr}(\\nabla_x^2 \\log p_\\theta(x))     + \\tfrac{1}{2} \\| \\nabla_x \\log p_\\theta(x) \\|_2^2 \\right] + \\text{const.},\\quad \\text{with }\\ \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x). \\] <p>where: - \\( E_\\theta(x) \\) is the energy function with parameters \\( \\theta \\) - \\( \\nabla_x E_\\theta(x) \\) is the score function (gradient of energy w.r.t. input) - \\( \\nabla_x^2 E_\\theta(x) \\) is the Hessian of the energy function - \\( \\text{tr}(\\cdot) \\) denotes the trace operator</p> <p>Computational Considerations</p> <p>The computational cost varies significantly with the choice of Hessian computation method: - Exact method: \\( O(d^2) \\) for d-dimensional data - Hutchinson method: \\( O(d) \\) with variance depending on number of samples - Approximation method: \\( O(d) \\) but may be less accurate</p>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching--implementation-details","title":"Implementation Details","text":"<p>This implementation provides three different methods for computing the Hessian trace:</p> <ol> <li> <p>exact: Computes the full Hessian diagonal elements directly. Most accurate but    computationally expensive for high-dimensional data.</p> </li> <li> <p>hutchinson: Uses Hutchinson's trace estimator, which approximates the trace using    random projections: \\( \\text{tr}(H) \\approx \\mathbb{E}_v[v^T H v] \\) where v is typically    sampled from a Rademacher distribution. More efficient for high dimensions.</p> </li> <li> <p>approx: Uses a finite-difference approximation of the Hessian trace which can be    more numerically stable in some cases.</p> </li> </ol> <p>Numerical Stability</p> <ul> <li>The exact method can be unstable with mixed precision training</li> <li>Large values in the Hessian can cause numerical issues</li> <li>Gradient clipping is applied automatically to prevent instability</li> </ul> <p>Basic Usage</p> <pre><code># Create a simple energy function\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n# Initialize score matching with Hutchinson estimator\nsm_loss = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",\n    hutchinson_samples=5\n)\n\n# Training loop\noptimizer = torch.optim.Adam(energy_fn.parameters(), lr=1e-3)\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = sm_loss(batch)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Advanced Configuration</p> <pre><code># With mixed precision training\nsm_loss = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",\n    use_mixed_precision=True\n)\n\n# With custom regularization\ndef l2_regularization(energy_fn, x):\n    return torch.mean(energy_fn(x)**2)\n\nsm_loss = ScoreMatching(\n    energy_function=energy_fn,\n    regularization_strength=0.1,\n    custom_regularization=l2_regularization\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to train</p> required <code>hessian_method</code> <code>str</code> <p>Method to compute Hessian trace. One of {\"exact\", \"hutchinson\", \"approx\"}</p> <code>'exact'</code> <code>regularization_strength</code> <code>float</code> <p>Coefficient for regularization terms</p> <code>0.0</code> <code>hutchinson_samples</code> <code>int</code> <p>Number of random vectors for Hutchinson's trace estimator</p> required <code>custom_regularization</code> <code>Optional[Callable]</code> <p>Optional function for custom regularization</p> <code>None</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training</p> <code>False</code> <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for computations</p> <code>None</code> References <p>Hyv\u00e4rinen, A. (2005). Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6, 695-709.</p> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>class ScoreMatching(BaseScoreMatching):\n    r\"\"\"\n    Implementation of the original Score Matching method by Hyv\u00e4rinen (2005).\n\n    Score Matching trains energy-based models by making the gradient of the model's\n    energy function (score) match the gradient of the data's log density. This method\n    avoids the need for MCMC sampling that is typically required in contrastive divergence.\n\n    !!! success \"Key Advantages\"\n        - No MCMC sampling required\n        - Direct estimation of score function\n        - More stable training dynamics\n        - Consistent estimator under mild conditions\n\n    ## Mathematical Formulation\n\n    The score matching objective minimizes:\n\n    \\[\n    J(\\theta) = \\frac{1}{2}\n    \\mathbb{E}_{p_{\\text{data}}} \\left[ \\| \\nabla_x E_\\theta(x) \\|_2^2 \\right]\n    - \\mathbb{E}_{p_{\\text{data}}} \\left[ \\operatorname{tr}(\\nabla_x^2 E_\\theta(x)) \\right]\n    \\]\n\n    which is equivalent (up to an additive constant independent of \\(\\theta\\)) to the\n    log-density formulation:\n\n    \\[\n    J(\\theta) = \\mathbb{E}_{p_{\\text{data}}} \\left[\n        \\operatorname{tr}(\\nabla_x^2 \\log p_\\theta(x))\n        + \\tfrac{1}{2} \\| \\nabla_x \\log p_\\theta(x) \\|_2^2\n    \\right] + \\text{const.},\\quad \\text{with }\\ \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x).\n    \\]\n\n    where:\n    - \\( E_\\theta(x) \\) is the energy function with parameters \\( \\theta \\)\n    - \\( \\nabla_x E_\\theta(x) \\) is the score function (gradient of energy w.r.t. input)\n    - \\( \\nabla_x^2 E_\\theta(x) \\) is the Hessian of the energy function\n    - \\( \\text{tr}(\\cdot) \\) denotes the trace operator\n\n    !!! tip \"Computational Considerations\"\n        The computational cost varies significantly with the choice of Hessian computation method:\n        - Exact method: \\( O(d^2) \\) for d-dimensional data\n        - Hutchinson method: \\( O(d) \\) with variance depending on number of samples\n        - Approximation method: \\( O(d) \\) but may be less accurate\n\n    ## Implementation Details\n\n    This implementation provides three different methods for computing the Hessian trace:\n\n    1. **exact**: Computes the full Hessian diagonal elements directly. Most accurate but\n       computationally expensive for high-dimensional data.\n\n    2. **hutchinson**: Uses Hutchinson's trace estimator, which approximates the trace using\n       random projections: \\( \\text{tr}(H) \\approx \\mathbb{E}_v[v^T H v] \\) where v is typically\n       sampled from a Rademacher distribution. More efficient for high dimensions.\n\n    3. **approx**: Uses a finite-difference approximation of the Hessian trace which can be\n       more numerically stable in some cases.\n\n    !!! warning \"Numerical Stability\"\n        - The exact method can be unstable with mixed precision training\n        - Large values in the Hessian can cause numerical issues\n        - Gradient clipping is applied automatically to prevent instability\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Create a simple energy function\n        energy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n        # Initialize score matching with Hutchinson estimator\n        sm_loss = ScoreMatching(\n            energy_function=energy_fn,\n            hessian_method=\"hutchinson\",\n            hutchinson_samples=5\n        )\n\n        # Training loop\n        optimizer = torch.optim.Adam(energy_fn.parameters(), lr=1e-3)\n\n        for batch in dataloader:\n            optimizer.zero_grad()\n            loss = sm_loss(batch)\n            loss.backward()\n            optimizer.step()\n        ```\n\n    !!! example \"Advanced Configuration\"\n        ```python\n        # With mixed precision training\n        sm_loss = ScoreMatching(\n            energy_function=energy_fn,\n            hessian_method=\"hutchinson\",\n            use_mixed_precision=True\n        )\n\n        # With custom regularization\n        def l2_regularization(energy_fn, x):\n            return torch.mean(energy_fn(x)**2)\n\n        sm_loss = ScoreMatching(\n            energy_function=energy_fn,\n            regularization_strength=0.1,\n            custom_regularization=l2_regularization\n        )\n        ```\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to train\n        hessian_method (str): Method to compute Hessian trace. One of {\"exact\", \"hutchinson\", \"approx\"}\n        regularization_strength (float): Coefficient for regularization terms\n        hutchinson_samples (int): Number of random vectors for Hutchinson's trace estimator\n        custom_regularization (Optional[Callable]): Optional function for custom regularization\n        use_mixed_precision (bool): Whether to use mixed precision training\n        dtype (torch.dtype): Data type for computations\n        device (Optional[Union[str, torch.device]]): Device for computations\n\n    References:\n        Hyv\u00e4rinen, A. (2005). Estimation of non-normalized statistical models by score matching.\n        Journal of Machine Learning Research, 6, 695-709.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        hessian_method: str = \"exact\",\n        regularization_strength: float = 0.0,\n        custom_regularization: Optional[Callable] = None,\n        use_mixed_precision: bool = False,\n        is_training=True,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            energy_function=energy_function,\n            regularization_strength=regularization_strength,\n            use_autograd=True,\n            custom_regularization=custom_regularization,\n            use_mixed_precision=use_mixed_precision,\n            dtype=dtype,\n            device=device,\n            *args,\n            **kwargs,\n        )\n\n        self.hessian_method = hessian_method\n        self.training = is_training\n        valid_methods = [\"exact\", \"approx\"]\n        if self.hessian_method not in valid_methods:\n            warnings.warn(\n                f\"Invalid hessian_method '{self.hessian_method}'. \"\n                f\"Using 'exact' instead. Valid options are: {valid_methods}\",\n                UserWarning,\n            )\n            self.hessian_method = \"exact\"\n\n        if self.use_mixed_precision and self.hessian_method == \"exact\":\n            warnings.warn(\n                \"Using 'exact' Hessian method with mixed precision may be unstable. \"\n                \"Consider using SlicedScoreMatching for better numerical stability.\",\n                UserWarning,\n            )\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the score matching loss for a batch of data.\n\n        This method first calculates the loss using the specified Hessian computation method,\n        then adds regularization if needed.\n\n        !!! note\n            The input tensor is automatically converted to the device and dtype specified\n            during initialization.\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n            *args: Additional positional arguments passed to compute_loss\n            **kwargs: Additional keyword arguments passed to compute_loss\n\n        Returns:\n            torch.Tensor: The score matching loss (scalar)\n\n        Examples:\n            &gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n            &gt;&gt;&gt; loss_fn = ScoreMatching(energy_fn, hessian_method=\"hutchinson\")\n            &gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n            &gt;&gt;&gt; loss = loss_fn(x)  # Compute the score matching loss\n            &gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n        \"\"\"\n        if (x.device != self.device) or (x.dtype != self.dtype):\n            x = x.to(device=self.device, dtype=self.dtype)\n\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                loss = self.compute_loss(x, *args, **kwargs)\n        else:\n            loss = self.compute_loss(x, *args, **kwargs)\n\n        if self.regularization_strength &gt; 0 or self.custom_regularization is not None:\n            loss = self.add_regularization(loss, x)\n\n        return loss\n\n    def compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the score matching loss using the specified Hessian computation method.\n\n        This method selects between different implementations of the score matching loss\n        based on the `hessian_method` attribute.\n\n        !!! note\n            For high-dimensional data, SlicedScoreMatching is recommended for better\n            computational efficiency and numerical stability.\n\n        !!! warning\n            The \"exact\" method requires computing the full Hessian diagonal, which can be\n            computationally expensive for high-dimensional data.\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n            *args: Additional arguments passed to the specific method implementation\n            **kwargs: Additional keyword arguments passed to the specific method implementation\n\n        Returns:\n            torch.Tensor: The score matching loss (scalar)\n\n        Examples:\n            &gt;&gt;&gt; # Different Hessian methods can be chosen at initialization:\n            &gt;&gt;&gt; # Exact method (computationally expensive but accurate)\n            &gt;&gt;&gt; loss_fn_exact = ScoreMatching(energy_fn, hessian_method=\"exact\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Approximation method (using finite differences)\n            &gt;&gt;&gt; loss_fn_approx = ScoreMatching(energy_fn, hessian_method=\"approx\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # For efficient high-dimensional computation, use SlicedScoreMatching:\n            &gt;&gt;&gt; loss_fn_sliced = SlicedScoreMatching(energy_fn, n_projections=10)\n        \"\"\"\n\n        if self.hessian_method == \"approx\":\n            return self._approx_score_matching(x)\n        else:\n            return self._exact_score_matching(x)\n\n    def _exact_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute score matching loss using exact Hessian trace computation.\n\n        This computes the score matching objective:\n\n        \\[\n        \\mathcal{L}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\| \\nabla_x E_\\theta(x) \\|_2^2 \\right]\n        - \\mathbb{E}_{p_{\\text{data}}} \\left[ \\operatorname{tr}(\\nabla_x^2 E_\\theta(x)) \\right]\n        \\]\n\n        where the trace of the Hessian is computed exactly by calculating each diagonal element.\n\n        !!! warning\n            This method computes the full Hessian diagonal elements and can be very\n            computationally expensive for high-dimensional data. Consider using the\n            Hutchinson estimator via `hessian_method=\"hutchinson\"` for high dimensions.\n\n        !!! note\n            For each dimension \\( i \\), this computes \\( \\frac{\\partial^2 E}{\\partial x_i^2} \\),\n            requiring a separate backward pass, making it \\( O(d^2) \\) in computational complexity\n            where \\( d \\) is the data dimension.\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n\n        Returns:\n            torch.Tensor: The score matching loss (scalar)\n        \"\"\"\n        batch_size = x.shape[0]\n        feature_dim = x.numel() // batch_size\n\n        x_leaf = x.detach().clone()\n        x_leaf.requires_grad_(True)\n\n        energy = self.energy_function(x_leaf)\n        logp_sum = (-energy).sum()\n        grad1 = torch.autograd.grad(\n            logp_sum, x_leaf, create_graph=True, retain_graph=True\n        )[0]\n\n        grad1_flat = grad1.view(batch_size, -1)\n        term1 = 0.5 * grad1_flat.pow(2).sum(dim=1)\n\n        laplacian = torch.zeros(batch_size, device=x.device, dtype=x.dtype)\n        for i in range(feature_dim):\n            comp_sum = grad1_flat[:, i].sum()\n            grad2_full = torch.autograd.grad(\n                comp_sum,\n                x_leaf,\n                create_graph=True,\n                retain_graph=True,\n                allow_unused=True,\n            )[0]\n            if grad2_full is None:\n                grad2_comp = torch.zeros(batch_size, device=x.device, dtype=x.dtype)\n            else:\n                grad2_comp = grad2_full.view(batch_size, -1)[:, i]\n            laplacian += grad2_comp\n\n        loss_per_sample = term1 + laplacian\n        return loss_per_sample.mean()\n\n    def _approx_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute score matching loss using a more efficient finite-difference approximation.\n\n        This method combines the exact computation of the score term with a more\n        efficient approximation of the Hessian trace using finite differences:\n\n        \\[\n        \\text{tr}(\\nabla_x^2 E_\\theta(x)) \\approx \\frac{1}{\\epsilon^2 d} \\mathbb{E}_{\\delta \\sim \\mathcal{N}(0, \\epsilon^2 I)}\n        \\left[ (\\nabla_x E_\\theta(x + \\delta) - \\nabla_x E_\\theta(x))^T \\delta \\right]\n        \\]\n\n        where \\( \\epsilon \\) is a small constant and \\( d \\) is the data dimension.\n\n        !!! note\n            This approximation requires only two score computations regardless of the\n            data dimensionality, making it more efficient than the exact method for\n            high-dimensional data.\n\n        !!! warning\n            The approximation quality depends on the choice of \\( \\epsilon \\). Too small\n            values may lead to numerical instability, while too large values may give\n            inaccurate estimates.\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n\n        Returns:\n            torch.Tensor: The score matching loss (scalar)\n        \"\"\"\n\n        batch_size = x.shape[0]\n        data_dim = x.numel() // batch_size\n\n        x_detached = x.detach().clone()\n        x_detached.requires_grad_(True)\n\n        score = self.compute_score(x_detached)\n        score_square_term = (\n            0.5 * torch.sum(score**2, dim=list(range(1, len(x.shape)))).mean()\n        )\n\n        epsilon = 1e-5\n        x_noise = x_detached + epsilon * torch.randn_like(x_detached)\n\n        score_x = self.compute_score(x_detached)\n        score_x_noise = self.compute_score(x_noise)\n\n        hessian_trace = torch.sum(\n            (score_x_noise - score_x) * (x_noise - x_detached),\n            dim=list(range(1, len(x.shape))),\n        ).mean() / (epsilon**2 * data_dim)\n\n        loss = score_square_term - hessian_trace\n\n        return loss\n\n    def _hutchinson_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        DEPRECATED: Use SlicedScoreMatching for efficient trace estimation.\n\n        This method has been deprecated in favor of SlicedScoreMatching which provides\n        a more efficient and theoretically sound implementation of Hutchinson's estimator.\n        \"\"\"\n        warnings.warn(\n            \"ScoreMatching._hutchinson_score_matching is deprecated. \"\n            \"Use SlicedScoreMatching for efficient trace estimation instead.\",\n            DeprecationWarning,\n        )\n        return self._exact_score_matching(x)\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching.hessian_method","title":"hessian_method  <code>instance-attribute</code>","text":"<pre><code>hessian_method = hessian_method\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching.training","title":"training  <code>instance-attribute</code>","text":"<pre><code>training = is_training\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching.forward","title":"forward","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the score matching loss for a batch of data.</p> <p>This method first calculates the loss using the specified Hessian computation method, then adds regularization if needed.</p> <p>Note</p> <p>The input tensor is automatically converted to the device and dtype specified during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <code>*args</code> <p>Additional positional arguments passed to compute_loss</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments passed to compute_loss</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The score matching loss (scalar)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n&gt;&gt;&gt; loss_fn = ScoreMatching(energy_fn, hessian_method=\"hutchinson\")\n&gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n&gt;&gt;&gt; loss = loss_fn(x)  # Compute the score matching loss\n&gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n</code></pre> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the score matching loss for a batch of data.\n\n    This method first calculates the loss using the specified Hessian computation method,\n    then adds regularization if needed.\n\n    !!! note\n        The input tensor is automatically converted to the device and dtype specified\n        during initialization.\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n        *args: Additional positional arguments passed to compute_loss\n        **kwargs: Additional keyword arguments passed to compute_loss\n\n    Returns:\n        torch.Tensor: The score matching loss (scalar)\n\n    Examples:\n        &gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n        &gt;&gt;&gt; loss_fn = ScoreMatching(energy_fn, hessian_method=\"hutchinson\")\n        &gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n        &gt;&gt;&gt; loss = loss_fn(x)  # Compute the score matching loss\n        &gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n    \"\"\"\n    if (x.device != self.device) or (x.dtype != self.dtype):\n        x = x.to(device=self.device, dtype=self.dtype)\n\n    if self.use_mixed_precision and self.autocast_available:\n        from torch.cuda.amp import autocast\n\n        with autocast():\n            loss = self.compute_loss(x, *args, **kwargs)\n    else:\n        loss = self.compute_loss(x, *args, **kwargs)\n\n    if self.regularization_strength &gt; 0 or self.custom_regularization is not None:\n        loss = self.add_regularization(loss, x)\n\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the score matching loss using the specified Hessian computation method.</p> <p>This method selects between different implementations of the score matching loss based on the <code>hessian_method</code> attribute.</p> <p>Note</p> <p>For high-dimensional data, SlicedScoreMatching is recommended for better computational efficiency and numerical stability.</p> <p>Warning</p> <p>The \"exact\" method requires computing the full Hessian diagonal, which can be computationally expensive for high-dimensional data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <code>*args</code> <p>Additional arguments passed to the specific method implementation</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the specific method implementation</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The score matching loss (scalar)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Different Hessian methods can be chosen at initialization:\n&gt;&gt;&gt; # Exact method (computationally expensive but accurate)\n&gt;&gt;&gt; loss_fn_exact = ScoreMatching(energy_fn, hessian_method=\"exact\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Approximation method (using finite differences)\n&gt;&gt;&gt; loss_fn_approx = ScoreMatching(energy_fn, hessian_method=\"approx\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # For efficient high-dimensional computation, use SlicedScoreMatching:\n&gt;&gt;&gt; loss_fn_sliced = SlicedScoreMatching(energy_fn, n_projections=10)\n</code></pre> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the score matching loss using the specified Hessian computation method.\n\n    This method selects between different implementations of the score matching loss\n    based on the `hessian_method` attribute.\n\n    !!! note\n        For high-dimensional data, SlicedScoreMatching is recommended for better\n        computational efficiency and numerical stability.\n\n    !!! warning\n        The \"exact\" method requires computing the full Hessian diagonal, which can be\n        computationally expensive for high-dimensional data.\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n        *args: Additional arguments passed to the specific method implementation\n        **kwargs: Additional keyword arguments passed to the specific method implementation\n\n    Returns:\n        torch.Tensor: The score matching loss (scalar)\n\n    Examples:\n        &gt;&gt;&gt; # Different Hessian methods can be chosen at initialization:\n        &gt;&gt;&gt; # Exact method (computationally expensive but accurate)\n        &gt;&gt;&gt; loss_fn_exact = ScoreMatching(energy_fn, hessian_method=\"exact\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Approximation method (using finite differences)\n        &gt;&gt;&gt; loss_fn_approx = ScoreMatching(energy_fn, hessian_method=\"approx\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # For efficient high-dimensional computation, use SlicedScoreMatching:\n        &gt;&gt;&gt; loss_fn_sliced = SlicedScoreMatching(energy_fn, n_projections=10)\n    \"\"\"\n\n    if self.hessian_method == \"approx\":\n        return self._approx_score_matching(x)\n    else:\n        return self._exact_score_matching(x)\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching._exact_score_matching","title":"_exact_score_matching","text":"<pre><code>_exact_score_matching(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute score matching loss using exact Hessian trace computation.</p> <p>This computes the score matching objective:</p> \\[ \\mathcal{L}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\| \\nabla_x E_\\theta(x) \\|_2^2 \\right] - \\mathbb{E}_{p_{\\text{data}}} \\left[ \\operatorname{tr}(\\nabla_x^2 E_\\theta(x)) \\right] \\] <p>where the trace of the Hessian is computed exactly by calculating each diagonal element.</p> <p>Warning</p> <p>This method computes the full Hessian diagonal elements and can be very computationally expensive for high-dimensional data. Consider using the Hutchinson estimator via <code>hessian_method=\"hutchinson\"</code> for high dimensions.</p> <p>Note</p> <p>For each dimension \\( i \\), this computes \\( \\frac{\\partial^2 E}{\\partial x_i^2} \\), requiring a separate backward pass, making it \\( O(d^2) \\) in computational complexity where \\( d \\) is the data dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The score matching loss (scalar)</p> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def _exact_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute score matching loss using exact Hessian trace computation.\n\n    This computes the score matching objective:\n\n    \\[\n    \\mathcal{L}(\\theta) = \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\| \\nabla_x E_\\theta(x) \\|_2^2 \\right]\n    - \\mathbb{E}_{p_{\\text{data}}} \\left[ \\operatorname{tr}(\\nabla_x^2 E_\\theta(x)) \\right]\n    \\]\n\n    where the trace of the Hessian is computed exactly by calculating each diagonal element.\n\n    !!! warning\n        This method computes the full Hessian diagonal elements and can be very\n        computationally expensive for high-dimensional data. Consider using the\n        Hutchinson estimator via `hessian_method=\"hutchinson\"` for high dimensions.\n\n    !!! note\n        For each dimension \\( i \\), this computes \\( \\frac{\\partial^2 E}{\\partial x_i^2} \\),\n        requiring a separate backward pass, making it \\( O(d^2) \\) in computational complexity\n        where \\( d \\) is the data dimension.\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n\n    Returns:\n        torch.Tensor: The score matching loss (scalar)\n    \"\"\"\n    batch_size = x.shape[0]\n    feature_dim = x.numel() // batch_size\n\n    x_leaf = x.detach().clone()\n    x_leaf.requires_grad_(True)\n\n    energy = self.energy_function(x_leaf)\n    logp_sum = (-energy).sum()\n    grad1 = torch.autograd.grad(\n        logp_sum, x_leaf, create_graph=True, retain_graph=True\n    )[0]\n\n    grad1_flat = grad1.view(batch_size, -1)\n    term1 = 0.5 * grad1_flat.pow(2).sum(dim=1)\n\n    laplacian = torch.zeros(batch_size, device=x.device, dtype=x.dtype)\n    for i in range(feature_dim):\n        comp_sum = grad1_flat[:, i].sum()\n        grad2_full = torch.autograd.grad(\n            comp_sum,\n            x_leaf,\n            create_graph=True,\n            retain_graph=True,\n            allow_unused=True,\n        )[0]\n        if grad2_full is None:\n            grad2_comp = torch.zeros(batch_size, device=x.device, dtype=x.dtype)\n        else:\n            grad2_comp = grad2_full.view(batch_size, -1)[:, i]\n        laplacian += grad2_comp\n\n    loss_per_sample = term1 + laplacian\n    return loss_per_sample.mean()\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching._approx_score_matching","title":"_approx_score_matching","text":"<pre><code>_approx_score_matching(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute score matching loss using a more efficient finite-difference approximation.</p> <p>This method combines the exact computation of the score term with a more efficient approximation of the Hessian trace using finite differences:</p> \\[ \\text{tr}(\\nabla_x^2 E_\\theta(x)) \\approx \\frac{1}{\\epsilon^2 d} \\mathbb{E}_{\\delta \\sim \\mathcal{N}(0, \\epsilon^2 I)} \\left[ (\\nabla_x E_\\theta(x + \\delta) - \\nabla_x E_\\theta(x))^T \\delta \\right] \\] <p>where \\( \\epsilon \\) is a small constant and \\( d \\) is the data dimension.</p> <p>Note</p> <p>This approximation requires only two score computations regardless of the data dimensionality, making it more efficient than the exact method for high-dimensional data.</p> <p>Warning</p> <p>The approximation quality depends on the choice of \\( \\epsilon \\). Too small values may lead to numerical instability, while too large values may give inaccurate estimates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The score matching loss (scalar)</p> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def _approx_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute score matching loss using a more efficient finite-difference approximation.\n\n    This method combines the exact computation of the score term with a more\n    efficient approximation of the Hessian trace using finite differences:\n\n    \\[\n    \\text{tr}(\\nabla_x^2 E_\\theta(x)) \\approx \\frac{1}{\\epsilon^2 d} \\mathbb{E}_{\\delta \\sim \\mathcal{N}(0, \\epsilon^2 I)}\n    \\left[ (\\nabla_x E_\\theta(x + \\delta) - \\nabla_x E_\\theta(x))^T \\delta \\right]\n    \\]\n\n    where \\( \\epsilon \\) is a small constant and \\( d \\) is the data dimension.\n\n    !!! note\n        This approximation requires only two score computations regardless of the\n        data dimensionality, making it more efficient than the exact method for\n        high-dimensional data.\n\n    !!! warning\n        The approximation quality depends on the choice of \\( \\epsilon \\). Too small\n        values may lead to numerical instability, while too large values may give\n        inaccurate estimates.\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n\n    Returns:\n        torch.Tensor: The score matching loss (scalar)\n    \"\"\"\n\n    batch_size = x.shape[0]\n    data_dim = x.numel() // batch_size\n\n    x_detached = x.detach().clone()\n    x_detached.requires_grad_(True)\n\n    score = self.compute_score(x_detached)\n    score_square_term = (\n        0.5 * torch.sum(score**2, dim=list(range(1, len(x.shape)))).mean()\n    )\n\n    epsilon = 1e-5\n    x_noise = x_detached + epsilon * torch.randn_like(x_detached)\n\n    score_x = self.compute_score(x_detached)\n    score_x_noise = self.compute_score(x_noise)\n\n    hessian_trace = torch.sum(\n        (score_x_noise - score_x) * (x_noise - x_detached),\n        dim=list(range(1, len(x.shape))),\n    ).mean() / (epsilon**2 * data_dim)\n\n    loss = score_square_term - hessian_trace\n\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/ScoreMatching/#torchebm.losses.score_matching.ScoreMatching._hutchinson_score_matching","title":"_hutchinson_score_matching","text":"<pre><code>_hutchinson_score_matching(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>DEPRECATED: Use SlicedScoreMatching for efficient trace estimation.</p> <p>This method has been deprecated in favor of SlicedScoreMatching which provides a more efficient and theoretically sound implementation of Hutchinson's estimator.</p> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def _hutchinson_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    DEPRECATED: Use SlicedScoreMatching for efficient trace estimation.\n\n    This method has been deprecated in favor of SlicedScoreMatching which provides\n    a more efficient and theoretically sound implementation of Hutchinson's estimator.\n    \"\"\"\n    warnings.warn(\n        \"ScoreMatching._hutchinson_score_matching is deprecated. \"\n        \"Use SlicedScoreMatching for efficient trace estimation instead.\",\n        DeprecationWarning,\n    )\n    return self._exact_score_matching(x)\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/","title":"SlicedScoreMatching","text":""},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseScoreMatching</code></p> <p>Implementation of Sliced Score Matching (SSM) by Song et al. (2019).</p> <p>SSM is a computationally efficient variant of score matching that uses random projections to estimate the score matching objective. It avoids computing the full Hessian matrix and instead uses random projections to estimate the trace of the Hessian.</p> <p>Key Advantages</p> <ul> <li>Significantly more efficient than exact score matching</li> <li>Scales well to high dimensions</li> <li>No need for MCMC sampling</li> <li>Works with any energy function architecture</li> </ul>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#torchebm.losses.score_matching.SlicedScoreMatching--mathematical-formulation","title":"Mathematical Formulation","text":"<p>For data \\( x \\) and random projection vectors \\( v \\), the SSM objective is:</p> \\[ J_{\\text{SSM}}(\\theta) = \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{v \\sim \\mathcal{N}(0, I)} \\left[ v^T \\nabla_x \\log p_\\theta(x) v + \\frac{1}{2} \\left( v^T \\nabla_x \\log p_\\theta(x) \\right)^2 \\right] + \\text{const.} \\] <p>The key insight is that this provides a tractable alternative to score matching by obtaining the following tractable alternative:</p> \\[ \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x) \\] <p>This allows us to compute the score matching objective using only first-order gradients and avoids computing the full Hessian matrix, which is much more efficient.</p> <p>Projection Selection</p> <p>The choice of projection type and number of projections affects the accuracy: - Gaussian projections: Most common, works well in practice - Rademacher projections: Binary values, can be more efficient - sphere projections: - More projections: Better accuracy but higher computational cost - Fewer projections: Faster but may be less accurate</p>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#torchebm.losses.score_matching.SlicedScoreMatching--practical-considerations","title":"Practical Considerations","text":"<ul> <li>The number of projections \\( n_{\\text{projections}} \\) is a key hyperparameter</li> <li>More projections lead to better accuracy but higher computational cost</li> <li>The projection type (Gaussian, Rademacher, or sphere) can affect performance</li> <li>SSM is particularly useful for high-dimensional data where exact score matching is infeasible</li> </ul> <p>Common Issues</p> <ul> <li>Too few projections can lead to high variance in the gradient estimates</li> <li>Projection type should be chosen based on the data characteristics</li> <li>May require more iterations than exact score matching for convergence</li> </ul> <p>Basic Usage</p> <pre><code># Create energy function\nenergy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n# Initialize SSM with default parameters\nssm_loss = SlicedScoreMatching(\n    energy_function=energy_fn,\n    n_projections=10,\n    projection_type=\"gaussian\"\n)\n\n# Training loop\noptimizer = torch.optim.Adam(energy_fn.parameters(), lr=1e-3)\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = ssm_loss(batch)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Advanced Configuration</p> <pre><code># With Rademacher projections\nssm_loss = SlicedScoreMatching(\n    energy_function=energy_fn,\n    n_projections=20,\n    projection_type=\"rademacher\"\n)\n\n# With mixed precision training\nssm_loss = SlicedScoreMatching(\n    energy_function=energy_fn,\n    n_projections=10,\n    projection_type=\"gaussian\",\n    use_mixed_precision=True\n)\n\n# With custom regularization\ndef custom_reg(energy_fn, x):\n    return torch.mean(energy_fn(x)**2)\n\nssm_loss = SlicedScoreMatching(\n    energy_function=energy_fn,\n    n_projections=10,\n    projection_type=\"gaussian\",\n    custom_regularization=custom_reg\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to train</p> required <code>n_projections</code> <code>int</code> <p>Number of random projections to use</p> <code>5</code> <code>projection_type</code> <code>str</code> <p>Type of random projections (\"gaussian\", \"rademacher\", or \"sphere\")</p> <code>'rademacher'</code> <code>regularization_strength</code> <code>float</code> <p>Coefficient for regularization terms</p> <code>0.0</code> <code>custom_regularization</code> <code>Optional[Callable]</code> <p>Optional function for custom regularization</p> <code>None</code> <code>use_mixed_precision</code> <code>bool</code> <p>Whether to use mixed precision training</p> <code>False</code> <code>dtype</code> <code>dtype</code> <p>Data type for computations</p> <code>float32</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device for computations</p> <code>None</code> References <p>Song, Y., Garg, S., Shi, J., &amp; Ermon, S. (2019). Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence (pp. 574-584).</p> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>class SlicedScoreMatching(BaseScoreMatching):\n    r\"\"\"\n    Implementation of Sliced Score Matching (SSM) by Song et al. (2019).\n\n    SSM is a computationally efficient variant of score matching that uses random projections\n    to estimate the score matching objective. It avoids computing the full Hessian matrix and\n    instead uses random projections to estimate the trace of the Hessian.\n\n    !!! success \"Key Advantages\"\n        - Significantly more efficient than exact score matching\n        - Scales well to high dimensions\n        - No need for MCMC sampling\n        - Works with any energy function architecture\n\n    ## Mathematical Formulation\n\n    For data \\( x \\) and random projection vectors \\( v \\), the SSM objective is:\n\n    \\[\n    J_{\\text{SSM}}(\\theta) = \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{v \\sim \\mathcal{N}(0, I)}\n    \\left[ v^T \\nabla_x \\log p_\\theta(x) v + \\frac{1}{2} \\left( v^T \\nabla_x \\log p_\\theta(x) \\right)^2 \\right] + \\text{const.}\n    \\]\n\n    The key insight is that this provides a tractable alternative to score matching by obtaining\n    the following tractable alternative:\n\n    \\[\n    \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x)\n    \\]\n\n    This allows us to compute the score matching objective using only first-order gradients\n    and avoids computing the full Hessian matrix, which is much more efficient.\n\n    !!! tip \"Projection Selection\"\n        The choice of projection type and number of projections affects the accuracy:\n        - Gaussian projections: Most common, works well in practice\n        - Rademacher projections: Binary values, can be more efficient\n        - sphere projections:\n        - More projections: Better accuracy but higher computational cost\n        - Fewer projections: Faster but may be less accurate\n\n    ## Practical Considerations\n\n    - The number of projections \\( n_{\\text{projections}} \\) is a key hyperparameter\n    - More projections lead to better accuracy but higher computational cost\n    - The projection type (Gaussian, Rademacher, or sphere) can affect performance\n    - SSM is particularly useful for high-dimensional data where exact score matching is infeasible\n\n    !!! warning \"Common Issues\"\n        - Too few projections can lead to high variance in the gradient estimates\n        - Projection type should be chosen based on the data characteristics\n        - May require more iterations than exact score matching for convergence\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Create energy function\n        energy_fn = MLPEnergyFunction(input_dim=2, hidden_dim=64)\n\n        # Initialize SSM with default parameters\n        ssm_loss = SlicedScoreMatching(\n            energy_function=energy_fn,\n            n_projections=10,\n            projection_type=\"gaussian\"\n        )\n\n        # Training loop\n        optimizer = torch.optim.Adam(energy_fn.parameters(), lr=1e-3)\n\n        for batch in dataloader:\n            optimizer.zero_grad()\n            loss = ssm_loss(batch)\n            loss.backward()\n            optimizer.step()\n        ```\n\n    !!! example \"Advanced Configuration\"\n        ```python\n        # With Rademacher projections\n        ssm_loss = SlicedScoreMatching(\n            energy_function=energy_fn,\n            n_projections=20,\n            projection_type=\"rademacher\"\n        )\n\n        # With mixed precision training\n        ssm_loss = SlicedScoreMatching(\n            energy_function=energy_fn,\n            n_projections=10,\n            projection_type=\"gaussian\",\n            use_mixed_precision=True\n        )\n\n        # With custom regularization\n        def custom_reg(energy_fn, x):\n            return torch.mean(energy_fn(x)**2)\n\n        ssm_loss = SlicedScoreMatching(\n            energy_function=energy_fn,\n            n_projections=10,\n            projection_type=\"gaussian\",\n            custom_regularization=custom_reg\n        )\n        ```\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to train\n        n_projections (int): Number of random projections to use\n        projection_type (str): Type of random projections (\"gaussian\", \"rademacher\", or \"sphere\")\n        regularization_strength (float): Coefficient for regularization terms\n        custom_regularization (Optional[Callable]): Optional function for custom regularization\n        use_mixed_precision (bool): Whether to use mixed precision training\n        dtype (torch.dtype): Data type for computations\n        device (Optional[Union[str, torch.device]]): Device for computations\n\n    References:\n        Song, Y., Garg, S., Shi, J., &amp; Ermon, S. (2019). Sliced score matching: A scalable approach\n        to density and score estimation. In Uncertainty in Artificial Intelligence (pp. 574-584).\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        n_projections: int = 5,\n        projection_type: str = \"rademacher\",\n        regularization_strength: float = 0.0,\n        custom_regularization: Optional[Callable] = None,\n        use_mixed_precision: bool = False,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            energy_function=energy_function,\n            regularization_strength=regularization_strength,\n            use_autograd=True,\n            custom_regularization=custom_regularization,\n            use_mixed_precision=use_mixed_precision,\n            dtype=dtype,\n            device=device,\n            *args,\n            **kwargs,\n        )\n\n        self.n_projections = n_projections\n        self.projection_type = projection_type\n\n        # Validate projection_type\n        valid_types = [\"rademacher\", \"sphere\", \"gaussian\"]\n        if self.projection_type not in valid_types:\n            warnings.warn(\n                f\"Invalid projection_type '{self.projection_type}'. \"\n                f\"Using 'rademacher' instead. Valid options are: {valid_types}\",\n                UserWarning,\n            )\n            self.projection_type = \"rademacher\"\n\n    def _get_random_projections(self, shape: torch.Size) -&gt; torch.Tensor:\n        r\"\"\"\n        Generate random vectors for projection-based score matching.\n\n        This function samples vectors from either a Rademacher or Gaussian distribution\n        based on the `projection_type` parameter.\n\n        !!! note\n            Rademacher distributions (values in \\(\\{-1, +1\\}\\)) often have lower variance\n            in the trace estimator compared to Gaussian distributions.\n\n        Args:\n            shape (torch.Size): Shape of vectors to generate\n\n        Returns:\n            torch.Tensor: Random projection vectors of the specified shape\n\n        Examples:\n            &gt;&gt;&gt; loss_fn = SlicedScoreMatching(energy_fn)\n            &gt;&gt;&gt; # Internally used to generate projection vectors:\n            &gt;&gt;&gt; v = loss_fn._get_random_projections((32, 2))  # 32 samples of dim 2\n            &gt;&gt;&gt; # v will be of shape (32, 2) with values in {-1, +1} (Rademacher)\n        \"\"\"\n        vectors = torch.randn_like(shape)\n        if self.projection_type == \"rademacher\":\n            return vectors.sign()\n        elif self.projection_type == \"sphere\":\n            return (\n                vectors\n                / torch.norm(vectors, dim=-1, keepdim=True)\n                * torch.sqrt(vectors.shape[-1])\n            )\n        else:\n            return vectors\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the sliced score matching loss for a batch of data.\n\n        This method first calculates the sliced score matching loss using random\n        projections, then adds regularization if needed.\n\n        !!! note\n            The input tensor is automatically converted to the device and dtype specified\n            during initialization.\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n            *args: Additional positional arguments passed to compute_loss\n            **kwargs: Additional keyword arguments passed to compute_loss\n\n        Returns:\n            torch.Tensor: The sliced score matching loss (scalar)\n\n        Examples:\n            &gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n            &gt;&gt;&gt; loss_fn = SlicedScoreMatching(\n            ...     energy_fn,\n            ...     n_projections=10,  # Number of random projections to use\n            ...     projection_type=\"rademacher\"  # Type of random vectors\n            ... )\n            &gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n            &gt;&gt;&gt; loss = loss_fn(x)  # Compute the SSM loss\n            &gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n        \"\"\"\n        if (x.device != self.device) or (x.dtype != self.dtype):\n            x = x.to(device=self.device, dtype=self.dtype)\n\n        if self.use_mixed_precision and self.autocast_available:\n            from torch.cuda.amp import autocast\n\n            with autocast():\n                loss = self.compute_loss(x, *args, **kwargs)\n        else:\n            loss = self.compute_loss(x, *args, **kwargs)\n\n        if self.regularization_strength &gt; 0 or self.custom_regularization is not None:\n            loss = self.add_regularization(loss, x)\n\n        return loss\n\n    def compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the sliced score matching loss using random projections efficiently.\n\n        This implementation computes the sliced score matching objective efficiently using\n        the tractable alternative formulation. The key insight is that we can compute the\n        objective using only first-order gradients by leveraging the relationship:\n\n        \\[\n        \\nabla_x \\log p_\\theta(x) = - \\nabla_x E_\\theta(x)\n        \\]\n\n        The objective is:\n\n        \\[\n        \\mathcal{L}_{\\text{SSM}}(\\theta) = \\mathbb{E}_{p_{\\text{data}}(x)}\n        \\left[ v^T \\nabla_x \\log p_\\theta(x) v + \\frac{1}{2} \\left( v^T \\nabla_x \\log p_\\theta(x) \\right)^2 \\right] + \\text{const.}\n        \\]\n\n        !!! tip\n            This method is computationally efficient for high-dimensional data with O(d)\n            complexity rather than O(d\u00b2) for exact score matching, where d is the data dimension.\n\n        Args:\n            x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n            *args: Additional arguments (not used)\n            **kwargs: Additional keyword arguments (not used)\n\n        Returns:\n            torch.Tensor: The sliced score matching loss (scalar)\n        \"\"\"\n\n        dup_x = (\n            x.unsqueeze(0)\n            .expand(self.n_projections, *x.shape)\n            .contiguous()\n            .view(-1, *x.shape[1:])\n        ).requires_grad_(\n            True\n        )  # final shape: (n_particles * batch_size, d). tracing the shape: (batch_size, d) -&gt; (1, batch_size, d)\n        # -&gt; (n_particles, batch_size, d) -&gt; (n_particles, batch_size, d) -&gt; (n_particles * batch_size, d)\n\n        n_vectors = self._get_random_projections(dup_x)\n\n        logp = (-self.energy_function(dup_x)).sum()\n        grad1 = torch.autograd.grad(logp, dup_x, create_graph=True)[0]\n        v_score = torch.sum(grad1 * n_vectors, dim=-1)\n        term1 = 0.5 * (v_score**2)\n\n        grad_v = torch.autograd.grad(v_score.sum(), dup_x, create_graph=True)[0]\n        term2 = torch.sum(n_vectors * grad_v, dim=-1)\n\n        term1 = term1.view(self.n_projections, -1).mean(dim=0)\n        term2 = term2.view(self.n_projections, -1).mean(dim=0)\n\n        loss = term2 + term1\n\n        return loss.mean()\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#torchebm.losses.score_matching.SlicedScoreMatching.n_projections","title":"n_projections  <code>instance-attribute</code>","text":"<pre><code>n_projections = n_projections\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#torchebm.losses.score_matching.SlicedScoreMatching.projection_type","title":"projection_type  <code>instance-attribute</code>","text":"<pre><code>projection_type = projection_type\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#torchebm.losses.score_matching.SlicedScoreMatching._get_random_projections","title":"_get_random_projections","text":"<pre><code>_get_random_projections(shape: Size) -&gt; torch.Tensor\n</code></pre> <p>Generate random vectors for projection-based score matching.</p> <p>This function samples vectors from either a Rademacher or Gaussian distribution based on the <code>projection_type</code> parameter.</p> <p>Note</p> <p>Rademacher distributions (values in \\(\\{-1, +1\\}\\)) often have lower variance in the trace estimator compared to Gaussian distributions.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Size</code> <p>Shape of vectors to generate</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Random projection vectors of the specified shape</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss_fn = SlicedScoreMatching(energy_fn)\n&gt;&gt;&gt; # Internally used to generate projection vectors:\n&gt;&gt;&gt; v = loss_fn._get_random_projections((32, 2))  # 32 samples of dim 2\n&gt;&gt;&gt; # v will be of shape (32, 2) with values in {-1, +1} (Rademacher)\n</code></pre> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def _get_random_projections(self, shape: torch.Size) -&gt; torch.Tensor:\n    r\"\"\"\n    Generate random vectors for projection-based score matching.\n\n    This function samples vectors from either a Rademacher or Gaussian distribution\n    based on the `projection_type` parameter.\n\n    !!! note\n        Rademacher distributions (values in \\(\\{-1, +1\\}\\)) often have lower variance\n        in the trace estimator compared to Gaussian distributions.\n\n    Args:\n        shape (torch.Size): Shape of vectors to generate\n\n    Returns:\n        torch.Tensor: Random projection vectors of the specified shape\n\n    Examples:\n        &gt;&gt;&gt; loss_fn = SlicedScoreMatching(energy_fn)\n        &gt;&gt;&gt; # Internally used to generate projection vectors:\n        &gt;&gt;&gt; v = loss_fn._get_random_projections((32, 2))  # 32 samples of dim 2\n        &gt;&gt;&gt; # v will be of shape (32, 2) with values in {-1, +1} (Rademacher)\n    \"\"\"\n    vectors = torch.randn_like(shape)\n    if self.projection_type == \"rademacher\":\n        return vectors.sign()\n    elif self.projection_type == \"sphere\":\n        return (\n            vectors\n            / torch.norm(vectors, dim=-1, keepdim=True)\n            * torch.sqrt(vectors.shape[-1])\n        )\n    else:\n        return vectors\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#torchebm.losses.score_matching.SlicedScoreMatching.forward","title":"forward","text":"<pre><code>forward(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the sliced score matching loss for a batch of data.</p> <p>This method first calculates the sliced score matching loss using random projections, then adds regularization if needed.</p> <p>Note</p> <p>The input tensor is automatically converted to the device and dtype specified during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <code>*args</code> <p>Additional positional arguments passed to compute_loss</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments passed to compute_loss</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The sliced score matching loss (scalar)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n&gt;&gt;&gt; loss_fn = SlicedScoreMatching(\n...     energy_fn,\n...     n_projections=10,  # Number of random projections to use\n...     projection_type=\"rademacher\"  # Type of random vectors\n... )\n&gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n&gt;&gt;&gt; loss = loss_fn(x)  # Compute the SSM loss\n&gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n</code></pre> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def forward(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the sliced score matching loss for a batch of data.\n\n    This method first calculates the sliced score matching loss using random\n    projections, then adds regularization if needed.\n\n    !!! note\n        The input tensor is automatically converted to the device and dtype specified\n        during initialization.\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n        *args: Additional positional arguments passed to compute_loss\n        **kwargs: Additional keyword arguments passed to compute_loss\n\n    Returns:\n        torch.Tensor: The sliced score matching loss (scalar)\n\n    Examples:\n        &gt;&gt;&gt; energy_fn = MLPEnergyFunction(dim=2, hidden_dim=32)\n        &gt;&gt;&gt; loss_fn = SlicedScoreMatching(\n        ...     energy_fn,\n        ...     n_projections=10,  # Number of random projections to use\n        ...     projection_type=\"rademacher\"  # Type of random vectors\n        ... )\n        &gt;&gt;&gt; x = torch.randn(128, 2)  # 128 samples of 2D data\n        &gt;&gt;&gt; loss = loss_fn(x)  # Compute the SSM loss\n        &gt;&gt;&gt; loss.backward()  # Backpropagate the loss\n    \"\"\"\n    if (x.device != self.device) or (x.dtype != self.dtype):\n        x = x.to(device=self.device, dtype=self.dtype)\n\n    if self.use_mixed_precision and self.autocast_available:\n        from torch.cuda.amp import autocast\n\n        with autocast():\n            loss = self.compute_loss(x, *args, **kwargs)\n    else:\n        loss = self.compute_loss(x, *args, **kwargs)\n\n    if self.regularization_strength &gt; 0 or self.custom_regularization is not None:\n        loss = self.add_regularization(loss, x)\n\n    return loss\n</code></pre>"},{"location":"api/torchebm/losses/score_matching/classes/SlicedScoreMatching/#torchebm.losses.score_matching.SlicedScoreMatching.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(x: Tensor, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the sliced score matching loss using random projections efficiently.</p> <p>This implementation computes the sliced score matching objective efficiently using the tractable alternative formulation. The key insight is that we can compute the objective using only first-order gradients by leveraging the relationship:</p> \\[ \\nabla_x \\log p_\\theta(x) = - \\nabla_x E_\\theta(x) \\] <p>The objective is:</p> \\[ \\mathcal{L}_{\\text{SSM}}(\\theta) = \\mathbb{E}_{p_{\\text{data}}(x)} \\left[ v^T \\nabla_x \\log p_\\theta(x) v + \\frac{1}{2} \\left( v^T \\nabla_x \\log p_\\theta(x) \\right)^2 \\right] + \\text{const.} \\] <p>Tip</p> <p>This method is computationally efficient for high-dimensional data with O(d) complexity rather than O(d\u00b2) for exact score matching, where d is the data dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor of shape (batch_size, *data_dims)</p> required <code>*args</code> <p>Additional arguments (not used)</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments (not used)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The sliced score matching loss (scalar)</p> Source code in <code>torchebm/losses/score_matching.py</code> <pre><code>def compute_loss(self, x: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the sliced score matching loss using random projections efficiently.\n\n    This implementation computes the sliced score matching objective efficiently using\n    the tractable alternative formulation. The key insight is that we can compute the\n    objective using only first-order gradients by leveraging the relationship:\n\n    \\[\n    \\nabla_x \\log p_\\theta(x) = - \\nabla_x E_\\theta(x)\n    \\]\n\n    The objective is:\n\n    \\[\n    \\mathcal{L}_{\\text{SSM}}(\\theta) = \\mathbb{E}_{p_{\\text{data}}(x)}\n    \\left[ v^T \\nabla_x \\log p_\\theta(x) v + \\frac{1}{2} \\left( v^T \\nabla_x \\log p_\\theta(x) \\right)^2 \\right] + \\text{const.}\n    \\]\n\n    !!! tip\n        This method is computationally efficient for high-dimensional data with O(d)\n        complexity rather than O(d\u00b2) for exact score matching, where d is the data dimension.\n\n    Args:\n        x (torch.Tensor): Input data tensor of shape (batch_size, *data_dims)\n        *args: Additional arguments (not used)\n        **kwargs: Additional keyword arguments (not used)\n\n    Returns:\n        torch.Tensor: The sliced score matching loss (scalar)\n    \"\"\"\n\n    dup_x = (\n        x.unsqueeze(0)\n        .expand(self.n_projections, *x.shape)\n        .contiguous()\n        .view(-1, *x.shape[1:])\n    ).requires_grad_(\n        True\n    )  # final shape: (n_particles * batch_size, d). tracing the shape: (batch_size, d) -&gt; (1, batch_size, d)\n    # -&gt; (n_particles, batch_size, d) -&gt; (n_particles, batch_size, d) -&gt; (n_particles * batch_size, d)\n\n    n_vectors = self._get_random_projections(dup_x)\n\n    logp = (-self.energy_function(dup_x)).sum()\n    grad1 = torch.autograd.grad(logp, dup_x, create_graph=True)[0]\n    v_score = torch.sum(grad1 * n_vectors, dim=-1)\n    term1 = 0.5 * (v_score**2)\n\n    grad_v = torch.autograd.grad(v_score.sum(), dup_x, create_graph=True)[0]\n    term2 = torch.sum(n_vectors * grad_v, dim=-1)\n\n    term1 = term1.view(self.n_projections, -1).mean(dim=0)\n    term2 = term2.view(self.n_projections, -1).mean(dim=0)\n\n    loss = term2 + term1\n\n    return loss.mean()\n</code></pre>"},{"location":"api/torchebm/models/","title":"Torchebm &gt; Models","text":""},{"location":"api/torchebm/models/#torchebm-models","title":"Torchebm &gt; Models","text":""},{"location":"api/torchebm/models/#contents","title":"Contents","text":""},{"location":"api/torchebm/models/#modules","title":"Modules","text":"<ul> <li>Base_model</li> </ul>"},{"location":"api/torchebm/models/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/models/#torchebm.models","title":"torchebm.models","text":"<p>Model implementations for energy-based learning, including base abstract class.</p>"},{"location":"api/torchebm/models/base_model/","title":"Torchebm &gt; Models &gt; Base_model","text":""},{"location":"api/torchebm/models/base_model/#torchebm-models-base_model","title":"Torchebm &gt; Models &gt; Base_model","text":""},{"location":"api/torchebm/models/base_model/#contents","title":"Contents","text":""},{"location":"api/torchebm/models/base_model/#classes","title":"Classes","text":"<ul> <li><code>BaseModel</code> - Base class for models.</li> </ul>"},{"location":"api/torchebm/models/base_model/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/models/base_model/#torchebm.models.base_model","title":"torchebm.models.base_model","text":""},{"location":"api/torchebm/models/base_model/classes/BaseModel/","title":"BaseModel","text":""},{"location":"api/torchebm/models/base_model/classes/BaseModel/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for models.</p> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to sample from.</p> required <code>sampler</code> <code>BaseSampler</code> <p>Sampler to use for sampling.</p> required <p>Methods:</p> Name Description <code>energy</code> <p>Compute the energy of the input.</p> <code>sample</code> <p>Sample from the model.</p> <code>train_step</code> <p>Perform a single training step</p> Source code in <code>torchebm/models/base_model.py</code> <pre><code>class BaseModel(ABC):\n    \"\"\"\n    Base class for models.\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to sample from.\n        sampler (BaseSampler): Sampler to use for sampling.\n\n    Methods:\n        energy(x): Compute the energy of the input.\n        sample(num_samples): Sample from the model.\n        train_step(real_data): Perform a single training step\n    \"\"\"\n\n    def __init__(self, energy_function: BaseEnergyFunction, sampler: BaseSampler):\n        self.energy_function = energy_function\n        self.sampler = sampler\n\n    @abstractmethod\n    def energy(self, x: torch.Tensor) -&gt; torch.Tensor:\n        pass\n\n    @abstractmethod\n    def sample(self, num_samples: int) -&gt; torch.Tensor:\n        pass\n\n    @abstractmethod\n    def train_step(self, real_data: torch.Tensor) -&gt; dict:\n        pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.sampler","title":"sampler  <code>instance-attribute</code>","text":"<pre><code>sampler = sampler\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.energy","title":"energy  <code>abstractmethod</code>","text":"<pre><code>energy(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef energy(self, x: torch.Tensor) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(num_samples: int) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef sample(self, num_samples: int) -&gt; torch.Tensor:\n    pass\n</code></pre>"},{"location":"api/torchebm/models/base_model/classes/BaseModel/#torchebm.models.base_model.BaseModel.train_step","title":"train_step  <code>abstractmethod</code>","text":"<pre><code>train_step(real_data: Tensor) -&gt; dict\n</code></pre> Source code in <code>torchebm/models/base_model.py</code> <pre><code>@abstractmethod\ndef train_step(self, real_data: torch.Tensor) -&gt; dict:\n    pass\n</code></pre>"},{"location":"api/torchebm/samplers/","title":"Torchebm &gt; Samplers","text":""},{"location":"api/torchebm/samplers/#torchebm-samplers","title":"Torchebm &gt; Samplers","text":""},{"location":"api/torchebm/samplers/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/#modules","title":"Modules","text":"<ul> <li>Hmc</li> <li>Langevin_dynamics</li> </ul>"},{"location":"api/torchebm/samplers/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/#torchebm.samplers","title":"torchebm.samplers","text":"<p>Sampling algorithms for energy-based models, including Langevin Dynamics and Hamiltonian Monte Carlo.</p>"},{"location":"api/torchebm/samplers/hmc/","title":"Torchebm &gt; Samplers &gt; Hmc","text":""},{"location":"api/torchebm/samplers/hmc/#torchebm-samplers-hmc","title":"Torchebm &gt; Samplers &gt; Hmc","text":""},{"location":"api/torchebm/samplers/hmc/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/hmc/#classes","title":"Classes","text":"<ul> <li><code>HamiltonianMonteCarlo</code> - Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.</li> </ul>"},{"location":"api/torchebm/samplers/hmc/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc","title":"torchebm.samplers.hmc","text":"<p>Hamiltonian Monte Carlo Sampler Module.</p> <p>This module provides a robust implementation of the Hamiltonian Monte Carlo (HMC) algorithm, a powerful Markov Chain Monte Carlo (MCMC) technique. By leveraging Hamiltonian dynamics, HMC efficiently explores complex, high-dimensional probability distributions, making it ideal for Bayesian inference and statistical modeling.</p> <p>Key Features</p> <ul> <li>Efficient sampling using Hamiltonian dynamics.</li> <li>Customizable step sizes and leapfrog steps for fine-tuned performance.</li> <li>Diagnostic tools to monitor convergence and sampling quality.</li> </ul>"},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>HamiltonianMonteCarlo</code> <p>Implements the Hamiltonian Monte Carlo sampler.</p>"},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc--usage-example","title":"Usage Example","text":"<p>Sampling from a Gaussian Distribution</p> <pre><code>from torchebm.samplers.mcmc import HamiltonianMonteCarlo\nfrom torchebm.energy_functions.energy_function import GaussianEnergy\nimport torch\n\n# Define a 2D Gaussian energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize HMC sampler\nhmc = HamiltonianMonteCarlo(energy_fn, step_size=0.1, n_leapfrog_steps=10)\n\n# Starting points for 10 chains\ninitial_state = torch.randn(10, 2)\n\n# Run sampling\nsamples, diagnostics = hmc.sample_chain(initial_state, k_steps=100, return_diagnostics=True)\nprint(f\"Samples: {samples.batch_shape}\")\nprint(f\"Diagnostics: {diagnostics.keys()}\")\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Hamiltonian Dynamics in HMC</p> <p>HMC combines statistical sampling with concepts from classical mechanics. It introduces an auxiliary momentum variable \\( p \\) and defines a Hamiltonian:</p> \\[ H(q, p) = U(q) + K(p) \\] <ul> <li>Potential Energy: \\( U(q) = -\\log \\pi(q) \\), where \\( \\pi(q) \\) is the target distribution.</li> <li>Kinetic Energy: \\( K(p) = \\frac{1}{2} p^T M^{-1} p \\), with \\( M \\) as the mass matrix (often set to the identity matrix).</li> </ul> <p>This formulation allows HMC to propose new states by simulating trajectories along the energy landscape.</p> <p>Why Hamiltonian Dynamics?</p> <ul> <li>Efficient Exploration: HMC uses gradient information to propose new states, allowing it to explore the state space more efficiently, especially in high-dimensional and complex distributions.</li> <li>Reduced Correlation: By simulating Hamiltonian dynamics, HMC reduces the correlation between successive samples, leading to faster convergence to the target distribution.</li> <li>High Acceptance Rate: The use of Hamiltonian dynamics and a Metropolis acceptance step ensures that proposed moves are accepted with high probability, provided the numerical integration is accurate.</li> </ul>"},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc--leapfrog-integration","title":"Leapfrog Integration","text":"<p>Numerical Simulation of Dynamics</p> <p>HMC approximates Hamiltonian trajectories using the leapfrog integrator, a symplectic method that preserves energy. The steps are:</p> <ol> <li>Momentum Half-Step:     $$     p_{t + \\frac{\\epsilon}{2}} = p_t - \\frac{\\epsilon}{2} \\nabla U(q_t)     $$</li> <li>Position Full-Step:     $$     q_{t + 1} = q_t + \\epsilon M^{-1} p_{t + \\frac{\\epsilon}{2}}     $$</li> <li>Momentum Half-Step:     $$     p_{t + 1} = p_{t + \\frac{\\epsilon}{2}} - \\frac{\\epsilon}{2} \\nabla U(q_{t + 1})     $$</li> </ol> <p>Here, \\( \\epsilon \\) is the step size, and the process is repeated for \\( L \\) leapfrog steps.</p>"},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc--acceptance-step","title":"Acceptance Step","text":"<p>Metropolis-Hastings Correction</p> <p>After proposing a new state \\( (q_{t + 1}, p_{t + 1}) \\), HMC applies an acceptance criterion to ensure detailed balance:</p> \\[ \\alpha = \\min \\left( 1, \\exp \\left( H(q_t, p_t) - H(q_{t + 1}, p_{t + 1}) \\right) \\right) \\] <p>The proposal is accepted with probability \\( \\alpha \\), correcting for numerical errors in the leapfrog integration.</p>"},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc--practical-considerations","title":"Practical Considerations","text":"<p>Tuning Parameters</p> <ul> <li>Step Size (\\( \\epsilon \\)): Too large a step size can lead to unstable trajectories; too small reduces efficiency.</li> <li>Number of Leapfrog Steps (\\( L \\)): Affects the distance traveled per proposal\u2014balance exploration vs. computational cost.</li> <li>Mass Matrix (\\( M \\)): Adjusting \\( M \\) can improve sampling in distributions with varying scales.</li> </ul> <p>How to Diagnose Issues?</p> <p>Use diagnostics to check: - Acceptance rates (ideal: 0.6\u20130.8). - Energy conservation (should be relatively stable). - Autocorrelation of samples (should decrease with lag).</p> <p>Common Pitfalls</p> <ul> <li>Low Acceptance Rate: If the acceptance rate is too low, it may indicate that the step size is too large or the number of leapfrog steps is too high. Try reducing the step size or decreasing the number of leapfrog steps.</li> <li>High Correlation Between Samples: If samples are highly correlated, it may indicate that the step size is too small or the number of leapfrog steps is too few. Increase the step size or the number of leapfrog steps to improve exploration.</li> <li>Divergence or NaN Values: Numerical instability or poor parameter choices can lead to divergent behavior or NaN values. Ensure that the energy function and its gradients are correctly implemented and that parameters are appropriately scaled.</li> </ul>"},{"location":"api/torchebm/samplers/hmc/#torchebm.samplers.hmc--useful-insights","title":"Useful Insights","text":"<p>Why HMC Outperforms Other MCMC Methods</p> <p>HMC's use of gradients and dynamics reduces random-walk behavior, making it particularly effective for: - High-dimensional spaces. - Multimodal distributions (with proper tuning). - Models with strong correlations between variables.</p> Further Reading <ul> <li>Hamiltonian Mechanics Explained</li> <li>Neal, R. M. (2011). \"MCMC using Hamiltonian dynamics.\" Handbook of Markov Chain Monte Carlo.</li> </ul>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/","title":"HamiltonianMonteCarlo","text":""},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.</p> <p>This class implements the Hamiltonian Monte Carlo algorithm, which uses concepts from Hamiltonian mechanics to generate more efficient proposals than traditional random-walk methods. By introducing an auxiliary momentum variable and simulating Hamiltonian dynamics, HMC can make distant proposals with high acceptance probability, particularly in high-dimensional spaces.</p> <p>The method works by: 1. Augmenting the state space with momentum variables 2. Simulating Hamiltonian dynamics using leapfrog integration 3. Accepting or rejecting proposals using a Metropolis-Hastings criterion</p> <p>Algorithm Summary</p> <ol> <li>If <code>x</code> is not provided, initialize it with Gaussian noise.</li> <li>For each step:    a. Sample momentum from Gaussian distribution.    b. Perform leapfrog integration for <code>n_leapfrog_steps</code> steps.    c. Accept or reject the proposal based on Metropolis-Hastings criterion.</li> <li>Optionally track trajectory and diagnostics.</li> </ol> <p>Key Advantages</p> <ul> <li>Efficiency: Performs well in high dimensions by avoiding random walk behavior</li> <li>Exploration: Can efficiently traverse complex probability landscapes</li> <li>Energy Conservation: Uses symplectic integrators that approximately preserve energy</li> <li>Adaptability: Can be adjusted through mass matrices to handle varying scales</li> </ul> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to sample from.</p> required <code>step_size</code> <code>float</code> <p>Step size for leapfrog updates.</p> <code>0.001</code> <code>n_leapfrog_steps</code> <code>int</code> <p>Number of leapfrog steps per proposal.</p> <code>10</code> <code>mass</code> <code>Optional[Tuple[float, Tensor]]</code> <p>Optional mass matrix or scalar for momentum sampling. If float: Uses scalar mass for all dimensions. If Tensor: Uses diagonal mass matrix. If None: Uses identity mass matrix.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type to use for computations.</p> <code>float32</code> <code>device</code> <code>Optional[Union[Tuple[str, device]]]</code> <p>Device to run computations on.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>For invalid parameter ranges</p> <p>Methods:</p> Name Description <code>_initialize_momentum</code> <p>Generate initial momentum from Gaussian distribution.</p> <code>_compute_kinetic_energy</code> <p>Compute the kinetic energy of the momentum.</p> <code>_leapfrog_step</code> <p>Perform a single leapfrog step.</p> <code>_leapfrog_integration</code> <p>Perform full leapfrog integration.</p> <code>hmc_step</code> <p>Perform one HMC step with Metropolis-Hastings acceptance.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics.</p> <p>Basic Usage</p> <pre><code># Define energy function for a 2D Gaussian\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize HMC sampler\nsampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10\n)\n\n# Sample 100 points from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=2,\n    k_steps=100,\n    n_samples=5\n)\n</code></pre> <p>Parameter Relationships</p> <ul> <li>Decreasing <code>step_size</code> improves stability but may reduce mixing.</li> <li>Increasing <code>n_leapfrog_steps</code> allows exploring more distant regions but increases computation.</li> <li>The <code>mass</code> parameter can be tuned to match the geometry of the target distribution.</li> </ul> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>class HamiltonianMonteCarlo(BaseSampler):\n    r\"\"\"\n    Hamiltonian Monte Carlo sampler for efficient exploration of complex probability distributions.\n\n    This class implements the Hamiltonian Monte Carlo algorithm, which uses concepts from\n    Hamiltonian mechanics to generate more efficient proposals than traditional random-walk\n    methods. By introducing an auxiliary momentum variable and simulating Hamiltonian dynamics,\n    HMC can make distant proposals with high acceptance probability, particularly in\n    high-dimensional spaces.\n\n    The method works by:\n    1. Augmenting the state space with momentum variables\n    2. Simulating Hamiltonian dynamics using leapfrog integration\n    3. Accepting or rejecting proposals using a Metropolis-Hastings criterion\n\n    !!! note \"Algorithm Summary\"\n        1. If `x` is not provided, initialize it with Gaussian noise.\n        2. For each step:\n           a. Sample momentum from Gaussian distribution.\n           b. Perform leapfrog integration for `n_leapfrog_steps` steps.\n           c. Accept or reject the proposal based on Metropolis-Hastings criterion.\n        3. Optionally track trajectory and diagnostics.\n\n    !!! tip \"Key Advantages\"\n        - **Efficiency**: Performs well in high dimensions by avoiding random walk behavior\n        - **Exploration**: Can efficiently traverse complex probability landscapes\n        - **Energy Conservation**: Uses symplectic integrators that approximately preserve energy\n        - **Adaptability**: Can be adjusted through mass matrices to handle varying scales\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to sample from.\n        step_size (float): Step size for leapfrog updates.\n        n_leapfrog_steps (int): Number of leapfrog steps per proposal.\n        mass (Optional[Tuple[float, torch.Tensor]]): Optional mass matrix or scalar for momentum sampling.\n            If float: Uses scalar mass for all dimensions.\n            If Tensor: Uses diagonal mass matrix.\n            If None: Uses identity mass matrix.\n        dtype (torch.dtype): Data type to use for computations.\n        device (Optional[Union[Tuple[str, torch.device]]]): Device to run computations on.\n\n    Raises:\n        ValueError: For invalid parameter ranges\n\n    Methods:\n        _initialize_momentum(batch_shape): Generate initial momentum from Gaussian distribution.\n        _compute_kinetic_energy(p): Compute the kinetic energy of the momentum.\n        _leapfrog_step(position, momentum, gradient_fn): Perform a single leapfrog step.\n        _leapfrog_integration(position, momentum): Perform full leapfrog integration.\n        hmc_step(current_position): Perform one HMC step with Metropolis-Hastings acceptance.\n        sample_chain(x, dim, k_steps, n_samples, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(dim, k_steps, n_samples): Initialize the diagnostics.\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Define energy function for a 2D Gaussian\n        energy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n        # Initialize HMC sampler\n        sampler = HamiltonianMonteCarlo(\n            energy_function=energy_fn,\n            step_size=0.1,\n            n_leapfrog_steps=10\n        )\n\n        # Sample 100 points from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=2,\n            k_steps=100,\n            n_samples=5\n        )\n        ```\n\n    !!! warning \"Parameter Relationships\"\n        - Decreasing `step_size` improves stability but may reduce mixing.\n        - Increasing `n_leapfrog_steps` allows exploring more distant regions but increases computation.\n        - The `mass` parameter can be tuned to match the geometry of the target distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        step_size: Union[float, BaseScheduler] = 1e-3,\n        n_leapfrog_steps: int = 10,\n        mass: Optional[Tuple[float, torch.Tensor]] = None,\n        dtype: torch.Tensor = torch.float32,\n        device: Optional[Union[Tuple[str, torch.device]]] = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initialize the Hamiltonian Monte Carlo sampler.\n\n        Args:\n            energy_function: Energy function to sample from.\n            step_size: Step size for leapfrog integration (epsilon in equations).\n            n_leapfrog_steps: Number of leapfrog steps per HMC trajectory.\n            mass: Optional mass parameter or matrix for momentum.\n                If float: Uses scalar mass for all dimensions.\n                If Tensor: Uses diagonal mass matrix.\n                If None: Uses identity mass matrix.\n            dtype: Data type for computations.\n            device: Device to run computations on (\"cpu\" or \"cuda\").\n\n        Raises:\n            ValueError: If step_size or n_leapfrog_steps is non-positive.\n        \"\"\"\n        super().__init__(energy_function=energy_function, dtype=dtype, device=device)\n        if isinstance(step_size, BaseScheduler):\n            self.register_scheduler(\"step_size\", step_size)\n        else:\n            if step_size &lt;= 0:\n                raise ValueError(\"step_size must be positive\")\n            self.register_scheduler(\"step_size\", ConstantScheduler(step_size))\n\n        if n_leapfrog_steps &lt;= 0:\n            raise ValueError(\"n_leapfrog_steps must be positive\")\n\n        # Ensure device consistency: convert device to torch.device and move energy_function\n        # if device is not None:\n        #     self.device = torch.device(device)\n        #     energy_function = energy_function.to(self.device)\n        # else:\n        #     self.device = torch.device(\"cpu\")\n\n        # Respect user-provided dtype\n        self.dtype = dtype\n        self.n_leapfrog_steps = n_leapfrog_steps\n        if mass is not None and not isinstance(mass, float):\n            self.mass = mass.to(self.device)\n        else:\n            self.mass = mass\n\n    def _initialize_momentum(self, shape: torch.Size) -&gt; torch.Tensor:\n        \"\"\"Initialize momentum variables from Gaussian distribution.\n\n        For HMC, momentum variables are sampled from a multivariate Gaussian distribution\n        determined by the mass matrix. The kinetic energy is then:\n        K(p) = p^T M^(-1) p / 2\n\n        Args:\n            shape: Size of the momentum tensor to generate.\n\n        Returns:\n            Momentum tensor drawn from appropriate Gaussian distribution.\n\n        Note:\n            When using a mass matrix M, we sample from N(0, M) rather than\n            transforming samples from N(0, I).\n        \"\"\"\n        p = torch.randn(shape, dtype=self.dtype, device=self.device)\n\n        if self.mass is not None:\n            # Apply mass matrix (equivalent to sampling from N(0, M))\n            if isinstance(self.mass, float):\n                p = p * torch.sqrt(\n                    torch.tensor(self.mass, dtype=self.dtype, device=self.device)\n                )\n            else:\n                mass_sqrt = torch.sqrt(self.mass)\n                p = p * mass_sqrt.view(*([1] * (len(shape) - 1)), -1).expand_as(p)\n        return p\n\n    def _compute_kinetic_energy(self, p: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the kinetic energy given momentum.\n\n        The kinetic energy is defined as:\n        $$ K(p) = p^T M^(-1) p / 2 $$\n\n        Args:\n            p: Momentum tensor.\n\n        Returns:\n            Kinetic energy for each sample in the batch.\n        \"\"\"\n        if self.mass is None:\n            return 0.5 * torch.sum(p**2, dim=-1)\n        elif isinstance(self.mass, float):\n            return 0.5 * torch.sum(p**2, dim=-1) / self.mass\n        else:\n            return 0.5 * torch.sum(\n                p**2 / self.mass.view(*([1] * (len(p.shape) - 1)), -1), dim=-1\n            )\n\n    def _leapfrog_step(\n        self, position: torch.Tensor, momentum: torch.Tensor, gradient_fn: Callable\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Perform a single leapfrog integration step.\n\n        Implements the symplectic leapfrog integrator for Hamiltonian dynamics:\n\n        1. Half-step momentum update: \\(p(t+\u03b5/2) = p(t) - (\u03b5/2)\u2207U(q(t))\\)\n        2. Full-step position update: \\(q(t+\u03b5) = q(t) + \u03b5M^(-1)p(t+\u03b5/2)\\)\n        3. Half-step momentum update: \\(p(t+\u03b5) = p(t+\u03b5/2) - (\u03b5/2)\u2207U(q(t+\u03b5))\\)\n\n        Args:\n            position: Current position tensor.\n            momentum: Current momentum tensor.\n            gradient_fn: Function to compute gradient of potential energy.\n\n        Returns:\n            Tuple of (new_position, new_momentum).\n        \"\"\"\n        step_size = self.get_scheduled_value(\"step_size\")\n\n        # Calculate gradient for half-step momentum update with numerical safeguards\n        gradient = gradient_fn(position)\n        # Clip extreme gradient values to prevent instability\n        gradient = torch.clamp(gradient, min=-1e6, max=1e6)\n\n        # Half-step momentum update\n        p_half = momentum - 0.5 * step_size * gradient\n\n        # Full-step position update with mass matrix adjustment\n        if self.mass is None:\n            x_new = position + step_size * p_half\n        else:\n            if isinstance(self.mass, float):\n                # Ensure mass is positive to avoid division issues\n                safe_mass = max(self.mass, 1e-10)\n                x_new = position + step_size * p_half / safe_mass\n            else:\n                # Create safe mass tensor avoiding zeros or negative values\n                safe_mass = torch.clamp(self.mass, min=1e-10)\n                x_new = position + step_size * p_half / safe_mass.view(\n                    *([1] * (len(position.shape) - 1)), -1\n                )\n\n        # Half-step momentum update with gradient clamping\n        grad_new = gradient_fn(x_new)\n        grad_new = torch.clamp(grad_new, min=-1e6, max=1e6)\n        p_new = p_half - 0.5 * step_size * grad_new\n\n        return x_new, p_new\n\n    def _leapfrog_integration(\n        self, position: torch.Tensor, momentum: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a full leapfrog integration for n_leapfrog_steps.\n\n        Applies multiple leapfrog steps to simulate Hamiltonian dynamics\n        for a trajectory of specified length. This is the energy_functions of the HMC\n        proposal generation.\n\n        Args:\n            position: Initial position tensor.\n            momentum: Initial momentum tensor.\n\n        Returns:\n            Tuple of (final_position, final_momentum) after integration.\n        \"\"\"\n        gradient_fn = partial(self.energy_function.gradient)\n        x = position\n        p = momentum\n\n        # Add check for NaN values before starting integration\n        if torch.isnan(x).any() or torch.isnan(p).any():\n            # Replace NaN values with zeros\n            x = torch.nan_to_num(x, nan=0.0)\n            p = torch.nan_to_num(p, nan=0.0)\n\n        for _ in range(self.n_leapfrog_steps):\n            x, p = self._leapfrog_step(x, p, gradient_fn)\n\n            # Check for NaN values after each step\n            if torch.isnan(x).any() or torch.isnan(p).any():\n                # If NaN values appear, break the integration\n                # Replace NaN with zeros and return current state\n                x = torch.nan_to_num(x, nan=0.0)\n                p = torch.nan_to_num(p, nan=0.0)\n                break\n\n        return x, p\n\n    def hmc_step(\n        self, current_position: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a single HMC step with Metropolis-Hastings acceptance.\n\n        This implements the energy_functions HMC algorithm:\n\n        1. Sample initial momentum\n        2. Compute initial Hamiltonian\n        3. Perform leapfrog integration to propose new state\n        4. Compute final Hamiltonian\n        5. Accept/reject based on Metropolis-Hastings criterion\n\n        Args:\n            current_position: Current position tensor of batch_shape (batch_size, dim).\n\n        Returns:\n            new_position: Updated position tensor\n            acceptance_prob: Probability of accepting each proposal\n            accepted: Boolean mask indicating which proposals were accepted\n        \"\"\"\n        batch_size = current_position.shape[0]\n\n        # Sample initial momentum\n        current_momentum = self._initialize_momentum(current_position.shape)\n\n        # Compute current Hamiltonian: H = U(q) + K(p)\n        # Add numerical stability with clamping\n        current_energy = self.energy_function(current_position)\n        current_energy = torch.clamp(\n            current_energy, min=-1e10, max=1e10\n        )  # Prevent extreme energy values\n\n        current_kinetic = self._compute_kinetic_energy(current_momentum)\n        current_kinetic = torch.clamp(\n            current_kinetic, min=0, max=1e10\n        )  # Kinetic energy should be non-negative\n\n        current_hamiltonian = current_energy + current_kinetic\n\n        # Perform leapfrog integration to get proposal\n        proposed_position, proposed_momentum = self._leapfrog_integration(\n            current_position, current_momentum\n        )\n\n        # Compute proposed Hamiltonian with similar numerical stability\n        proposed_energy = self.energy_function(proposed_position)\n        proposed_energy = torch.clamp(proposed_energy, min=-1e10, max=1e10)\n\n        proposed_kinetic = self._compute_kinetic_energy(proposed_momentum)\n        proposed_kinetic = torch.clamp(proposed_kinetic, min=0, max=1e10)\n\n        proposed_hamiltonian = proposed_energy + proposed_kinetic\n\n        # Metropolis-Hastings acceptance criterion\n        # Clamp hamiltonian_diff to avoid overflow in exp()\n        hamiltonian_diff = current_hamiltonian - proposed_hamiltonian\n        hamiltonian_diff = torch.clamp(hamiltonian_diff, max=50, min=-50)\n\n        acceptance_prob = torch.min(\n            torch.ones(batch_size, device=self.device), torch.exp(hamiltonian_diff)\n        )\n\n        # Accept/reject based on acceptance probability\n        random_uniform = torch.rand(batch_size, device=self.device)\n        accepted = random_uniform &lt; acceptance_prob\n        accepted_mask = accepted.float().view(\n            -1, *([1] * (len(current_position.shape) - 1))\n        )\n\n        # Update position based on acceptance\n        new_position = (\n            accepted_mask * proposed_position + (1.0 - accepted_mask) * current_position\n        )\n\n        return new_position, acceptance_prob, accepted\n\n    @torch.no_grad()\n    def sample(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = None,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Generate samples using Hamiltonian Monte Carlo.\n\n        Runs an HMC chain for a specified number of steps, optionally returning\n        the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian\n        dynamics with leapfrog integration to propose samples efficiently, particularly\n        in high-dimensional spaces.\n\n        Args:\n            x: Initial state to start sampling from. If None, random initialization is used.\n            dim: Dimension of the state space when x is None. If None, will attempt to infer from the energy function.\n            n_steps: Number of HMC steps to perform.\n            n_samples: Number of parallel chains to run.\n            return_trajectory: If True, return the entire trajectory of samples.\n            return_diagnostics: If True, return diagnostics about the sampling process.\n\n        Returns:\n            Final samples:\n\n                - If return_trajectory=False and return_diagnostics=False:\n                    Tensor of batch_shape (n_samples, dim) with final samples.\n                - If return_trajectory=True and return_diagnostics=False:\n                    Tensor of batch_shape (n_samples, k_steps, dim) with the trajectory of all samples.\n                - If return_diagnostics=True:\n                    Tuple of (samples, diagnostics) where diagnostics contains information about\n                    the sampling process, including mean, variance, energy values, and acceptance rates.\n\n        Note:\n            This method uses automatic mixed precision when available on CUDA devices\n            to improve performance while maintaining numerical stability for the\n            Hamiltonian dynamics simulation.\n\n        Example:\n            ```python\n            # Run 10 parallel chains for 1000 steps\n            samples, diagnostics = hmc.sample_chain(\n                dim=10,\n                k_steps=1000,\n                n_samples=10,\n                return_diagnostics=True\n            )\n\n            # Plot acceptance rates\n            import matplotlib.pyplot as plt\n            plt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\n            plt.ylabel('Acceptance Rate')\n            plt.xlabel('Step')\n            plt.show()\n            ```\n        \"\"\"\n        # Reset schedulers to their initial values at the start of sampling\n        self.reset_schedulers()\n\n        if x is None:\n            # If dim is not provided, try to infer from the energy function\n            if dim is None:\n                # Check if it's GaussianEnergy which has mean attribute\n                if hasattr(self.energy_function, \"mean\"):\n                    dim = self.energy_function.mean.shape[0]\n                else:\n                    raise ValueError(\n                        \"dim must be provided when x is None and cannot be inferred from the energy function\"\n                    )\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(device=self.device, dtype=self.dtype)\n\n        # Get dimension from x for later use\n        dim = x.shape[1]\n\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        if return_diagnostics:\n            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n            acceptance_rates = torch.zeros(\n                n_steps, device=self.device, dtype=self.dtype\n            )\n\n        with torch.amp.autocast(\n            device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\",\n            dtype=self.dtype if self.device.type == \"cuda\" else None,\n        ):\n            for i in range(n_steps):\n                # Perform single HMC step\n                x, acceptance_prob, accepted = self.hmc_step(x)\n\n                # Step all schedulers after each HMC step\n                scheduler_values = self.step_schedulers()\n\n                if return_trajectory:\n                    trajectory[:, i, :] = x\n\n                if return_diagnostics:\n                    # Calculate diagnostics with numerical stability safeguards\n\n                    if n_samples &gt; 1:\n                        # mean_x = x.mean(dim=0).unsqueeze(0).expand_as(x)\n                        mean_x = x.mean(dim=0, keepdim=True)\n\n                        # Clamp variance calculations to prevent NaN values\n                        # First compute variance in a numerically stable way\n                        # and then clamp to ensure positive finite values\n                        # x_centered = x - mean_x\n                        # var_x = torch.mean(x_centered**2, dim=0)\n                        var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                        var_x = torch.clamp(\n                            var_x, min=1e-10, max=1e10\n                        )  # Prevent zero/extreme variances\n                        # var_x = var_x.unsqueeze(0).expand_as(x)\n                    else:\n                        # For single sample, mean and variance are trivial\n                        mean_x = x.clone()\n                        var_x = torch.zeros_like(x)\n\n                    # Energy values (ensure finite values)\n                    energy = self.energy_function(\n                        x\n                    )  # assumed to have batch_shape (n_samples,)\n                    energy = torch.clamp(\n                        energy, min=-1e10, max=1e10\n                    )  # Prevent extreme energy values\n                    energy = energy.unsqueeze(1).expand_as(x)\n\n                    # Acceptance rate is already between 0 and 1\n                    acceptance_rate = accepted.float().mean()\n                    acceptance_rate_expanded = torch.ones_like(x) * acceptance_rate\n\n                    # Stack diagnostics\n                    diagnostics[i, 0, :, :] = mean_x\n                    diagnostics[i, 1, :, :] = var_x\n                    diagnostics[i, 2, :, :] = energy\n                    diagnostics[i, 3, :, :] = acceptance_rate_expanded\n\n        if return_trajectory:\n            if return_diagnostics:\n                return trajectory.to(dtype=self.dtype), diagnostics.to(\n                    dtype=self.dtype\n                )  # , acceptance_rates\n            return trajectory.to(dtype=self.dtype)\n\n        if return_diagnostics:\n            return x.to(dtype=self.dtype), diagnostics.to(\n                dtype=self.dtype\n            )  # , acceptance_rates\n\n        return x.to(dtype=self.dtype)\n\n    def _setup_diagnostics(\n        self, dim: int, n_steps: int, n_samples: int = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Initialize diagnostics tensor to track HMC sampling metrics.\n\n        Creates a tensor to store diagnostics information during sampling, including:\n\n        - Mean of samples (dimension 0)\n        - Variance of samples (dimension 1)\n        - Energy values (dimension 2)\n        - Acceptance rates (dimension 3)\n\n        Args:\n            dim: Dimensionality of the state space.\n            n_steps: Number of sampling steps.\n            n_samples: Number of parallel chains (if None, assumed to be 1).\n\n        Returns:\n            Empty tensor of batch_shape (k_steps, 4, n_samples, dim) to store diagnostics.\n        \"\"\"\n        if n_samples is not None:\n            return torch.empty(\n                (n_steps, 4, n_samples, dim), device=self.device, dtype=self.dtype\n            )\n        else:\n            return torch.empty((n_steps, 4, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo.n_leapfrog_steps","title":"n_leapfrog_steps  <code>instance-attribute</code>","text":"<pre><code>n_leapfrog_steps = n_leapfrog_steps\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo.mass","title":"mass  <code>instance-attribute</code>","text":"<pre><code>mass = to(device)\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo._initialize_momentum","title":"_initialize_momentum","text":"<pre><code>_initialize_momentum(shape: Size) -&gt; torch.Tensor\n</code></pre> <p>Initialize momentum variables from Gaussian distribution.</p> <p>For HMC, momentum variables are sampled from a multivariate Gaussian distribution determined by the mass matrix. The kinetic energy is then: K(p) = p^T M^(-1) p / 2</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Size</code> <p>Size of the momentum tensor to generate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Momentum tensor drawn from appropriate Gaussian distribution.</p> Note <p>When using a mass matrix M, we sample from N(0, M) rather than transforming samples from N(0, I).</p> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>def _initialize_momentum(self, shape: torch.Size) -&gt; torch.Tensor:\n    \"\"\"Initialize momentum variables from Gaussian distribution.\n\n    For HMC, momentum variables are sampled from a multivariate Gaussian distribution\n    determined by the mass matrix. The kinetic energy is then:\n    K(p) = p^T M^(-1) p / 2\n\n    Args:\n        shape: Size of the momentum tensor to generate.\n\n    Returns:\n        Momentum tensor drawn from appropriate Gaussian distribution.\n\n    Note:\n        When using a mass matrix M, we sample from N(0, M) rather than\n        transforming samples from N(0, I).\n    \"\"\"\n    p = torch.randn(shape, dtype=self.dtype, device=self.device)\n\n    if self.mass is not None:\n        # Apply mass matrix (equivalent to sampling from N(0, M))\n        if isinstance(self.mass, float):\n            p = p * torch.sqrt(\n                torch.tensor(self.mass, dtype=self.dtype, device=self.device)\n            )\n        else:\n            mass_sqrt = torch.sqrt(self.mass)\n            p = p * mass_sqrt.view(*([1] * (len(shape) - 1)), -1).expand_as(p)\n    return p\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo._compute_kinetic_energy","title":"_compute_kinetic_energy","text":"<pre><code>_compute_kinetic_energy(p: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute the kinetic energy given momentum.</p> <p>The kinetic energy is defined as: $$ K(p) = p^T M^(-1) p / 2 $$</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Tensor</code> <p>Momentum tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Kinetic energy for each sample in the batch.</p> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>def _compute_kinetic_energy(self, p: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the kinetic energy given momentum.\n\n    The kinetic energy is defined as:\n    $$ K(p) = p^T M^(-1) p / 2 $$\n\n    Args:\n        p: Momentum tensor.\n\n    Returns:\n        Kinetic energy for each sample in the batch.\n    \"\"\"\n    if self.mass is None:\n        return 0.5 * torch.sum(p**2, dim=-1)\n    elif isinstance(self.mass, float):\n        return 0.5 * torch.sum(p**2, dim=-1) / self.mass\n    else:\n        return 0.5 * torch.sum(\n            p**2 / self.mass.view(*([1] * (len(p.shape) - 1)), -1), dim=-1\n        )\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo._leapfrog_step","title":"_leapfrog_step","text":"<pre><code>_leapfrog_step(position: Tensor, momentum: Tensor, gradient_fn: Callable) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Perform a single leapfrog integration step.</p> <p>Implements the symplectic leapfrog integrator for Hamiltonian dynamics:</p> <ol> <li>Half-step momentum update: \\(p(t+\u03b5/2) = p(t) - (\u03b5/2)\u2207U(q(t))\\)</li> <li>Full-step position update: \\(q(t+\u03b5) = q(t) + \u03b5M^(-1)p(t+\u03b5/2)\\)</li> <li>Half-step momentum update: \\(p(t+\u03b5) = p(t+\u03b5/2) - (\u03b5/2)\u2207U(q(t+\u03b5))\\)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>Tensor</code> <p>Current position tensor.</p> required <code>momentum</code> <code>Tensor</code> <p>Current momentum tensor.</p> required <code>gradient_fn</code> <code>Callable</code> <p>Function to compute gradient of potential energy.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (new_position, new_momentum).</p> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>def _leapfrog_step(\n    self, position: torch.Tensor, momentum: torch.Tensor, gradient_fn: Callable\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Perform a single leapfrog integration step.\n\n    Implements the symplectic leapfrog integrator for Hamiltonian dynamics:\n\n    1. Half-step momentum update: \\(p(t+\u03b5/2) = p(t) - (\u03b5/2)\u2207U(q(t))\\)\n    2. Full-step position update: \\(q(t+\u03b5) = q(t) + \u03b5M^(-1)p(t+\u03b5/2)\\)\n    3. Half-step momentum update: \\(p(t+\u03b5) = p(t+\u03b5/2) - (\u03b5/2)\u2207U(q(t+\u03b5))\\)\n\n    Args:\n        position: Current position tensor.\n        momentum: Current momentum tensor.\n        gradient_fn: Function to compute gradient of potential energy.\n\n    Returns:\n        Tuple of (new_position, new_momentum).\n    \"\"\"\n    step_size = self.get_scheduled_value(\"step_size\")\n\n    # Calculate gradient for half-step momentum update with numerical safeguards\n    gradient = gradient_fn(position)\n    # Clip extreme gradient values to prevent instability\n    gradient = torch.clamp(gradient, min=-1e6, max=1e6)\n\n    # Half-step momentum update\n    p_half = momentum - 0.5 * step_size * gradient\n\n    # Full-step position update with mass matrix adjustment\n    if self.mass is None:\n        x_new = position + step_size * p_half\n    else:\n        if isinstance(self.mass, float):\n            # Ensure mass is positive to avoid division issues\n            safe_mass = max(self.mass, 1e-10)\n            x_new = position + step_size * p_half / safe_mass\n        else:\n            # Create safe mass tensor avoiding zeros or negative values\n            safe_mass = torch.clamp(self.mass, min=1e-10)\n            x_new = position + step_size * p_half / safe_mass.view(\n                *([1] * (len(position.shape) - 1)), -1\n            )\n\n    # Half-step momentum update with gradient clamping\n    grad_new = gradient_fn(x_new)\n    grad_new = torch.clamp(grad_new, min=-1e6, max=1e6)\n    p_new = p_half - 0.5 * step_size * grad_new\n\n    return x_new, p_new\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo._leapfrog_integration","title":"_leapfrog_integration","text":"<pre><code>_leapfrog_integration(position: Tensor, momentum: Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Perform a full leapfrog integration for n_leapfrog_steps.</p> <p>Applies multiple leapfrog steps to simulate Hamiltonian dynamics for a trajectory of specified length. This is the energy_functions of the HMC proposal generation.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>Tensor</code> <p>Initial position tensor.</p> required <code>momentum</code> <code>Tensor</code> <p>Initial momentum tensor.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (final_position, final_momentum) after integration.</p> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>def _leapfrog_integration(\n    self, position: torch.Tensor, momentum: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Perform a full leapfrog integration for n_leapfrog_steps.\n\n    Applies multiple leapfrog steps to simulate Hamiltonian dynamics\n    for a trajectory of specified length. This is the energy_functions of the HMC\n    proposal generation.\n\n    Args:\n        position: Initial position tensor.\n        momentum: Initial momentum tensor.\n\n    Returns:\n        Tuple of (final_position, final_momentum) after integration.\n    \"\"\"\n    gradient_fn = partial(self.energy_function.gradient)\n    x = position\n    p = momentum\n\n    # Add check for NaN values before starting integration\n    if torch.isnan(x).any() or torch.isnan(p).any():\n        # Replace NaN values with zeros\n        x = torch.nan_to_num(x, nan=0.0)\n        p = torch.nan_to_num(p, nan=0.0)\n\n    for _ in range(self.n_leapfrog_steps):\n        x, p = self._leapfrog_step(x, p, gradient_fn)\n\n        # Check for NaN values after each step\n        if torch.isnan(x).any() or torch.isnan(p).any():\n            # If NaN values appear, break the integration\n            # Replace NaN with zeros and return current state\n            x = torch.nan_to_num(x, nan=0.0)\n            p = torch.nan_to_num(p, nan=0.0)\n            break\n\n    return x, p\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo.hmc_step","title":"hmc_step","text":"<pre><code>hmc_step(current_position: Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Perform a single HMC step with Metropolis-Hastings acceptance.</p> <p>This implements the energy_functions HMC algorithm:</p> <ol> <li>Sample initial momentum</li> <li>Compute initial Hamiltonian</li> <li>Perform leapfrog integration to propose new state</li> <li>Compute final Hamiltonian</li> <li>Accept/reject based on Metropolis-Hastings criterion</li> </ol> <p>Parameters:</p> Name Type Description Default <code>current_position</code> <code>Tensor</code> <p>Current position tensor of batch_shape (batch_size, dim).</p> required <p>Returns:</p> Name Type Description <code>new_position</code> <code>Tensor</code> <p>Updated position tensor</p> <code>acceptance_prob</code> <code>Tensor</code> <p>Probability of accepting each proposal</p> <code>accepted</code> <code>Tensor</code> <p>Boolean mask indicating which proposals were accepted</p> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>def hmc_step(\n    self, current_position: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Perform a single HMC step with Metropolis-Hastings acceptance.\n\n    This implements the energy_functions HMC algorithm:\n\n    1. Sample initial momentum\n    2. Compute initial Hamiltonian\n    3. Perform leapfrog integration to propose new state\n    4. Compute final Hamiltonian\n    5. Accept/reject based on Metropolis-Hastings criterion\n\n    Args:\n        current_position: Current position tensor of batch_shape (batch_size, dim).\n\n    Returns:\n        new_position: Updated position tensor\n        acceptance_prob: Probability of accepting each proposal\n        accepted: Boolean mask indicating which proposals were accepted\n    \"\"\"\n    batch_size = current_position.shape[0]\n\n    # Sample initial momentum\n    current_momentum = self._initialize_momentum(current_position.shape)\n\n    # Compute current Hamiltonian: H = U(q) + K(p)\n    # Add numerical stability with clamping\n    current_energy = self.energy_function(current_position)\n    current_energy = torch.clamp(\n        current_energy, min=-1e10, max=1e10\n    )  # Prevent extreme energy values\n\n    current_kinetic = self._compute_kinetic_energy(current_momentum)\n    current_kinetic = torch.clamp(\n        current_kinetic, min=0, max=1e10\n    )  # Kinetic energy should be non-negative\n\n    current_hamiltonian = current_energy + current_kinetic\n\n    # Perform leapfrog integration to get proposal\n    proposed_position, proposed_momentum = self._leapfrog_integration(\n        current_position, current_momentum\n    )\n\n    # Compute proposed Hamiltonian with similar numerical stability\n    proposed_energy = self.energy_function(proposed_position)\n    proposed_energy = torch.clamp(proposed_energy, min=-1e10, max=1e10)\n\n    proposed_kinetic = self._compute_kinetic_energy(proposed_momentum)\n    proposed_kinetic = torch.clamp(proposed_kinetic, min=0, max=1e10)\n\n    proposed_hamiltonian = proposed_energy + proposed_kinetic\n\n    # Metropolis-Hastings acceptance criterion\n    # Clamp hamiltonian_diff to avoid overflow in exp()\n    hamiltonian_diff = current_hamiltonian - proposed_hamiltonian\n    hamiltonian_diff = torch.clamp(hamiltonian_diff, max=50, min=-50)\n\n    acceptance_prob = torch.min(\n        torch.ones(batch_size, device=self.device), torch.exp(hamiltonian_diff)\n    )\n\n    # Accept/reject based on acceptance probability\n    random_uniform = torch.rand(batch_size, device=self.device)\n    accepted = random_uniform &lt; acceptance_prob\n    accepted_mask = accepted.float().view(\n        -1, *([1] * (len(current_position.shape) - 1))\n    )\n\n    # Update position based on acceptance\n    new_position = (\n        accepted_mask * proposed_position + (1.0 - accepted_mask) * current_position\n    )\n\n    return new_position, acceptance_prob, accepted\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo.sample","title":"sample","text":"<pre><code>sample(x: Optional[Tensor] = None, dim: int = None, n_steps: int = 100, n_samples: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Generate samples using Hamiltonian Monte Carlo.</p> <p>Runs an HMC chain for a specified number of steps, optionally returning the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian dynamics with leapfrog integration to propose samples efficiently, particularly in high-dimensional spaces.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Initial state to start sampling from. If None, random initialization is used.</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of the state space when x is None. If None, will attempt to infer from the energy function.</p> <code>None</code> <code>n_steps</code> <code>int</code> <p>Number of HMC steps to perform.</p> <code>100</code> <code>n_samples</code> <code>int</code> <p>Number of parallel chains to run.</p> <code>1</code> <code>return_trajectory</code> <code>bool</code> <p>If True, return the entire trajectory of samples.</p> <code>False</code> <code>return_diagnostics</code> <code>bool</code> <p>If True, return diagnostics about the sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Final samples:</p> <ul> <li>If return_trajectory=False and return_diagnostics=False:     Tensor of batch_shape (n_samples, dim) with final samples.</li> <li>If return_trajectory=True and return_diagnostics=False:     Tensor of batch_shape (n_samples, k_steps, dim) with the trajectory of all samples.</li> <li>If return_diagnostics=True:     Tuple of (samples, diagnostics) where diagnostics contains information about     the sampling process, including mean, variance, energy values, and acceptance rates.</li> </ul> Note <p>This method uses automatic mixed precision when available on CUDA devices to improve performance while maintaining numerical stability for the Hamiltonian dynamics simulation.</p> Example <pre><code># Run 10 parallel chains for 1000 steps\nsamples, diagnostics = hmc.sample_chain(\n    dim=10,\n    k_steps=1000,\n    n_samples=10,\n    return_diagnostics=True\n)\n\n# Plot acceptance rates\nimport matplotlib.pyplot as plt\nplt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\nplt.ylabel('Acceptance Rate')\nplt.xlabel('Step')\nplt.show()\n</code></pre> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>@torch.no_grad()\ndef sample(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = None,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Generate samples using Hamiltonian Monte Carlo.\n\n    Runs an HMC chain for a specified number of steps, optionally returning\n    the entire trajectory and/or diagnostics. The HMC algorithm uses Hamiltonian\n    dynamics with leapfrog integration to propose samples efficiently, particularly\n    in high-dimensional spaces.\n\n    Args:\n        x: Initial state to start sampling from. If None, random initialization is used.\n        dim: Dimension of the state space when x is None. If None, will attempt to infer from the energy function.\n        n_steps: Number of HMC steps to perform.\n        n_samples: Number of parallel chains to run.\n        return_trajectory: If True, return the entire trajectory of samples.\n        return_diagnostics: If True, return diagnostics about the sampling process.\n\n    Returns:\n        Final samples:\n\n            - If return_trajectory=False and return_diagnostics=False:\n                Tensor of batch_shape (n_samples, dim) with final samples.\n            - If return_trajectory=True and return_diagnostics=False:\n                Tensor of batch_shape (n_samples, k_steps, dim) with the trajectory of all samples.\n            - If return_diagnostics=True:\n                Tuple of (samples, diagnostics) where diagnostics contains information about\n                the sampling process, including mean, variance, energy values, and acceptance rates.\n\n    Note:\n        This method uses automatic mixed precision when available on CUDA devices\n        to improve performance while maintaining numerical stability for the\n        Hamiltonian dynamics simulation.\n\n    Example:\n        ```python\n        # Run 10 parallel chains for 1000 steps\n        samples, diagnostics = hmc.sample_chain(\n            dim=10,\n            k_steps=1000,\n            n_samples=10,\n            return_diagnostics=True\n        )\n\n        # Plot acceptance rates\n        import matplotlib.pyplot as plt\n        plt.plot(diagnostics[:-1, 3, 0, 0].cpu().numpy())\n        plt.ylabel('Acceptance Rate')\n        plt.xlabel('Step')\n        plt.show()\n        ```\n    \"\"\"\n    # Reset schedulers to their initial values at the start of sampling\n    self.reset_schedulers()\n\n    if x is None:\n        # If dim is not provided, try to infer from the energy function\n        if dim is None:\n            # Check if it's GaussianEnergy which has mean attribute\n            if hasattr(self.energy_function, \"mean\"):\n                dim = self.energy_function.mean.shape[0]\n            else:\n                raise ValueError(\n                    \"dim must be provided when x is None and cannot be inferred from the energy function\"\n                )\n        x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n    else:\n        x = x.to(device=self.device, dtype=self.dtype)\n\n    # Get dimension from x for later use\n    dim = x.shape[1]\n\n    if return_trajectory:\n        trajectory = torch.empty(\n            (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n        )\n\n    if return_diagnostics:\n        diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n        acceptance_rates = torch.zeros(\n            n_steps, device=self.device, dtype=self.dtype\n        )\n\n    with torch.amp.autocast(\n        device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\",\n        dtype=self.dtype if self.device.type == \"cuda\" else None,\n    ):\n        for i in range(n_steps):\n            # Perform single HMC step\n            x, acceptance_prob, accepted = self.hmc_step(x)\n\n            # Step all schedulers after each HMC step\n            scheduler_values = self.step_schedulers()\n\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n            if return_diagnostics:\n                # Calculate diagnostics with numerical stability safeguards\n\n                if n_samples &gt; 1:\n                    # mean_x = x.mean(dim=0).unsqueeze(0).expand_as(x)\n                    mean_x = x.mean(dim=0, keepdim=True)\n\n                    # Clamp variance calculations to prevent NaN values\n                    # First compute variance in a numerically stable way\n                    # and then clamp to ensure positive finite values\n                    # x_centered = x - mean_x\n                    # var_x = torch.mean(x_centered**2, dim=0)\n                    var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                    var_x = torch.clamp(\n                        var_x, min=1e-10, max=1e10\n                    )  # Prevent zero/extreme variances\n                    # var_x = var_x.unsqueeze(0).expand_as(x)\n                else:\n                    # For single sample, mean and variance are trivial\n                    mean_x = x.clone()\n                    var_x = torch.zeros_like(x)\n\n                # Energy values (ensure finite values)\n                energy = self.energy_function(\n                    x\n                )  # assumed to have batch_shape (n_samples,)\n                energy = torch.clamp(\n                    energy, min=-1e10, max=1e10\n                )  # Prevent extreme energy values\n                energy = energy.unsqueeze(1).expand_as(x)\n\n                # Acceptance rate is already between 0 and 1\n                acceptance_rate = accepted.float().mean()\n                acceptance_rate_expanded = torch.ones_like(x) * acceptance_rate\n\n                # Stack diagnostics\n                diagnostics[i, 0, :, :] = mean_x\n                diagnostics[i, 1, :, :] = var_x\n                diagnostics[i, 2, :, :] = energy\n                diagnostics[i, 3, :, :] = acceptance_rate_expanded\n\n    if return_trajectory:\n        if return_diagnostics:\n            return trajectory.to(dtype=self.dtype), diagnostics.to(\n                dtype=self.dtype\n            )  # , acceptance_rates\n        return trajectory.to(dtype=self.dtype)\n\n    if return_diagnostics:\n        return x.to(dtype=self.dtype), diagnostics.to(\n            dtype=self.dtype\n        )  # , acceptance_rates\n\n    return x.to(dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/hmc/classes/HamiltonianMonteCarlo/#torchebm.samplers.hmc.HamiltonianMonteCarlo._setup_diagnostics","title":"_setup_diagnostics","text":"<pre><code>_setup_diagnostics(dim: int, n_steps: int, n_samples: int = None) -&gt; torch.Tensor\n</code></pre> <p>Initialize diagnostics tensor to track HMC sampling metrics.</p> <p>Creates a tensor to store diagnostics information during sampling, including:</p> <ul> <li>Mean of samples (dimension 0)</li> <li>Variance of samples (dimension 1)</li> <li>Energy values (dimension 2)</li> <li>Acceptance rates (dimension 3)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimensionality of the state space.</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps.</p> required <code>n_samples</code> <code>int</code> <p>Number of parallel chains (if None, assumed to be 1).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Empty tensor of batch_shape (k_steps, 4, n_samples, dim) to store diagnostics.</p> Source code in <code>torchebm/samplers/hmc.py</code> <pre><code>def _setup_diagnostics(\n    self, dim: int, n_steps: int, n_samples: int = None\n) -&gt; torch.Tensor:\n    \"\"\"Initialize diagnostics tensor to track HMC sampling metrics.\n\n    Creates a tensor to store diagnostics information during sampling, including:\n\n    - Mean of samples (dimension 0)\n    - Variance of samples (dimension 1)\n    - Energy values (dimension 2)\n    - Acceptance rates (dimension 3)\n\n    Args:\n        dim: Dimensionality of the state space.\n        n_steps: Number of sampling steps.\n        n_samples: Number of parallel chains (if None, assumed to be 1).\n\n    Returns:\n        Empty tensor of batch_shape (k_steps, 4, n_samples, dim) to store diagnostics.\n    \"\"\"\n    if n_samples is not None:\n        return torch.empty(\n            (n_steps, 4, n_samples, dim), device=self.device, dtype=self.dtype\n        )\n    else:\n        return torch.empty((n_steps, 4, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/","title":"Torchebm &gt; Samplers &gt; Langevin_dynamics","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm-samplers-langevin_dynamics","title":"Torchebm &gt; Samplers &gt; Langevin_dynamics","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#contents","title":"Contents","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#classes","title":"Classes","text":"<ul> <li><code>LangevinDynamics</code> - Langevin Dynamics sampler implementing discretized gradient-based MCMC.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics","title":"torchebm.samplers.langevin_dynamics","text":"<p>Langevin Dynamics Sampler Module.</p> <p>This module provides an implementation of the Langevin Dynamics algorithm, a gradient-based Markov Chain Monte Carlo (MCMC) method. It leverages stochastic differential equations to sample from complex probability distributions, making it a lightweight yet effective tool for Bayesian inference and generative modeling.</p> <p>Key Features</p> <ul> <li>Gradient-based sampling with stochastic updates.</li> <li>Customizable step sizes and noise scales for flexible tuning.</li> <li>Optional diagnostics and trajectory tracking for analysis.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--module-components","title":"Module Components","text":"<p>Classes:</p> Name Description <code>LangevinDynamics</code> <p>Core class implementing the Langevin Dynamics sampler.</p>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--usage-example","title":"Usage Example","text":"<p>Sampling from a Custom Energy Function</p> <pre><code>from torchebm.samplers.mcmc.langevin import LangevinDynamics\nfrom torchebm.energy_functions.energy_function import GaussianEnergy\nimport torch\n\n# Define a 2D Gaussian energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Initialize Langevin sampler\nsampler = LangevinDynamics(energy_fn, step_size=0.01, noise_scale=0.1)\n\n# Starting points for 5 chains\ninitial_state = torch.randn(5, 2)\n\n# Run sampling\nsamples, diagnostics = sampler.sample_chain(\n    x=initial_state, k_steps=100, n_samples=5, return_diagnostics=True\n)\nprint(f\"Samples batch_shape: {samples.batch_shape}\")\nprint(f\"Diagnostics keys: {diagnostics.batch_shape}\")\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--mathematical-foundations","title":"Mathematical Foundations","text":"<p>Langevin Dynamics Overview</p> <p>Langevin Dynamics simulates a stochastic process governed by the Langevin equation. For a state \\( x_t \\), the discretized update rule is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla U(x_t) + \\sqrt{2\\eta} \\epsilon_t \\] <ul> <li>\\( U(x) \\): Potential energy, where \\( U(x) = -\\log p(x) \\) and \\( p(x) \\) is the target distribution.</li> <li>\\( \\eta \\): Step size controlling the gradient descent.</li> <li>\\( \\epsilon_t \\sim \\mathcal{N}(0, I) \\): Gaussian noise introducing stochasticity.</li> </ul> <p>Over time, this process converges to samples from the Boltzmann distribution:</p> \\[ p(x) \\propto e^{-U(x)} \\] <p>Why Use Langevin Dynamics?</p> <ul> <li>Simplicity: Requires only first-order gradients, making it computationally lighter than methods like HMC.</li> <li>Exploration: The noise term prevents the sampler from getting stuck in local minima.</li> <li>Flexibility: Applicable to a wide range of energy-based models and score-based generative tasks.</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/#torchebm.samplers.langevin_dynamics--practical-considerations","title":"Practical Considerations","text":"<p>Parameter Tuning Guide</p> <ul> <li>Step Size (\\(\\eta\\)):<ul> <li>Too large: Instability and divergence</li> <li>Too small: Slow convergence</li> <li>Rule of thumb: Start with \\(\\eta \\approx 10^{-3}\\) to \\(10^{-5}\\)</li> </ul> </li> <li>Noise Scale (\\(\\beta^{-1/2}\\)):<ul> <li>Controls exploration-exploitation tradeoff</li> <li>Higher values help escape local minima</li> </ul> </li> <li>Decay Rate (future implementation):<ul> <li>Momentum-like term for accelerated convergence</li> </ul> </li> </ul> <p>Diagnostics Interpretation</p> <p>Use <code>return_diagnostics=True</code> to monitor: - Mean/Variance: Track distribution stationarity - Energy Gradients: Check for vanishing/exploding gradients - Autocorrelation: Assess mixing efficiency</p> <p>When to Choose Langevin Over HMC?</p> Criterion Langevin HMC Computational Cost Lower Higher Tuning Complexity Simpler More involved High Dimensions Efficient More efficient Multimodal Targets May need annealing Better exploration <p>How to Diagnose Sampling?</p> <p>Check diagnostics for: - Sample mean and variance convergence. - Gradient magnitudes (should stabilize). - Energy trends over iterations.</p> Further Reading <ul> <li>Langevin Dynamics Basics</li> <li>Score-Based Models and Langevin</li> <li>Practical Langevin Tutorial</li> </ul>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/","title":"LangevinDynamics","text":""},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#methods-and-attributes","title":"Methods and Attributes","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Langevin Dynamics sampler implementing discretized gradient-based MCMC.</p> <p>This class implements the Langevin Dynamics algorithm, a gradient-based MCMC method that samples from a target distribution defined by an energy function. It uses a stochastic update rule combining gradient descent with Gaussian noise to explore the energy landscape.</p> <p>Each step updates the state \\(x_t\\) according to the discretized Langevin equation:</p> \\[x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t\\] <p>where \\(\\epsilon_t \\sim \\mathcal{N}(0, I)\\) and \\(\\eta\\) is the step size.</p> <p>This process generates samples that asymptotically follow the Boltzmann distribution:</p> \\[p(x) \\propto e^{-U(x)}\\] <p>where \\(U(x)\\) defines the energy landscape.</p> <p>Algorithm Summary</p> <ol> <li>If <code>x</code> is not provided, initialize it with Gaussian noise.</li> <li>Iteratively update <code>x</code> for <code>k_steps</code> using <code>self.langevin_step()</code>.</li> <li>Optionally track trajectory (<code>return_trajectory=True</code>).</li> <li>Optionally collect diagnostics such as mean, variance, and energy gradients.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>energy_function</code> <code>BaseEnergyFunction</code> <p>Energy function to sample from.</p> required <code>step_size</code> <code>float</code> <p>Step size for the Langevin update.</p> <code>0.001</code> <code>noise_scale</code> <code>float</code> <p>Scale of the Gaussian noise.</p> <code>1.0</code> <code>decay</code> <code>float</code> <p>Damping coefficient (not supported yet).</p> <code>0.0</code> <code>dtype</code> <code>dtype</code> <p>Data type to use for the computations.</p> <code>float32</code> <code>device</code> <code>str</code> <p>Device to run the computations on (e.g., \"cpu\" or \"cuda\").</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>For invalid parameter ranges</p> <p>Methods:</p> Name Description <code>langevin_step</code> <p>Perform a Langevin step.</p> <code>sample_chain</code> <p>Run the sampling process.</p> <code>_setup_diagnostics</code> <p>Initialize the diagnostics</p> <p>Basic Usage</p> <pre><code># Define energy function\nenergy_fn = QuadraticEnergy(A=torch.eye(2), b=torch.zeros(2))\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=0.1\n)\n\n# Sample 100 points from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=2,\n    k_steps=50,\n    n_samples=100\n)\n</code></pre> <p>Parameter Relationships</p> <p>The effective temperature is controlled by: \\(\\text{Temperature} = \\frac{\\text{noise_scale}^2}{2 \\cdot \\text{step_size}}\\) Adjust both parameters together to maintain constant temperature.</p> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>class LangevinDynamics(BaseSampler):\n    r\"\"\"\n    Langevin Dynamics sampler implementing discretized gradient-based MCMC.\n\n    This class implements the Langevin Dynamics algorithm, a gradient-based MCMC method that samples from a target\n    distribution defined by an energy function. It uses a stochastic update rule combining gradient descent with Gaussian noise to explore the energy landscape.\n\n    Each step updates the state $x_t$ according to the discretized Langevin equation:\n\n    $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n    where $\\epsilon_t \\sim \\mathcal{N}(0, I)$ and $\\eta$ is the step size.\n\n    This process generates samples that asymptotically follow the Boltzmann distribution:\n\n\n    $$p(x) \\propto e^{-U(x)}$$\n\n    where $U(x)$ defines the energy landscape.\n\n    !!! note \"Algorithm Summary\"\n\n        1. If `x` is not provided, initialize it with Gaussian noise.\n        2. Iteratively update `x` for `k_steps` using `self.langevin_step()`.\n        3. Optionally track trajectory (`return_trajectory=True`).\n        4. Optionally collect diagnostics such as mean, variance, and energy gradients.\n\n    Args:\n        energy_function (BaseEnergyFunction): Energy function to sample from.\n        step_size (float): Step size for the Langevin update.\n        noise_scale (float): Scale of the Gaussian noise.\n        decay (float): Damping coefficient (not supported yet).\n        dtype (torch.dtype): Data type to use for the computations.\n        device (str): Device to run the computations on (e.g., \"cpu\" or \"cuda\").\n\n    Raises:\n        ValueError: For invalid parameter ranges\n\n    Methods:\n        langevin_step(prev_x, noise): Perform a Langevin step.\n        sample_chain(x, dim, k_steps, n_samples, return_trajectory, return_diagnostics): Run the sampling process.\n        _setup_diagnostics(dim, k_steps, n_samples): Initialize the diagnostics\n\n    !!! example \"Basic Usage\"\n        ```python\n        # Define energy function\n        energy_fn = QuadraticEnergy(A=torch.eye(2), b=torch.zeros(2))\n\n        # Initialize sampler\n        sampler = LangevinDynamics(\n            energy_function=energy_fn,\n            step_size=0.01,\n            noise_scale=0.1\n        )\n\n        # Sample 100 points from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=2,\n            k_steps=50,\n            n_samples=100\n        )\n        ```\n    !!! warning \"Parameter Relationships\"\n        The effective temperature is controlled by:\n        \\(\\text{Temperature} = \\frac{\\text{noise_scale}^2}{2 \\cdot \\text{step_size}}\\)\n        Adjust both parameters together to maintain constant temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        step_size: Union[float, BaseScheduler] = 1e-3,\n        noise_scale: Union[float, BaseScheduler] = 1.0,\n        decay: float = 0.0,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(energy_function=energy_function, dtype=dtype, device=device)\n\n        # Register schedulers for step_size and noise_scale\n        if isinstance(step_size, BaseScheduler):\n            self.register_scheduler(\"step_size\", step_size)\n        else:\n            if step_size &lt;= 0:\n                raise ValueError(\"step_size must be positive\")\n            self.register_scheduler(\"step_size\", ConstantScheduler(step_size))\n\n        if isinstance(noise_scale, BaseScheduler):\n            self.register_scheduler(\"noise_scale\", noise_scale)\n        else:\n            if noise_scale &lt;= 0:\n                raise ValueError(\"noise_scale must be positive\")\n            self.register_scheduler(\"noise_scale\", ConstantScheduler(noise_scale))\n\n        # if device is not None:\n        #     self.device = torch.device(device)\n        #     energy_function = energy_function.to(self.device)\n        # else:\n        #     self.device = torch.device(\"cpu\")\n        # Respect dtype from BaseSampler; do not override based on device\n        self.energy_function = energy_function\n        self.step_size = step_size\n        self.noise_scale = noise_scale\n        self.decay = decay\n\n    def langevin_step(self, prev_x: torch.Tensor, noise: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Perform a single Langevin dynamics update step.\n\n        Implements the discrete Langevin equation:\n\n        $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n        Args:\n            prev_x (torch.Tensor): Current state tensor of batch_shape (batch_size, dim)\n            noise (torch.Tensor): Gaussian noise tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            torch.Tensor: Updated state tensor of same batch_shape as prev_x\n\n        Example:\n            ```python\n            # Single step for 10 particles in 2D space\n            current_state = torch.randn(10, 2)\n            noise = torch.randn_like(current_state)\n            next_state = langevin.langevin_step(current_state, noise)\n            ```\n        \"\"\"\n\n        step_size = self.get_scheduled_value(\"step_size\")\n        noise_scale = self.get_scheduled_value(\"noise_scale\")\n\n        gradient = self.energy_function.gradient(prev_x)\n\n        # Apply noise scaling\n        scaled_noise = noise_scale * noise\n\n        # Apply proper step size and noise scaling\n        new_x = (\n            prev_x\n            - step_size * gradient\n            + torch.sqrt(torch.tensor(2.0 * step_size, device=prev_x.device))\n            * scaled_noise\n        )\n        return new_x\n\n    @torch.no_grad()\n    def sample(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        \"\"\"\n        Generate Markov chain samples using Langevin dynamics.\n\n        Args:\n            x: Initial state to start the sampling from.\n            dim: Dimension of the state space.\n            n_steps: Number of steps to take between samples.\n            n_samples: Number of samples to generate.\n            return_trajectory: Whether to return the trajectory of the samples.\n            return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n        Returns:\n            Final samples:\n\n                - If `return_trajectory=False` and `return_diagnostics=False`, returns the final\n                  samples of batch_shape `(n_samples, dim)`.\n                - If `return_trajectory=True`, returns a tensor of batch_shape `(n_samples, k_steps, dim)`,\n                  containing the sampled trajectory.\n                - If `return_diagnostics=True`, returns a tuple `(samples, diagnostics)`, where\n                  `diagnostics` is a list of dictionaries storing per-step statistics.\n\n        Raises:\n            ValueError: If input dimensions mismatch\n\n        Note:\n            - Automatically handles device placement (CPU/GPU)\n            - Uses mixed-precision training when available\n            - Diagnostics include:\n                * Mean and variance across dimensions\n                * Energy gradients\n                * Noise statistics\n\n        Example:\n            ```python\n            # Generate 100 samples from 5 parallel chains\n            samples = sampler.sample_chain(\n                dim=32,\n                k_steps=500,\n                n_samples=100,\n                return_diagnostics=True\n            )\n            ```\n        \"\"\"\n\n        self.reset_schedulers()\n\n        if x is None:\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(self.device)  # Initial batch\n            dim = x.shape[-1]\n            n_samples = x.shape[0]\n\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        if return_diagnostics:\n            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n\n        if self.use_mixed_precision:\n            with torch.amp.autocast(\n                device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"\n            ):\n                for i in range(n_steps):\n                    # todo: Add decay logic\n                    # Generate fresh noise for each step\n                    noise = torch.randn_like(x, device=self.device)\n\n                    # Step all schedulers before each MCMC step\n                    scheduler_values = self.step_schedulers()\n\n                    x = self.langevin_step(x, noise)\n\n                    if return_trajectory:\n                        trajectory[:, i, :] = x\n\n                    if return_diagnostics:\n                        # Handle mean and variance safely regardless of batch size\n                        if n_samples &gt; 1:\n                            mean_x = x.mean(dim=0, keepdim=True)\n                            var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                            var_x = torch.clamp(var_x, min=1e-10, max=1e10)\n                        else:\n                            # For single sample, just use the value and zeros for variance\n                            mean_x = x.clone()\n                            var_x = torch.zeros_like(x)\n\n                        # Compute energy values\n                        energy = self.energy_function(x)\n\n                        # Store the diagnostics safely\n                        for b in range(n_samples):\n                            diagnostics[i, 0, b, :] = mean_x[b if n_samples &gt; 1 else 0]\n                            diagnostics[i, 1, b, :] = var_x[b if n_samples &gt; 1 else 0]\n                            diagnostics[i, 2, b, :] = energy[b].reshape(-1)\n        else:\n            for i in range(n_steps):\n                # todo: Add decay logic\n                # Generate fresh noise for each step\n                noise = torch.randn_like(x, device=self.device)\n\n                # Step all schedulers before each MCMC step\n                scheduler_values = self.step_schedulers()\n\n                x = self.langevin_step(x, noise)\n\n                if return_trajectory:\n                    trajectory[:, i, :] = x\n\n                if return_diagnostics:\n                    # Handle mean and variance safely regardless of batch size\n                    if n_samples &gt; 1:\n                        mean_x = x.mean(dim=0, keepdim=True)\n                        var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                        var_x = torch.clamp(var_x, min=1e-10, max=1e10)\n                    else:\n                        # For single sample, just use the value and zeros for variance\n                        mean_x = x.clone()\n                        var_x = torch.zeros_like(x)\n\n                    # Compute energy values\n                    energy = self.energy_function(x)\n\n                    # Store the diagnostics safely\n                    for b in range(n_samples):\n                        diagnostics[i, 0, b, :] = mean_x[b if n_samples &gt; 1 else 0]\n                        diagnostics[i, 1, b, :] = var_x[b if n_samples &gt; 1 else 0]\n                        diagnostics[i, 2, b, :] = energy[b].reshape(-1)\n\n        if return_trajectory:\n            if return_diagnostics:\n                return trajectory, diagnostics\n            return trajectory\n        if return_diagnostics:\n            return x, diagnostics\n        return x\n\n    def _setup_diagnostics(\n        self, dim: int, n_steps: int, n_samples: int = None\n    ) -&gt; torch.Tensor:\n        if n_samples is not None:\n            return torch.empty(\n                (n_steps, 3, n_samples, dim), device=self.device, dtype=self.dtype\n            )\n        else:\n            return torch.empty((n_steps, 3, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.energy_function","title":"energy_function  <code>instance-attribute</code>","text":"<pre><code>energy_function = energy_function\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.step_size","title":"step_size  <code>instance-attribute</code>","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.noise_scale","title":"noise_scale  <code>instance-attribute</code>","text":"<pre><code>noise_scale = noise_scale\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.decay","title":"decay  <code>instance-attribute</code>","text":"<pre><code>decay = decay\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.langevin_step","title":"langevin_step","text":"<pre><code>langevin_step(prev_x: Tensor, noise: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Perform a single Langevin dynamics update step.</p> <p>Implements the discrete Langevin equation:</p> \\[x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t\\] <p>Parameters:</p> Name Type Description Default <code>prev_x</code> <code>Tensor</code> <p>Current state tensor of batch_shape (batch_size, dim)</p> required <code>noise</code> <code>Tensor</code> <p>Gaussian noise tensor of batch_shape (batch_size, dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Updated state tensor of same batch_shape as prev_x</p> Example <pre><code># Single step for 10 particles in 2D space\ncurrent_state = torch.randn(10, 2)\nnoise = torch.randn_like(current_state)\nnext_state = langevin.langevin_step(current_state, noise)\n</code></pre> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>def langevin_step(self, prev_x: torch.Tensor, noise: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Perform a single Langevin dynamics update step.\n\n    Implements the discrete Langevin equation:\n\n    $$x_{t+1} = x_t - \\eta \\nabla_x U(x_t) + \\sqrt{2\\eta} \\epsilon_t$$\n\n    Args:\n        prev_x (torch.Tensor): Current state tensor of batch_shape (batch_size, dim)\n        noise (torch.Tensor): Gaussian noise tensor of batch_shape (batch_size, dim)\n\n    Returns:\n        torch.Tensor: Updated state tensor of same batch_shape as prev_x\n\n    Example:\n        ```python\n        # Single step for 10 particles in 2D space\n        current_state = torch.randn(10, 2)\n        noise = torch.randn_like(current_state)\n        next_state = langevin.langevin_step(current_state, noise)\n        ```\n    \"\"\"\n\n    step_size = self.get_scheduled_value(\"step_size\")\n    noise_scale = self.get_scheduled_value(\"noise_scale\")\n\n    gradient = self.energy_function.gradient(prev_x)\n\n    # Apply noise scaling\n    scaled_noise = noise_scale * noise\n\n    # Apply proper step size and noise scaling\n    new_x = (\n        prev_x\n        - step_size * gradient\n        + torch.sqrt(torch.tensor(2.0 * step_size, device=prev_x.device))\n        * scaled_noise\n    )\n    return new_x\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics.sample","title":"sample","text":"<pre><code>sample(x: Optional[Tensor] = None, dim: int = 10, n_steps: int = 100, n_samples: int = 1, thin: int = 1, return_trajectory: bool = False, return_diagnostics: bool = False, *args, **kwargs) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]\n</code></pre> <p>Generate Markov chain samples using Langevin dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Initial state to start the sampling from.</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of the state space.</p> <code>10</code> <code>n_steps</code> <code>int</code> <p>Number of steps to take between samples.</p> <code>100</code> <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>1</code> <code>return_trajectory</code> <code>bool</code> <p>Whether to return the trajectory of the samples.</p> <code>False</code> <code>return_diagnostics</code> <code>bool</code> <p>Whether to return the diagnostics of the sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, List[dict]]]</code> <p>Final samples:</p> <ul> <li>If <code>return_trajectory=False</code> and <code>return_diagnostics=False</code>, returns the final   samples of batch_shape <code>(n_samples, dim)</code>.</li> <li>If <code>return_trajectory=True</code>, returns a tensor of batch_shape <code>(n_samples, k_steps, dim)</code>,   containing the sampled trajectory.</li> <li>If <code>return_diagnostics=True</code>, returns a tuple <code>(samples, diagnostics)</code>, where   <code>diagnostics</code> is a list of dictionaries storing per-step statistics.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input dimensions mismatch</p> Note <ul> <li>Automatically handles device placement (CPU/GPU)</li> <li>Uses mixed-precision training when available</li> <li>Diagnostics include:<ul> <li>Mean and variance across dimensions</li> <li>Energy gradients</li> <li>Noise statistics</li> </ul> </li> </ul> Example <pre><code># Generate 100 samples from 5 parallel chains\nsamples = sampler.sample_chain(\n    dim=32,\n    k_steps=500,\n    n_samples=100,\n    return_diagnostics=True\n)\n</code></pre> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>@torch.no_grad()\ndef sample(\n    self,\n    x: Optional[torch.Tensor] = None,\n    dim: int = 10,\n    n_steps: int = 100,\n    n_samples: int = 1,\n    thin: int = 1,\n    return_trajectory: bool = False,\n    return_diagnostics: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n    \"\"\"\n    Generate Markov chain samples using Langevin dynamics.\n\n    Args:\n        x: Initial state to start the sampling from.\n        dim: Dimension of the state space.\n        n_steps: Number of steps to take between samples.\n        n_samples: Number of samples to generate.\n        return_trajectory: Whether to return the trajectory of the samples.\n        return_diagnostics: Whether to return the diagnostics of the sampling process.\n\n    Returns:\n        Final samples:\n\n            - If `return_trajectory=False` and `return_diagnostics=False`, returns the final\n              samples of batch_shape `(n_samples, dim)`.\n            - If `return_trajectory=True`, returns a tensor of batch_shape `(n_samples, k_steps, dim)`,\n              containing the sampled trajectory.\n            - If `return_diagnostics=True`, returns a tuple `(samples, diagnostics)`, where\n              `diagnostics` is a list of dictionaries storing per-step statistics.\n\n    Raises:\n        ValueError: If input dimensions mismatch\n\n    Note:\n        - Automatically handles device placement (CPU/GPU)\n        - Uses mixed-precision training when available\n        - Diagnostics include:\n            * Mean and variance across dimensions\n            * Energy gradients\n            * Noise statistics\n\n    Example:\n        ```python\n        # Generate 100 samples from 5 parallel chains\n        samples = sampler.sample_chain(\n            dim=32,\n            k_steps=500,\n            n_samples=100,\n            return_diagnostics=True\n        )\n        ```\n    \"\"\"\n\n    self.reset_schedulers()\n\n    if x is None:\n        x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n    else:\n        x = x.to(self.device)  # Initial batch\n        dim = x.shape[-1]\n        n_samples = x.shape[0]\n\n    if return_trajectory:\n        trajectory = torch.empty(\n            (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n        )\n\n    if return_diagnostics:\n        diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n\n    if self.use_mixed_precision:\n        with torch.amp.autocast(\n            device_type=\"cuda\" if self.device.type == \"cuda\" else \"cpu\"\n        ):\n            for i in range(n_steps):\n                # todo: Add decay logic\n                # Generate fresh noise for each step\n                noise = torch.randn_like(x, device=self.device)\n\n                # Step all schedulers before each MCMC step\n                scheduler_values = self.step_schedulers()\n\n                x = self.langevin_step(x, noise)\n\n                if return_trajectory:\n                    trajectory[:, i, :] = x\n\n                if return_diagnostics:\n                    # Handle mean and variance safely regardless of batch size\n                    if n_samples &gt; 1:\n                        mean_x = x.mean(dim=0, keepdim=True)\n                        var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                        var_x = torch.clamp(var_x, min=1e-10, max=1e10)\n                    else:\n                        # For single sample, just use the value and zeros for variance\n                        mean_x = x.clone()\n                        var_x = torch.zeros_like(x)\n\n                    # Compute energy values\n                    energy = self.energy_function(x)\n\n                    # Store the diagnostics safely\n                    for b in range(n_samples):\n                        diagnostics[i, 0, b, :] = mean_x[b if n_samples &gt; 1 else 0]\n                        diagnostics[i, 1, b, :] = var_x[b if n_samples &gt; 1 else 0]\n                        diagnostics[i, 2, b, :] = energy[b].reshape(-1)\n    else:\n        for i in range(n_steps):\n            # todo: Add decay logic\n            # Generate fresh noise for each step\n            noise = torch.randn_like(x, device=self.device)\n\n            # Step all schedulers before each MCMC step\n            scheduler_values = self.step_schedulers()\n\n            x = self.langevin_step(x, noise)\n\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n            if return_diagnostics:\n                # Handle mean and variance safely regardless of batch size\n                if n_samples &gt; 1:\n                    mean_x = x.mean(dim=0, keepdim=True)\n                    var_x = x.var(dim=0, unbiased=False, keepdim=True)\n                    var_x = torch.clamp(var_x, min=1e-10, max=1e10)\n                else:\n                    # For single sample, just use the value and zeros for variance\n                    mean_x = x.clone()\n                    var_x = torch.zeros_like(x)\n\n                # Compute energy values\n                energy = self.energy_function(x)\n\n                # Store the diagnostics safely\n                for b in range(n_samples):\n                    diagnostics[i, 0, b, :] = mean_x[b if n_samples &gt; 1 else 0]\n                    diagnostics[i, 1, b, :] = var_x[b if n_samples &gt; 1 else 0]\n                    diagnostics[i, 2, b, :] = energy[b].reshape(-1)\n\n    if return_trajectory:\n        if return_diagnostics:\n            return trajectory, diagnostics\n        return trajectory\n    if return_diagnostics:\n        return x, diagnostics\n    return x\n</code></pre>"},{"location":"api/torchebm/samplers/langevin_dynamics/classes/LangevinDynamics/#torchebm.samplers.langevin_dynamics.LangevinDynamics._setup_diagnostics","title":"_setup_diagnostics","text":"<pre><code>_setup_diagnostics(dim: int, n_steps: int, n_samples: int = None) -&gt; torch.Tensor\n</code></pre> Source code in <code>torchebm/samplers/langevin_dynamics.py</code> <pre><code>def _setup_diagnostics(\n    self, dim: int, n_steps: int, n_samples: int = None\n) -&gt; torch.Tensor:\n    if n_samples is not None:\n        return torch.empty(\n            (n_steps, 3, n_samples, dim), device=self.device, dtype=self.dtype\n        )\n    else:\n        return torch.empty((n_steps, 3, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"api/torchebm/utils/","title":"Torchebm &gt; Utils","text":""},{"location":"api/torchebm/utils/#torchebm-utils","title":"Torchebm &gt; Utils","text":""},{"location":"api/torchebm/utils/#contents","title":"Contents","text":""},{"location":"api/torchebm/utils/#modules","title":"Modules","text":"<ul> <li>Visualization</li> </ul>"},{"location":"api/torchebm/utils/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/utils/#torchebm.utils","title":"torchebm.utils","text":"<p>Utility functions for working with energy-based models, including visualization tools.</p>"},{"location":"api/torchebm/utils/visualization/","title":"Visualization","text":""},{"location":"api/torchebm/utils/visualization/#torchebm-utils-visualization","title":"Torchebm &gt; Utils &gt; Visualization","text":""},{"location":"api/torchebm/utils/visualization/#contents","title":"Contents","text":""},{"location":"api/torchebm/utils/visualization/#functions","title":"Functions","text":"<ul> <li><code>plot_2d_energy_landscape()</code> - Plot a 2D energy landscape.</li> <li><code>plot_3d_energy_landscape()</code> - Plot a 3D surface visualization of a 2D energy landscape.</li> <li><code>plot_sample_trajectories()</code> - Plot sample trajectories, optionally on an energy landscape background.</li> <li><code>plot_samples_on_energy()</code> - Plot samples on a 2D energy landscape.</li> </ul>"},{"location":"api/torchebm/utils/visualization/#api-reference","title":"API Reference","text":""},{"location":"api/torchebm/utils/visualization/#torchebm.utils.visualization","title":"torchebm.utils.visualization","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#torchebm-blog","title":"TorchEBM Blog","text":"<p>Welcome to the TorchEBM blog! Here you'll find latest news, tutorials, research insights, and updates about the project.</p>"},{"location":"blog/#categories","title":"Categories","text":"<ul> <li> <p> Tutorials</p> <p>Step-by-step guides to help you learn how to use TorchEBM effectively.</p> <p> View Tutorials</p> </li> <li> <p> Research</p> <p>Cutting-edge research using energy-based models and insights from the field.</p> <p> View Research</p> </li> <li> <p> Announcements</p> <p>Updates about TorchEBM releases, features, and roadmap.</p> <p> View Announcements</p> </li> <li> <p> Examples</p> <p>Real-world examples and case studies using TorchEBM.</p> <p> View Examples</p> </li> </ul>"},{"location":"blog/#recent-posts","title":"Recent Posts","text":""},{"location":"blog/hamiltonian-mechanics/","title":"Hamiltonian Mechanics","text":"","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltonian-mechanics","title":"Hamiltonian Mechanics","text":"<p> Hamiltonian mechanics is a way to describe how physical systems, like planets or pendulums, move over  time, focusing on energy rather than just forces. By reframing complex dynamics through energy lenses,  this 19th-century physics framework now powers cutting-edge generative AI. It uses generalized coordinates \\( q \\) (like position) and their  conjugate momenta \\( p \\) (related to momentum), forming a phase space that captures the system's state. This approach  is particularly useful for complex systems with many parts, making it easier to find patterns and conservation laws.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#mathematical-reformation-from-second-order-to-phase-flow","title":"Mathematical Reformation: From Second-Order to Phase Flow","text":"<p>Newton's \\( F = m\\ddot{q} \\) requires solving second-order differential equations, which become unwieldy for constrained systems or when identifying conserved quantities. </p> <p>The Core Idea</p> <p>Hamiltonian mechanics splits \\( \\ddot{q} = F(q)/m \\) into two first-order equations by introducing conjugate momentum \\( p \\):</p> \\[\\begin{align*} \\dot{q} = \\frac{\\partial H}{\\partial p} &amp; \\text{(Position)}, \\quad \\dot{p} = -\\frac{\\partial H}{\\partial q} &amp; \\text{(Momentum)} \\end{align*}\\] <p>It decomposes acceleration into complementary momentum/position flows. This phase space perspective reveals hidden geometric structure.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#lagrangian-prelude-action-principles","title":"Lagrangian Prelude: Action Principles","text":"<p>The Lagrangian \\( \\mathcal{L}(q, \\dot{q}) = K - U \\) leads to Euler-Lagrange equations via variational calculus: $$ \\frac{d}{dt}\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}} \\right) - \\frac{\\partial \\mathcal{L}}{\\partial q} = 0 $$</p> <p>Kinetic Energy Symbol</p> <p>Note that the \\( K \\) in the \\( \\mathcal{L}(q, \\dot{q}) = K - U \\) is also represented as \\( T \\).</p> <p>But these remain second-order. The critical leap comes through Legendre Transformation \\( (\\dot{q} \\rightarrow p) \\). The Hamiltonian is derived from the Lagrangian through a Legendre transformation by defining the conjugate momentum  as \\( p_i = \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}_i} \\); then the Hamiltonian can be written as: $$ H(q,p) = \\sum_i p_i \\dot{q}_i - \\mathcal{L}(q, \\dot{q}) $$</p> Annotated \\( H(q,p) \\) \\[ H(q,p) = \\sum_i \\underbrace{p_i}_{\\text{Conjugate Momentum}} \\underbrace{\\dot{q}_i}_{\\text{Generalized Velocity}} - \\underbrace{\\mathcal{L}(q, \\dot{q})}_{\\text{Lagrangian}} \\] <p>We can write \\( H(q,p) \\) more intuitively as: $$ H(q,p) = K(p) + U(q) $$</p> <p>Proof?</p> <p>T is the Kinetic energy; for simplicity, I'll replace it with K.</p> <p>The negative sign arises because we are subtracting the Lagrangian from the sum of the products of momenta and  velocities. This ensures that the Hamiltonian represents the total energy of the system for conservative systems,  where the Lagrangian is \\( K - U \\) and the Hamiltonian becomes \\( K + U \\).</p> <p>For a simple system where \\( K = \\frac{1}{2}m\\dot{q}^2 \\) and \\( U = U(q) \\), the Hamiltonian would be:</p> <ul> <li>Kinetic Energy: \\( K = \\frac{1}{2}m\\dot{q}^2 \\)</li> <li>Potential Energy: \\( U = U(q) \\)</li> <li>Lagrangian: \\( \\mathcal{L} = \\frac{1}{2}m\\dot{q}^2 - U(q) \\)</li> <li>Conjugate Momentum: \\( p = m\\dot{q} \\)</li> <li>Hamiltonian: \\( H = p\\dot{q} - \\mathcal{L} = p\\frac{p}{m} - \\left(\\frac{1}{2}m\\left(\\frac{p}{m}\\right)^2 - U(q)\\right) = \\frac{p^2}{m} - \\frac{p^2}{2m} + U(q) = \\frac{p^2}{2m} + U(q) = K(p) + U(q) \\)</li> </ul> <p>This flips the script: instead of \\( \\dot{q} \\)-centric dynamics, we get symplectic phase flow.</p> <p>Why This Matters</p> <p>The Hamiltonian becomes the system's total energy \\( H = K + U \\) for many physical systems.  It also provides a framework where time evolution is a canonical transformation -  a symmetry preserving the fundamental Poisson bracket structure \\( \\{q_i, p_j\\} = \\delta_{ij} \\).</p> Canonical and Non-Canonical Transformations <p>A canonical transformation is a change of variables that preserves the form of Hamilton's equations. It's like changing the map projection without altering the landscape.</p> <p>Consider a simple translation: $$ Q = q + a, \\quad P = p + b $$ This transformation preserves the Hamiltonian structure and Poisson bracket: \\( \\{Q, P\\} = \\{q + a, p + b\\} = \\{q, p\\} = 1 = \\delta_{ij} \\)</p> <ul> <li>Preserves Hamiltonian structure.</li> <li>Maintains Poisson bracket invariance.</li> <li>Time evolution can be viewed as a canonical transformation.</li> </ul> <p>On the other hand, non-canonical transformation changes the form of Hamilton's equations.</p> <p>For example, consider the transformation:</p> \\[ Q = q^3, \\quad P = p^3 \\] <p>The Poisson bracket is:</p> <p>\\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = 3q^2 \\cdot 3p^2 - 0 = 9q^2p^2 \\neq 1 \\)</p> How to calculate those formula? <p>The Poisson bracket of two functions \\( f \\) and \\( g \\) is defined as: \\( \\{f, g\\} = \\sum_i \\left( \\frac{\\partial f}{\\partial q_i} \\frac{\\partial g}{\\partial p_i} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i} \\right) \\)</p> <p>Transformation 1: \\( Q = q + a, \\quad P = p + b \\)</p> <p>Partial Derivatives: </p> <ul> <li>\\( \\frac{\\partial Q}{\\partial q} = 1 \\)</li> <li>\\( \\frac{\\partial Q}{\\partial p} = 0 \\)</li> <li>\\( \\frac{\\partial P}{\\partial q} = 0 \\)</li> <li>\\( \\frac{\\partial P}{\\partial p} = 1 \\)</li> </ul> <p>These derivatives can be represented in matrix form as: \\( \\frac{\\partial (Q, P)}{\\partial (q, p)} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\)</p> <p>This is a diagonal identity matrix, indicating that the transformation preserves the original structure.</p> <p>Poisson Bracket Calculation \\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = (1)(1) - (0)(0) = 1 \\)</p> <p>Transformation 2: \\( Q = q^3, \\quad P = p^3 \\)</p> <p>Partial Derivatives - \\( \\frac{\\partial Q}{\\partial q} = 3q^2 \\) - \\( \\frac{\\partial Q}{\\partial p} = 0 \\) - \\( \\frac{\\partial P}{\\partial q} = 0 \\) - \\( \\frac{\\partial P}{\\partial p} = 3p^2 \\)</p> <p>These derivatives can be represented as: \\( \\frac{\\partial (Q, P)}{\\partial (q, p)} = \\begin{pmatrix} 3q^2 &amp; 0 \\\\ 0 &amp; 3p^2 \\end{pmatrix} \\)</p> <p>This is a diagonal matrix but not the identity matrix, indicating that the transformation does not preserve the original structure.</p> <p>Poisson Bracket Calculation \\( \\{Q, P\\} = \\frac{\\partial Q}{\\partial q} \\frac{\\partial P}{\\partial p} - \\frac{\\partial Q}{\\partial p} \\frac{\\partial P}{\\partial q} = (3q^2)(3p^2) - (0)(0) = 9q^2p^2 \\)</p> <ul> <li>Transformation 1 preserves the Poisson bracket structure because it results in a constant value of 1, represented by an identity matrix.</li> <li>Transformation 2 does not preserve the Poisson bracket structure because the result depends on \\( q \\) and \\( p \\), represented by a non-identity diagonal matrix.</li> </ul> <p>This transformation is not canonical because it does not preserve the Poisson bracket structure.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#newton-vs-lagrange-vs-hamilton-a-philosophical-showdown","title":"Newton vs. Lagrange vs. Hamilton: A Philosophical Showdown","text":"Aspect Newtonian Lagrangian Hamiltonian State Variables Position \\( x \\) and velocity \\( \\dot{x} \\) Generalized coordinates \\( q \\) and velocities \\( \\dot{q} \\) Generalized coordinates \\( q \\) and conjugate momenta \\( p \\) Formulation Second-order differential equations \\( (F=ma) \\) Principle of least action (\\( \\delta \\int L \\, dt = 0 \\)) First-order differential equations from Hamiltonian function (Phase flow \\( (dH) \\)) Identifying Symmetries Manual identification or through specific methods Noether's theorem Canonical transformations and Poisson brackets Machine Learning Connection Physics-informed neural networks, simulations Optimal control, reinforcement learning Hamiltonian Monte Carlo (HMC) sampling, energy-based models Energy Conservation Not inherent (must be derived) Built-in through conservation laws Central (Hamiltonian is energy) General Coordinates Possible, but often cumbersome Natural fit Natural fit Time Reversibility Yes Yes Yes, especially in symplectic formulations","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltons-equations-the-geometry-of-phase-space","title":"Hamilton's Equations: The Geometry of Phase Space","text":"<p>The phase space is a mathematical space where we can represent the set of possible states of a physical system. For a system with \\( n \\) degrees of freedom, the phase space is a \\( 2n \\)-dimensional space, often visualized as a map where each point \\( (q, p) \\) represents a unique state. The evolution of the system is described by the motion of a point in this space, governed by Hamilton's equations.</p>      Your browser does not support the video tag.        Phase space portrait of a nonlinear pendulum showing oscillatory motion (closed orbits), rotational motion (wavy trajectories), and separatrices (red curves) connecting unstable equilibrium points. Position (q) and momentum (p) dynamics illustrate energy conservation principles fundamental to Hamiltonian systems.   <p>This formulation offers several advantages. It makes it straightforward to identify conserved quantities and symmetries through canonical transformations and Poisson brackets, which provides deeper insights into the system's behavior. For instance, Liouville's theorem states that the volume in phase space occupied by an ensemble of systems remains constant over time, expressed as:</p> \\[ \\frac{\\partial \\rho}{\\partial t} + \\{\\rho, H\\} = 0 \\] <p>or equivalently:</p> \\[ \\frac{\\partial \\rho}{\\partial t} + \\sum_i \\left(\\frac{\\partial \\rho}{\\partial q_i}\\frac{\\partial H}{\\partial p_i} - \\frac{\\partial \\rho}{\\partial p_i}\\frac{\\partial H}{\\partial q_i}\\right) = 0 \\] <p>where \\( \\rho(q, p, t) \\) is the density function. This helps us to represent the phase space flows and how they  preserve area under symplectic transformations. Its relation to symplectic geometry enables mathematical properties that are directly relevant to many numerical methods. For instance, it enables Hamiltonian Monte Carlo to perform well in high-dimensions by defining MCMC strategies that increases the chances of accepting a sample (particle). </p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplecticity-the-sacred-invariant","title":"Symplecticity: The Sacred Invariant","text":"<p>Hamiltonian flows preserve the symplectic 2-form \\( \\omega = \\sum_i dq_i \\wedge dp_i \\). </p> Symplectic 2-form \\( \\omega \\) <p>The symplectic 2-form, denoted by \\( \\omega = \\sum_i dq_i \\wedge dp_i \\), is a mathematical object used in  symplectic geometry. It measures the area of parallelograms formed by vectors in the tangent space of a phase space.</p> <ul> <li>\\( dq_i \\) and \\( dp_i \\): Infinitesimal changes in position and momentum coordinates.</li> <li>\\( \\wedge \\): The wedge product, which combines differential forms in an antisymmetric way meaning that \\( dq_i \\wedge dp_i = -dp_i \\wedge dq_i \\).</li> <li>\\( \\sum_i \\): Sum over all degrees of freedom.</li> </ul> <p>Imagine a phase space where each point represents a state of a physical system. The symplectic form assigns a  value to each pair of vectors, effectively measuring the area of the parallelogram they span. This area is  preserved under Hamiltonian flows.</p> <p>Key Properties</p> <ol> <li>Closed: \\( d\\omega = 0 \\) which means its exterior derivative is zero \\( d\\omega=0 \\). This property ensures that the form does not change under continuous transformations.</li> <li>Non-degenerate: The form is non-degenerate if \\( d\\omega(X,Y)=0 \\) for all \\( Y \\)s, then \\( X=0 \\). This ensures that every vector has a unique \"partner\" vector such that their pairing under \\( \\omega \\) is non-zero.</li> </ol> <p>Example</p> <p>For a simple harmonic oscillator with one degree of freedom, \\( \\omega = dq \\wedge dp \\). This measures the area of parallelograms in the phase space spanned by vectors representing changes in position and momentum.</p> <p>A Very Simplistic PyTorch Code: While PyTorch doesn't directly handle differential forms, you can conceptually represent the symplectic form using tensors:</p> <pre><code>import torch\n\n# Conceptual representation of dq and dp as tensors\ndq = torch.tensor([1.0])  \ndp = torch.tensor([1.0])  \n\n# \"Wedge product\" conceptually represented using an outer product\nomega = torch.outer(dq, dp) - torch.outer(dp, dq)\n\nprint(omega)\n</code></pre> <p>This code illustrates the antisymmetric nature of the wedge product.</p> <p>Numerically, this means good integrators must respect:</p> \\[ \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} = J \\quad \\text{where } J = \\begin{pmatrix} 0 &amp; I \\\\ -I &amp; 0 \\end{pmatrix} \\] Breaking Down the Formula <ul> <li>Geometric numerical integration solves differential equations while preserving geometric properties of the system.</li> <li> <p>Symplecticity is a geometric property inherent to Hamiltonian systems. It ensures that the area of geometric  structures (e.g., parallelograms) in phase space \\( (q, p) \\) remains constant over time. This is encoded in the  symplectic form \\( \\omega = \\sum_i dq_i \\wedge dp_i \\).</p> </li> <li> <p>A numerical method is symplectic if it preserves \\( \\omega \\). The Jacobian matrix of the transformation  from \\( (q(t), p(t)) \\) to \\( (q(t + \\epsilon), p(t + \\epsilon)) \\) must satisfy the condition above.</p> </li> <li> <p>The Jacobian matrix \\( \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\)  quantifies how small changes in the initial state \\( (q(t), p(t)) \\) propagate to the next state \\( (q(t + \\epsilon), p(t + \\epsilon)) \\).</p> </li> <li> <p>\\( q(t) \\) and \\( p(t) \\) : Position and momentum at time \\( t \\).</p> </li> <li>\\( q(t + \\epsilon) \\) and \\( p(t + \\epsilon) \\) : Updated position and momentum after one time step \\( \\epsilon \\).</li> <li>\\( \\frac{\\partial}{\\partial (q(t), p(t))} \\) : Partial derivatives with respect to the initial state.</li> </ul>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#how-are-we-going-to-solve-it","title":"How are We Going to Solve it?","text":"<p>Numerical solvers for differential equations inevitably introduce errors that affect solution accuracy. These errors manifest as deviations from the true trajectory in phase space, particularly noticeable in energy-conserving systems like the harmonic oscillator. The errors fall into two main categories: local truncation error, arising from the approximation of continuous derivatives with discrete steps (proportional to \\( \\mathcal{O}(\\epsilon^n+1) \\) where \\( \\epsilon \\) is the step size and n depends on the method); and global accumulation error, which compounds over integration time.</p> <p>Forward Euler Method Fails at This!</p> <p>To overcome this, we turn to symplectic integrators\u2014methods that respect the underlying geometry of Hamiltonian  systems, leading us naturally to the Leapfrog Verlet method, a powerful symplectic alternative. \ud83d\ude80</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#key-issue-energy-drift-from-non-symplectic-updates","title":"Key Issue: Energy Drift from Non-Symplectic Updates","text":"<p>The forward Euler method (FEM) violates the geometric structure of Hamiltonian systems,  leading to energy drift in long-term simulations. Let\u2019s dissect why.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#forward-euler-in-hamiltonian-systems","title":"Forward Euler in Hamiltonian Systems","text":"<p>For a Hamiltonian \\( H(q, p) \\), the forward Euler updates position and momentum as: \\( q(t + \\epsilon) = q(t) + \\epsilon \\frac{\\partial H}{\\partial p}(q(t), p(t)),\\quad  p(t + \\epsilon) = p(t) - \\epsilon \\frac{\\partial H}{\\partial q}(q(t), p(t)) \\)</p> <p>Unlike Leapfrog Verlet, these updates do not split the position/momentum dependencies, breaking symplecticity.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#step-by-step-failure-harmonic-oscillator-example","title":"Step-by-Step Failure: Harmonic Oscillator Example","text":"<p>Hamiltonian: \\( H = \\frac{1}{2}(q^2 + p^2) \\quad \\text{(mass-spring system with m = k = 1 )} \\)</p> <p>Forward Euler Updates: \\( q(t + \\epsilon) = q(t) + \\epsilon p(t) \\quad \\text{(position update)} \\quad p(t + \\epsilon) = p(t) - \\epsilon q(t) \\quad \\text{(momentum update)} \\)</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#jacobian-matrix-analysis","title":"Jacobian Matrix Analysis","text":"<p>The Jacobian \\( M = \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\) becomes: \\( M = \\begin{pmatrix} 1 &amp; \\epsilon \\\\ -\\epsilon &amp; 1 \\end{pmatrix} \\)</p> <p>Symplectic Condition Check: Does \\( M^T J M = J \\), where \\( J = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\)?</p> <ol> <li> <p>Transpose \\( M^T \\): \\(    M^T = \\begin{pmatrix}    1 &amp; -\\epsilon \\\\    \\epsilon &amp; 1    \\end{pmatrix}    \\)</p> </li> <li> <p>Compute \\( J M \\): \\(       J M = \\begin{pmatrix}       -\\epsilon &amp; 1 \\\\       -1 &amp; -\\epsilon       \\end{pmatrix}       \\)</p> </li> <li> <p>Final Product \\( M^T J M \\): \\(       M^T J M = \\begin{pmatrix}       0 &amp; 1 + \\epsilon^2 \\\\       -1 - \\epsilon^2 &amp; 0       \\end{pmatrix} \\neq J       \\)</p> </li> </ol> <p>The result violates \\( M^T J M = J \\), proving symplecticity fails unless \\( \\epsilon = 0 \\).</p> \\[ \\boxed{\\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\neq J} \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#practical-consequences-energy-drift","title":"Practical Consequences: Energy Drift","text":"<p>Why This Matters</p> <ul> <li>Energy Drift: FEM artificially injects/dissipates energy over time because \\( H(q,p) \\) is not conserved.  </li> <li>Phase Space Distortion: Volume preservation fails, corrupting long-term trajectories.  </li> <li>Unusable for HMC: Sampling in Hamiltonian Monte Carlo relies on symplectic integrators to maintain detailed balance.  </li> </ul> <p>Example: Simulating a harmonic oscillator with FEM shows spiraling/non-closing orbits in phase space, unlike the stable ellipses from Leapfrog Verlet.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplectic-numerical-integrators","title":"Symplectic Numerical Integrators","text":"","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#leapfrog-verlet","title":"Leapfrog Verlet","text":"<p>For a separable Hamiltonian \\( H(q,p) = K(p) + U(q) \\), where the corresponding probability distribution is given by:</p> \\[ P(q,p) = \\frac{1}{Z} e^{-U(q)} e^{-K(p)}, \\] <p>the Leapfrog Verlet integrator proceeds as follows:</p> \\[ \\begin{aligned} p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) &amp;= p_{i}(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t)) \\\\ q_{i}(t + \\epsilon) &amp;= q_{i}(t) + \\epsilon \\frac{\\partial K}{\\partial p_{i}}\\left(p\\left(t + \\frac{\\epsilon}{2}\\right)\\right) \\\\ p_{i}(t + \\epsilon) &amp;= p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t + \\epsilon)) \\end{aligned} \\] <p>This St\u00f6rmer-Verlet scheme preserves symplecticity exactly, with local error \\( \\mathcal{O}(\\epsilon^3) \\) and global  error \\( \\mathcal{O}(\\epsilon^2) \\). You can read more about   numerical methods and analysis in Python here.</p> <p>How Exactly?</p> <ol> <li> <p>Leapfrog Verlet Update Equations For a separable Hamiltonian \\( H(q, p) = K(p) + U(q) \\), the method splits into three component-wise steps:</p> <ol> <li> <p>Half-step momentum update: \\(    p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) = p_{i}(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t))    \\)</p> </li> <li> <p>Full-step position update: \\(    q_{i}(t + \\epsilon) = q_{i}(t) + \\epsilon \\frac{\\partial K}{\\partial p_{i}}\\left(p\\left(t + \\frac{\\epsilon}{2}\\right)\\right)    \\)</p> </li> <li> <p>Full-step momentum update: \\(    p_{i}(t + \\epsilon) = p_{i}\\left(t + \\frac{\\epsilon}{2}\\right) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q_{i}}(q(t + \\epsilon))    \\)</p> </li> </ol> </li> <li> <p>Jacobian Matrix Calculation For the harmonic oscillator \\( H(q, p) = \\frac{1}{2}(q^2 + p^2) \\), the updates simplify to:  </p> \\[ q(t + \\epsilon) = q(t) + \\epsilon p(t) - \\frac{\\epsilon^2}{2} q(t), \\quad p(t + \\epsilon) = p(t) - \\epsilon q(t) - \\frac{\\epsilon^2}{2} p(t). \\] <p>The Jacobian matrix \\( M = \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} \\) becomes: \\( M = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; \\epsilon \\\\ -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix}. \\)</p> </li> <li> <p>Transpose of \\( M \\) The transpose \\( M^T \\) swaps off-diagonal terms: \\( M^T = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; -\\epsilon \\\\ \\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix}. \\)</p> </li> <li> <p>Verify \\( M^T J M = J \\) Let \\( J = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\). Compute \\( M^T J M \\):</p> <ol> <li> <p>Calculate: \\( J M = \\begin{pmatrix} -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\\\ -\\left(1 - \\frac{\\epsilon^2}{2}\\right) &amp; -\\epsilon \\end{pmatrix}. \\)</p> </li> <li> <p>Calculate: \\( M^T J M = \\begin{pmatrix} 1 - \\frac{\\epsilon^2}{2} &amp; -\\epsilon \\\\ \\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\end{pmatrix} \\begin{pmatrix} -\\epsilon &amp; 1 - \\frac{\\epsilon^2}{2} \\\\ -\\left(1 - \\frac{\\epsilon^2}{2}\\right) &amp; -\\epsilon \\end{pmatrix}. \\)</p> </li> </ol> <p>After matrix multiplication: \\( M^T J M = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} = J. \\)</p> </li> </ol> <p>The Leapfrog Verlet method satisfies \\( M^T J M = J \\), proving it preserves the symplectic structure. This matches its theoretical property as a symplectic integrator.</p> \\[ \\boxed{\\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))}^T J \\frac{\\partial (q(t + \\epsilon), p(t + \\epsilon))}{\\partial (q(t), p(t))} = J} \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#why-symplectic-matters","title":"Why Symplectic Matters","text":"<p>They're the reversible neural nets of physics simulations!</p> <p>Symplectic integrators like Leapfrog Verlet are critical for long-term stability in Hamiltonian systems.  </p> <ul> <li>Phase space preservation: The volume in \\( (q, p) \\)-space is conserved exactly, avoiding artificial energy drift.  </li> <li>Approximate energy conservation: While energy \\( H(q,p) \\) is not perfectly conserved (due to \\( \\mathcal{O}(\\epsilon^2) \\) error), it oscillates near the true value over exponentially long timescales.  </li> <li>Practical relevance: This makes symplectic integrators indispensable in molecular dynamics and Hamiltonian Monte Carlo (HMC), where accurate sampling relies on stable trajectories.  </li> </ul>  Comparison of numerical integration methods for a simple harmonic oscillator in phase space. Color gradients  indicate error magnitude with brighter colors showing larger divergence from the exact solution (white).  Euler's method (a) exhibits energy growth, Modified Euler's method (b) shows improved stability, while  Leapfrog maintains excellent energy conservation at small stepsize (c) but develops geometric distortion  at larger stepsize (d).  <p>Euler's method (first-order) systematically injects energy into the system, causing the characteristic outward spiral seen in the plots. Modified Euler's method (second-order) significantly reduces this energy drift. Most importantly, symplectic integrators like the Leapfrog method preserve the geometric structure of Hamiltonian systems even with relatively large step sizes by maintaining phase space volume conservation. This structural preservation is why Leapfrog remains the preferred method for long-time simulations in molecular dynamics and astronomy, where energy conservation is critical despite the visible polygon-like discretization artifacts at large step sizes.</p> <p>Non-symplectic methods (e.g., Euler-Maruyama) often fail catastrophically in these settings.</p> Integrator Symplecticity Order Type Local Error Global Error Suitable For Computational Cost Euler Method 1 Explicit O(\u03b5\u00b2) O(\u03b5) Quick prototypes and Short-term simulations of general ODEs Low Symplectically Euler 1 Explicit O(\u03b5\u00b2) O(\u03b5) Simple Hamiltonian systems Low Leapfrog (Verlet) 2 Explicit O(\u03b5\u00b3) O(\u03b5\u00b2) Molecular dynamics, Long-term simulations of Hamiltonian systems Moderate Runge-Kutta 4 4 Explicit O(\u03b5\u2075) O(\u03b5\u2074) Short-term accuracy, General ODEs, but not recommended for long-term Hamiltonian systems High Forest-Ruth Integrator 4 Explicit O(\u03b5\u2075) O(\u03b5\u2074) High-accuracy long-term simulations High Yoshida 6<sup>th</sup>-order 6 Explicit O(\u03b5\u2077) O(\u03b5\u2076) High-accuracy High Heun\u2019s Method (RK2) 2 Explicit O(\u03b5\u00b3) O(\u03b5\u00b2) General ODEs requiring moderate accuracy Moderate Third-order Runge-Kutta 3 Explicit O(\u03b5\u2074) O(\u03b5\u00b3) When higher accuracy than RK2 is needed without the cost of RK4 High Implicit Midpoint Rule 2 Implicit (solving equations) O(\u03b5\u00b3) O(\u03b5\u00b2) Hamiltonian systems, stiff problems High Fourth-order Adams-Bashforth 4 Multi-step (explicit) O(\u03b5\u2075) O(\u03b5\u2074) Non-stiff problems with smooth solutions, after initial steps Low Backward Euler Method 1 Implicit (solving equations) O(\u03b5\u00b2) O(\u03b5) Stiff problems, where stability is crucial High","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":"<p>Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that leverages Hamiltonian dynamics to  efficiently sample from complex probability distributions, particularly in Bayesian statistics and machine learning.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#from-phase-space-to-probability-space","title":"From Phase Space to Probability Space","text":"<p>HMC interprets target distribution \\( P(z) \\) as a Boltzmann distribution:</p> \\[ P(z) = \\frac{1}{Z} e^{\\frac{-E(z)}{T}} \\] <p>Substituting into this formulation, the Hamiltonian gives us a joint density:</p> \\[  P(q,p) = \\frac{1}{Z} e^{-U(q)} e^{-K(p)} \\quad \\text{where } U(q) = -\\log[p(q), p(q|D)] \\] <p>where \\( p(q|D) \\) is the likelihood of the given data \\( D \\) and T=1 and therefore removed. We estimate our posterior distribution using the potential energy \\( U(q) \\) since \\( P(q,p) \\) consists of two independent probability distributions.</p> <p>Augment with artificial momentum \\( p \\sim \\mathcal{N}(0,M) \\), then simulate Hamiltonian dynamics to propose new \\( q' \\)  based on the distribution of the position variables \\( U(q) \\) which acts as the \"potential energy\" of the target distribution \\( P(q) \\), thereby creating valleys at high-probability regions.</p> <p>For more on HMC, check out this explanation or  this tutorial.</p> <ul> <li>Physical Systems: \\( H(q,p) = U(q) + K(p) \\) represents total energy  </li> <li>Sampling Systems: \\( H(q,p) = -\\log P(q) + \\frac{1}{2}p^T M^{-1} p \\) defines exploration dynamics  </li> </ul> <p>The kinetic energy with the popular form of \\( K(p) = \\frac{1}{2}p^T M^{-1} p \\), often Gaussian,  injects momentum to traverse these landscapes. Crucially, the mass matrix \\( M \\) plays the role of a  preconditioner - diagonal \\( M \\) adapts to parameter scales, while dense \\( M \\) can align with correlation  structure. \\( M \\) is symmetric, positive definite and typically diagonal.</p> <p>What is Positive Definite?</p> <p>Positive Definite: For any non-zero vector \\( x \\), the expression \\( x^T M x \\) is always positive. This ensures stability and efficiency.</p> <p></p>      Illustration of different quadratic forms in two variables that shows how different covariance matrices      influence the shape of these forms. The plots depict:     a) Positive Definite Form: A bowl-shaped surface where all eigenvalues are positive, indicating a minimum.     b) Negative Definite Form: An inverted bowl where all eigenvalues are negative, indicating a maximum.     c) Indefinite Form: A saddle-shaped surface with both positive and negative eigenvalues, indicating neither a maximum nor a minimum.     Each subplot includes the matrix \\( M \\) and the corresponding quadratic form \\( Q(x) = x^T M x \\).  <p></p> \\[ x^T M x &gt; 0 \\] Kinetic Energy Choices <ul> <li>Gaussian (Standard HMC): \\( K(p) = \\frac{1}{2}p^T M^{-1} p \\)   Yields Euclidean trajectories, efficient for moderate dimensions.  </li> <li>Relativistic (Riemannian HMC): \\( K(p) = \\sqrt{p^T M^{-1} p + c^2} \\)   Limits maximum velocity, preventing divergences in ill-conditioned spaces.  </li> <li>Adaptive (Surrogate Gradients): Learn \\( K(p) \\) via neural networks to match target geometry.</li> </ul> <p>Key Intuition</p> <p>The Hamiltonian \\( H(q,p) = U(q) + \\frac{1}{2}p^T M^{-1} p \\) creates an energy landscape where momentum carries  the sampler through high-probability regions, avoiding random walk behavior.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#the-hmc-algorithm","title":"The HMC Algorithm","text":"<p>The algorithm involves:</p> <ol> <li> <p>Initialization: Start with an initial position \\( q_0 \\) and sample momentum \\( p_0 \\sim \\mathcal{N}(0,M) \\).</p> </li> <li> <p>Leapfrog Integration: Use the leapfrog method to approximate Hamiltonian dynamics. For a step size \\( \\epsilon \\) and L steps, update:</p> </li> <li> <p>Half-step momentum: \\( p(t + \\frac{\\epsilon}{2}) = p(t) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t)) \\)</p> </li> <li>Full-step position: \\( q(t + \\epsilon) = q(t) + \\epsilon \\frac{\\partial K}{\\partial p}(p(t + \\frac{\\epsilon}{2})) \\), where \\( K(p) = \\frac{1}{2} p^T M^{-1} p \\), so \\( \\frac{\\partial K}{\\partial p} = M^{-1} p \\)</li> <li>Full-step momentum: \\( p(t + \\epsilon) = p(t + \\frac{\\epsilon}{2}) - \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t + \\epsilon)) \\)</li> </ol> <p>This is repeated L times to get proposed \\( \\dot{q} \\) and \\( \\dot{p} \\).</p> <ol> <li>Metropolis-Hastings Acceptance: Accept the proposed \\( \\dot{q} \\) with probability \\( \\min(1, e^{H(q_0,p_0) - H(\\dot{q},\\dot{p})}) \\), where \\( H(q,p) = U(q) + K(p) \\).</li> </ol> <p>This process generates a Markov chain with stationary distribution \\( P(q) \\), leveraging Hamiltonian dynamics to take larger, more efficient steps compared to random-walk methods.</p> Why Better Than Random Walk? <p>HMC navigates high-dimensional spaces along energy contours -  like following mountain paths instead of wandering randomly!</p> Recap of the Hamilton's equations? \\[ \\begin{cases} \\dot{q} = \\nabla_p K(p) = M^{-1}p &amp; \\text{(Guided exploration)} \\\\ \\dot{p} = -\\nabla_q U(q) = \\nabla_q \\log P(q) &amp; \\text{(Bayesian updating)} \\end{cases} \\] <p>This coupled system drives \\( (q,p) \\) along iso-probability contours of \\( P(q) \\), with momentum rotating rather  than resetting at each step like in Random Walk Metropolis--think of following mountain paths instead of wandering randomly! The key parameters - integration time \\( \\tau = L\\epsilon \\) and step size \\( \\epsilon \\) - balance exploration vs. computational cost:  </p> <ul> <li>Short \\( \\tau \\): Local exploration, higher acceptance  </li> <li>Long \\( \\tau \\): Global moves, risk of U-turns (periodic orbits)  </li> </ul> <p>Key Parameters and Tuning</p> <p>Tuning \\( M \\) to match the covariance of \\( P(q) \\) (e.g., via warmup adaptation) and setting \\( \\tau \\sim \\mathcal{O}(1/\\lambda_{\\text{max}}) \\), where \\( \\lambda_{\\text{max}} \\) is the largest eigenvalue of \\( \\nabla^2 U \\), often yields optimal mixing.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#connection-with-energy-based-models","title":"Connection with Energy-Based Models","text":"<p>Energy-based models (EBMs) are a class of generative models that define a probability distribution over data points  using an energy function. The probability of a data point is proportional to \\( e^{-E(x)} \\), where \\( E(x) \\) is the  energy function. This formulation is directly analogous to the Boltzmann distribution in statistical physics, where the  probability is related to the energy of a state. In Hamiltonian mechanics, the Hamiltonian function \\( H(q, p) \\)  represents the total energy of the system, and the probability distribution in phase space is given  by \\( e^{-H(q,p)/T} \\), where \\( T \\) is the temperature.</p> <p>In EBMs, Hamiltonian Monte Carlo (HMC) is often used to sample from the model's distribution. HMC leverages  Hamiltonian dynamics to propose new states, which are then accepted or rejected based on the Metropolis-Hastings  criterion. This method is particularly effective for high-dimensional problems, as it reduces the correlation between  samples and allows for more efficient exploration of the state space. For instance, in image generation tasks, HMC  can sample from the distribution defined by the energy function, facilitating the generation of high-quality images.</p> <p>EBMs define probability through Hamiltonians:</p> \\[ p(x) = \\frac{1}{Z}e^{-E(x)} \\quad \\leftrightarrow \\quad H(q,p) = E(q) + K(p) \\]","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#potential-research-directions","title":"Potential Research Directions","text":"","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#symplecticity-in-machine-learning-models","title":"Symplecticity in Machine Learning Models","text":"Overview of the Hamiltonian Neural Networks architecture. Image from the HNN paper. <p>Incorporate the symplectic structure of Hamiltonian mechanics into machine learning models to preserve properties  like energy conservation, which is crucial for long-term predictions. Generalizing Hamiltonian Neural Networks (HNNs),  as discussed in Hamiltonian Neural Networks, to more complex systems or developing new architectures that preserve symplecticity</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hmc-for-complex-distributions","title":"HMC for Complex Distributions:","text":"<p>HMC for sampling from complex, high-dimensional, and multimodal distributions, such as those encountered in deep learning. Combining HMC with other techniques, like parallel tempering, could handle distributions with multiple modes more  effectively.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#combining-hamiltonian-mechanics-with-other-ml-techniques","title":"Combining Hamiltonian Mechanics with Other ML Techniques:","text":"<p>Integrate Hamiltonian mechanics with reinforcement learning to guide exploration in continuous state and action spaces. Using it to model the environment could improve exploration strategies, as seen in potential applications in robotics.  Additionally, using Hamiltonian mechanics to define approximate posteriors in variational inference could lead to more  flexible and accurate approximations.   </p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#hamiltonian-gans","title":"Hamiltonian GANs","text":"<p>Employing Hamiltonian formalism as an inductive bias for the generation of physically plausible videos with neural networks. Imagine generator-discriminator dynamics governed by:</p> <p>The possibilities make phase space feel infinite...</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#want-to-team-up-on-this","title":"Want to Team Up on This \ud83e\udd13","text":"<p>If you are an ML researcher and are interested in collaborating on researching EBMs,  diffusion- and flow-based models, or have other relevant ideas in mind for generalization over out-of-distribution  data (downstream tasks can be anything in from molecular design to robotics motion planning to LLMs), please feel free to reach out!</p> <p>Also follow me on  Twitter /  BlueSky or  GitHub\u2014I\u2019m usually rambling about this stuff there.  Also on LinkedIn and  Medium /  TDS if you\u2019re curious.  To find more about my research interests, check out my  personal website.</p>","tags":["hamiltonian","sampling"]},{"location":"blog/hamiltonian-mechanics/#useful-links","title":"Useful Links","text":"<ul> <li>An Introduction to Multistep Methods: Leap-frog</li> <li>The beginners guide to Hamiltonian Monte Carlo</li> <li>Hamiltonian Monte Carlo</li> <li>Hamiltonian Monte Carlo - Stan - Stan explained</li> <li>Hamiltonian Mechanics For Dummies: An Intuitive Introduction.</li> <li>Hamiltonian mechanics Wikipedia page</li> <li>An introduction to Lagrangian and Hamiltonian mechanics Lecture notes</li> <li> <p>Hamiltonian Mechanics - Jeremy Tatum, University of Victoria</p> </li> <li> <p>Hamiltonian Neural Networks - Blog</p> </li> <li>Hamiltonian Neural Networks</li> <li>Other: Natural Intelligence - A blog by Sam Greydanus - Many interesting topics </li> </ul>","tags":["hamiltonian","sampling"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/","title":"Langevin Dynamics Sampling with TorchEBM","text":"","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#langevin-dynamics-sampling-with-torchebm","title":"Langevin Dynamics Sampling with TorchEBM","text":"<p>Langevin dynamics is a powerful sampling technique that allows us to draw samples from complex probability distributions. In this tutorial, we'll explore how to use TorchEBM's implementation of Langevin dynamics for sampling from various energy landscapes.</p>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#basic-example-sampling-from-a-2d-gaussian","title":"Basic Example: Sampling from a 2D Gaussian","text":"<p>Let's start with a simple example of sampling from a 2D Gaussian distribution:</p> Basic Langevin Dynamics Sampling<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create energy function for a 2D Gaussian\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndim = 2  # dimension of the state space\nn_steps = 100  # steps between samples\nn_samples = 1000  # num of samples\nmean = torch.tensor([1.0, -1.0])\ncov = torch.tensor([[1.0, 0.5], [0.5, 2.0]])\nenergy_fn = GaussianEnergy(mean, cov, device=device)\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=0.1,\n    device=device,\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = sampler.sample(\n    x=initial_state,\n    n_steps=n_steps,\n    n_samples=n_samples,\n)\n\n# Plot results\nsamples = samples.cpu().numpy()\nplt.figure(figsize=(10, 5))\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\nplt.title(\"Samples from 2D Gaussian using Langevin Dynamics\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.show()\n</code></pre>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#advanced-example-double-well-potential","title":"Advanced Example: Double Well Potential","text":"<p>For a more interesting example, let's sample from a double well potential, which has two local minima:</p> Double Well Energy Sampling<pre><code>from torchebm.core import DoubleWellEnergy\n\n# Create energy function and sampler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.001,\n    noise_scale=0.1,\n    decay=0.1,  # for stability\n    device=device,\n)\n\n# Generate trajectory with diagnostics\ninitial_state = torch.tensor([0.0], device=device)\ntrajectory, diagnostics = sampler.sample(\n    x=initial_state,\n    n_steps=1000,\n    return_trajectory=True,\n    return_diagnostics=True,\n)\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot trajectory\nax1.plot(trajectory[0, :, 0].cpu().numpy())\nax1.set_title(\"Single Chain Trajectory\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Position\")\n\n# Plot energy over time\nax2.plot(diagnostics[:, 2, 0, 0].cpu().numpy())\nax2.set_title(\"Energy Evolution\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Energy\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#key-benefits-of-torchebms-langevin-dynamics-implementation","title":"Key Benefits of TorchEBM's Langevin Dynamics Implementation","text":"<ol> <li>GPU Acceleration - Sampling is performed efficiently on GPUs when available</li> <li>Flexible API - Easy to use with various energy functions and initialization strategies</li> <li>Diagnostic Tools - Track energy, gradient norms, and acceptance rates during sampling</li> <li>Configurable Parameters - Fine-tune step size, noise scale, and decay for optimal performance</li> </ol>","tags":["langevin","sampling","tutorial"]},{"location":"blog/langevin-dynamics-sampling-with-torchebm/#conclusion","title":"Conclusion","text":"<p>Langevin dynamics is a versatile sampling method for energy-based models, and TorchEBM makes it easy to use in your projects. Whether you're sampling from simple analytical distributions or complex neural network energy functions, the same API works seamlessly.</p> <p>Stay tuned for more tutorials on other samplers and energy functions! </p><p></p>","tags":["langevin","sampling","tutorial"]},{"location":"developer_guide/","title":"Developer Guide","text":""},{"location":"developer_guide/#torchebm-developer-guide","title":"TorchEBM Developer Guide","text":"<p>Welcome to the TorchEBM developer guide! This comprehensive resource is designed to help you understand the project's architecture, contribute effectively, and follow best practices when working with the codebase.</p>"},{"location":"developer_guide/#getting-started-with-development","title":"Getting Started with Development","text":"<ul> <li> <p> Development Setup</p> <p>Set up your development environment and prepare for contribution.</p> <p> Development Setup</p> </li> <li> <p> Code Style</p> <p>Learn about the coding standards and style guidelines.</p> <p> Code Style</p> </li> <li> <p> Testing Guide</p> <p>Discover how to write and run tests for TorchEBM.</p> <p> Testing Guide</p> </li> <li> <p>:material-git-commit:{ .lg .middle } Commit Conventions</p> <p>Understand the commit message format and conventions.</p> <p> Commit Conventions</p> </li> </ul>"},{"location":"developer_guide/#understanding-the-architecture","title":"Understanding the Architecture","text":"<ul> <li> <p> Project Structure</p> <p>Explore the organization of the TorchEBM codebase.</p> <p> Project Structure</p> </li> <li> <p> Design Principles</p> <p>Learn about the guiding principles behind TorchEBM.</p> <p> Design Principles</p> </li> <li> <p> Core Components</p> <p>Understand the core components and their interactions.</p> <p> Core Components</p> </li> </ul>"},{"location":"developer_guide/#implementation-details","title":"Implementation Details","text":"<ul> <li> <p> Energy Functions</p> <p>Detailed information about energy function implementations.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Understand how samplers are implemented in TorchEBM.</p> <p> Samplers</p> </li> <li> <p> Loss Functions</p> <p>Learn about the implementation of various loss functions.</p> <p> BaseLoss Functions</p> </li> <li> <p> Model Architecture</p> <p>Explore the implementation of neural network models.</p> <p> Model Architecture</p> </li> <li> <p>:material-gpu:{ .lg .middle } CUDA Optimizations</p> <p>Discover performance optimizations using CUDA.</p> <p> CUDA Optimizations</p> </li> </ul>"},{"location":"developer_guide/#contributing","title":"Contributing","text":"<ul> <li> <p> Contributing</p> <p>Guidelines for contributing to TorchEBM.</p> <p> Contributing</p> </li> <li> <p> API Design</p> <p>Learn about API design principles in TorchEBM.</p> <p> API Design</p> </li> <li> <p> API Generation</p> <p>Understand how API documentation is generated.</p> <p> API Generation</p> </li> <li> <p> Performance</p> <p>Best practices for optimizing performance.</p> <p> Performance</p> </li> </ul>"},{"location":"developer_guide/#contributing-process","title":"Contributing Process","text":"<p>The general process for contributing to TorchEBM involves:</p> <ol> <li> <p>Set up the development environment: Follow the development setup guide to prepare your workspace.</p> </li> <li> <p>Understand the codebase: Familiarize yourself with the project structure, design principles, and core components.</p> </li> <li> <p>Make changes: Implement your feature or bug fix, following the code style guidelines.</p> </li> <li> <p>Write tests: Add tests for your changes as described in the testing guide.</p> </li> <li> <p>Submit a pull request: Follow the contributing guidelines to submit your changes.</p> </li> </ol> <p>We welcome contributions from the community and are grateful for your help in improving TorchEBM!</p>"},{"location":"developer_guide/#development-philosophy","title":"Development Philosophy","text":"<p>TorchEBM aims to be:</p> <ul> <li>Modular: Components should be easy to combine and extend</li> <li>Performant: Critical operations should be optimized for speed</li> <li>User-friendly: APIs should be intuitive and well-documented</li> <li>Well-tested: Code should be thoroughly tested to ensure reliability</li> </ul> <p>Learn more about our Design Principles.</p>"},{"location":"developer_guide/#quick-reference","title":"Quick Reference","text":"<ul> <li>Installation for development: <code>pip install -e \".[dev]\"</code></li> <li>Run tests: <code>pytest</code></li> <li>Check code style: <code>black torchebm/ &amp;&amp; isort torchebm/ &amp;&amp; flake8 torchebm/</code></li> <li>Build documentation: <code>mkdocs serve</code></li> <li>Pre-commit hooks: <code>pre-commit install</code></li> </ul>"},{"location":"developer_guide/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during development, you can:</p> <ul> <li>Open an issue on GitHub</li> <li>Ask questions in the GitHub Discussions</li> <li>Reach out to maintainers via email or GitHub </li> </ul>"},{"location":"developer_guide/api_design/","title":"API Design","text":""},{"location":"developer_guide/api_design/#api-design","title":"API Design","text":"<p>This document outlines the design principles and patterns used in TorchEBM's API, providing guidelines for contributors and insights for users building on top of the library.</p>"},{"location":"developer_guide/api_design/#api-design-philosophy","title":"API Design Philosophy","text":"<p>Design Goals</p> <p>TorchEBM's API is designed with these goals in mind:</p> <ol> <li>Intuitive: APIs should be easy to understand and use</li> <li>Consistent: Similar operations should have similar interfaces</li> <li>Pythonic: Follow Python conventions and best practices</li> <li>Flexible: Allow for customization and extension</li> <li>Type-Safe: Use type hints for better IDE support and error checking</li> </ol>"},{"location":"developer_guide/api_design/#core-abstractions","title":"Core Abstractions","text":"<ul> <li> <p> Energy Functions</p> <p>Energy functions define the energy landscape that characterizes a probability distribution.</p> <pre><code>class BaseEnergyFunction(nn.Module):\n    def forward(self, x):\n        # Return energy values for inputs x\n        pass\n\n    def gradient(self, x):\n        # Return energy gradients for inputs x\n        pass\n</code></pre> </li> <li> <p> Samplers</p> <p>Samplers generate samples from energy functions via various algorithms.</p> <pre><code>class BaseSampler:\n    def __init__(self, energy_function, device=\"cpu\"):\n        self.energy_function = energy_function\n        self.device = device\n\n    def sample_chain(self, dim, n_steps, n_samples=1):\n        # Generate samples\n        pass\n</code></pre> </li> <li> <p> Loss Functions</p> <p>BaseLoss functions are used to train energy-based models, often using samplers.</p> <pre><code>class ContrastiveDivergence(nn.Module):\n    def __init__(self, energy_fn, sampler, mcmc_steps=1):\n        self.energy_fn = energy_fn\n        self.sampler = sampler\n        self.mcmc_steps = mcmc_steps\n\n    def forward(self, data_samples):\n        # Compute loss\n        pass\n</code></pre> </li> <li> <p> Models</p> <p>Neural network models that can be used as energy functions.</p> <pre><code>class BaseModel(BaseEnergyFunction):\n    def __init__(self):\n        super().__init__()\n        # Define model architecture\n\n    def forward(self, x):\n        # Compute energy\n        pass\n</code></pre> </li> </ul>"},{"location":"developer_guide/api_design/#interface-design-patterns","title":"Interface Design Patterns","text":""},{"location":"developer_guide/api_design/#method-naming-conventions","title":"Method Naming Conventions","text":"<p>TorchEBM follows consistent naming patterns:</p> Pattern Example Purpose <code>forward()</code> <code>energy_fn.forward(x)</code> Core computation (energy) <code>gradient()</code> <code>energy_fn.gradient(x)</code> Compute gradients <code>sample_chain()</code> <code>sampler.sample_chain(dim, n_steps)</code> Generate samples <code>step()</code> <code>sampler.step(x)</code> Single sampling step"},{"location":"developer_guide/api_design/#parameter-ordering","title":"Parameter Ordering","text":"<p>Parameters follow a consistent ordering pattern:</p> <ol> <li>Required parameters (e.g., input data, dimensions)</li> <li>Algorithm-specific parameters (e.g., step size, number of steps)</li> <li>Optional parameters with defaults (e.g., device, random seed)</li> </ol>"},{"location":"developer_guide/api_design/#return-types","title":"Return Types","text":"<p>Return values are consistently structured:</p> <ul> <li>Single values are returned directly</li> <li>Multiple return values use tuples</li> <li>Complex returns use dictionaries for named access</li> <li>Diagnostic information is returned in a separate dictionary</li> </ul> <p>Example:</p> <pre><code># Return samples and diagnostics\nsamples, diagnostics = sampler.sample(dim=2, n_steps=100)\n\n# Access diagnostic information\nacceptance_rate = diagnostics['acceptance_rate']\nenergy_trajectory = diagnostics['energy_trajectory']\n</code></pre>"},{"location":"developer_guide/api_design/#extension-patterns","title":"Extension Patterns","text":""},{"location":"developer_guide/api_design/#subclassing-base-classes","title":"Subclassing Base Classes","text":"<p>The primary extension pattern is to subclass the appropriate base class:</p> <pre><code>class MyCustomSampler(BaseSampler):\n    def __init__(self, energy_function, special_param, device=\"cpu\"):\n        super().__init__(energy_function, device)\n        self.special_param = special_param\n\n    def sample_chain(self, x, step_idx=None):\n        # Custom sampling logic\n        return x_new, diagnostics\n</code></pre>"},{"location":"developer_guide/api_design/#composition-pattern","title":"Composition Pattern","text":"<p>For more complex extensions, composition can be used:</p> <pre><code>class HybridSampler(BaseSampler):\n    def __init__(self, energy_function, sampler1, sampler2, switch_freq=10):\n        super().__init__(energy_function)\n        self.sampler1 = sampler1\n        self.sampler2 = sampler2\n        self.switch_freq = switch_freq\n\n    def sample_chain(self, x, step_idx=None):\n        # Choose sampler based on step index\n        if step_idx % self.switch_freq &lt; self.switch_freq // 2:\n            return self.sampler1.step(x, step_idx)\n        else:\n            return self.sampler2.step(x, step_idx)\n</code></pre>"},{"location":"developer_guide/api_design/#configuration-and-customization","title":"Configuration and Customization","text":""},{"location":"developer_guide/api_design/#constructor-parameters","title":"Constructor Parameters","text":"<p>Features are enabled and configured primarily through constructor parameters:</p> <pre><code># Configure through constructor parameters\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=0.1,\n    thinning=5,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n</code></pre>"},{"location":"developer_guide/api_design/#method-parameters","title":"Method Parameters","text":"<p>Runtime behavior is controlled through method parameters:</p> <pre><code># Control sampling behavior through method parameters\nsamples = sampler.sample(\n    dim=2,\n    n_steps=1000,\n    n_samples=100,\n    initial_samples=None,  # If None, random initialization\n    burn_in=100,\n    verbose=True\n)\n</code></pre>"},{"location":"developer_guide/api_design/#handling-errors-and-edge-cases","title":"Handling Errors and Edge Cases","text":"<p>TorchEBM follows these practices for error handling:</p> <ol> <li>Input Validation: Validate inputs early and raise descriptive exceptions</li> <li>Graceful Degradation: Fall back to simpler algorithms when necessary</li> <li>Informative Exceptions: Provide clear error messages with suggestions</li> <li>Default Safety: Choose safe default values that work in most cases</li> </ol> <p>Example: </p><pre><code>def sample_chain(self, dim, n_steps, n_samples=1):\n    if n_steps &lt;= 0:\n        raise ValueError(\"k_steps must be positive\")\n\n    if dim &lt;= 0:\n        raise ValueError(\"dim must be positive\")\n\n    # Implementation\n</code></pre><p></p>"},{"location":"developer_guide/api_design/#api-evolution-guidelines","title":"API Evolution Guidelines","text":"<p>When evolving the API, we follow these guidelines:</p> <ol> <li>Backward Compatibility: Avoid breaking changes when possible</li> <li>Deprecation Cycle: Use deprecation warnings before removing features</li> <li>Default Arguments: Add new parameters with sensible defaults</li> <li>Feature Flags: Use boolean flags to enable/disable new features</li> </ol> <p>Example of deprecation: </p><pre><code>def old_method(self, param):\n    warnings.warn(\n        \"old_method is deprecated and will be removed in a future version. \"\n        \"Use new_method instead.\",\n        DeprecationWarning,\n        stacklevel=2\n    )\n    return self.new_method(param)\n</code></pre><p></p>"},{"location":"developer_guide/api_design/#documentation-standards","title":"Documentation Standards","text":"<p>All APIs should include:</p> <ul> <li>Docstrings for all public classes and methods</li> <li>Type hints for parameters and return values</li> <li>Examples showing common usage patterns</li> <li>Notes on performance implications</li> <li>References to relevant papers or algorithms</li> </ul> <p>Example: </p><pre><code>def sample_chain(\n    self, \n    dim: int, \n    n_steps: int, \n    n_samples: int = 1\n) -&gt; Tuple[torch.Tensor, Dict[str, Any]]:\n    \"\"\"\n    Generate samples using Langevin dynamics.\n\n    Args:\n        dim: Dimensionality of samples\n        n_steps: Number of sampling steps\n        n_samples: Number of parallel chains\n\n    Returns:\n        tuple: (samples, diagnostics)\n            - samples: Tensor of batch_shape [n_samples, dim]\n            - diagnostics: Dict with sampling statistics\n\n    Example:\n        &gt;&gt;&gt; energy_fn = GaussianEnergy(torch.zeros(2), torch.eye(2))\n        &gt;&gt;&gt; sampler = LangevinDynamics(energy_fn, step_size=0.1)\n        &gt;&gt;&gt; samples, _ = sampler.sample_chain(dim=2, k_steps=100, n_samples=10)\n    \"\"\"\n    # Implementation\n</code></pre><p></p> <p>By following these API design principles, TorchEBM maintains a consistent, intuitive, and extensible interface for energy-based modeling in PyTorch. </p>"},{"location":"developer_guide/api_generation/","title":"API Generation","text":""},{"location":"developer_guide/api_generation/#developer-guide-for-torchebm","title":"Developer Guide for <code>torchebm</code>","text":""},{"location":"developer_guide/api_generation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Setting Up the Development Environment</li> <li>Prerequisites</li> <li>Cloning the Repository</li> <li>Installing Dependencies</li> <li>Code Style and Quality</li> <li>Code Formatting</li> <li>Linting</li> <li>Type Checking</li> <li>Testing</li> <li>Documentation</li> <li>Docstring Conventions</li> <li>API Documentation Generation</li> <li>Contributing</li> <li>Branching Strategy</li> <li>Commit Messages</li> <li>Pull Requests</li> <li>Additional Resources</li> </ol>"},{"location":"developer_guide/api_generation/#introduction","title":"Introduction","text":"<p>Welcome to the developer guide for <code>torchebm</code>, a Python library focused on components and algorithms for energy-based models. This document provides instructions and best practices for contributing to the project.</p>"},{"location":"developer_guide/api_generation/#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":""},{"location":"developer_guide/api_generation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed:</p> <ul> <li>Python: Version 3.9 or higher.</li> <li>Git: For version control.</li> <li>pip: Python package installer.</li> </ul>"},{"location":"developer_guide/api_generation/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Clone the repository using SSH:</p> <pre><code>git clone git@github.com:soran-ghaderi/torchebm.git\n</code></pre> <p>Or using HTTPS:</p> <pre><code>git clone https://github.com/soran-ghaderi/torchebm.git\n</code></pre> <p>Navigate to the project directory:</p> <pre><code>cd torchebm\n</code></pre>"},{"location":"developer_guide/api_generation/#installing-dependencies","title":"Installing Dependencies","text":"<p>Install the development dependencies:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>This command installs the package in editable mode along with all development dependencies specified in the <code>pyproject.toml</code> file.</p>"},{"location":"developer_guide/api_generation/#code-style-and-quality","title":"Code Style and Quality","text":"<p>Maintaining a consistent code style ensures readability and ease of collaboration.</p> <p>To streamline code formatting and quality checks in your development workflow, integrating tools like Black, isort, and mypy directly into your Integrated Development Environment (IDE) can be highly beneficial. Many modern IDEs, such as PyCharm, offer built-in support or plugins for these tools, enabling automatic code formatting and linting as you write.</p> <p>PyCharm Integration:</p> <ul> <li> <p>Black Integration: Starting from PyCharm 2023.2, Black integration is built-in. To enable it:</p> </li> <li> <p>Navigate to <code>Preferences</code> or <code>Settings</code> &gt; <code>Tools</code> &gt; <code>Black</code>.</p> </li> <li>Configure the settings as desired.</li> <li> <p>Ensure Black is installed in your environment:</p> <pre><code>pip install black\n</code></pre> </li> <li> <p>isort Integration: While PyCharm doesn't have built-in isort support, you can set it up using File Watchers:</p> </li> <li> <p>Install the File Watchers plugin if it's not already installed.</p> </li> <li>Navigate to <code>Preferences</code> or <code>Settings</code> &gt; <code>Tools</code> &gt; <code>File Watchers</code>.</li> <li> <p>Add a new watcher for isort with the following configuration:</p> <ul> <li>File Type: Python</li> <li>Scope: Current File</li> <li>Program: Path to isort executable (e.g., <code>$PyInterpreterDirectory$/isort</code>)</li> <li>Arguments: <code>$FilePath$</code></li> <li>Output Paths to Refresh: <code>$FilePath$</code></li> <li>Working Directory: <code>$ProjectFileDir$</code></li> </ul> </li> <li> <p>mypy Integration: To integrate mypy:</p> </li> <li> <p>Install mypy in your environment:</p> <pre><code>pip install mypy\n</code></pre> </li> <li> <p>Set up a File Watcher in PyCharm similar to isort, replacing the program path with the mypy executable path.</p> </li> </ul> <p>VSCode Integration:</p> <p>For Visual Studio Code users, extensions are available for seamless integration:</p> <ul> <li>Black Formatter: Install the \"Python\" extension by Microsoft, which supports Black formatting.</li> <li>isort: Use the \"Python\" extension's settings to enable isort integration.</li> <li>mypy: Install the \"mypy\" extension for real-time type checking.</li> </ul> <p>By configuring your IDE with these tools, you ensure consistent code quality and adhere to the project's coding standards effortlessly. </p> <p>If you prefer to do these steps manually, please read the following steps, if not, you can ignore this section.</p>"},{"location":"developer_guide/api_generation/#code-formatting","title":"Code Formatting","text":"<p>We use Black for code formatting. To format your code, run:</p> <pre><code>black .\n</code></pre>"},{"location":"developer_guide/api_generation/#linting","title":"Linting","text":"<p>isort is used for sorting imports. To sort imports, execute:</p> <pre><code>isort .\n</code></pre>"},{"location":"developer_guide/api_generation/#type-checking","title":"Type Checking","text":"<p>mypy is employed for static type checking. To check types, run:</p> <pre><code>mypy torchebm/\n</code></pre>"},{"location":"developer_guide/api_generation/#testing","title":"Testing","text":"<p>We utilize pytest for testing. To run tests, execute:</p> <pre><code>pytest\n</code></pre> <p>For test coverage, use:</p> <pre><code>pytest --cov=torchebm\n</code></pre>"},{"location":"developer_guide/api_generation/#documentation","title":"Documentation","text":""},{"location":"developer_guide/api_generation/#docstring-conventions","title":"Docstring Conventions","text":"<p>All docstrings should adhere to the Google style guide. For example:</p> <pre><code>def function_name(param1, param2):\n    \"\"\"Short description of the function.\n\n    Longer description if needed.\n\n    Args:\n        param1 (type): Description of param1.\n        param2 (type): Description of param2.\n\n    Returns:\n        type: Description of return value.\n\n    Raises:\n        ExceptionType: Explanation of when this exception is raised.\n\n    Examples:\n        result = function_name(1, \"test\")\n    \"\"\"\n    # Function implementation\n</code></pre>"},{"location":"developer_guide/api_generation/#api-documentation-generation","title":"API Documentation Generation","text":"<p>The API documentation is automatically generated from docstrings using <code>generate_api_docs.py</code>, MkDocs, and the MkDocstrings plugin.</p> <p>To update the API documentation:</p> <ol> <li> <p>Run the API documentation generator script:</p> <pre><code>python gen_ref_pages.py\n</code></pre> </li> <li> <p>Build the documentation to preview changes:</p> <pre><code>mkdocs serve\n</code></pre> </li> </ol>"},{"location":"developer_guide/api_generation/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please follow the guidelines below.</p>"},{"location":"developer_guide/api_generation/#branching-strategy","title":"Branching Strategy","text":"<ul> <li>main: Contains stable code.</li> <li>feature/branch-name: For developing new features.</li> <li>bugfix/branch-name: For fixing bugs.</li> </ul>"},{"location":"developer_guide/api_generation/#commit-messages","title":"Commit Messages","text":"<p>Use clear and concise commit messages. Follow the format:</p> <pre><code>Subject line (imperative, 50 characters or less)\n\nOptional detailed description, wrapped at 72 characters.\n</code></pre>"},{"location":"developer_guide/api_generation/#pull-requests","title":"Pull Requests","text":"<p>Before submitting a pull request:</p> <ol> <li>Ensure all tests pass.</li> <li>Update documentation if applicable.</li> <li>Adhere to code style guidelines.</li> </ol>"},{"location":"developer_guide/api_generation/#additional-resources","title":"Additional Resources","text":"<ul> <li>Python's PEP 8 Style Guide</li> <li>Google Python Style Guide</li> <li>Black Code Formatter</li> <li>pytest Documentation</li> </ul> <p>By following this guide, you will help maintain the quality and consistency of the <code>torchebm</code> library. Happy coding! </p>"},{"location":"developer_guide/architecture/","title":"Architecture","text":""},{"location":"developer_guide/architecture/#architecture","title":"Architecture","text":"<p>This document outlines the architecture and design principles of TorchEBM, providing insights into how the library is structured and how its components interact.</p>"},{"location":"developer_guide/architecture/#core-components","title":"Core Components","text":"<p>TorchEBM is designed around several key components that work together to provide a flexible and powerful framework for energy-based modeling:</p>"},{"location":"developer_guide/architecture/#energy-functions","title":"Energy Functions","text":"<p>The <code>BaseEnergyFunction</code> base class defines the interface for all energy functions. It provides methods for:</p> <ul> <li>Computing energy values</li> <li>Computing gradients</li> <li>Handling batches of inputs</li> <li>CUDA acceleration</li> </ul> <p>Implemented energy functions include Gaussian, Double Well, Rastrigin, Rosenbrock, and more.</p>"},{"location":"developer_guide/architecture/#samplers","title":"Samplers","text":"<p>The <code>BaseSampler</code> class defines the interface for sampling algorithms. Key features include:</p> <ul> <li>Generating samples from energy functions</li> <li>Running sampling chains</li> <li>Collecting diagnostics</li> <li>Parallelized sampling</li> </ul> <p>Implemented samplers include Langevin Dynamics and Hamiltonian Monte Carlo.</p>"},{"location":"developer_guide/architecture/#baseloss-functions","title":"BaseLoss Functions","text":"<p>BaseLoss functions are used to train energy-based models. They include:</p> <ul> <li>Contrastive Divergence (CD)</li> <li>Persistent Contrastive Divergence (PCD)</li> <li>Parallel Tempering Contrastive Divergence</li> <li>Score Matching (planned)</li> </ul>"},{"location":"developer_guide/architecture/#models","title":"Models","text":"<p>Neural network models that can be used as energy functions:</p> <ul> <li>Base model interfaces</li> <li>Integration with PyTorch modules</li> <li>Support for custom architectures</li> <li>GPU acceleration</li> </ul>"},{"location":"developer_guide/architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TD\n    A[Energy Functions] --&gt; C[Samplers]\n    B[Models] --&gt; A\n    C --&gt; D[BaseLoss Functions]\n    D --&gt; B\n    E[CUDA Accelerators] --&gt; A\n    E --&gt; C\n    F[Utils] --&gt; A\n    F --&gt; B\n    F --&gt; C\n    F --&gt; D</code></pre>"},{"location":"developer_guide/architecture/#design-principles","title":"Design Principles","text":"<p>Key Design Principles</p> <p>TorchEBM follows these core design principles:</p> <ol> <li>Modularity: Components can be used independently and combined flexibly</li> <li>Extensibility: Easy to add new energy functions, samplers, and loss functions</li> <li>Performance: Optimized for both CPU and GPU execution</li> <li>Compatibility: Seamless integration with PyTorch ecosystem</li> <li>Usability: Clear, consistent API with comprehensive documentation</li> </ol>"},{"location":"developer_guide/architecture/#component-interactions","title":"Component Interactions","text":""},{"location":"developer_guide/architecture/#energy-function-and-sampler-interaction","title":"Energy Function and Sampler Interaction","text":"<p>The energy function provides the landscape that the sampler traverses:</p> <pre><code># Energy function computes energy and gradients\nenergy = energy_fn(x)  # Forward pass\ngradient = energy_fn.gradient(x)  # Gradient computation\n\n# Sampler uses gradients for updates\nx_new = x - step_size * gradient + noise\n</code></pre>"},{"location":"developer_guide/architecture/#sampler-and-baseloss-function-interaction","title":"Sampler and BaseLoss Function Interaction","text":"<p>Samplers are used by loss functions to generate negative samples during training:</p> <pre><code># BaseLoss function uses sampler to generate negative samples\nnegative_samples = sampler.sample(x_init, n_steps=10)\n\n# BaseLoss computation uses both data samples and negative samples\nloss = loss_fn(data_samples, negative_samples)\n</code></pre>"},{"location":"developer_guide/architecture/#module-organization","title":"Module Organization","text":"<p>TorchEBM's codebase is organized into the following modules:</p> Module Description Key Classes <code>torchebm.core</code> Core functionality and base classes <code>BaseEnergyFunction</code>, <code>BaseSampler</code> <code>torchebm.samplers</code> Sampling algorithms <code>LangevinDynamics</code>, <code>HamiltonianMonteCarlo</code> <code>torchebm.losses</code> BaseLoss functions <code>ContrastiveDivergence</code>, <code>PersistentContrastiveDivergence</code> <code>torchebm.models</code> Neural network models <code>BaseModel</code> <code>torchebm.cuda</code> CUDA-accelerated implementations Various CUDA kernels <code>torchebm.utils</code> Utility functions and helpers Visualization tools, diagnostics"},{"location":"developer_guide/architecture/#performance-considerations","title":"Performance Considerations","text":"<p>TorchEBM is designed with performance in mind:</p> <ul> <li>Vectorization: Operations are vectorized for efficient batch processing</li> <li>GPU Acceleration: Most operations can run on CUDA devices</li> <li>Memory Management: Careful memory management to avoid unnecessary allocations</li> <li>Parallel Sampling: Samples can be generated in parallel for better utilization of hardware</li> </ul>"},{"location":"developer_guide/architecture/#extension-points","title":"Extension Points","text":"<p>TorchEBM is designed to be extended in several ways:</p> <ol> <li>Custom Energy Functions: Create your own energy functions by subclassing <code>BaseEnergyFunction</code></li> <li>Custom Samplers: Implement new sampling algorithms by subclassing <code>BaseSampler</code></li> <li>Custom BaseLoss Functions: Create new training objectives for energy-based models</li> <li>Neural Network Energy Functions: Use neural networks as energy functions</li> </ol> <p>For more details on implementing extensions, see our API Design documentation. </p>"},{"location":"developer_guide/code_style/","title":"Code Style Guide","text":""},{"location":"developer_guide/code_style/#code-style-guide","title":"Code Style Guide","text":"<p>Consistent Style</p> <p>Following a consistent code style ensures our codebase remains readable and maintainable. This guide outlines the style conventions used in TorchEBM.</p>"},{"location":"developer_guide/code_style/#python-style-guidelines","title":"Python Style Guidelines","text":"<p>TorchEBM follows PEP 8 with some project-specific guidelines.</p>"},{"location":"developer_guide/code_style/#automatic-formatting","title":"Automatic Formatting","text":"<p>We use several tools to automatically format and check our code:</p> <ul> <li> <p> Black</p> <p>Automatic code formatter with a focus on consistency.</p> <pre><code>black torchebm/\n</code></pre> </li> <li> <p> isort</p> <p>Sorts imports alphabetically and separates them into sections.</p> <pre><code>isort torchebm/\n</code></pre> </li> <li> <p> Flake8</p> <p>Linter to catch logical and stylistic issues.</p> <pre><code>flake8 torchebm/\n</code></pre> </li> </ul>"},{"location":"developer_guide/code_style/#code-structure","title":"Code Structure","text":"Function DefinitionsClass Definitions <pre><code>def function_name(\n    param1: type,\n    param2: type,\n    param3: Optional[type] = None\n) -&gt; ReturnType:\n    \"\"\"Short description of the function.\n\n    More detailed explanation if needed.\n\n    Args:\n        param1: Description of parameter 1\n        param2: Description of parameter 2\n        param3: Description of parameter 3\n\n    Returns:\n        Description of the return value\n\n    Raises:\n        ExceptionType: When and why this exception is raised\n    \"\"\"\n    # Function implementation\n    pass\n</code></pre> <pre><code>class ClassName(BaseClass):\n    \"\"\"Short description of the class.\n\n    More detailed explanation if needed.\n\n    Args:\n        attr1: Description of attribute 1\n        attr2: Description of attribute 2\n    \"\"\"\n\n    def __init__(\n        self,\n        attr1: type,\n        attr2: type = default_value\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            attr1: Description of attribute 1\n            attr2: Description of attribute 2\n        \"\"\"\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def method_name(self, param: type) -&gt; ReturnType:\n        \"\"\"Short description of the method.\n\n        Args:\n            param: Description of parameter\n\n        Returns:\n            Description of the return value\n        \"\"\"\n        # Method implementation\n        pass\n</code></pre>"},{"location":"developer_guide/code_style/#naming-conventions","title":"Naming Conventions","text":""},{"location":"developer_guide/code_style/#classes","title":"Classes","text":"<p>Use <code>CamelCase</code> for class names:</p> <pre><code>class BaseEnergyFunction:\n    pass\n\nclass LangevinDynamics:\n    pass\n</code></pre>"},{"location":"developer_guide/code_style/#functions-and-variables","title":"Functions and Variables","text":"<p>Use <code>snake_case</code> for functions and variables:</p> <pre><code>def compute_energy(x):\n    pass\n\nsample_count = 1000\n</code></pre>"},{"location":"developer_guide/code_style/#constants","title":"Constants","text":"<p>Use <code>UPPER_CASE</code> for constants:</p> <pre><code>DEFAULT_STEP_SIZE = 0.01\nMAX_ITERATIONS = 1000\n</code></pre>"},{"location":"developer_guide/code_style/#documentation-style","title":"Documentation Style","text":"<p>TorchEBM uses Google-style docstrings for all code documentation.</p> <p>Docstring Example</p> <pre><code>def sample_chain(\n    self, \n    dim: int, \n    n_steps: int, \n    n_samples: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"Generate samples using a Markov chain of specified length.\n\n    Args:\n        dim: Dimensionality of samples\n        n_steps: Number of steps in the chain\n        n_samples: Number of parallel chains to run\n\n    Returns:\n        Tensor of shape (n_samples, dim) containing final samples\n\n    Examples:\n        &gt;&gt;&gt; energy_fn = GaussianEnergy(torch.zeros(2), torch.eye(2))\n        &gt;&gt;&gt; sampler = LangevinDynamics(energy_fn, step_size=0.01)\n        &gt;&gt;&gt; samples = sampler.sample_chain(dim=2, n_steps=100, n_samples=10)\n    \"\"\"\n</code></pre>"},{"location":"developer_guide/code_style/#type-annotations","title":"Type Annotations","text":"<p>We use Python's type hints to improve code readability and enable static type checking:</p> <pre><code>from typing import Optional, List, Union, Dict, Tuple, Callable\n\ndef function(\n    tensor: torch.Tensor,\n    scale: float = 1.0,\n    use_cuda: bool = False,\n    callback: Optional[Callable[[torch.Tensor], None]] = None\n) -&gt; Tuple[torch.Tensor, float]:\n    # Implementation\n    pass\n</code></pre>"},{"location":"developer_guide/code_style/#cuda-code-style","title":"CUDA Code Style","text":"<p>For CUDA extensions, we follow these conventions:</p> File OrganizationCUDA Naming Conventions <pre><code>torchebm/cuda/\n\u251c\u2500\u2500 kernels/\n\u2502   \u251c\u2500\u2500 kernel_name.cu\n\u2502   \u2514\u2500\u2500 kernel_name.cuh\n\u251c\u2500\u2500 bindings.cpp\n\u2514\u2500\u2500 __init__.py\n</code></pre> <pre><code>// Function names in snake_case\n__global__ void compute_energy_kernel(float* input, float* output, int n) {\n    // Implementation\n}\n\n// Constants in UPPER_CASE\n#define BLOCK_SIZE 256\n</code></pre>"},{"location":"developer_guide/code_style/#imports-organization","title":"Imports Organization","text":"<p>Organize imports in the following order:</p> <ol> <li>Standard library imports</li> <li>Related third-party imports</li> <li>Local application/library specific imports</li> </ol> <pre><code># Standard library\nimport os\nimport sys\nfrom typing import Optional, Dict\n\n# Third-party\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# Local application\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.utils import get_device\n</code></pre>"},{"location":"developer_guide/code_style/#comments","title":"Comments","text":"<ul> <li>Use comments sparingly - prefer self-documenting code with clear variable names</li> <li>Add comments for complex algorithms or non-obvious implementations</li> <li>Update comments when you change code</li> </ul> <p>Good Comments Example</p> <pre><code># Correcting for numerical instability by adding a small epsilon\nnormalized_weights = weights / (torch.sum(weights, dim=1, keepdim=True) + 1e-8)\n</code></pre>"},{"location":"developer_guide/code_style/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>TorchEBM uses pre-commit hooks to enforce code style. Make sure to install them as described in the Development Setup guide.</p>"},{"location":"developer_guide/code_style/#recommended-editor-settings","title":"Recommended Editor Settings","text":"VS CodePyCharm <pre><code>{\n  \"editor.formatOnSave\": true,\n  \"editor.codeActionsOnSave\": {\n    \"source.organizeImports\": true\n  },\n  \"python.linting.enabled\": true,\n  \"python.linting.flake8Enabled\": true,\n  \"python.formatting.provider\": \"black\"\n}\n</code></pre> <ol> <li>Install Black and isort plugins</li> <li>Configure Code Style for Python to match PEP 8</li> <li>Set Black as the external formatter</li> <li>Enable \"Reformat code on save\"</li> <li>Configure isort for import optimization</li> </ol>"},{"location":"developer_guide/code_style/#style-enforcement","title":"Style Enforcement","text":"<p>Our CI pipeline checks for style compliance. Pull requests failing style checks will be automatically rejected.</p> <p>CI Pipeline Failure</p> <p>If your PR fails CI due to style issues, run the following commands locally to fix them:</p> <pre><code># Format code with Black\nblack torchebm/\n\n# Sort imports\nisort torchebm/\n\n# Run flake8\nflake8 torchebm/\n\n# Run mypy for type checking\nmypy torchebm/\n</code></pre>"},{"location":"developer_guide/commit_conventions/","title":"Commit Message Conventions","text":""},{"location":"developer_guide/commit_conventions/#commit-message-conventions","title":"Commit Message Conventions","text":"<p>TorchEBM follows a specific format for commit messages to maintain a clear project history and generate meaningful changelogs. This document outlines the conventions that all contributors should follow when making commits to the project.</p>"},{"location":"developer_guide/commit_conventions/#format","title":"Format","text":"<p>Each commit message should have a specific format:</p> <ol> <li>The first line should be a maximum of 50-60 characters</li> <li>It should begin with an emoji followed by a type, then a colon and a brief description</li> <li>Any further details should be in the subsequent lines, separated by an empty line</li> </ol>"},{"location":"developer_guide/commit_conventions/#types-and-emojis","title":"Types and Emojis","text":"<p>We use the following types and emojis to categorize commits:</p> Type Emoji Description feat \u2728 Introduces a new feature fix \ud83d\udc1b Patches a bug in the codebase docs \ud83d\udcd6 Changes related to documentation style \ud83d\udc8e Changes that do not affect the meaning of the code (formatting) refactor \ud83d\udce6 Code changes that neither fix a bug nor add a feature perf \ud83d\ude80 Improvements to performance test \ud83d\udea8 Adding or correcting tests build \ud83d\udc77 Changes affecting the build system or external dependencies ci \ud83d\udcbb Changes to Continuous Integration configuration chore \ud83c\udfab Miscellaneous changes that don't modify source or test files revert \ud83d\udd19 Reverts a previous commit"},{"location":"developer_guide/commit_conventions/#examples","title":"Examples","text":"<pre><code>\u2728 feat: add Hamiltonian Monte Carlo sampler\n</code></pre> <pre><code>\ud83d\udc1b fix: correct gradient calculation in Langevin dynamics\n\nThis fixes an issue where gradients were not being properly scaled\nby the step size, leading to instability in long sampling chains.\n</code></pre> <pre><code>\ud83d\udcd6 docs: improve installation instructions\n\nUpdate pip installation command and add conda installation option.\n</code></pre>"},{"location":"developer_guide/commit_conventions/#version-bumping-and-releasing","title":"Version Bumping and Releasing","text":"<p>For Maintainers</p> <p>Version bumping and release tags are primarily for project maintainers. As a contributor, you don't need to worry about these when submitting pull requests. Project maintainers will handle versioning and releases.</p> <p>For project maintainers, our CI/CD workflow supports the following tags:</p> <ul> <li>Use <code>#major</code> for breaking changes requiring a major version bump (e.g., 1.0.0 to 2.0.0)</li> <li>Use <code>#minor</code> for new features requiring a minor version bump (e.g., 1.0.0 to 1.1.0)</li> <li>Default is patch level for bug fixes (e.g., 1.0.0 to 1.0.1)</li> <li>Include <code>#release</code> to trigger a release to PyPI</li> </ul> <p>Example (for maintainers): </p><pre><code>\u2728 feat: add comprehensive API for custom energy functions #minor #release\n</code></pre><p></p>"},{"location":"developer_guide/commit_conventions/#best-practices","title":"Best Practices","text":"<ol> <li>Be descriptive but concise in your commit message</li> <li>Focus on the why, not just the what</li> <li>Use present tense (\"add feature\" not \"added feature\")</li> <li>Separate commits logically - one commit per logical change</li> <li>Reference issues in commit messages when appropriate (e.g., \"Fixes #123\")</li> </ol> <p>Following these conventions helps maintain a clean project history and facilitates automated changelog generation. </p>"},{"location":"developer_guide/contributing/","title":"Contributing Guidelines","text":""},{"location":"developer_guide/contributing/#contributing-to-torchebm","title":"Contributing to TorchEBM","text":"<p>Thank you for your interest in contributing to TorchEBM! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"developer_guide/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We expect all contributors to follow our Code of Conduct. Please be respectful and inclusive in your interactions with others.</p>"},{"location":"developer_guide/contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are many ways to contribute to TorchEBM:</p> <ol> <li>Report bugs: Report bugs or issues by opening an issue on GitHub</li> <li>Suggest features: Suggest new features or improvements</li> <li>Improve documentation: Fix typos, clarify explanations, or add examples</li> <li>Write code: Implement new features, fix bugs, or improve performance</li> <li>Review pull requests: Help review code from other contributors</li> </ol>"},{"location":"developer_guide/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"developer_guide/contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/your-username/torchebm.git\ncd torchebm\n</code></pre></li> <li>Set the original repository as an upstream remote:    <pre><code>git remote add upstream https://github.com/soran-ghaderi/torchebm.git\n</code></pre></li> <li>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> </ol>"},{"location":"developer_guide/contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Create a new branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes</li> <li>Run tests to make sure your changes don't break existing functionality:    <pre><code>pytest\n</code></pre></li> <li>Add and commit your changes using our commit message conventions</li> </ol>"},{"location":"developer_guide/contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 for Python code style with some modifications:</p> <ul> <li>Line length limit: 88 characters</li> <li>Use double quotes for strings</li> <li>Follow naming conventions:</li> <li>Classes: <code>CamelCase</code></li> <li>Functions and variables: <code>snake_case</code></li> <li>Constants: <code>UPPER_CASE</code></li> </ul> <p>We use <code>black</code> for code formatting and <code>isort</code> for sorting imports.</p>"},{"location":"developer_guide/contributing/#commit-message-conventions","title":"Commit Message Conventions","text":"<p>We follow a specific format for commit messages to make the project history clear and generate meaningful changelogs. Each commit message should have a specific format:</p> <p>The first line should be max 50-60 chars. Any further details should be in the next lines separated by an empty line.</p> <ul> <li>\u2728 feat: Introduces a new feature</li> <li>\ud83d\udc1b fix: Patches a bug in the codebase</li> <li>\ud83d\udcd6 docs: Changes related to documentation</li> <li>\ud83d\udc8e style: Changes that do not affect the meaning of the code (formatting)</li> <li>\ud83d\udce6 refactor: Code changes that neither fix a bug nor add a feature</li> <li>\ud83d\ude80 perf: Improvements to performance</li> <li>\ud83d\udea8 test: Adding or correcting tests</li> <li>\ud83d\udc77 build: Changes affecting the build system or external dependencies</li> <li>\ud83d\udcbb ci: Changes to Continuous Integration configuration</li> <li>\ud83c\udfab chore: Miscellaneous changes that don't modify source or test files</li> <li>\ud83d\udd19 revert: Reverts a previous commit</li> </ul> <p>Example: </p><pre><code>\u2728 feat: new feature implemented\n\nThe details of the commit (if any) go here.\n</code></pre><p></p> <p>For version bumping, include one of these tags in your commit message: - Use <code>#major</code> for breaking changes - Use <code>#minor</code> for new features - Default is patch level for bug fixes</p> <p>For releasing to PyPI, include <code>#release</code> in your commit message.</p> <p>For more detailed information about our commit message conventions, please see our Commit Message Conventions guide.</p>"},{"location":"developer_guide/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Push your changes to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Create a pull request on GitHub</li> <li>In your pull request description, explain the changes and link to any related issues</li> <li>Wait for a review and address any feedback</li> </ol>"},{"location":"developer_guide/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Keep pull requests focused on a single task</li> <li>Add tests for new features or bug fixes</li> <li>Update documentation as needed</li> <li>Ensure all tests pass</li> <li>Follow the code style guidelines</li> </ul>"},{"location":"developer_guide/contributing/#issue-guidelines","title":"Issue Guidelines","text":"<p>When opening an issue, please provide:</p> <ul> <li>A clear and descriptive title</li> <li>A detailed description of the issue</li> <li>Steps to reproduce (for bugs)</li> <li>Expected vs. actual behavior</li> <li>Version information (TorchEBM, PyTorch, Python, CUDA)</li> <li>Any relevant code snippets or error messages</li> </ul>"},{"location":"developer_guide/contributing/#implementing-new-features","title":"Implementing New Features","text":""},{"location":"developer_guide/contributing/#samplers","title":"Samplers","text":"<p>When implementing a new sampler:</p> <ol> <li>Create a new file in <code>torchebm/samplers/</code></li> <li>Extend the <code>BaseSampler</code> class</li> <li>Implement the required methods:</li> <li><code>__init__</code>: Initialize the sampler with appropriate parameters</li> <li><code>step</code>: Implement the sampling step</li> <li><code>sample_chain</code>: Implement a full sampling chain (or use the default implementation)</li> <li>Add tests in <code>tests/samplers/</code></li> <li>Update documentation</li> </ol> <p>Example:</p> <pre><code>from torchebm.core import BaseSampler\nimport torch\n\nclass MySampler(BaseSampler):\n    def __init__(self, energy_function, param1, param2, device=\"cpu\"):\n        super().__init__(energy_function, device)\n        self.param1 = param1\n        self.param2 = param2\n\n    def sample_chain(self, x, step_idx=None):\n        # Implement your sampling algorithm here\n        # x batch_shape: [n_samples, dim]\n\n        # Your sampler logic\n        x_new = ...\n\n        # Return updated samples and any diagnostics\n        return x_new, {\"diagnostic1\": value1, \"diagnostic2\": value2}\n</code></pre>"},{"location":"developer_guide/contributing/#energy-functions","title":"Energy Functions","text":"<p>When implementing a new energy function:</p> <ol> <li>Create a new class in <code>torchebm/core/energy_function.py</code> or a new file in <code>torchebm/core/</code></li> <li>Extend the <code>BaseEnergyFunction</code> class</li> <li>Implement the required methods:</li> <li><code>__init__</code>: Initialize the energy function with appropriate parameters</li> <li><code>forward</code>: Compute the energy value for a given input</li> <li>Add tests in <code>tests/core/</code></li> <li>Update documentation</li> </ol> <p>Example:</p> <pre><code>from torchebm.core import BaseEnergyFunction\nimport torch\n\n\nclass MyEnergyFunction(BaseEnergyFunction):\n    def __init__(self, param1, param2):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def forward(self, x):\n        # Implement your energy function here\n        # x batch_shape: [batch_size, dimension]\n        # Return batch_shape: [batch_size]\n        return torch.sum(self.param1 * x ** 2 + self.param2 * torch.sin(x), dim=-1)\n</code></pre>"},{"location":"developer_guide/contributing/#baseloss-functions","title":"BaseLoss Functions","text":"<p>When implementing a new loss function:</p> <ol> <li>Create a new class in <code>torchebm/losses/</code></li> <li>Implement the required methods:</li> <li><code>__init__</code>: Initialize the loss function with appropriate parameters</li> <li><code>forward</code>: Compute the loss value</li> <li>Add tests in <code>tests/losses/</code></li> <li>Update documentation</li> </ol> <p>Example:</p> <pre><code>import torch\nimport torch.nn as nn\n\nclass MyLossFunction(nn.Module):\n    def __init__(self, param1, param2):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def forward(self, model, data_samples):\n        # Implement your loss function here\n        # Return a scalar loss value\n        return loss\n</code></pre>"},{"location":"developer_guide/contributing/#documentation-guidelines","title":"Documentation Guidelines","text":"<ul> <li>Use clear, concise language</li> <li>Include examples for complex functionality</li> <li>Document parameters, return values, and exceptions</li> <li>Add docstrings to classes and functions</li> <li>Update the roadmap when implementing new features</li> </ul>"},{"location":"developer_guide/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help or have questions:</p> <ul> <li>Check existing documentation</li> <li>Search for similar issues on GitHub</li> <li>Ask for help in your pull request or issue</li> </ul>"},{"location":"developer_guide/contributing/#thank-you","title":"Thank You!","text":"<p>Thank you for contributing to TorchEBM! Your help is greatly appreciated and makes the library better for everyone. </p>"},{"location":"developer_guide/core_components/","title":"Core Components","text":""},{"location":"developer_guide/core_components/#core-components","title":"Core Components","text":"<p>Building Blocks</p> <p>TorchEBM is built around several core components that form the foundation of the library. This guide provides in-depth information about these components and how they interact.</p>"},{"location":"developer_guide/core_components/#component-overview","title":"Component Overview","text":"<ul> <li> <p> Energy Functions</p> <p>Define the energy landscape for probability distributions.</p> <pre><code>energy = energy_fn(x)  # Evaluate energy at point x\n</code></pre> </li> <li> <p> Samplers</p> <p>Generate samples from energy-based distributions.</p> <pre><code>samples = sampler.sample(n_samples=1000)  # Generate 1000 samples\n</code></pre> </li> <li> <p> Loss Functions</p> <p>Train energy-based models from data.</p> <pre><code>loss = loss_fn(model, data_samples)  # Compute training loss\n</code></pre> </li> <li> <p> Models</p> <p>Parameterize energy functions with neural networks.</p> <pre><code>model = EnergyModel(network=nn.Sequential(...))\n</code></pre> </li> </ul>"},{"location":"developer_guide/core_components/#energy-functions","title":"Energy Functions","text":"<p>Energy functions are the core building block of TorchEBM. They define a scalar energy value for each point in the sample space.</p>"},{"location":"developer_guide/core_components/#base-energy-function","title":"Base Energy Function","text":"<p>The <code>BaseEnergyFunction</code> class is the foundation for all energy functions:</p> <pre><code>class BaseEnergyFunction(nn.Module):\n    \"\"\"Base class for all energy functions.\n\n    An energy function maps points in the sample space to scalar energy values.\n    Lower energy corresponds to higher probability density.\n    \"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy for input points.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        raise NotImplementedError\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score function (gradient of energy) for input points.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size, dim) containing score values\n        \"\"\"\n        x = x.requires_grad_(True)\n        energy = self.forward(x)\n        return torch.autograd.grad(energy.sum(), x, create_graph=True)[0]\n</code></pre>"},{"location":"developer_guide/core_components/#analytical-energy-functions","title":"Analytical Energy Functions","text":"<p>TorchEBM provides several analytical energy functions for testing and benchmarking:</p> Gaussian EnergyDouble Well Energy <pre><code>class GaussianEnergy(BaseEnergyFunction):\n    \"\"\"Gaussian energy function.\n\n    Energy function defined by a multivariate Gaussian distribution:\n    E(x) = 0.5 * (x - mean)^T * precision * (x - mean)\n    \"\"\"\n\n    def __init__(self, mean: torch.Tensor, cov: torch.Tensor):\n        \"\"\"Initialize Gaussian energy function.\n\n        Args:\n            mean: Mean vector of shape (dim,)\n            cov: Covariance matrix of shape (dim, dim)\n        \"\"\"\n        super().__init__()\n        self.register_buffer(\"mean\", mean)\n        self.register_buffer(\"cov\", cov)\n        self.register_buffer(\"precision\", torch.inverse(cov))\n        self._dim = mean.size(0)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute Gaussian energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        centered = x - self.mean\n        return 0.5 * torch.sum(\n            centered * (self.precision @ centered.T).T,\n            dim=1\n        )\n</code></pre> <pre><code>class DoubleWellEnergy(BaseEnergyFunction):\n    \"\"\"Double well energy function.\n\n    Energy function with two local minima:\n    E(x) = a * (x^2 - b)^2\n    \"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 2.0):\n        \"\"\"Initialize double well energy function.\n\n        Args:\n            a: Scale parameter\n            b: Parameter controlling the distance between wells\n        \"\"\"\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute double well energy.\n\n        Args:\n            x: Input tensor of shape (batch_size, dim)\n\n        Returns:\n            Tensor of shape (batch_size,) containing energy values\n        \"\"\"\n        return self.a * torch.sum((x**2 - self.b)**2, dim=1)\n</code></pre>"},{"location":"developer_guide/core_components/#composite-energy-functions","title":"Composite Energy Functions","text":"<p>Energy functions can be composed to create more complex landscapes:</p> <pre><code>class CompositeEnergy(BaseEnergyFunction):\n    \"\"\"Composite energy function.\n\n    Combines multiple energy functions through addition.\n    \"\"\"\n\n    def __init__(self, energy_functions: List[BaseEnergyFunction], weights: Optional[List[float]] = None):\n        \"\"\"Initialize composite energy function.\n\n        Args:\n            energy_functions: List of energy functions to combine\n            weights: Optional weights for each energy function\n        \"\"\"\n        super().__init__()\n        self.energy_functions = nn.ModuleList(energy_functions)\n        if weights is None:\n            weights = [1.0] * len(energy_functions)\n        self.weights = weights\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute composite energy.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        return sum(w * f(x) for w, f in zip(self.weights, self.energy_functions))\n</code></pre>"},{"location":"developer_guide/core_components/#samplers","title":"Samplers","text":"<p>Samplers generate samples from energy-based distributions. They provide methods to initialize and update samples based on the energy landscape.</p>"},{"location":"developer_guide/core_components/#base-sampler","title":"Base Sampler","text":"<p>The <code>Sampler</code> class is the foundation for all sampling algorithms:</p> <pre><code>class Sampler(ABC):\n    \"\"\"Base class for all samplers.\n\n    A sampler generates samples from an energy-based distribution.\n    \"\"\"\n\n    def __init__(self, energy_function: BaseEnergyFunction):\n        \"\"\"Initialize sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n        \"\"\"\n        self.energy_function = energy_function\n\n    @abstractmethod\n    def sample(self, n_samples: int, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of batch_shape (n_samples, dim) containing samples\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using a Markov chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of batch_shape (n_samples, dim) containing final samples\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/core_components/#langevin-dynamics","title":"Langevin Dynamics","text":"<p>The <code>LangevinDynamics</code> sampler implements Langevin Monte Carlo:</p> <pre><code>class LangevinDynamics(Sampler):\n    \"\"\"Langevin dynamics sampler.\n\n    Uses Langevin dynamics to sample from an energy-based distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        step_size: float = 0.01,\n        noise_scale: float = 1.0\n    ):\n        \"\"\"Initialize Langevin dynamics sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            step_size: Step size for updates\n            noise_scale: Scale of noise added at each step\n        \"\"\"\n        super().__init__(energy_function)\n        self.step_size = step_size\n        self.noise_scale = noise_scale\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Langevin dynamics.\n\n        Args:\n            x: Current samples of batch_shape (n_samples, dim)\n\n        Returns:\n            Updated samples of batch_shape (n_samples, dim)\n        \"\"\"\n        # Compute score (gradient of energy)\n        score = self.energy_function.score(x)\n\n        # Update samples\n        noise = torch.randn_like(x) * np.sqrt(2 * self.step_size * self.noise_scale)\n        x_new = x - self.step_size * score + noise\n\n        return x_new\n\n    def sample_chain(\n        self,\n        dim: int,\n        n_steps: int,\n        n_samples: int = 1,\n        initial_samples: Optional[torch.Tensor] = None,\n        return_trajectory: bool = False\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Generate samples using a Langevin dynamics chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            initial_samples: Optional initial samples\n            return_trajectory: Whether to return the full trajectory\n\n        Returns:\n            Tensor of batch_shape (n_samples, dim) containing final samples,\n            or a tuple of (samples, trajectory) if return_trajectory is True\n        \"\"\"\n        # Initialize samples\n        if initial_samples is None:\n            x = torch.randn(n_samples, dim)\n        else:\n            x = initial_samples.clone()\n\n        # Initialize trajectory if needed\n        if return_trajectory:\n            trajectory = torch.zeros(n_steps + 1, n_samples, dim)\n            trajectory[0] = x\n\n        # Run chain\n        for i in range(n_steps):\n            x = self.sample_step(x)\n            if return_trajectory:\n                trajectory[i + 1] = x\n\n        if return_trajectory:\n            return x, trajectory\n        else:\n            return x\n\n    def sample(self, n_samples: int, dim: int, n_steps: int = 100, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\n\n        Args:\n            n_samples: Number of samples to generate\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            **kwargs: Additional parameters passed to sample_chain\n\n        Returns:\n            Tensor of batch_shape (n_samples, dim) containing samples\n        \"\"\"\n        return self.sample_chain(dim=dim, n_steps=n_steps, n_samples=n_samples, **kwargs)\n</code></pre>"},{"location":"developer_guide/core_components/#baseloss-functions","title":"BaseLoss Functions","text":"<p>BaseLoss functions are used to train energy-based models from data. They provide methods to compute gradients for model updates.</p>"},{"location":"developer_guide/core_components/#base-baseloss-function","title":"Base BaseLoss Function","text":"<p>The <code>BaseLoss</code> class is the foundation for all loss functions:</p> <pre><code>class BaseLoss(ABC):\n    \"\"\"Base class for all loss functions.\n\n    A loss function computes a loss value for an energy-based model.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(\n        self,\n        model: nn.Module,\n        data_samples: torch.Tensor,\n        **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute loss for the model.\n\n        Args:\n            model: Energy-based model\n            data_samples: Samples from the target distribution\n            **kwargs: Additional loss-specific parameters\n\n        Returns:\n            Scalar loss value\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/core_components/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>The <code>ContrastiveDivergence</code> loss implements the contrastive divergence algorithm:</p> <pre><code>class ContrastiveDivergence(BaseLoss):\n    \"\"\"Contrastive divergence loss.\n\n    Uses contrastive divergence to train energy-based models.\n    \"\"\"\n\n    def __init__(\n            self,\n            sampler: Sampler,\n            k: int = 1,\n            batch_size: Optional[int] = None\n    ):\n        \"\"\"Initialize contrastive divergence loss.\n\n        Args:\n            sampler: Sampler to generate model samples\n            k: Number of sampling steps (CD-k_steps)\n            batch_size: Optional batch size for sampling\n        \"\"\"\n        super().__init__()\n        self.sampler = sampler\n        self.k = k\n        self.batch_size = batch_size\n\n    def __call__(\n            self,\n            model: nn.Module,\n            data_samples: torch.Tensor,\n            **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute contrastive divergence loss.\n\n        Args:\n            model: Energy-based model\n            data_samples: Samples from the target distribution\n            **kwargs: Additional parameters passed to the sampler\n\n        Returns:\n            Scalar loss value\n        \"\"\"\n        # Get data statistics\n        batch_size = self.batch_size or data_samples.size(0)\n        dim = data_samples.size(1)\n\n        # Set the model as the sampler's energy function\n        self.sampler.energy_function = model\n\n        # Generate model samples\n        model_samples = self.sampler.sample(\n            dim=dim,\n            n_steps=self.k,\n            n_samples=batch_size,\n            **kwargs\n        )\n\n        # Compute energies\n        data_energy = model(data_samples).mean()\n        model_energy = model(model_samples).mean()\n\n        # Compute loss\n        loss = data_energy - model_energy\n\n        return loss\n</code></pre>"},{"location":"developer_guide/core_components/#models","title":"Models","text":"<p>Models parameterize energy functions using neural networks.</p>"},{"location":"developer_guide/core_components/#energy-model","title":"Energy Model","text":"<p>The <code>EnergyModel</code> class wraps a neural network as an energy function:</p> <pre><code>class EnergyModel(BaseEnergyFunction):\n    \"\"\"Neural network-based energy model.\n\n    Uses a neural network to parameterize an energy function.\n    \"\"\"\n\n    def __init__(self, network: nn.Module):\n        \"\"\"Initialize energy model.\n\n        Args:\n            network: Neural network that outputs scalar energy values\n        \"\"\"\n        super().__init__()\n        self.network = network\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy using the neural network.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"developer_guide/core_components/#component-interactions","title":"Component Interactions","text":"<p>The following diagram illustrates how the core components interact:</p> <pre><code>graph TD\n    A[Energy Function] --&gt;|Defines landscape| B[Sampler]\n    B --&gt;|Generates samples| C[Training Process]\n    D[BaseLoss Function] --&gt;|Guides training| C\n    C --&gt;|Updates| E[Energy Model]\n    E --&gt;|Parameterizes| A</code></pre>"},{"location":"developer_guide/core_components/#typical-usage-flow","title":"Typical Usage Flow","text":"<ol> <li>Define an energy function - Either analytical or neural network-based</li> <li>Create a sampler - Using the energy function</li> <li>Generate samples - Using the sampler</li> <li>Train a model - Using the loss function and sampler</li> <li>Use the trained model - For tasks like generation or density estimation</li> </ol> <pre><code># Define energy function\nenergy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n# Create sampler\nsampler = LangevinDynamics(energy_function=energy_fn, step_size=0.01)\n\n# Generate samples\nsamples = sampler.sample(dim=2, n_steps=1000, n_samples=100)\n\n# Create and train a model\nmodel = EnergyModel(network=MLP(input_dim=2, hidden_dims=[32, 32], output_dim=1))\nloss_fn = ContrastiveDivergence(sampler=sampler, k=10)\n\n# Training loop\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nfor epoch in range(100):\n    optimizer.zero_grad()\n    loss = loss_fn(model, data_samples)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"developer_guide/core_components/#extension-points","title":"Extension Points","text":"<p>TorchEBM is designed to be extensible at several points:</p> <ul> <li>New Energy Functions - Create by subclassing <code>BaseEnergyFunction</code></li> <li>New Samplers - Create by subclassing <code>Sampler</code></li> <li>New BaseLoss Functions - Create by subclassing <code>BaseLoss</code></li> <li>New Models - Create by subclassing <code>EnergyModel</code> or using custom networks</li> </ul>"},{"location":"developer_guide/core_components/#component-lifecycle","title":"Component Lifecycle","text":"<p>Each component in TorchEBM has a typical lifecycle:</p> <ol> <li>Initialization - Configure the component with parameters</li> <li>Usage - Use the component to perform its intended function</li> <li>Composition - Combine with other components</li> <li>Extension - Extend with new functionality</li> </ol> <p>Understanding this lifecycle helps when implementing new components or extending existing ones.</p>"},{"location":"developer_guide/core_components/#best-practices","title":"Best Practices","text":"<p>When working with TorchEBM components, follow these best practices:</p> <ul> <li>Energy Functions: Ensure they're properly normalized for stable training</li> <li>Samplers: Check mixing time and adjust parameters accordingly</li> <li>BaseLoss Functions: Monitor training stability and adjust hyperparameters</li> <li>Models: Use appropriate architecture for the problem domain</li> </ul> <p>Performance Optimization</p> <p>For large-scale applications, consider using CUDA-optimized implementations and batch processing for better performance.</p> <ul> <li> <p> Energy Functions</p> <p>Learn about energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Explore sampler implementation details.</p> <p> Samplers</p> </li> <li> <p> Loss Functions</p> <p>Understand loss function implementation details.</p> <p> BaseLoss Functions</p> </li> </ul>"},{"location":"developer_guide/design_principles/","title":"Design Principles","text":""},{"location":"developer_guide/design_principles/#design-principles","title":"Design Principles","text":"<p>Project Philosophy</p> <p>TorchEBM is built on a set of core design principles that guide its development. Understanding these principles will help you contribute in a way that aligns with the project's vision.</p>"},{"location":"developer_guide/design_principles/#core-philosophy","title":"Core Philosophy","text":"<p>TorchEBM aims to be:</p> <ul> <li> <p> Performant</p> <p>High-performance implementations that leverage PyTorch's capabilities and CUDA acceleration.</p> </li> <li> <p> Modular</p> <p>Components that can be easily combined, extended, and customized.</p> </li> <li> <p> Intuitive</p> <p>Clear, well-documented APIs that are easy to understand and use.</p> </li> <li> <p> Educational</p> <p>Serves as both a practical tool and a learning resource for energy-based modeling.</p> </li> </ul>"},{"location":"developer_guide/design_principles/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"developer_guide/design_principles/#composable-base-classes","title":"Composable Base Classes","text":"<p>TorchEBM is built around a set of extensible base classes that provide common interface:</p> <pre><code>class BaseEnergyFunction(nn.Module):\n    \"\"\"Base class for all energy functions.\"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy for input x.\"\"\"\n        raise NotImplementedError\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score (gradient of energy) for input x.\"\"\"\n        x = x.requires_grad_(True)\n        energy = self.forward(x)\n        return torch.autograd.grad(energy.sum(), x, create_graph=True)[0]\n</code></pre> <p>This design allows for:</p> <ul> <li>Composition: Combining energy functions via addition, multiplication, etc.</li> <li>Extension: Creating new energy functions by subclassing</li> <li>Integration: Using energy functions with any sampler that follows the interface</li> </ul>"},{"location":"developer_guide/design_principles/#factory-methods","title":"Factory Methods","text":"<p>Factory methods create configured instances with sensible defaults:</p> <pre><code>@classmethod\ndef create_standard(cls, dim: int = 2) -&gt; 'GaussianEnergy':\n    \"\"\"Create a standard Gaussian energy function.\"\"\"\n    return cls(mean=torch.zeros(dim), cov=torch.eye(dim))\n</code></pre>"},{"location":"developer_guide/design_principles/#configuration-through-constructor","title":"Configuration through Constructor","text":"<p>Classes are configured through their constructor rather than setter methods:</p> <pre><code>sampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    noise_scale=1.0\n)\n</code></pre> <p>This approach:</p> <ul> <li>Makes the configuration explicit and clear</li> <li>Encourages immutability of key parameters</li> <li>Simplifies object creation and usage</li> </ul>"},{"location":"developer_guide/design_principles/#method-chaining","title":"Method Chaining","text":"<p>Methods return the object itself when appropriate to allow method chaining:</p> <pre><code>result = (\n    sampler\n    .set_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    .set_seed(42)\n    .sample(dim=2, n_steps=1000)\n)\n</code></pre>"},{"location":"developer_guide/design_principles/#lazily-evaluated-operations","title":"Lazily-Evaluated Operations","text":"<p>Computations are performed lazily when possible to avoid unnecessary work:</p> <pre><code># Create a sampler with a sampling trajectory\nsampler = LangevinDynamics(energy_fn)\ntrajectory = sampler.sample_trajectory(dim=2, n_steps=1000)\n\n# Compute statistics only when needed\nmean = trajectory.mean()  # Computation happens here\nvariance = trajectory.variance()  # Computation happens here\n</code></pre>"},{"location":"developer_guide/design_principles/#architecture-principles","title":"Architecture Principles","text":""},{"location":"developer_guide/design_principles/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Components have clearly defined responsibilities:</p> <ul> <li>Energy Functions: Define the energy landscape</li> <li>Samplers: Generate samples from energy functions</li> <li>Losses: Train energy functions from data</li> <li>Models: Parameterize energy functions using neural networks</li> <li>Utils: Provide supporting functionality</li> </ul>"},{"location":"developer_guide/design_principles/#minimizing-dependencies","title":"Minimizing Dependencies","text":"<p>Each module has minimal dependencies on other modules:</p> <ul> <li>Core modules (e.g., <code>core</code>, <code>samplers</code>) don't depend on higher-level modules</li> <li>Utility modules are designed to be used by all other modules</li> <li>CUDA implementations are separated to allow for CPU-only usage</li> </ul>"},{"location":"developer_guide/design_principles/#consistent-error-handling","title":"Consistent Error Handling","text":"<p>Error handling follows consistent patterns:</p> <ul> <li>Use descriptive error messages that suggest solutions</li> <li>Validate inputs early with helpful validation errors</li> <li>Provide debug information when operations fail</li> </ul> <pre><code>def validate_dimensions(tensor: torch.Tensor, expected_dims: int) -&gt; None:\n    \"\"\"Validate that tensor has the expected number of dimensions.\"\"\"\n    if tensor.dim() != expected_dims:\n        raise ValueError(\n            f\"Expected tensor with {expected_dims} dimensions, \"\n            f\"but got tensor with batch_shape {tensor.shape}\"\n        )\n</code></pre>"},{"location":"developer_guide/design_principles/#consistent-api-design","title":"Consistent API Design","text":"<p>APIs are designed consistently across the library:</p> <ul> <li>Similar operations have similar interfaces</li> <li>Parameters follow consistent naming conventions</li> <li>Return types are consistent and well-documented</li> </ul>"},{"location":"developer_guide/design_principles/#progressive-disclosure","title":"Progressive Disclosure","text":"<p>Simple use cases are simple, while advanced functionality is available but not required:</p> <pre><code># Simple usage\nsampler = LangevinDynamics(energy_fn)\nsamples = sampler.sample(n_samples=1000)\n\n# Advanced usage\nsampler = LangevinDynamics(\n    energy_fn,\n    step_size=0.01,\n    noise_scale=1.0,\n    step_size_schedule=LinearSchedule(0.01, 0.001),\n    metropolis_correction=True\n)\nsamples = sampler.sample(\n    n_samples=1000,\n    initial_samples=initial_x,\n    callback=logging_callback\n)\n</code></pre>"},{"location":"developer_guide/design_principles/#implementation-principles","title":"Implementation Principles","text":""},{"location":"developer_guide/design_principles/#pytorch-first","title":"PyTorch First","text":"<p>TorchEBM is built on PyTorch and follows PyTorch patterns:</p> <ul> <li>Use PyTorch's tensor operations whenever possible</li> <li>Follow PyTorch's model design patterns (e.g., <code>nn.Module</code>)</li> <li>Leverage PyTorch's autograd for gradient computation</li> <li>Support both CPU and CUDA execution</li> </ul>"},{"location":"developer_guide/design_principles/#vectorized-operations","title":"Vectorized Operations","text":"<p>Operations are vectorized where possible for efficiency:</p> <pre><code># Good: Vectorized operations\ndef compute_pairwise_distances(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    return torch.cdist(x, y, p=2)\n\n# Avoid: Explicit loops\ndef compute_pairwise_distances_slow(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    result = torch.zeros(x.size(0), y.size(0))\n    for i in range(x.size(0)):\n        for j in range(y.size(0)):\n            result[i, j] = torch.norm(x[i] - y[j])\n    return result\n</code></pre>"},{"location":"developer_guide/design_principles/#cuda-optimization","title":"CUDA Optimization","text":"<p>Performance-critical operations are optimized with CUDA when appropriate:</p> <ul> <li>CPU implementations as fallback</li> <li>CUDA implementations for performance</li> <li>Automatic selection based on available hardware</li> </ul>"},{"location":"developer_guide/design_principles/#type-annotations","title":"Type Annotations","text":"<p>Code uses type annotations for clarity and static analysis:</p> <pre><code>def sample_chain(\n    self,\n    dim: int,\n    n_steps: int,\n    n_samples: int = 1,\n    initial_samples: Optional[torch.Tensor] = None,\n    return_trajectory: bool = False\n) -&gt; Union[torch.Tensor, Trajectory]:\n    \"\"\"Generate samples using a Markov chain.\"\"\"\n    # Implementation\n</code></pre>"},{"location":"developer_guide/design_principles/#testing-principles","title":"Testing Principles","text":"<ul> <li>Unit Testing: Individual components are thoroughly tested</li> <li>Integration Testing: Component interactions are tested</li> <li>Property Testing: Properties of algorithms are tested</li> <li>Numerical Testing: Numerical algorithms are tested for stability and accuracy</li> </ul>"},{"location":"developer_guide/design_principles/#documentation-principles","title":"Documentation Principles","text":"<p>Documentation is comprehensive and includes:</p> <ul> <li>API Documentation: Clear documentation of all public APIs</li> <li>Tutorials: Step-by-step guides for common tasks</li> <li>Examples: Real-world examples of using the library</li> <li>Theory: Explanations of the underlying mathematical concepts</li> </ul>"},{"location":"developer_guide/design_principles/#future-compatibility","title":"Future Compatibility","text":"<p>TorchEBM is designed with future compatibility in mind:</p> <ul> <li>API Stability: Breaking changes are minimized and clearly documented</li> <li>Feature Flags: Experimental features are clearly marked</li> <li>Deprecation Warnings: Deprecated features emit warnings before removal</li> </ul>"},{"location":"developer_guide/design_principles/#contributing-guidelines","title":"Contributing Guidelines","text":"<p>When contributing to TorchEBM, adhere to these design principles:</p> <ul> <li>Make sure new components follow existing patterns</li> <li>Keep interfaces consistent with the rest of the library</li> <li>Write thorough tests for new functionality</li> <li>Document public APIs clearly</li> <li>Optimize for readability and maintainability</li> </ul> <p>Design Example: Adding a New Sampler</p> <p>When adding a new sampler:</p> <ol> <li>Subclass the <code>Sampler</code> base class</li> <li>Implement required methods (<code>sample</code>, <code>sample_chain</code>)</li> <li>Follow the existing parameter naming conventions</li> <li>Add comprehensive documentation</li> <li>Write tests that verify the sampler's properties</li> <li>Optimize performance-critical sections</li> </ol> <ul> <li> <p> Project Structure</p> <p>Understand how the project is organized.</p> <p> Project Structure</p> </li> <li> <p> Core Components</p> <p>Learn about the core components in detail.</p> <p> Core Components</p> </li> <li> <p> Code Style</p> <p>Follow the project's coding standards.</p> <p> Code Style</p> </li> </ul>"},{"location":"developer_guide/development_setup/","title":"Development Setup","text":""},{"location":"developer_guide/development_setup/#development-setup","title":"Development Setup","text":"<p>Quick Start</p> <p>If you're just getting started with TorchEBM development, this guide will help you set up your environment properly.</p>"},{"location":"developer_guide/development_setup/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the TorchEBM development environment, make sure you have the following:</p> <ul> <li> <p> Python 3.9+</p> <p>TorchEBM requires Python 3.9 or higher.</p> <p> Install Python</p> </li> <li> <p> Git</p> <p>You'll need Git for version control.</p> <p> Install Git</p> </li> <li> <p> GitHub Account</p> <p>For contributing to the repository.</p> <p> Create GitHub Account</p> </li> </ul>"},{"location":"developer_guide/development_setup/#setting-up-your-environment","title":"Setting Up Your Environment","text":""},{"location":"developer_guide/development_setup/#1-fork-and-clone-the-repository","title":"1. Fork and Clone the Repository","text":"GitHub UIGitHub CLI <ol> <li>Navigate to TorchEBM repository</li> <li>Click the Fork button in the top-right corner</li> <li>Clone your fork to your local machine:    <pre><code>git clone https://github.com/YOUR-USERNAME/torchebm.git\ncd torchebm\n</code></pre></li> </ol> <pre><code>gh repo fork soran-ghaderi/torchebm --clone=true\ncd torchebm\n</code></pre>"},{"location":"developer_guide/development_setup/#2-set-up-virtual-environment","title":"2. Set Up Virtual Environment","text":"<p>It's recommended to use a virtual environment to manage dependencies:</p> venvconda <pre><code>python -m venv venv\n# Activate on Windows\nvenv\\Scripts\\activate\n# Activate on macOS/Linux\nsource venv/bin/activate\n</code></pre> <pre><code>conda create -n torchebm python=3.9\nconda activate torchebm\n</code></pre>"},{"location":"developer_guide/development_setup/#3-install-development-dependencies","title":"3. Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This will install TorchEBM in development mode along with all development dependencies.</p>"},{"location":"developer_guide/development_setup/#development-workflow","title":"Development Workflow","text":""},{"location":"developer_guide/development_setup/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"developer_guide/development_setup/#2-make-changes","title":"2. Make Changes","text":"<p>Make your changes to the codebase.</p>"},{"location":"developer_guide/development_setup/#3-run-tests","title":"3. Run Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"developer_guide/development_setup/#4-commit-changes","title":"4. Commit Changes","text":"<p>Follow our commit conventions.</p> <pre><code>git commit -m \"feat: add new feature\"\n</code></pre>"},{"location":"developer_guide/development_setup/#5-push-changes-and-create-a-pull-request","title":"5. Push Changes and Create a Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then go to GitHub and create a pull request.</p>"},{"location":"developer_guide/development_setup/#documentation-development","title":"Documentation Development","text":"<p>To preview documentation locally:</p> <pre><code>pip install -e \".[docs]\"\nmkdocs serve\n</code></pre> <p>This will start a local server at http://127.0.0.1:8000/.</p>"},{"location":"developer_guide/development_setup/#common-issues","title":"Common Issues","text":"<p>Missing CUDA?</p> <p>If you're developing CUDA extensions, ensure you have the right CUDA toolkit installed:</p> <pre><code>pip install torch==1.12.0+cu116 -f https://download.pytorch.org/whl/cu116/torch_stable.html\n</code></pre>"},{"location":"developer_guide/development_setup/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Code Style</p> <p>Learn about our coding standards.</p> <p> Code Style</p> </li> <li> <p> Testing Guide</p> <p>Learn how to write effective tests.</p> <p> Testing Guide</p> </li> <li> <p> API Design</p> <p>Understand our API design principles.</p> <p> API Design</p> </li> </ul>"},{"location":"developer_guide/implementation_energy/","title":"Energy Functions Implementation","text":""},{"location":"developer_guide/implementation_energy/#energy-functions-implementation","title":"Energy Functions Implementation","text":"<p>Implementation Details</p> <p>This guide provides detailed information about the implementation of energy functions in TorchEBM, including mathematical foundations, code structure, and optimization techniques.</p>"},{"location":"developer_guide/implementation_energy/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Energy-based models define a probability distribution through an energy function:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>where \\(E(x)\\) is the energy function and \\(Z = \\int e^{-E(x)} dx\\) is the normalization constant (partition function).</p> <p>The score function is the gradient of the log-probability:</p> \\[\\nabla_x \\log p(x) = -\\nabla_x E(x)\\] <p>This relationship is fundamental to many sampling and training methods in TorchEBM.</p>"},{"location":"developer_guide/implementation_energy/#base-energy-function-implementation","title":"Base Energy Function Implementation","text":"<p>The <code>BaseEnergyFunction</code> base class provides the foundation for all energy functions:</p> <pre><code>class BaseEnergyFunction(nn.Module):\n    \"\"\"Base class for all energy functions.\"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy for input x.\"\"\"\n        raise NotImplementedError\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score (gradient of energy) for input x.\"\"\"\n        x = x.requires_grad_(True)\n        energy = self.forward(x)\n        return torch.autograd.grad(energy.sum(), x, create_graph=True)[0]\n</code></pre> <p>Key design decisions:</p> <ol> <li>PyTorch <code>nn.Module</code> Base: Allows energy functions to have learnable parameters and use PyTorch's optimization tools</li> <li>Automatic Differentiation: Uses PyTorch's autograd for computing the score function</li> <li>Batched Computation: All methods support batched inputs for efficiency</li> </ol>"},{"location":"developer_guide/implementation_energy/#analytical-energy-functions","title":"Analytical Energy Functions","text":"<p>TorchEBM includes several analytical energy functions for testing and benchmarking. Here are detailed implementations of some key ones:</p>"},{"location":"developer_guide/implementation_energy/#gaussian-energy","title":"Gaussian Energy","text":"<p>The Gaussian energy function is defined as:</p> \\[E(x) = \\frac{1}{2}(x - \\mu)^T\\Sigma^{-1}(x - \\mu)\\] <p>Where \\(\\mu\\) is the mean vector and \\(\\Sigma\\) is the covariance matrix.</p> <pre><code>class GaussianEnergy(BaseEnergyFunction):\n    \"\"\"Gaussian energy function.\"\"\"\n\n    def __init__(self, mean: torch.Tensor, cov: torch.Tensor):\n        \"\"\"Initialize Gaussian energy function.\n\n        Args:\n            mean: Mean vector of batch_shape (dim,)\n            cov: Covariance matrix of batch_shape (dim, dim)\n        \"\"\"\n        super().__init__()\n        self.register_buffer(\"mean\", mean)\n        self.register_buffer(\"cov\", cov)\n        self.register_buffer(\"precision\", torch.inverse(cov))\n        self._dim = mean.size(0)\n\n        # Compute log determinant for normalization (optional)\n        self.register_buffer(\"log_det\", torch.logdet(cov))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute Gaussian energy.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        # Ensure x has the right batch_shape\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        # Center the data\n        centered = x - self.mean\n\n        # Compute quadratic form efficiently\n        return 0.5 * torch.sum(\n            centered * torch.matmul(centered, self.precision),\n            dim=1\n        )\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute score function analytically.\n\n        This is more efficient than using automatic differentiation.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size, dim) containing score values\n        \"\"\"\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        return -torch.matmul(x - self.mean, self.precision)\n</code></pre> <p>Implementation notes:</p> <ul> <li>We precompute the precision matrix (inverse covariance) for efficiency</li> <li>A specialized <code>score</code> method is provided that uses the analytical formula rather than automatic differentiation</li> <li>Input shape handling ensures both single samples and batches work correctly</li> </ul>"},{"location":"developer_guide/implementation_energy/#double-well-energy","title":"Double Well Energy","text":"<p>The double well energy function creates a bimodal distribution:</p> \\[E(x) = a(x^2 - b)^2\\] <pre><code>class DoubleWellEnergy(BaseEnergyFunction):\n    \"\"\"Double well energy function.\"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 2.0):\n        \"\"\"Initialize double well energy function.\n\n        Args:\n            a: Scale parameter controlling depth of wells\n            b: Parameter controlling the distance between wells\n        \"\"\"\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute double well energy.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        # Compute (x^2 - b)^2 for each dimension, then sum\n        return self.a * torch.sum((x**2 - self.b)**2, dim=1)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#rosenbrock-energy","title":"Rosenbrock Energy","text":"<p>The Rosenbrock function is a challenging test case with a narrow curved valley:</p> \\[E(x) = \\sum_{i=1}^{d-1} \\left[ a(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\] <pre><code>class RosenbrockEnergy(BaseEnergyFunction):\n    \"\"\"Rosenbrock energy function.\"\"\"\n\n    def __init__(self, a: float = 1.0, b: float = 100.0):\n        \"\"\"Initialize Rosenbrock energy function.\n\n        Args:\n            a: Scale parameter for the first term\n            b: Scale parameter for the second term (usually 100)\n        \"\"\"\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute Rosenbrock energy.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        batch_size, dim = x.shape\n        energy = torch.zeros(batch_size, device=x.device)\n\n        for i in range(dim - 1):\n            term1 = self.b * (x[:, i+1] - x[:, i]**2)**2\n            term2 = (x[:, i] - 1)**2\n            energy += term1 + term2\n\n        return energy\n</code></pre>"},{"location":"developer_guide/implementation_energy/#composite-energy-functions","title":"Composite Energy Functions","text":"<p>TorchEBM supports composing energy functions to create more complex landscapes:</p> <pre><code>class CompositeEnergy(BaseEnergyFunction):\n    \"\"\"Composite energy function.\"\"\"\n\n    def __init__(\n        self,\n        energy_functions: List[BaseEnergyFunction],\n        weights: Optional[List[float]] = None,\n        operation: str = \"sum\"\n    ):\n        \"\"\"Initialize composite energy function.\n\n        Args:\n            energy_functions: List of energy functions to combine\n            weights: Optional weights for each energy function\n            operation: How to combine energy functions (\"sum\", \"product\", \"min\", \"max\")\n        \"\"\"\n        super().__init__()\n        self.energy_functions = nn.ModuleList(energy_functions)\n\n        if weights is None:\n            weights = [1.0] * len(energy_functions)\n        self.register_buffer(\"weights\", torch.tensor(weights))\n\n        if operation not in [\"sum\", \"product\", \"min\", \"max\"]:\n            raise ValueError(f\"Unknown operation: {operation}\")\n        self.operation = operation\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute composite energy.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        energies = [f(x) * w for f, w in zip(self.energy_functions, self.weights)]\n\n        if self.operation == \"sum\":\n            return torch.sum(torch.stack(energies), dim=0)\n        elif self.operation == \"product\":\n            return torch.prod(torch.stack(energies), dim=0)\n        elif self.operation == \"min\":\n            return torch.min(torch.stack(energies), dim=0)[0]\n        elif self.operation == \"max\":\n            return torch.max(torch.stack(energies), dim=0)[0]\n</code></pre>"},{"location":"developer_guide/implementation_energy/#neural-network-energy-functions","title":"Neural Network Energy Functions","text":"<p>Neural networks can parameterize energy functions for flexibility:</p> <pre><code>class MLPEnergy(BaseEnergyFunction):\n    \"\"\"Multi-layer perceptron energy function.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dims: List[int],\n        activation: Callable = nn.SiLU\n    ):\n        \"\"\"Initialize MLP energy function.\n\n        Args:\n            input_dim: Input dimensionality\n            hidden_dims: List of hidden layer dimensions\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n\n        # Build MLP layers\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(activation())\n            prev_dim = hidden_dim\n\n        # Final layer with scalar output\n        layers.append(nn.Linear(prev_dim, 1))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy using the MLP.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, input_dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"developer_guide/implementation_energy/#efficient-gradient-computation","title":"Efficient Gradient Computation","text":"<p>For gradients, TorchEBM provides optimized implementations:</p> <pre><code>def efficient_grad(energy_fn: BaseEnergyFunction, x: torch.Tensor, create_graph: bool = False) -&gt; torch.Tensor:\n    \"\"\"Compute gradient of energy function efficiently.\n\n    Args:\n        energy_fn: Energy function\n        x: Input tensor of batch_shape (batch_size, dim)\n        create_graph: Whether to create gradient graph (for higher-order gradients)\n\n    Returns:\n        Gradient tensor of batch_shape (batch_size, dim)\n    \"\"\"\n    x.requires_grad_(True)\n    with torch.enable_grad():\n        energy = energy_fn(x)\n\n    grad = torch.autograd.grad(\n        energy.sum(), x, create_graph=create_graph\n    )[0]\n\n    return grad\n</code></pre>"},{"location":"developer_guide/implementation_energy/#cuda-implementations","title":"CUDA Implementations","text":"<p>For performance-critical operations, TorchEBM includes CUDA implementations:</p> <pre><code>def cuda_score_function(energy_fn, x):\n    \"\"\"CUDA-optimized score function computation.\"\"\"\n    # Use energy_fn's custom CUDA implementation if available\n    if hasattr(energy_fn, 'cuda_score') and torch.cuda.is_available():\n        return energy_fn.cuda_score(x)\n    else:\n        # Fall back to autograd\n        return energy_fn.score(x)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#factory-methods","title":"Factory Methods","text":"<p>Factory methods provide convenient ways to create energy functions:</p> <pre><code>@classmethod\ndef create_standard_gaussian(cls, dim: int) -&gt; 'GaussianEnergy':\n    \"\"\"Create a standard Gaussian energy function.\n\n    Args:\n        dim: Dimensionality\n\n    Returns:\n        GaussianEnergy with zero mean and identity covariance\n    \"\"\"\n    return cls(mean=torch.zeros(dim), cov=torch.eye(dim))\n\n@classmethod\ndef from_samples(cls, samples: torch.Tensor, regularization: float = 1e-4) -&gt; 'GaussianEnergy':\n    \"\"\"Create a Gaussian energy function from data samples.\n\n    Args:\n        samples: Data samples of batch_shape (n_samples, dim)\n        regularization: Small value added to diagonal for numerical stability\n\n    Returns:\n        GaussianEnergy fit to the samples\n    \"\"\"\n    mean = samples.mean(dim=0)\n    cov = torch.cov(samples.T) + regularization * torch.eye(samples.size(1))\n    return cls(mean=mean, cov=cov)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#implementation-challenges-and-solutions","title":"Implementation Challenges and Solutions","text":""},{"location":"developer_guide/implementation_energy/#numerical-stability","title":"Numerical Stability","text":"<p>Energy functions must be numerically stable:</p> <pre><code>class NumericallyStableEnergy(BaseEnergyFunction):\n    \"\"\"Energy function with numerical stability considerations.\"\"\"\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy with numerical stability.\n\n        Uses log-sum-exp trick for numerical stability.\n        \"\"\"\n        # Example of numerical stability in computation\n        terms = self.compute_terms(x)\n        max_term = torch.max(terms, dim=1, keepdim=True)[0]\n        stable_energy = max_term + torch.log(torch.sum(\n            torch.exp(terms - max_term), dim=1\n        ))\n        return stable_energy\n</code></pre>"},{"location":"developer_guide/implementation_energy/#handling-multi-modal-distributions","title":"Handling Multi-Modal Distributions","text":"<p>For multi-modal distributions:</p> <pre><code>class MixtureEnergy(BaseEnergyFunction):\n    \"\"\"Mixture of energy functions.\"\"\"\n\n    def __init__(self, components: List[BaseEnergyFunction], weights: Optional[List[float]] = None):\n        \"\"\"Initialize mixture energy function.\n\n        Args:\n            components: List of component energy functions\n            weights: Optional weights for each component\n        \"\"\"\n        super().__init__()\n        self.components = nn.ModuleList(components)\n\n        if weights is None:\n            weights = [1.0] * len(components)\n        self.register_buffer(\"log_weights\", torch.log(torch.tensor(weights)))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute mixture energy using log-sum-exp for stability.\"\"\"\n        energies = torch.stack([f(x) for f in self.components], dim=1)\n        weighted_energies = -self.log_weights - energies\n\n        # Use log-sum-exp trick for numerical stability\n        max_val = torch.max(weighted_energies, dim=1, keepdim=True)[0]\n        stable_energy = -max_val - torch.log(torch.sum(\n            torch.exp(weighted_energies - max_val), dim=1\n        ))\n\n        return stable_energy\n</code></pre>"},{"location":"developer_guide/implementation_energy/#testing-energy-functions","title":"Testing Energy Functions","text":"<p>TorchEBM includes comprehensive testing utilities for energy functions:</p> <pre><code>def test_energy_function(energy_fn: BaseEnergyFunction, dim: int, n_samples: int = 1000) -&gt; dict:\n    \"\"\"Test an energy function for correctness and properties.\n\n    Args:\n        energy_fn: Energy function to test\n        dim: Input dimensionality\n        n_samples: Number of test samples\n\n    Returns:\n        Dictionary with test results\n    \"\"\"\n    # Generate random samples\n    x = torch.randn(n_samples, dim)\n\n    # Test energy computation\n    energy = energy_fn(x)\n    assert energy.shape == (n_samples,)\n\n    # Test score computation\n    score = energy_fn.score(x)\n    assert score.shape == (n_samples, dim)\n\n    # Test gradient consistency\n    manual_grad = torch.autograd.grad(\n        energy_fn(x).sum(), x, create_graph=True\n    )[0]\n    assert torch.allclose(score, -manual_grad, atol=1e-5, rtol=1e-5)\n\n    return {\n        \"energy_mean\": energy.mean().item(),\n        \"energy_std\": energy.std().item(),\n        \"score_mean\": score.mean().item(),\n        \"score_std\": score.std().item(),\n    }\n</code></pre>"},{"location":"developer_guide/implementation_energy/#best-practices-for-custom-energy-functions","title":"Best Practices for Custom Energy Functions","text":"<p>When implementing custom energy functions, follow these best practices:</p> <p>Custom Energy Function Example</p> <pre><code>class CustomEnergy(BaseEnergyFunction):\n    \"\"\"Custom energy function example.\"\"\"\n\n    def __init__(self, scale: float = 1.0):\n        super().__init__()\n        self.scale = scale\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Ensure correct input shape\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        # Compute energy using vectorized operations\n        return self.scale * torch.sum(torch.sin(x) ** 2, dim=1)\n\n    def score(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Analytical gradient\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n\n        return -2 * self.scale * torch.sin(x) * torch.cos(x)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#do","title":"Do","text":"<ul> <li>Implement a custom <code>score</code> method if an analytical gradient is available</li> <li>Use vectorized operations for performance</li> <li>Register parameters and buffers properly</li> <li>Handle batched inputs consistently</li> <li>Add factory methods for common use cases</li> </ul>"},{"location":"developer_guide/implementation_energy/#dont","title":"Don't","text":"<ul> <li>Use loops when vectorized operations are possible</li> <li>Recompute values that could be cached</li> <li>Modify inputs in-place</li> <li>Forget to handle edge cases</li> <li>Ignore numerical stability</li> </ul>"},{"location":"developer_guide/implementation_energy/#debugging-energy-functions","title":"Debugging Energy Functions","text":"<p>Common issues with energy functions include:</p> <ol> <li>NaN/Inf Values: Check for division by zero or log of negative numbers</li> <li>Poor Sampling: Energy may not be well-defined or have numerical issues</li> <li>Training Instability: Energy might grow unbounded or collapse</li> </ol> <p>Debugging techniques:</p> <pre><code>def debug_energy_function(energy_fn: BaseEnergyFunction, x: torch.Tensor) -&gt; None:\n    \"\"\"Debug an energy function for common issues.\"\"\"\n    # Check for NaN/Inf in energy\n    energy = energy_fn(x)\n    if torch.isnan(energy).any() or torch.isinf(energy).any():\n        print(\"Warning: Energy contains NaN or Inf values\")\n\n    # Check for NaN/Inf in score\n    score = energy_fn.score(x)\n    if torch.isnan(score).any() or torch.isinf(score).any():\n        print(\"Warning: Score contains NaN or Inf values\")\n\n    # Check score magnitude\n    score_norm = torch.norm(score, dim=1)\n    if (score_norm &gt; 1e3).any():\n        print(\"Warning: Score has very large values\")\n\n    # Check energy range\n    if energy.max() - energy.min() &gt; 1e6:\n        print(\"Warning: Energy has a very large range\")\n</code></pre>"},{"location":"developer_guide/implementation_energy/#advanced-topics","title":"Advanced Topics","text":""},{"location":"developer_guide/implementation_energy/#spherical-energy-functions","title":"Spherical Energy Functions","text":"<p>Energy functions on constrained domains:</p> <pre><code>class SphericalEnergy(BaseEnergyFunction):\n    \"\"\"Energy function defined on a unit sphere.\"\"\"\n\n    def __init__(self, base_energy: BaseEnergyFunction):\n        \"\"\"Initialize spherical energy function.\n\n        Args:\n            base_energy: Base energy function\n        \"\"\"\n        super().__init__()\n        self.base_energy = base_energy\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy on unit sphere.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        # Project to unit sphere\n        x_normalized = F.normalize(x, p=2, dim=1)\n        return self.base_energy(x_normalized)\n</code></pre>"},{"location":"developer_guide/implementation_energy/#energy-from-density-model","title":"Energy from Density Model","text":"<p>Creating an energy function from a density model:</p> <pre><code>class DensityModelEnergy(BaseEnergyFunction):\n    \"\"\"Energy function from a density model.\"\"\"\n\n    def __init__(self, density_model: Callable):\n        \"\"\"Initialize energy function from density model.\n\n        Args:\n            density_model: Model that computes log probability\n        \"\"\"\n        super().__init__()\n        self.density_model = density_model\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy as negative log probability.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, dim)\n\n        Returns:\n            Tensor of batch_shape (batch_size,) containing energy values\n        \"\"\"\n        log_prob = self.density_model.log_prob(x)\n        return -log_prob\n</code></pre>"},{"location":"developer_guide/implementation_energy/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Samplers</p> <p>Explore how samplers work with energy functions.</p> <p> Samplers</p> </li> <li> <p> Code Style</p> <p>Follow coding standards when implementing energy functions.</p> <p> Code Style</p> </li> </ul>"},{"location":"developer_guide/implementation_losses/","title":"BaseLoss Functions Implementation","text":""},{"location":"developer_guide/implementation_losses/#baseloss-functions-implementation","title":"BaseLoss Functions Implementation","text":"<p>Implementation Details</p> <p>This guide provides detailed information about the implementation of loss functions in TorchEBM, including mathematical foundations, code structure, and optimization techniques.</p>"},{"location":"developer_guide/implementation_losses/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Energy-based models can be trained using various loss functions, each with different properties. The primary goal is to shape the energy landscape such that observed data has low energy while other regions have high energy.</p>"},{"location":"developer_guide/implementation_losses/#base-baseloss-implementation","title":"Base BaseLoss Implementation","text":"<p>The <code>BaseLoss</code> base class provides the foundation for all loss functions:</p> <pre><code>from abc import ABC, abstractmethod\nimport torch\nfrom typing import Optional, Dict, Any, Tuple\n\nfrom torchebm.core import BaseEnergyFunction\n\n\nclass BaseLoss(ABC):\n    \"\"\"Base class for all loss functions.\"\"\"\n\n    def __init__(self, energy_function: BaseEnergyFunction):\n        \"\"\"Initialize loss with an energy function.\n\n        Args:\n            energy_function: The energy function to train\n        \"\"\"\n        self.energy_function = energy_function\n\n    @abstractmethod\n    def __call__(\n            self,\n            pos_samples: torch.Tensor,\n            neg_samples: torch.Tensor,\n            **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Negative samples from the model distribution\n            **kwargs: Additional loss-specific parameters\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/implementation_losses/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background","title":"Mathematical Background","text":"<p>The MLE approach aims to maximize the log-likelihood of the data under the model:</p> \\[\\mathcal{L}_{\\text{MLE}} = \\mathbb{E}_{p_{\\text{data}}(x)}[E(x)] - \\mathbb{E}_{p_{\\text{model}}(x)}[E(x)]\\]"},{"location":"developer_guide/implementation_losses/#implementation","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses.base import BaseLoss\n\n\nclass MLELoss(BaseLoss):\n    \"\"\"Maximum Likelihood Estimation loss.\"\"\"\n\n    def __init__(\n            self,\n            energy_function: BaseEnergyFunction,\n            alpha: float = 1.0,\n            regularization: Optional[str] = None,\n            reg_strength: float = 0.0\n    ):\n        \"\"\"Initialize MLE loss.\n\n        Args:\n            energy_function: Energy function to train\n            alpha: Weight for the negative phase\n            regularization: Type of regularization ('l1', 'l2', or None)\n            reg_strength: Strength of regularization\n        \"\"\"\n        super().__init__(energy_function)\n        self.alpha = alpha\n        self.regularization = regularization\n        self.reg_strength = reg_strength\n\n    def __call__(\n            self,\n            pos_samples: torch.Tensor,\n            neg_samples: torch.Tensor,\n            **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the MLE loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Negative samples from the model distribution\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Compute loss components\n        pos_term = pos_energy.mean()\n        neg_term = neg_energy.mean()\n\n        # Full loss\n        loss = pos_term - self.alpha * neg_term\n\n        # Add regularization if specified\n        reg_loss = torch.tensor(0.0, device=pos_energy.device)\n        if self.regularization is not None and self.reg_strength &gt; 0:\n            if self.regularization == 'l2':\n                for param in self.energy_function.parameters():\n                    reg_loss += torch.sum(param ** 2)\n            elif self.regularization == 'l1':\n                for param in self.energy_function.parameters():\n                    reg_loss += torch.sum(torch.abs(param))\n\n            loss = loss + self.reg_strength * reg_loss\n\n        # Metrics to track\n        metrics = {\n            'pos_energy': pos_term.detach(),\n            'neg_energy': neg_term.detach(),\n            'energy_gap': (neg_term - pos_term).detach(),\n            'loss': loss.detach(),\n            'reg_loss': reg_loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#contrastive-divergence-cd","title":"Contrastive Divergence (CD)","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_1","title":"Mathematical Background","text":"<p>Contrastive Divergence is a variant of MLE that uses a specific sampling scheme where negative samples are obtained by starting from positive samples and running MCMC for a few steps:</p> \\[\\mathcal{L}_{\\text{CD}} = \\mathbb{E}_{p_{\\text{data}}(x)}[E(x)] - \\mathbb{E}_{p_{K}(x|x_{\\text{data}})}[E(x)]\\] <p>where \\(p_{K}(x|x_{\\text{data}})\\) is the distribution after \\(K\\) steps of MCMC starting from data samples.</p>"},{"location":"developer_guide/implementation_losses/#implementation_1","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple, Optional\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import Sampler, LangevinDynamics\nfrom torchebm.losses.base import BaseLoss\n\n\nclass ContrastiveDivergenceLoss(BaseLoss):\n    \"\"\"Contrastive Divergence loss.\"\"\"\n\n    def __init__(\n            self,\n            energy_function: BaseEnergyFunction,\n            sampler: Optional[Sampler] = None,\n            n_steps: int = 10,\n            alpha: float = 1.0\n    ):\n        \"\"\"Initialize CD loss.\n\n        Args:\n            energy_function: Energy function to train\n            sampler: Sampler for generating negative samples\n            n_steps: Number of sampling steps for negative samples\n            alpha: Weight for the negative phase\n        \"\"\"\n        super().__init__(energy_function)\n        self.sampler = sampler or LangevinDynamics(energy_function)\n        self.n_steps = n_steps\n        self.alpha = alpha\n\n    def __call__(\n            self,\n            pos_samples: torch.Tensor,\n            neg_samples: Optional[torch.Tensor] = None,\n            **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the CD loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Optional negative samples (if None, will be generated)\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Generate negative samples if not provided\n        if neg_samples is None:\n            with torch.no_grad():\n                neg_samples = self.sampler.sample(\n                    pos_samples.shape[1],\n                    self.n_steps,\n                    n_samples=pos_samples.shape[0],\n                    initial_samples=pos_samples.detach()\n                )\n\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Compute loss components\n        pos_term = pos_energy.mean()\n        neg_term = neg_energy.mean()\n\n        # Full loss\n        loss = pos_term - self.alpha * neg_term\n\n        # Metrics to track\n        metrics = {\n            'pos_energy': pos_term.detach(),\n            'neg_energy': neg_term.detach(),\n            'energy_gap': (neg_term - pos_term).detach(),\n            'loss': loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#noise-contrastive-estimation-nce","title":"Noise Contrastive Estimation (NCE)","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_2","title":"Mathematical Background","text":"<p>NCE treats the problem as a binary classification task, distinguishing between data samples and noise samples:</p> \\[\\mathcal{L}_{\\text{NCE}} = -\\mathbb{E}_{p_{\\text{data}}(x)}[\\log \\sigma(f_\\theta(x))] - \\mathbb{E}_{p_{\\text{noise}}(x)}[\\log (1 - \\sigma(f_\\theta(x)))]\\] <p>where \\(f_\\theta(x) = -E(x) - \\log Z\\) and \\(\\sigma\\) is the sigmoid function.</p>"},{"location":"developer_guide/implementation_losses/#implementation_2","title":"Implementation","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses.base import BaseLoss\n\n\nclass NCELoss(BaseLoss):\n    \"\"\"Noise Contrastive Estimation loss.\"\"\"\n\n    def __init__(\n            self,\n            energy_function: BaseEnergyFunction,\n            log_partition: float = 0.0,\n            learn_partition: bool = True\n    ):\n        \"\"\"Initialize NCE loss.\n\n        Args:\n            energy_function: Energy function to train\n            log_partition: Initial value of log partition function\n            learn_partition: Whether to learn the partition function\n        \"\"\"\n        super().__init__(energy_function)\n        if learn_partition:\n            self.log_z = torch.nn.Parameter(torch.tensor([log_partition], dtype=torch.float32))\n        else:\n            self.register_buffer('log_z', torch.tensor([log_partition], dtype=torch.float32))\n        self.learn_partition = learn_partition\n\n    def __call__(\n            self,\n            pos_samples: torch.Tensor,\n            neg_samples: torch.Tensor,\n            **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the NCE loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Negative samples from noise distribution\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Compute logits\n        pos_logits = -pos_energy - self.log_z\n        neg_logits = -neg_energy - self.log_z\n\n        # Binary classification loss\n        pos_loss = F.binary_cross_entropy_with_logits(\n            pos_logits,\n            torch.ones_like(pos_logits)\n        )\n        neg_loss = F.binary_cross_entropy_with_logits(\n            neg_logits,\n            torch.zeros_like(neg_logits)\n        )\n\n        # Full loss\n        loss = pos_loss + neg_loss\n\n        # Metrics to track\n        metrics = {\n            'pos_loss': pos_loss.detach(),\n            'neg_loss': neg_loss.detach(),\n            'loss': loss.detach(),\n            'log_z': self.log_z.detach(),\n            'pos_energy': pos_energy.mean().detach(),\n            'neg_energy': neg_energy.mean().detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#score-matching","title":"Score Matching","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_3","title":"Mathematical Background","text":"<p>Score Matching minimizes the difference between the model's score function (gradient of log-probability) and the data score:</p> \\[\\mathcal{L}_{\\text{SM}} = \\frac{1}{2}\\mathbb{E}_{p_{\\text{data}}(x)}\\left[\\left\\|\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\text{model}}(x)\\right\\|^2\\right]\\] <p>This can be simplified to:</p> \\[\\mathcal{L}_{\\text{SM}} = \\mathbb{E}_{p_{\\text{data}}(x)}\\left[\\text{tr}(\\nabla_x^2 E(x)) + \\frac{1}{2}\\|\\nabla_x E(x)\\|^2\\right]\\]"},{"location":"developer_guide/implementation_losses/#implementation_3","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses.base import BaseLoss\n\n\nclass ScoreMatchingLoss(BaseLoss):\n    \"\"\"Score Matching loss.\"\"\"\n\n    def __init__(\n            self,\n            energy_function: BaseEnergyFunction,\n            implicit: bool = True\n    ):\n        \"\"\"Initialize Score Matching loss.\n\n        Args:\n            energy_function: Energy function to train\n            implicit: Whether to use implicit score matching\n        \"\"\"\n        super().__init__(energy_function)\n        self.implicit = implicit\n\n    def _compute_explicit_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute explicit score matching loss.\n\n        This requires computing both the score and the Hessian trace.\n\n        Args:\n            x: Input samples of batch_shape (n_samples, dim)\n\n        Returns:\n            BaseLoss value\n        \"\"\"\n        x.requires_grad_(True)\n\n        # Compute energy\n        energy = self.energy_function(x)\n\n        # Compute score (first derivatives)\n        score = torch.autograd.grad(\n            energy.sum(), x, create_graph=True\n        )[0]\n\n        # Compute trace of Hessian (second derivatives)\n        trace = 0.0\n        for i in range(x.shape[1]):\n            grad_score_i = torch.autograd.grad(\n                score[:, i].sum(), x, create_graph=True\n            )[0]\n            trace += grad_score_i[:, i]\n\n        # Compute squared norm of score\n        score_norm = torch.sum(score ** 2, dim=1)\n\n        # Full loss\n        loss = trace + 0.5 * score_norm\n\n        return loss.mean()\n\n    def _compute_implicit_score_matching(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute implicit score matching loss.\n\n        This avoids computing the Hessian trace.\n\n        Args:\n            x: Input samples of batch_shape (n_samples, dim)\n\n        Returns:\n            BaseLoss value\n        \"\"\"\n        # Add noise to inputs\n        x_noise = x + torch.randn_like(x) * 0.01\n        x_noise.requires_grad_(True)\n\n        # Compute energy and its gradient\n        energy = self.energy_function(x_noise)\n        score = torch.autograd.grad(\n            energy.sum(), x_noise, create_graph=True\n        )[0]\n\n        # Compute loss as squared difference between gradient and vector field\n        vector_field = (x_noise - x) / (0.01 ** 2)\n        loss = 0.5 * torch.sum((score + vector_field) ** 2, dim=1)\n\n        return loss.mean()\n\n    def __call__(\n            self,\n            pos_samples: torch.Tensor,\n            neg_samples: torch.Tensor = None,\n            **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the Score Matching loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Not used in Score Matching\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        # Compute loss based on method\n        if self.implicit:\n            loss = self._compute_implicit_score_matching(pos_samples)\n        else:\n            loss = self._compute_explicit_score_matching(pos_samples)\n\n        # Metrics to track\n        metrics = {\n            'loss': loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#denoising-score-matching","title":"Denoising Score Matching","text":""},{"location":"developer_guide/implementation_losses/#mathematical-background_4","title":"Mathematical Background","text":"<p>Denoising Score Matching is a variant of score matching that adds noise to the data and tries to predict the score of the noisy distribution:</p> \\[\\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{p_{\\text{data}}(x)}\\mathbb{E}_{q_\\sigma(\\tilde{x}|x)}\\left[\\left\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x) - \\nabla_{\\tilde{x}} \\log p_{\\text{model}}(\\tilde{x})\\right\\|^2\\right]\\] <p>where \\(q_\\sigma(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}|x, \\sigma^2\\mathbf{I})\\).</p>"},{"location":"developer_guide/implementation_losses/#implementation_4","title":"Implementation","text":"<pre><code>import torch\nfrom typing import Dict, Tuple, Union, List\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses.base import BaseLoss\n\n\nclass DenoisingScoreMatchingLoss(BaseLoss):\n    \"\"\"Denoising Score Matching loss.\"\"\"\n\n    def __init__(\n            self,\n            energy_function: BaseEnergyFunction,\n            sigma: Union[float, List[float]] = 0.01\n    ):\n        \"\"\"Initialize DSM loss.\n\n        Args:\n            energy_function: Energy function to train\n            sigma: Noise level(s) for denoising\n        \"\"\"\n        super().__init__(energy_function)\n        if isinstance(sigma, (int, float)):\n            self.sigma = [float(sigma)]\n        else:\n            self.sigma = sigma\n\n    def __call__(\n            self,\n            pos_samples: torch.Tensor,\n            neg_samples: torch.Tensor = None,\n            **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the DSM loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Not used in DSM\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        total_loss = 0.0\n        metrics = {}\n\n        for i, sigma in enumerate(self.sigma):\n            # Add noise to inputs\n            noise = torch.randn_like(pos_samples) * sigma\n            x_noisy = pos_samples + noise\n\n            # Compute score of model\n            x_noisy.requires_grad_(True)\n            energy = self.energy_function(x_noisy)\n            score_model = torch.autograd.grad(\n                energy.sum(), x_noisy, create_graph=True\n            )[0]\n\n            # Target score (gradient of log density of noise model)\n            # For Gaussian noise, this is -(x_noisy - pos_samples) / sigma^2\n            score_target = -noise / (sigma ** 2)\n\n            # Compute loss\n            loss_sigma = 0.5 * torch.sum((score_model + score_target) ** 2, dim=1).mean()\n            total_loss += loss_sigma\n\n            metrics[f'loss_sigma_{sigma}'] = loss_sigma.detach()\n\n        # Average loss over all noise levels\n        avg_loss = total_loss / len(self.sigma)\n        metrics['loss'] = avg_loss.detach()\n\n        return avg_loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#slicedscorematching","title":"SlicedScoreMatching","text":"<pre><code>import torch\nfrom typing import Dict, Tuple\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses.base import BaseLoss\n\n\nclass SlicedScoreMatchingLoss(BaseLoss):\n    \"\"\"Sliced Score Matching loss.\"\"\"\n\n    def __init__(\n            self,\n            energy_function: BaseEnergyFunction,\n            n_projections: int = 1\n    ):\n        \"\"\"Initialize SSM loss.\n\n        Args:\n            energy_function: Energy function to train\n            n_projections: Number of random projections\n        \"\"\"\n        super().__init__(energy_function)\n        self.n_projections = n_projections\n\n    def __call__(\n            self,\n            pos_samples: torch.Tensor,\n            neg_samples: torch.Tensor = None,\n            **kwargs\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Compute the SSM loss.\n\n        Args:\n            pos_samples: Positive samples from the data distribution\n            neg_samples: Not used in SSM\n\n        Returns:\n            Tuple of (loss value, dictionary of metrics)\n        \"\"\"\n        x = pos_samples.detach().requires_grad_(True)\n\n        # Compute energy\n        energy = self.energy_function(x)\n\n        # Compute score (first derivatives)\n        score = torch.autograd.grad(\n            energy.sum(), x, create_graph=True\n        )[0]\n\n        total_loss = 0.0\n        for _ in range(self.n_projections):\n            # Generate random vectors\n            v = torch.randn_like(x)\n            v = v / torch.norm(v, p=2, dim=1, keepdim=True)\n\n            # Compute directional derivative\n            Jv = torch.sum(score * v, dim=1)\n\n            # Compute second directional derivative\n            J2v = torch.autograd.grad(\n                Jv.sum(), x, create_graph=True\n            )[0]\n\n            # Compute sliced score matching loss terms\n            loss_1 = torch.sum(J2v * v, dim=1)\n            loss_2 = 0.5 * torch.sum(score ** 2, dim=1)\n\n            # Full loss\n            loss = loss_1 + loss_2\n            total_loss += loss.mean()\n\n        # Average loss over projections\n        avg_loss = total_loss / self.n_projections\n\n        # Metrics to track\n        metrics = {\n            'loss': avg_loss.detach()\n        }\n\n        return avg_loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#performance-optimizations","title":"Performance Optimizations","text":"<p>For computationally intensive loss functions like Score Matching, we can use vectorized operations and CUDA optimizations:</p> <pre><code>def batched_hessian_trace(energy_function, x, batch_size=16):\n    \"\"\"Compute the trace of the Hessian in batches to save memory.\"\"\"\n    x.requires_grad_(True)\n    trace = torch.zeros(x.size(0), device=x.device)\n\n    # Compute energy and score\n    energy = energy_function(x)\n    score = torch.autograd.grad(\n        energy.sum(), x, create_graph=True\n    )[0]\n\n    # Compute trace of Hessian in batches\n    for i in range(0, x.size(1), batch_size):\n        end_i = min(i + batch_size, x.size(1))\n        sub_dims = list(range(i, end_i))\n\n        for j in sub_dims:\n            # Compute diagonal elements of Hessian\n            grad_score_j = torch.autograd.grad(\n                score[:, j].sum(), x, create_graph=True\n            )[0]\n            trace += grad_score_j[:, j]\n\n    return trace\n</code></pre>"},{"location":"developer_guide/implementation_losses/#factory-methods","title":"Factory Methods","text":"<p>Factory methods simplify loss creation:</p> <pre><code>def create_loss(\n    loss_type: str,\n    energy_function: BaseEnergyFunction,\n    **kwargs\n) -&gt; BaseLoss:\n    \"\"\"Create a loss function instance.\n\n    Args:\n        loss_type: Type of loss function\n        energy_function: Energy function to train\n        **kwargs: BaseLoss-specific parameters\n\n    Returns:\n        BaseLoss instance\n    \"\"\"\n    if loss_type.lower() == 'mle':\n        return MLELoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'cd':\n        return ContrastiveDivergenceLoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'nce':\n        return NCELoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'sm':\n        return ScoreMatchingLoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'dsm':\n        return DenoisingScoreMatchingLoss(energy_function, **kwargs)\n    elif loss_type.lower() == 'ssm':\n        return SlicedScoreMatchingLoss(energy_function, **kwargs)\n    else:\n        raise ValueError(f\"Unknown loss type: {loss_type}\")\n</code></pre>"},{"location":"developer_guide/implementation_losses/#testing-baseloss-functions","title":"Testing BaseLoss Functions","text":"<p>For testing loss implementations:</p> <pre><code>def validate_loss_gradients(\n    loss_fn: BaseLoss,\n    dim: int = 2,\n    n_samples: int = 10,\n    seed: int = 42\n) -&gt; bool:\n    \"\"\"Validate that loss function produces valid gradients.\n\n    Args:\n        loss_fn: BaseLoss function to test\n        dim: Dimensionality of test samples\n        n_samples: Number of test samples\n        seed: Random seed\n\n    Returns:\n        True if validation passes, False otherwise\n    \"\"\"\n    torch.manual_seed(seed)\n\n    # Generate test samples\n    pos_samples = torch.randn(n_samples, dim)\n    neg_samples = torch.randn(n_samples, dim)\n\n    # Ensure parameters require grad\n    for param in loss_fn.energy_function.parameters():\n        param.requires_grad_(True)\n\n    # Compute loss\n    loss, _ = loss_fn(pos_samples, neg_samples)\n\n    # Check if loss is scalar\n    if not isinstance(loss, torch.Tensor) or loss.numel() != 1:\n        print(f\"BaseLoss is not a scalar: {loss}\")\n        return False\n\n    # Check if loss produces gradients\n    try:\n        loss.backward()\n        has_grad = all(p.grad is not None for p in loss_fn.energy_function.parameters())\n        if not has_grad:\n            print(\"Some parameters did not receive gradients\")\n            return False\n    except Exception as e:\n        print(f\"Error during backward pass: {e}\")\n        return False\n\n    return True\n</code></pre>"},{"location":"developer_guide/implementation_losses/#best-practices-for-custom-baseloss-functions","title":"Best Practices for Custom BaseLoss Functions","text":"<p>When implementing custom loss functions, follow these best practices:</p> <p>Custom BaseLoss Example</p> <pre><code>class CustomLoss(BaseLoss):\n    \"\"\"Custom loss example.\"\"\"\n\n    def __init__(self, energy_function, alpha=1.0, beta=0.5):\n        super().__init__(energy_function)\n        self.alpha = alpha\n        self.beta = beta\n\n    def __call__(self, pos_samples, neg_samples, **kwargs):\n        # Compute energies\n        pos_energy = self.energy_function(pos_samples)\n        neg_energy = self.energy_function(neg_samples)\n\n        # Custom loss logic\n        loss = (pos_energy.mean() - self.alpha * neg_energy.mean()) + \\\n               self.beta * torch.abs(pos_energy.mean() - neg_energy.mean())\n\n        # Return loss and metrics\n        metrics = {\n            'pos_energy': pos_energy.mean().detach(),\n            'neg_energy': neg_energy.mean().detach(),\n            'loss': loss.detach()\n        }\n\n        return loss, metrics\n</code></pre>"},{"location":"developer_guide/implementation_losses/#do","title":"Do","text":"<ul> <li>Subclass the <code>BaseLoss</code> base class</li> <li>Return both the loss and metrics dictionary</li> <li>Validate inputs</li> <li>Use autograd for derivatives</li> <li>Consider numerical stability</li> </ul>"},{"location":"developer_guide/implementation_losses/#dont","title":"Don't","text":"<ul> <li>Modify input tensors in-place</li> <li>Compute unnecessary gradients</li> <li>Forget to detach metrics</li> <li>Mix device types</li> <li>Ignore potential NaN values</li> </ul>"},{"location":"developer_guide/implementation_losses/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Energy Functions</p> <p>Explore energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Understand sampler implementation details.</p> <p> Implementation Samplers</p> </li> </ul>"},{"location":"developer_guide/implementation_models/","title":"Models Implementation","text":""},{"location":"developer_guide/implementation_models/#models-implementation","title":"Models Implementation","text":"<p>Implementation Details</p> <p>This guide explains the implementation of neural network models in TorchEBM, including architecture designs, training workflows, and integration with energy functions.</p>"},{"location":"developer_guide/implementation_models/#base-model-architecture","title":"Base Model Architecture","text":"<p>The <code>BaseModel</code> class provides the foundation for all neural networks in TorchEBM:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom typing import Tuple, List, Dict, Any, Optional, Union\n\nclass BaseModel(nn.Module):\n    \"\"\"Base class for all neural network models.\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dims: List[int], activation: Optional[nn.Module] = None):\n        \"\"\"Initialize base model.\n\n        Args:\n            input_dim: Input dimension\n            hidden_dims: List of hidden dimensions\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dims = hidden_dims\n        self.activation = activation or nn.ReLU()\n\n        # Build network architecture\n        self._build_network()\n\n    def _build_network(self):\n        \"\"\"Build the neural network architecture.\"\"\"\n        layers = []\n\n        # Input layer\n        prev_dim = self.input_dim\n\n        # Hidden layers\n        for dim in self.hidden_dims:\n            layers.append(nn.Linear(prev_dim, dim))\n            layers.append(self.activation)\n            prev_dim = dim\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the network.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, input_dim)\n\n        Returns:\n            Output tensor\n        \"\"\"\n        return self.network(x)\n</code></pre>"},{"location":"developer_guide/implementation_models/#mlp-energy-model","title":"MLP Energy Model","text":"<pre><code>class MLPEnergyModel(BaseModel):\n    \"\"\"Multi-layer perceptron energy model.\"\"\"\n\n    def __init__(\n        self, \n        input_dim: int, \n        hidden_dims: List[int], \n        activation: Optional[nn.Module] = None,\n        use_spectral_norm: bool = False\n    ):\n        \"\"\"Initialize MLP energy model.\n\n        Args:\n            input_dim: Input dimension\n            hidden_dims: List of hidden dimensions\n            activation: Activation function\n            use_spectral_norm: Whether to use spectral normalization\n        \"\"\"\n        super().__init__(input_dim, hidden_dims, activation)\n        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n\n        # Apply spectral normalization if requested\n        if use_spectral_norm:\n            self._apply_spectral_norm()\n\n    def _apply_spectral_norm(self):\n        \"\"\"Apply spectral normalization to all linear layers.\"\"\"\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear):\n                setattr(\n                    self, \n                    name, \n                    nn.utils.spectral_norm(module)\n                )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass to compute energy.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, input_dim)\n\n        Returns:\n            Energy values of batch_shape (batch_size,)\n        \"\"\"\n        features = super().forward(x)\n        energy = self.output_layer(features)\n        return energy.squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#convolutional-energy-models","title":"Convolutional Energy Models","text":"<p>For image data, convolutional architectures are more appropriate:</p> <pre><code>class ConvEnergyModel(nn.Module):\n    \"\"\"Convolutional energy model for image data.\"\"\"\n\n    def __init__(\n        self,\n        input_channels: int,\n        image_size: int,\n        channels: List[int] = [32, 64, 128, 256],\n        kernel_size: int = 3,\n        activation: Optional[nn.Module] = None\n    ):\n        \"\"\"Initialize convolutional energy model.\n\n        Args:\n            input_channels: Number of input channels\n            image_size: Size of input images (assumed square)\n            channels: List of channel dimensions for conv layers\n            kernel_size: Size of convolutional kernel\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n        self.input_channels = input_channels\n        self.image_size = image_size\n        self.activation = activation or nn.LeakyReLU(0.2)\n\n        # Build convolutional layers\n        layers = []\n        in_channels = input_channels\n\n        for out_channels in channels:\n            layers.append(\n                nn.Conv2d(\n                    in_channels, \n                    out_channels, \n                    kernel_size=kernel_size,\n                    stride=2,\n                    padding=kernel_size // 2\n                )\n            )\n            layers.append(self.activation)\n            in_channels = out_channels\n\n        self.conv_net = nn.Sequential(*layers)\n\n        # Calculate feature size after convolutions\n        feature_size = image_size // (2 ** len(channels))\n\n        # Final layers\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_channels * feature_size * feature_size, 128),\n            self.activation,\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass to compute energy.\n\n        Args:\n            x: Input tensor of batch_shape (batch_size, channels, height, width)\n\n        Returns:\n            Energy values of batch_shape (batch_size,)\n        \"\"\"\n        # Ensure correct input batch_shape\n        if len(x.shape) == 2:\n            x = x.view(-1, self.input_channels, self.image_size, self.image_size)\n\n        features = self.conv_net(x)\n        energy = self.fc(features)\n        return energy.squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#neural-energy-functions","title":"Neural Energy Functions","text":"<p>Neural networks can be used to create energy functions:</p> <pre><code>from torchebm.core import BaseEnergyFunction\n\n\nclass NeuralEnergyFunction(BaseEnergyFunction):\n    \"\"\"Energy function implemented using a neural network.\"\"\"\n\n    def __init__(self, model: nn.Module):\n        \"\"\"Initialize neural energy function.\n\n        Args:\n            model: Neural network model\n        \"\"\"\n        super().__init__()\n        self.model = model\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute energy values for inputs.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Energy values\n        \"\"\"\n        return self.model(x)\n</code></pre>"},{"location":"developer_guide/implementation_models/#advanced-architectures","title":"Advanced Architectures","text":""},{"location":"developer_guide/implementation_models/#residual-blocks","title":"Residual Blocks","text":"<pre><code>class ResidualBlock(nn.Module):\n    \"\"\"Residual block for energy models.\"\"\"\n\n    def __init__(self, dim: int, activation: nn.Module = nn.ReLU()):\n        \"\"\"Initialize residual block.\n\n        Args:\n            dim: Feature dimension\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Linear(dim, dim),\n            activation,\n            nn.Linear(dim, dim)\n        )\n        self.activation = activation\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass with residual connection.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Output tensor\n        \"\"\"\n        return x + self.block(x)\n\nclass ResNetEnergyModel(nn.Module):\n    \"\"\"ResNet-style energy model.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 128,\n        n_blocks: int = 4,\n        activation: nn.Module = nn.ReLU()\n    ):\n        \"\"\"Initialize ResNet energy model.\n\n        Args:\n            input_dim: Input dimension\n            hidden_dim: Hidden dimension\n            n_blocks: Number of residual blocks\n            activation: Activation function\n        \"\"\"\n        super().__init__()\n\n        # Input projection\n        self.input_proj = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            activation\n        )\n\n        # Residual blocks\n        self.blocks = nn.ModuleList([\n            ResidualBlock(hidden_dim, activation)\n            for _ in range(n_blocks)\n        ])\n\n        # Output projection\n        self.output_proj = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through ResNet energy model.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Energy values\n        \"\"\"\n        h = self.input_proj(x)\n\n        for block in self.blocks:\n            h = block(h)\n\n        energy = self.output_proj(h)\n        return energy.squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#integration-with-trainers","title":"Integration with Trainers","text":"<p>Models are integrated with trainer classes:</p> <pre><code>class EBMTrainer:\n    \"\"\"Trainer for energy-based models.\"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        sampler: \"Sampler\",\n        optimizer: torch.optim.Optimizer,\n        loss_fn: \"BaseLoss\"\n    ):\n        \"\"\"Initialize EBM trainer.\n\n        Args:\n            energy_function: Energy function to train\n            sampler: Sampler for negative samples\n            optimizer: Optimizer for model parameters\n            loss_fn: BaseLoss function\n        \"\"\"\n        self.energy_function = energy_function\n        self.sampler = sampler\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n\n    def train_step(\n        self,\n        pos_samples: torch.Tensor,\n        neg_samples: Optional[torch.Tensor] = None\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Perform one training step.\n\n        Args:\n            pos_samples: Positive samples from data\n            neg_samples: Optional negative samples\n\n        Returns:\n            Dictionary of metrics\n        \"\"\"\n        # Generate negative samples if not provided\n        if neg_samples is None:\n            with torch.no_grad():\n                neg_samples = self.sampler.sample(\n                    n_samples=pos_samples.shape[0],\n                    dim=pos_samples.shape[1]\n                )\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Compute loss\n        loss, metrics = self.loss_fn(pos_samples, neg_samples)\n\n        # Backward and optimize\n        loss.backward()\n        self.optimizer.step()\n\n        return metrics\n</code></pre>"},{"location":"developer_guide/implementation_models/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"developer_guide/implementation_models/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\nclass MixedPrecisionEBMTrainer(EBMTrainer):\n    \"\"\"Trainer with mixed precision for faster training.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scaler = GradScaler()\n\n    def train_step(\n        self,\n        pos_samples: torch.Tensor,\n        neg_samples: Optional[torch.Tensor] = None\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Perform one training step with mixed precision.\"\"\"\n        # Generate negative samples if not provided\n        if neg_samples is None:\n            with torch.no_grad():\n                neg_samples = self.sampler.sample(\n                    n_samples=pos_samples.shape[0],\n                    dim=pos_samples.shape[1]\n                )\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Forward pass with mixed precision\n        with autocast():\n            loss, metrics = self.loss_fn(pos_samples, neg_samples)\n\n        # Backward and optimize with gradient scaling\n        self.scaler.scale(loss).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n\n        return metrics\n</code></pre>"},{"location":"developer_guide/implementation_models/#best-practices-for-custom-models","title":"Best Practices for Custom Models","text":"<p>When implementing custom models, follow these best practices:</p> <p>Custom Model Example</p> <pre><code>class CustomEnergyModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SiLU(),  # SiLU/Swish activation\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Initialize weights properly\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.orthogonal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"developer_guide/implementation_models/#do","title":"Do","text":"<ul> <li>Subclass appropriate base classes</li> <li>Handle device placement correctly</li> <li>Use proper initialization</li> <li>Consider normalization techniques</li> <li>Document architecture clearly</li> </ul>"},{"location":"developer_guide/implementation_models/#dont","title":"Don't","text":"<ul> <li>Create overly complex architectures</li> <li>Ignore numerical stability</li> <li>Forget to validate inputs</li> <li>Mix different PyTorch versions</li> <li>Ignore gradient flow issues</li> </ul>"},{"location":"developer_guide/implementation_models/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Energy Functions</p> <p>Explore energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Samplers</p> <p>Understand sampler implementation details.</p> <p> Implementation Samplers</p> </li> </ul>"},{"location":"developer_guide/implementation_samplers/","title":"Samplers Implementation","text":""},{"location":"developer_guide/implementation_samplers/#samplers-implementation","title":"Samplers Implementation","text":"<p>Implementation Details</p> <p>This guide provides detailed information about the implementation of sampling algorithms in TorchEBM, including mathematical foundations, code structure, and optimization techniques.</p>"},{"location":"developer_guide/implementation_samplers/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Sampling algorithms in energy-based models aim to generate samples from the distribution:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>where \\(E(x)\\) is the energy function and \\(Z = \\int e^{-E(x)} dx\\) is the normalization constant.</p>"},{"location":"developer_guide/implementation_samplers/#base-sampler-implementation","title":"Base Sampler Implementation","text":"<p>The <code>Sampler</code> base class provides the foundation for all sampling algorithms:</p> <pre><code>from abc import ABC, abstractmethod\nimport torch\nfrom typing import Optional, Union, Tuple\n\nfrom torchebm.core import BaseEnergyFunction\n\n\nclass Sampler(ABC):\n    \"\"\"Base class for all sampling algorithms.\"\"\"\n\n    def __init__(self, energy_function: BaseEnergyFunction):\n        \"\"\"Initialize sampler with an energy function.\n\n        Args:\n            energy_function: The energy function to sample from\n        \"\"\"\n        self.energy_function = energy_function\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def to(self, device):\n        \"\"\"Move sampler to specified device.\"\"\"\n        self.device = device\n        return self\n\n    @abstractmethod\n    def sample(self, n_samples: int, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of batch_shape (n_samples, dim) containing samples\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using a Markov chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            **kwargs: Additional sampler-specific parameters\n\n        Returns:\n            Tensor of batch_shape (n_samples, dim) containing final samples\n        \"\"\"\n        pass\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#langevin-dynamics","title":"Langevin Dynamics","text":""},{"location":"developer_guide/implementation_samplers/#mathematical-background","title":"Mathematical Background","text":"<p>Langevin dynamics uses the score function (gradient of log-probability) to guide sampling with Brownian motion:</p> \\[dx_t = -\\nabla E(x_t)dt + \\sqrt{2}dW_t\\] <p>where \\(W_t\\) is the Wiener process (Brownian motion).</p>"},{"location":"developer_guide/implementation_samplers/#implementation","title":"Implementation","text":"<pre><code>import torch\nimport numpy as np\nfrom typing import Optional, Union, Tuple\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers.base import Sampler\n\n\nclass LangevinDynamics(Sampler):\n    \"\"\"Langevin dynamics sampler.\"\"\"\n\n    def __init__(\n            self,\n            energy_function: BaseEnergyFunction,\n            step_size: float = 0.01,\n            noise_scale: float = 1.0\n    ):\n        \"\"\"Initialize Langevin dynamics sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            step_size: Step size for updates\n            noise_scale: Scale of noise added at each step\n        \"\"\"\n        super().__init__(energy_function)\n        self.step_size = step_size\n        self.noise_scale = noise_scale\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Langevin dynamics.\n\n        Args:\n            x: Current samples of batch_shape (n_samples, dim)\n\n        Returns:\n            Updated samples of batch_shape (n_samples, dim)\n        \"\"\"\n        # Compute score (gradient of log probability)\n        score = -self.energy_function.score(x)\n\n        # Add drift term and noise\n        noise = torch.randn_like(x) * np.sqrt(2 * self.step_size * self.noise_scale)\n        x_new = x + self.step_size * score + noise\n\n        return x_new\n\n    def sample_chain(\n            self,\n            dim: int,\n            n_steps: int,\n            n_samples: int = 1,\n            initial_samples: Optional[torch.Tensor] = None,\n            return_trajectory: bool = False\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Generate samples using a Langevin dynamics chain.\n\n        Args:\n            dim: Dimensionality of samples\n            n_steps: Number of steps in the chain\n            n_samples: Number of parallel chains to run\n            initial_samples: Optional initial samples\n            return_trajectory: Whether to return the full trajectory\n\n        Returns:\n            Samples or (samples, trajectory)\n        \"\"\"\n        # Initialize samples\n        if initial_samples is None:\n            x = torch.randn(n_samples, dim, device=self.device)\n        else:\n            x = initial_samples.clone().to(self.device)\n\n        # Initialize trajectory if needed\n        if return_trajectory:\n            trajectory = torch.zeros(n_steps + 1, n_samples, dim, device=self.device)\n            trajectory[0] = x\n\n        # Run sampling chain\n        for i in range(n_steps):\n            x = self.sample_step(x)\n            if return_trajectory:\n                trajectory[i + 1] = x\n\n        if return_trajectory:\n            return x, trajectory\n        else:\n            return x\n\n    def sample(self, n_samples: int, dim: int, n_steps: int = 100, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\"\"\"\n        return self.sample_chain(dim=dim, n_steps=n_steps, n_samples=n_samples, **kwargs)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":""},{"location":"developer_guide/implementation_samplers/#mathematical-background_1","title":"Mathematical Background","text":"<p>Hamiltonian Monte Carlo (HMC) introduces momentum variables to help explore the distribution more efficiently:</p> \\[H(x, p) = E(x) + \\frac{1}{2}p^Tp\\] <p>where \\(p\\) is the momentum variable and \\(H\\) is the Hamiltonian.</p>"},{"location":"developer_guide/implementation_samplers/#implementation_1","title":"Implementation","text":"<pre><code>class HamiltonianMonteCarlo(Sampler):\n    \"\"\"Hamiltonian Monte Carlo sampler.\"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        step_size: float = 0.1,\n        n_leapfrog_steps: int = 10,\n        mass_matrix: Optional[torch.Tensor] = None\n    ):\n        \"\"\"Initialize HMC sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            step_size: Step size for leapfrog integration\n            n_leapfrog_steps: Number of leapfrog steps\n            mass_matrix: Mass matrix for momentum (identity by default)\n        \"\"\"\n        super().__init__(energy_function)\n        self.step_size = step_size\n        self.n_leapfrog_steps = n_leapfrog_steps\n        self.mass_matrix = mass_matrix\n\n    def _leapfrog_step(self, x: torch.Tensor, p: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Perform one leapfrog step.\n\n        Args:\n            x: Position tensor of batch_shape (n_samples, dim)\n            p: Momentum tensor of batch_shape (n_samples, dim)\n\n        Returns:\n            New position and momentum\n        \"\"\"\n        # Half step for momentum\n        grad_x = self.energy_function.score(x)\n        p = p - 0.5 * self.step_size * grad_x\n\n        # Full step for position\n        if self.mass_matrix is not None:\n            x = x + self.step_size * torch.matmul(p, self.mass_matrix)\n        else:\n            x = x + self.step_size * p\n\n        # Half step for momentum\n        grad_x = self.energy_function.score(x)\n        p = p - 0.5 * self.step_size * grad_x\n\n        return x, p\n\n    def _compute_hamiltonian(self, x: torch.Tensor, p: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Hamiltonian value.\n\n        Args:\n            x: Position tensor of batch_shape (n_samples, dim)\n            p: Momentum tensor of batch_shape (n_samples, dim)\n\n        Returns:\n            Hamiltonian value of batch_shape (n_samples,)\n        \"\"\"\n        energy = self.energy_function(x)\n\n        if self.mass_matrix is not None:\n            kinetic = 0.5 * torch.sum(p * torch.matmul(p, self.mass_matrix), dim=1)\n        else:\n            kinetic = 0.5 * torch.sum(p * p, dim=1)\n\n        return energy + kinetic\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of HMC.\n\n        Args:\n            x: Current samples of batch_shape (n_samples, dim)\n\n        Returns:\n            Updated samples of batch_shape (n_samples, dim)\n        \"\"\"\n        # Sample initial momentum\n        p = torch.randn_like(x)\n\n        # Compute initial Hamiltonian\n        x_old, p_old = x.clone(), p.clone()\n        h_old = self._compute_hamiltonian(x_old, p_old)\n\n        # Leapfrog integration\n        x_new, p_new = x_old.clone(), p_old.clone()\n        for _ in range(self.n_leapfrog_steps):\n            x_new, p_new = self._leapfrog_step(x_new, p_new)\n\n        # Metropolis-Hastings correction\n        h_new = self._compute_hamiltonian(x_new, p_new)\n        accept_prob = torch.exp(h_old - h_new)\n        accept = torch.rand_like(accept_prob) &lt; accept_prob\n\n        # Accept or reject\n        x_out = torch.where(accept.unsqueeze(1), x_new, x_old)\n\n        return x_out\n\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using an HMC chain.\"\"\"\n        # Implementation similar to LangevinDynamics.sample_chain\n        pass\n\n    def sample(self, n_samples: int, dim: int, n_steps: int = 100, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples from the energy-based distribution.\"\"\"\n        return self.sample_chain(dim=dim, n_steps=n_steps, n_samples=n_samples, **kwargs)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#metropolis-hastings-sampler","title":"Metropolis-Hastings Sampler","text":"<pre><code>class MetropolisHastings(Sampler):\n    \"\"\"Metropolis-Hastings sampler.\"\"\"\n\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        proposal_scale: float = 0.1\n    ):\n        \"\"\"Initialize Metropolis-Hastings sampler.\n\n        Args:\n            energy_function: Energy function to sample from\n            proposal_scale: Scale of proposal distribution\n        \"\"\"\n        super().__init__(energy_function)\n        self.proposal_scale = proposal_scale\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Metropolis-Hastings.\n\n        Args:\n            x: Current samples of batch_shape (n_samples, dim)\n\n        Returns:\n            Updated samples of batch_shape (n_samples, dim)\n        \"\"\"\n        # Compute energy of current state\n        energy_x = self.energy_function(x)\n\n        # Propose new state\n        proposal = x + self.proposal_scale * torch.randn_like(x)\n\n        # Compute energy of proposed state\n        energy_proposal = self.energy_function(proposal)\n\n        # Compute acceptance probability\n        accept_prob = torch.exp(energy_x - energy_proposal)\n        accept = torch.rand_like(accept_prob) &lt; accept_prob\n\n        # Accept or reject\n        x_new = torch.where(accept.unsqueeze(1), proposal, x)\n\n        return x_new\n\n    def sample_chain(self, dim: int, n_steps: int, n_samples: int = 1, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate samples using a Metropolis-Hastings chain.\"\"\"\n        # Implementation similar to LangevinDynamics.sample_chain\n        pass\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"developer_guide/implementation_samplers/#cuda-acceleration","title":"CUDA Acceleration","text":"<p>For performance-critical operations, we implement CUDA-optimized versions:</p> <pre><code>from torchebm.cuda import langevin_step_cuda\n\nclass CUDALangevinDynamics(LangevinDynamics):\n    \"\"\"CUDA-optimized Langevin dynamics sampler.\"\"\"\n\n    def sample_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform one step of Langevin dynamics with CUDA optimization.\"\"\"\n        if not torch.cuda.is_available() or not x.is_cuda:\n            return super().sample_step(x)\n\n        return langevin_step_cuda(\n            x, \n            self.energy_function,\n            self.step_size,\n            self.noise_scale\n        )\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#batch-processing","title":"Batch Processing","text":"<p>To handle large numbers of samples efficiently:</p> <pre><code>def batch_sample_chain(\n        sampler: Sampler,\n        dim: int,\n        n_steps: int,\n        n_samples: int,\n        batch_size: int = 1000\n) -&gt; torch.Tensor:\n    \"\"\"Sample in batches to avoid memory issues.\"\"\"\n    samples = []\n\n    for i in range(0, n_samples, batch_size):\n        batch_n = min(batch_size, n_samples - i)\n        batch_samples = sampler.sample(\n            dim=dim,\n            n_steps=n_steps,\n            n_samples=batch_n\n        )\n        samples.append(batch_samples)\n\n    return torch.cat(samples, dim=0)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#best-practices-for-custom-samplers","title":"Best Practices for Custom Samplers","text":"<p>When implementing custom samplers, follow these best practices:</p> <p>Custom Sampler Example</p> <pre><code>class CustomSampler(Sampler):\n    \"\"\"Custom sampler example.\"\"\"\n\n    def __init__(self, energy_function, step_size=0.01):\n        super().__init__(energy_function)\n        self.step_size = step_size\n\n    def sample_step(self, x):\n        # Custom sampling logic\n        return x + self.step_size * torch.randn_like(x)\n\n    def sample_chain(self, dim, n_steps, n_samples=1):\n        # Initialize\n        x = torch.randn(n_samples, dim, device=self.device)\n\n        # Run chain\n        for _ in range(n_steps):\n            x = self.sample_step(x)\n\n        return x\n\n    def sample(self, n_samples, dim, n_steps=100):\n        return self.sample_chain(dim, n_steps, n_samples)\n</code></pre>"},{"location":"developer_guide/implementation_samplers/#do","title":"Do","text":"<ul> <li>Subclass the <code>Sampler</code> base class</li> <li>Implement both <code>sample</code> and <code>sample_chain</code> methods</li> <li>Handle device placement correctly</li> <li>Support batched execution</li> <li>Add diagnostics when appropriate</li> </ul>"},{"location":"developer_guide/implementation_samplers/#dont","title":"Don't","text":"<ul> <li>Modify input tensors in-place</li> <li>Allocate new tensors unnecessarily</li> <li>Ignore numerical stability</li> <li>Forget to validate inputs</li> <li>Implement complex logic in sampling loops</li> </ul>"},{"location":"developer_guide/implementation_samplers/#resources","title":"Resources","text":"<ul> <li> <p> Core Components</p> <p>Learn about core components and their interactions.</p> <p> Core Components</p> </li> <li> <p> Energy Functions</p> <p>Explore energy function implementation details.</p> <p> Energy Functions</p> </li> <li> <p> Loss Functions</p> <p>Understand loss function implementation details.</p> <p> BaseLoss Functions</p> </li> </ul>"},{"location":"developer_guide/performance/","title":"Performance Optimization","text":""},{"location":"developer_guide/performance/#performance-optimization","title":"Performance Optimization","text":"<p>This document provides guidance on optimizing the performance of TorchEBM for both development and usage.</p>"},{"location":"developer_guide/performance/#performance-considerations","title":"Performance Considerations","text":"<p>Key Performance Areas</p> <p>When working with TorchEBM, pay special attention to these performance-critical areas:</p> <ol> <li>Sampling algorithms: These are iterative and typically the most compute-intensive</li> <li>Gradient calculations: Computing energy gradients is fundamental to many algorithms</li> <li>Batch processing: Effective vectorization for parallel processing</li> <li>GPU utilization: Proper device management and memory usage</li> </ol>"},{"location":"developer_guide/performance/#vectorization-techniques","title":"Vectorization Techniques","text":""},{"location":"developer_guide/performance/#batched-operations","title":"Batched Operations","text":"<p>TorchEBM extensively uses batching to improve performance:</p> <pre><code># Instead of looping over samples\nfor i in range(n_samples):\n    energy_i = energy_function(x[i])  # Slow\n\n# Use batched computation\nenergy = energy_function(x)  # Fast\n</code></pre>"},{"location":"developer_guide/performance/#parallel-sampling","title":"Parallel Sampling","text":"<p>Sample multiple chains in parallel by using batch dimensions:</p> <pre><code># Initialize batch of samples\nx = torch.randn(n_samples, dim, device=device)\n\n# One sampling step (all chains update together)\nx_new, _ = sampler.step(x)\n</code></pre>"},{"location":"developer_guide/performance/#gpu-acceleration","title":"GPU Acceleration","text":"<p>TorchEBM is designed to work efficiently on GPUs:</p>"},{"location":"developer_guide/performance/#device-management","title":"Device Management","text":"<pre><code># Create energy function and move to appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = GaussianEnergy(mean, cov).to(device)\n\n# Create sampler with the same device\nsampler = LangevinDynamics(energy_fn, device=device)\n\n# Generate samples (automatically on the correct device)\nsamples, _ = sampler.sample(dim=2, n_steps=1000, n_samples=10000)\n</code></pre>"},{"location":"developer_guide/performance/#memory-management","title":"Memory Management","text":"<p>Memory management is critical for performance, especially on GPUs:</p> <pre><code># Avoid creating new tensors in loops\nfor step in range(n_steps):\n    # Bad: Creates new tensors each iteration\n    x = x - step_size * energy_fn.gradient(x) + noise_scale * torch.randn_like(x)\n\n    # Good: In-place operations\n    grad = energy_fn.gradient(x)\n    x.sub_(step_size * grad)\n    x.add_(noise_scale * torch.randn_like(x))\n</code></pre>"},{"location":"developer_guide/performance/#custom-cuda-kernels-to-be-added-see-also-curblas","title":"Custom CUDA Kernels (to be Added--See Also: cuRBLAS)","text":""},{"location":"developer_guide/performance/#sampling-efficiency","title":"Sampling Efficiency","text":"<p>Sampling efficiency can be improved using several techniques:</p> <ul> <li> <p> Step Size Adaptation</p> <p>Automatically adjust step sizes based on acceptance rates or other metrics.</p> <pre><code>if acceptance_rate &lt; 0.3:\n    step_size *= 0.9  # Decrease step size\nelif acceptance_rate &gt; 0.7:\n    step_size *= 1.1  # Increase step size\n</code></pre> </li> <li> <p> Burn-in Period</p> <p>Discard initial samples to reduce the impact of initialization.</p> <pre><code># Run burn-in period\nx = torch.randn(n_samples, dim)\nfor _ in range(burn_in_steps):\n    x, _ = sampler.step(x)\n\n# Start collecting samples\nsamples = []\nfor _ in range(n_steps):\n    x, _ = sampler.step(x)\n    samples.append(x.clone())\n</code></pre> </li> <li> <p> Thinning</p> <p>Reduce correlation between samples by keeping only every Nth sample.</p> <pre><code># Collect samples with thinning\nsamples = []\nfor i in range(n_steps):\n    x, _ = sampler.step(x)\n    if i % thinning == 0:\n        samples.append(x.clone())\n</code></pre> </li> <li> <p> Warm Starting</p> <p>Initialize sampling from a distribution close to the target.</p> <pre><code># Warm start from approximate distribution\nx = approximate_sampler.sample(n_samples, dim)\nsamples = sampler.sample(\n  n_steps=n_steps, \n  initial_samples=x\n)\n</code></pre> </li> </ul>"},{"location":"developer_guide/performance/#profiling-and-benchmarking-planned","title":"Profiling and Benchmarking (Planned)","text":""},{"location":"developer_guide/performance/#performance-benchmarks-planned","title":"Performance Benchmarks (Planned)","text":""},{"location":"developer_guide/performance/#performance-tips-and-best-practices","title":"Performance Tips and Best Practices","text":"<p>Common Pitfalls</p> <p>Avoid these common performance issues:</p> <ol> <li>Unnecessary CPU-GPU transfers: Keep data on the same device</li> <li>Small batch sizes: Too small batches underutilize hardware</li> <li>Unneeded gradient tracking: Disable gradients when not training</li> <li>Excessive logging: Logging every step can significantly slow down sampling</li> </ol>"},{"location":"developer_guide/performance/#general-tips","title":"General Tips","text":"<ol> <li>Use the right device: Always move computation to GPU when available</li> <li>Batch processing: Process data in batches rather than individually</li> <li>Reuse tensors: Avoid creating new tensors in inner loops</li> <li>Monitor memory: Use <code>torch.cuda.memory_summary()</code> to track memory usage</li> </ol>"},{"location":"developer_guide/performance/#sampling-tips","title":"Sampling Tips","text":"<ol> <li>Tune step sizes: Optimal step sizes balance exploration and stability</li> <li>Parallel chains: Use multiple chains to improve sample diversity</li> <li>Adaptive methods: Use adaptive samplers for complex distributions</li> <li>Mixed precision: Consider using mixed precision for larger models</li> </ol>"},{"location":"developer_guide/performance/#algorithm-specific-optimizations","title":"Algorithm-Specific Optimizations","text":""},{"location":"developer_guide/performance/#langevin-dynamics","title":"Langevin Dynamics","text":"<pre><code># Optimize step size for Langevin dynamics\n# Rule of thumb: step_size \u2248 O(d^(-1/3)) where d is dimension\nstep_size = min(0.01, 0.1 * dim**(-1/3))\n\n# Noise scale should be sqrt(2 * step_size) for standard Langevin\nnoise_scale = np.sqrt(2 * step_size)\n</code></pre>"},{"location":"developer_guide/performance/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":"<pre><code># Optimize HMC parameters\n# Leapfrog steps should scale with dimension\nn_leapfrog_steps = max(5, int(np.sqrt(dim)))\n\n# Step size should decrease with dimension\nstep_size = min(0.01, 0.05 * dim**(-1/4))\n</code></pre>"},{"location":"developer_guide/performance/#multi-gpu-scaling-planned","title":"Multi-GPU Scaling (Planned)","text":""},{"location":"developer_guide/performance/#conclusion","title":"Conclusion","text":"<p>Performance optimization in TorchEBM involves careful attention to vectorization, GPU acceleration, memory management, and algorithm-specific tuning. By following these guidelines, you can achieve significant speedups in your energy-based modeling workflows. </p>"},{"location":"developer_guide/project_structure/","title":"Project Structure","text":""},{"location":"developer_guide/project_structure/#project-structure","title":"Project Structure","text":"<p>Codebase Organization</p> <p>Understanding the TorchEBM project structure helps you navigate the codebase and contribute effectively. This guide provides an overview of the repository organization.</p>"},{"location":"developer_guide/project_structure/#repository-overview","title":"Repository Overview","text":"<p>The TorchEBM repository is organized as follows:</p> <pre><code>torchebm/\n\u251c\u2500\u2500 torchebm/              # Main package source code\n\u2502   \u251c\u2500\u2500 core/              # Core functionality and base classes\n\u2502   \u251c\u2500\u2500 samplers/          # Sampling algorithms implementation\n\u2502   \u251c\u2500\u2500 losses/            # BaseLoss functions for training\n\u2502   \u251c\u2500\u2500 models/            # Neural network model implementations\n\u2502   \u251c\u2500\u2500 cuda/              # CUDA optimized implementations\n\u2502   \u2514\u2500\u2500 utils/             # Utility functions and helpers\n\u251c\u2500\u2500 tests/                 # Test directory\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 examples/              # Example applications\n\u251c\u2500\u2500 benchmarks/            # Performance benchmarks\n\u251c\u2500\u2500 setup.py               # Package setup script\n\u2514\u2500\u2500 README.md              # Project README\n</code></pre>"},{"location":"developer_guide/project_structure/#main-package-structure","title":"Main Package Structure","text":""},{"location":"developer_guide/project_structure/#torchebmcore","title":"<code>torchebm.core</code>","text":"<p>Contains the core functionality including base classes and essential components:</p> <ul> <li><code>base.py</code> - Base classes for the entire library</li> <li><code>energy_function.py</code> - Base energy function class and interface</li> <li><code>analytical_functions.py</code> - Analytical energy function implementations</li> <li><code>distributions.py</code> - Probability distribution implementations</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmsamplers","title":"<code>torchebm.samplers</code>","text":"<p>Implementations of various sampling algorithms:</p> <ul> <li><code>base.py</code> - Base sampler class</li> <li><code>langevin_dynamics.py</code> - Langevin dynamics implementation</li> <li><code>hmc.py</code> - Hamiltonian Monte Carlo</li> <li><code>metropolis_hastings.py</code> - Metropolis-Hastings</li> <li>[Other sampler implementations]</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmlosses","title":"<code>torchebm.losses</code>","text":"<p>BaseLoss functions for training energy-based models:</p> <ul> <li><code>base.py</code> - Base loss class</li> <li><code>contrastive_divergence.py</code> - Contrastive divergence implementations</li> <li><code>score_matching.py</code> - Score matching methods</li> <li>[Other loss implementations]</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmmodels","title":"<code>torchebm.models</code>","text":"<p>Neural network model implementations:</p> <ul> <li><code>mlp.py</code> - Multi-layer perceptron energy models</li> <li><code>cnn.py</code> - Convolutional neural network energy models</li> <li><code>ebm.py</code> - Generic energy-based model implementations</li> <li>[Other model architectures]</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmcuda","title":"<code>torchebm.cuda</code>","text":"<p>CUDA-optimized implementations for performance-critical operations:</p> <ul> <li><code>kernels/</code> - CUDA kernel implementations</li> <li><code>bindings.cpp</code> - PyTorch C++ bindings</li> <li><code>ops.py</code> - Python interfaces to CUDA operations</li> </ul>"},{"location":"developer_guide/project_structure/#torchebmutils","title":"<code>torchebm.utils</code>","text":"<p>Utility functions and helpers:</p> <ul> <li><code>device.py</code> - Device management utilities</li> <li><code>visualization.py</code> - Visualization tools</li> <li><code>data.py</code> - Data loading and processing utilities</li> <li><code>logging.py</code> - Logging utilities</li> </ul>"},{"location":"developer_guide/project_structure/#tests-structure","title":"Tests Structure","text":"<p>The tests directory mirrors the package structure:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/                  # Unit tests\n\u2502   \u251c\u2500\u2500 core/              # Tests for core module\n\u2502   \u251c\u2500\u2500 samplers/          # Tests for samplers module\n\u2502   \u251c\u2500\u2500 losses/            # Tests for losses module\n\u2502   \u2514\u2500\u2500 utils/             # Tests for utilities\n\u251c\u2500\u2500 integration/           # Integration tests\n\u251c\u2500\u2500 performance/           # Performance benchmarks\n\u251c\u2500\u2500 conftest.py            # Pytest configuration and fixtures\n\u2514\u2500\u2500 utils.py               # Test utilities\n</code></pre>"},{"location":"developer_guide/project_structure/#documentation-structure","title":"Documentation Structure","text":"<p>The documentation is built with MkDocs and organized as follows:</p> <pre><code>docs/\n\u251c\u2500\u2500 index.md               # Home page\n\u251c\u2500\u2500 getting_started.md     # Quick start guide\n\u251c\u2500\u2500 guides/                # User guides\n\u251c\u2500\u2500 api/                   # API reference (auto-generated)\n\u251c\u2500\u2500 examples/              # Example documentation\n\u251c\u2500\u2500 developer_guide/       # Developer documentation\n\u2502   \u251c\u2500\u2500 contributing.md    # Contributing guidelines\n\u2502   \u251c\u2500\u2500 code_style.md      # Code style guide\n\u2502   \u2514\u2500\u2500 ...                # Other developer docs\n\u2514\u2500\u2500 assets/                # Static assets\n    \u251c\u2500\u2500 images/            # Images\n    \u251c\u2500\u2500 stylesheets/       # Custom CSS\n    \u2514\u2500\u2500 javascripts/       # Custom JavaScript\n</code></pre>"},{"location":"developer_guide/project_structure/#examples-structure","title":"Examples Structure","text":"<p>Example applications to demonstrate TorchEBM usage:</p> <pre><code>examples/\n\u251c\u2500\u2500 basic/                 # Basic usage examples\n\u251c\u2500\u2500 advanced/              # Advanced examples\n\u2514\u2500\u2500 real_world/            # Real-world applications\n</code></pre>"},{"location":"developer_guide/project_structure/#dependencies-and-requirements","title":"Dependencies and Requirements","text":"<p>TorchEBM has the following dependencies:</p> <ul> <li> <p> PyTorch</p> <p>Primary framework for tensor operations and automatic differentiation.</p> </li> <li> <p> NumPy</p> <p>Used for numerical operations when PyTorch isn't needed.</p> </li> <li> <p> Matplotlib</p> <p>For visualization capabilities.</p> </li> <li> <p> tqdm</p> <p>For progress bars during long operations.</p> </li> </ul>"},{"location":"developer_guide/project_structure/#entry-points","title":"Entry Points","text":"<p>The main entry points to the library are:</p> <ul> <li><code>torchebm.core</code> - Import core functionality</li> <li><code>torchebm.samplers</code> - Import samplers</li> <li><code>torchebm.losses</code> - Import loss functions</li> <li><code>torchebm.models</code> - Import neural network models</li> </ul> <p>Example of typical import patterns:</p> <pre><code># Import energy_functions components\nfrom torchebm.core import BaseEnergyFunction, GaussianEnergy\n\n# Import samplers\nfrom torchebm.samplers import LangevinDynamics, HamiltonianMC\n\n# Import loss functions\nfrom torchebm.losses import ContrastiveDivergence\n\n# Import utilities\nfrom torchebm.utils import visualize_samples\n</code></pre>"},{"location":"developer_guide/project_structure/#package-management","title":"Package Management","text":"<p>TorchEBM uses setuptools for package management with setup.py:</p> <pre><code># setup.py excerpt\nsetup(\n    name=\"torchebm\",\n    version=__version__,\n    description=\"Energy-Based Modeling library for PyTorch\",\n    packages=find_packages(),\n    install_requires=[\n        \"torch&gt;=1.9.0\",\n        \"numpy&gt;=1.20.0\",\n        \"matplotlib&gt;=3.4.0\",\n        \"tqdm&gt;=4.60.0\",\n    ],\n    # Additional configuration...\n)\n</code></pre>"},{"location":"developer_guide/project_structure/#version-control-structure","title":"Version Control Structure","text":"<ul> <li>We follow Conventional Commits for our commit messages</li> <li>Feature branches should be named <code>feature/feature-name</code></li> <li>Bugfix branches should be named <code>bugfix/bug-name</code></li> <li>Release branches should be named <code>release/vX.Y.Z</code></li> </ul> <p>Finding Your Way Around</p> <p>When contributing to TorchEBM, start by exploring the relevant directory for your feature. For example, if you're adding a new sampler, look at the existing implementations in <code>torchebm/samplers/</code> to understand the pattern. </p>"},{"location":"developer_guide/testing_guide/","title":"Testing Guide","text":""},{"location":"developer_guide/testing_guide/#testing-guide","title":"Testing Guide","text":"<p>Quality Assurance</p> <p>Comprehensive testing is essential for maintaining the reliability and stability of TorchEBM. This guide outlines our testing approach and best practices.</p>"},{"location":"developer_guide/testing_guide/#testing-philosophy","title":"Testing Philosophy","text":"<p>TorchEBM follows test-driven development principles where appropriate, especially for core functionality. Our testing strategy includes:</p> <ul> <li> <p> Unit Tests</p> <p>Test individual components in isolation to ensure they work correctly.</p> </li> <li> <p> Integration Tests</p> <p>Test combinations of components to ensure they work together seamlessly.</p> </li> <li> <p> Performance Tests</p> <p>Measure the speed and resource usage of critical operations.</p> </li> <li> <p> Numerical Tests</p> <p>Verify numerical correctness of algorithms against known results.</p> </li> </ul>"},{"location":"developer_guide/testing_guide/#test-directory-structure","title":"Test Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                # Unit tests\n\u2502   \u251c\u2500\u2500 core/            # Tests for core module\n\u2502   \u251c\u2500\u2500 samplers/        # Tests for samplers module\n\u2502   \u251c\u2500\u2500 losses/          # Tests for losses module\n\u2502   \u2514\u2500\u2500 utils/           # Tests for utilities\n\u251c\u2500\u2500 integration/         # Integration tests\n\u251c\u2500\u2500 performance/         # Performance benchmarks\n\u251c\u2500\u2500 conftest.py          # Pytest configuration and fixtures\n\u2514\u2500\u2500 utils.py             # Test utilities\n</code></pre>"},{"location":"developer_guide/testing_guide/#running-tests","title":"Running Tests","text":"Basic UsageCoverageParallel Execution <pre><code># Run all tests\npytest\n\n# Run specific tests\npytest tests/unit/core/\npytest tests/unit/samplers/test_langevin.py\n\n# Run specific test class\npytest tests/unit/core/test_energy.py::TestGaussianEnergy\n\n# Run specific test method\npytest tests/unit/core/test_energy.py::TestGaussianEnergy::test_energy_computation\n</code></pre> <pre><code># Run tests with coverage\npytest --cov=torchebm\n\n# Generate HTML coverage report\npytest --cov=torchebm --cov-report=html\n</code></pre> <pre><code># Run tests in parallel (4 processes)\npytest -n 4\n</code></pre>"},{"location":"developer_guide/testing_guide/#writing-tests","title":"Writing Tests","text":"<p>We use pytest for all our tests. Here are guidelines for writing effective tests:</p>"},{"location":"developer_guide/testing_guide/#test-class-structure","title":"Test Class Structure","text":"<pre><code>import pytest\nimport torch\nfrom torchebm.core import GaussianEnergy\n\nclass TestGaussianEnergy:\n    @pytest.fixture\n    def energy_fn(self):\n        \"\"\"Fixture to create a standard Gaussian energy function.\"\"\"\n        return GaussianEnergy(\n            mean=torch.zeros(2),\n            cov=torch.eye(2)\n        )\n\n    def test_energy_computation(self, energy_fn):\n        \"\"\"Test that energy is correctly computed for known inputs.\"\"\"\n        x = torch.zeros(2)\n        energy = energy_fn(x)\n        assert energy.item() == 0.0\n\n        x = torch.ones(2)\n        energy = energy_fn(x)\n        assert torch.isclose(energy, torch.tensor(1.0))\n</code></pre>"},{"location":"developer_guide/testing_guide/#test-naming-conventions","title":"Test Naming Conventions","text":"<ul> <li>Test files should be named <code>test_*.py</code></li> <li>Test classes should be named <code>Test*</code></li> <li>Test methods should be named <code>test_*</code></li> <li>Use descriptive names that indicate what's being tested</li> </ul>"},{"location":"developer_guide/testing_guide/#parametrized-tests","title":"Parametrized Tests","text":"<p>Use <code>pytest.mark.parametrize</code> for testing multiple inputs:</p> <pre><code>import pytest\nimport torch\nfrom torchebm.core import GaussianEnergy\n\nclass TestGaussianEnergy:\n    @pytest.mark.parametrize(\"mean,cov,x,expected\", [\n        (torch.zeros(2), torch.eye(2), torch.zeros(2), 0.0),\n        (torch.zeros(2), torch.eye(2), torch.ones(2), 1.0),\n        (torch.ones(2), torch.eye(2), torch.zeros(2), 1.0),\n    ])\n    def test_energy_parametrized(self, mean, cov, x, expected):\n        energy_fn = GaussianEnergy(mean=mean, cov=cov)\n        energy = energy_fn(x)\n        assert torch.isclose(energy, torch.tensor(expected))\n</code></pre>"},{"location":"developer_guide/testing_guide/#fixtures","title":"Fixtures","text":"<p>Use fixtures for common setup code:</p> <pre><code>import pytest\nimport torch\n\n@pytest.fixture\ndef device():\n    \"\"\"Return the default device for testing.\"\"\"\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@pytest.fixture\ndef precision():\n    \"\"\"Return the default precision for comparison.\"\"\"\n    return 1e-5\n</code></pre>"},{"location":"developer_guide/testing_guide/#testing-cuda-code","title":"Testing CUDA Code","text":"<p>When testing CUDA code, follow these guidelines:</p> <pre><code>import pytest\nimport torch\nfrom torchebm.cuda import cuda_function\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA not available\")\ndef test_cuda_function():\n    # Prepare test data\n    x = torch.randn(100, device=\"cuda\")\n\n    # Call function\n    result = cuda_function(x)\n\n    # Verify result\n    expected = x * 2  # Hypothetical expected result\n    assert torch.allclose(result, expected)\n</code></pre>"},{"location":"developer_guide/testing_guide/#mocking","title":"Mocking","text":"<p>Use <code>unittest.mock</code> or <code>pytest-mock</code> for mocking dependencies:</p> <pre><code>def test_with_mock(mocker):\n    # Mock an expensive function\n    mock_compute = mocker.patch(\"torchebm.utils.compute_expensive_function\")\n    mock_compute.return_value = torch.tensor(1.0)\n\n    # Test code that uses the mocked function\n    # ...\n\n    # Verify the mock was called correctly\n    mock_compute.assert_called_once_with(torch.tensor(0.0))\n</code></pre>"},{"location":"developer_guide/testing_guide/#property-based-testing","title":"Property-Based Testing","text":"<p>For complex functions, consider using property-based testing with Hypothesis:</p> <pre><code>import hypothesis.strategies as st\nfrom hypothesis import given\nimport torch\nfrom torchebm.core import GaussianEnergy\n\n@given(\n    x=st.lists(st.floats(min_value=-10, max_value=10), min_size=2, max_size=2).map(torch.tensor)\n)\ndef test_gaussian_energy_properties(x):\n    \"\"\"Test properties of Gaussian energy function.\"\"\"\n    energy_fn = GaussianEnergy(mean=torch.zeros(2), cov=torch.eye(2))\n\n    # Property: energy is non-negative for standard Gaussian\n    energy = energy_fn(x)\n    assert energy &gt;= 0\n\n    # Property: energy is minimized at the mean\n    energy_at_mean = energy_fn(torch.zeros(2))\n    assert energy &gt;= energy_at_mean\n</code></pre>"},{"location":"developer_guide/testing_guide/#performance-testing","title":"Performance Testing","text":"<p>For critical components, include performance tests:</p> <pre><code>import pytest\nimport time\nimport torch\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.core import GaussianEnergy\n\n\n@pytest.mark.performance\ndef test_langevin_performance():\n    \"\"\"Test the performance of Langevin dynamics sampling.\"\"\"\n    energy_fn = GaussianEnergy(mean=torch.zeros(10), cov=torch.eye(10))\n    sampler = LangevinDynamics(energy_function=energy_fn, step_size=0.01)\n\n    # Warm-up\n    sampler.sample(dim=10, n_steps=10, n_samples=100)\n\n    # Timed test\n    start_time = time.time()\n    sampler.sample(dim=10, n_steps=1000, n_samples=1000)\n    end_time = time.time()\n\n    elapsed = end_time - start_time\n    print(f\"Sampling took {elapsed:.4f} seconds\")\n\n    # Ensure performance meets requirements\n    assert elapsed &lt; 2.0  # Adjust threshold as needed\n</code></pre>"},{"location":"developer_guide/testing_guide/#test-coverage-requirements","title":"Test Coverage Requirements","text":"<p>TorchEBM aims for high test coverage:</p> <ul> <li>Core modules: 90%+ coverage</li> <li>Samplers and losses: 85%+ coverage</li> <li>Utilities: 80%+ coverage</li> <li>CUDA code: 75%+ coverage</li> </ul> <p>Use <code>pytest-cov</code> to measure coverage:</p> <pre><code>pytest --cov=torchebm --cov-report=term-missing\n</code></pre>"},{"location":"developer_guide/testing_guide/#continuous-integration","title":"Continuous Integration","text":"<p>Our CI pipeline automatically runs tests on every pull request:</p> <ul> <li>All tests must pass before a PR can be merged</li> <li>Coverage should not decrease</li> <li>Performance tests should not show significant regressions</li> </ul> <p>Local CI</p> <p>Before submitting a PR, run the full test suite locally to ensure it passes:</p> <pre><code># Install test dependencies\npip install -e \".[test]\"\n\n# Run all tests\npytest\n\n# Check coverage\npytest --cov=torchebm\n</code></pre>"},{"location":"developer_guide/testing_guide/#resources","title":"Resources","text":"<ul> <li> <p> pytest Documentation</p> <p>Comprehensive guide to pytest features.</p> <p> pytest Docs</p> </li> <li> <p> pytest-cov</p> <p>Coverage plugin for pytest.</p> <p> pytest-cov Docs</p> </li> <li> <p> Hypothesis</p> <p>Property-based testing for Python.</p> <p> Hypothesis Docs</p> </li> </ul>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#torchebm-examples","title":"TorchEBM Examples","text":"<p>This section contains practical examples that demonstrate how to use TorchEBM for energy-based modeling. Each example is fully tested and focuses on a specific use case or feature.</p> <ul> <li> <p> Energy Functions</p> <p>Explore and visualize various energy functions and their properties.</p> <p> Energy Functions</p> </li> <li> <p> Langevin Dynamics</p> <p>Sample from various distributions using Langevin dynamics.</p> <p> Langevin Dynamics</p> </li> <li> <p> Hamiltonian Monte Carlo</p> <p>Learn to use Hamiltonian Monte Carlo for efficient sampling.</p> <p> Hamiltonian Monte Carlo</p> </li> <li> <p> Visualization Tools</p> <p>Advanced visualization tools for energy landscapes and sampling results.</p> <p> Visualization</p> </li> </ul>"},{"location":"examples/#example-structure","title":"Example Structure","text":"<p>Example Format</p> <p>Each example follows a consistent structure to help you understand and apply the concepts:</p> <ol> <li>Overview: Brief explanation of the example and its purpose</li> <li>Code: Complete, runnable code for the example</li> <li>Explanation: Detailed explanation of key concepts and code sections</li> <li>Extensions: Suggestions for extending or modifying the example</li> </ol>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>All examples can be run using the examples main.py script:</p> <pre><code># Clone the repository\ngit clone https://github.com/soran-ghaderi/torchebm.git\ncd torchebm\n\n# Set up your environment\npip install -e .\n\n# List all available examples\npython examples/main.py --list\n\n# Run a specific example\npython examples/main.py samplers/langevin/visualization_trajectory\n</code></pre>"},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<p>To run these examples, you'll need:</p> <ul> <li>Python 3.7+</li> <li>PyTorch 1.9+</li> <li>NumPy</li> <li>Matplotlib</li> </ul> <p>If you haven't installed TorchEBM yet, see the Installation guide.</p>"},{"location":"examples/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Most examples support GPU acceleration and will automatically use CUDA if available:</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = GaussianEnergy(mean, cov).to(device)\n</code></pre>"},{"location":"examples/#key-example-files","title":"Key Example Files","text":"<p>You'll find examples organized into the following categories:</p> Category Description Key Files <code>core/energy_functions/</code> Energy function visualization and properties <code>landscape_2d.py</code>, <code>multimodal.py</code>, <code>parametric.py</code> <code>samplers/langevin/</code> Langevin dynamics sampling examples <code>gaussian_sampling.py</code>, <code>multimodal_sampling.py</code>, <code>visualization_trajectory.py</code> <code>samplers/hmc/</code> Hamiltonian Monte Carlo examples <code>gaussian_sampling.py</code>, <code>advanced.py</code>, <code>mass_matrix.py</code> <code>visualization/</code> Advanced visualization tools <code>basic/contour_plots.py</code>, <code>advanced/trajectory_animation.py</code>, <code>advanced/parallel_chains.py</code>"},{"location":"examples/#additional-resources","title":"Additional Resources","text":"<p>For more in-depth information about the concepts demonstrated in these examples, see:</p> <ul> <li>Energy Functions Guide</li> <li>Samplers Guide</li> <li>API Reference</li> </ul>"},{"location":"examples/#whats-next","title":"What's Next?","text":"<p>After exploring these examples, you might want to:</p> <ol> <li>Check out the API Reference for detailed documentation</li> <li>Read the Developer Guide to learn about contributing</li> <li>Look at the roadmap for upcoming features </li> </ol>"},{"location":"examples/datasets/","title":"Dataset Classes","text":""},{"location":"examples/datasets/#dataset-classes","title":"Dataset Classes","text":"<p>The <code>torchebm</code> library provides a variety of 2D synthetic datasets through the <code>torchebm.datasets</code> module. These datasets are implemented as PyTorch <code>Dataset</code> classes for easy integration with DataLoaders. This walkthrough explores each dataset class with examples and visualizations.</p>"},{"location":"examples/datasets/#setup","title":"Setup","text":"<p>First, let's import the necessary packages:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.datasets import (\n    GaussianMixtureDataset, EightGaussiansDataset, TwoMoonsDataset,\n    SwissRollDataset, CircleDataset, CheckerboardDataset,\n    PinwheelDataset, GridDataset\n)\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Helper function to visualize a dataset\ndef visualize_dataset(data, title, figsize=(5, 5)):\n    plt.figure(figsize=figsize)\n    plt.scatter(data[:, 0], data[:, 1], s=5, alpha=0.6)\n    plt.title(title)\n    plt.grid(True, alpha=0.3)\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"examples/datasets/#dataset-types","title":"Dataset Types","text":""},{"location":"examples/datasets/#1-gaussian-mixture","title":"1. Gaussian Mixture","text":"<p>Gaussian Mixture</p> <p>Generate points from a mixture of Gaussian distributions arranged in a circle.</p> <p>This dataset generator is useful for testing mode-seeking behavior in energy-based models.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>n_components</code>: Number of Gaussian components (modes)</li> <li><code>std</code>: Standard deviation of each Gaussian</li> <li><code>radius</code>: Radius of the circle on which centers are placed</li> </ul> <pre><code># Generate 1000 samples from a 6-component Gaussian mixture\ngmm_dataset = GaussianMixtureDataset(\n    n_samples=1000, \n    n_components=6, \n    std=0.05, \n    radius=1.0, \n    seed=42\n)\ngmm_data = gmm_dataset.get_data()\nvisualize_dataset(gmm_data, \"Gaussian Mixture (6 components)\")\n</code></pre> <p> </p> Gaussian Mixture with 6 components"},{"location":"examples/datasets/#2-eight-gaussians","title":"2. Eight Gaussians","text":"<p>Eight Gaussians</p> <p>A specific case of Gaussian mixture with 8 components arranged at compass and diagonal points.</p> <p>This is a common benchmark distribution in energy-based modeling literature.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>std</code>: Standard deviation of each component</li> <li><code>scale</code>: Scaling factor for the centers</li> </ul> <pre><code># Generate 1000 samples from the 8 Gaussians distribution\neight_gauss_dataset = EightGaussiansDataset(\n    n_samples=1000, \n    std=0.02, \n    scale=2.0, \n    seed=42\n)\neight_gauss_data = eight_gauss_dataset.get_data()\nvisualize_dataset(eight_gauss_data, \"Eight Gaussians\")\n</code></pre> <p> </p> Eight Gaussians distribution"},{"location":"examples/datasets/#3-two-moons","title":"3. Two Moons","text":"<p>Two Moons</p> <p>Generate the classic \"two moons\" dataset with two interleaving half-circles.</p> <p>This dataset is excellent for testing classification, clustering, and density estimation algorithms  due to its non-linear separation boundary.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>noise</code>: Standard deviation of Gaussian noise added</li> </ul> <pre><code># Generate 1000 samples from the Two Moons distribution\nmoons_dataset = TwoMoonsDataset(\n    n_samples=1000, \n    noise=0.05, \n    seed=42\n)\nmoons_data = moons_dataset.get_data()\nvisualize_dataset(moons_data, \"Two Moons\")\n</code></pre> <p> </p> Two Moons distribution"},{"location":"examples/datasets/#4-swiss-roll","title":"4. Swiss Roll","text":"<p>Swiss Roll</p> <p>Generate the 2D Swiss roll dataset with a spiral structure.</p> <p>The Swiss roll is a classic example of a nonlinear manifold.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>noise</code>: Standard deviation of Gaussian noise added</li> <li><code>arclength</code>: Controls how many rolls (pi*arclength)</li> </ul> <pre><code># Generate 1000 samples from the Swiss Roll distribution\nswiss_roll_dataset = SwissRollDataset(\n    n_samples=1000, \n    noise=0.05, \n    arclength=3.0, \n    seed=42\n)\nswiss_roll_data = swiss_roll_dataset.get_data()\nvisualize_dataset(swiss_roll_data, \"Swiss Roll\")\n</code></pre> <p> </p> Swiss Roll distribution"},{"location":"examples/datasets/#5-circle","title":"5. Circle","text":"<p>Circle</p> <p>Generate points uniformly distributed on a circle with optional noise.</p> <p>This simple distribution is useful for testing density estimation on a 1D manifold embedded in 2D space.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>noise</code>: Standard deviation of Gaussian noise added</li> <li><code>radius</code>: Radius of the circle</li> </ul> <pre><code># Generate 1000 samples from a Circle distribution\ncircle_dataset = CircleDataset(\n    n_samples=1000, \n    noise=0.05, \n    radius=1.0, \n    seed=42\n)\ncircle_data = circle_dataset.get_data()\nvisualize_dataset(circle_data, \"Circle\")\n</code></pre> <p> </p> Circle distribution"},{"location":"examples/datasets/#6-checkerboard","title":"6. Checkerboard","text":"<p>Checkerboard</p> <p>Generate points in a 2D checkerboard pattern with alternating high and low density regions.</p> <p>The checkerboard pattern creates multiple modes in a regular structure, challenging an EBM's ability  to capture complex multimodal distributions.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Target number of samples</li> <li><code>range_limit</code>: Defines the square region [-lim, lim] x [-lim, lim]</li> <li><code>noise</code>: Small Gaussian noise added to points</li> </ul> <pre><code># Generate 1000 samples from a Checkerboard distribution\ncheckerboard_dataset = CheckerboardDataset(\n    n_samples=1000, \n    range_limit=4.0, \n    noise=0.01, \n    seed=42\n)\ncheckerboard_data = checkerboard_dataset.get_data()\nvisualize_dataset(checkerboard_data, \"Checkerboard\")\n</code></pre> <p> </p> Checkerboard distribution"},{"location":"examples/datasets/#7-pinwheel","title":"7. Pinwheel","text":"<p>Pinwheel</p> <p>Generate the pinwheel dataset with curved blades spiraling outward.</p> <p>The pinwheel dataset is highly configurable:</p> <ul> <li>Adjust the number of blades with <code>n_classes</code></li> <li>Control blade length with <code>radial_scale</code></li> <li>Control blade thickness with <code>angular_scale</code></li> <li>Control how tightly the blades spiral with <code>spiral_scale</code></li> </ul> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>n_classes</code>: Number of 'blades' in the pinwheel</li> <li><code>noise</code>: Standard deviation of Gaussian noise</li> <li><code>radial_scale</code>: Scales the maximum radius of the points</li> <li><code>angular_scale</code>: Controls blade thickness</li> <li><code>spiral_scale</code>: Controls how tightly blades spiral</li> </ul> <pre><code># Generate 1000 samples from a Pinwheel distribution with 5 blades\npinwheel_dataset = PinwheelDataset(\n    n_samples=1000, \n    n_classes=5, \n    noise=0.05, \n    radial_scale=2.0,\n    angular_scale=0.1,\n    spiral_scale=5.0,\n    seed=42\n)\npinwheel_data = pinwheel_dataset.get_data()\nvisualize_dataset(pinwheel_data, \"Pinwheel (5 blades)\")\n</code></pre> <p> </p> Pinwheel distribution with 5 blades"},{"location":"examples/datasets/#8-2d-grid","title":"8. 2D Grid","text":"<p>2D Grid</p> <p>Generate points on a regular 2D grid with optional noise.</p> <p>This is useful for creating test points to evaluate model predictions across a regular spatial arrangement.</p> <p>Parameters:</p> <ul> <li><code>n_samples_per_dim</code>: Number of points along each dimension</li> <li><code>range_limit</code>: Defines the square region [-lim, lim] x [-lim, lim]</li> <li><code>noise</code>: Standard deviation of Gaussian noise added</li> </ul> <pre><code># Generate a 20x20 grid of points\ngrid_dataset = GridDataset(\n    n_samples_per_dim=20, \n    range_limit=1.0, \n    noise=0.01, \n    seed=42\n)\ngrid_data = grid_dataset.get_data()\nvisualize_dataset(grid_data, \"2D Grid (20x20)\")\n</code></pre> <p> </p> 2D Grid with 20x20 points"},{"location":"examples/datasets/#usage-examples","title":"Usage Examples","text":""},{"location":"examples/datasets/#using-with-dataloader","title":"Using with DataLoader","text":"<p>One of the key advantages of the dataset classes is their compatibility with PyTorch's DataLoader for efficient batch processing:</p> <pre><code>from torch.utils.data import DataLoader\n\n# Create a dataset\ndataset = GaussianMixtureDataset(n_samples=2000, n_components=8, std=0.1, seed=42)\n\n# Create a DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    drop_last=True\n)\n\n# Iterate through batches\nfor batch in dataloader:\n    # Each batch is a tensor of batch_shape [batch_size, 2]\n    print(f\"Batch batch_shape: {batch.shape}\")\n    # Process the batch...\n    break  # Just showing the first batch\n</code></pre>"},{"location":"examples/datasets/#comparing-multiple-datasets","title":"Comparing Multiple Datasets","text":"<p>You can easily generate and compare multiple datasets:</p> CodeOutput <pre><code># Create a figure with multiple datasets\nplt.figure(figsize=(15, 10))\n\n# Generate datasets\ndatasets = [\n    (GaussianMixtureDataset(1000, 8, 0.05, seed=42).get_data(), \"Gaussian Mixture\"),\n    (TwoMoonsDataset(1000, 0.05, seed=42).get_data(), \"Two Moons\"),\n    (SwissRollDataset(1000, 0.05, seed=42).get_data(), \"Swiss Roll\"),\n    (CircleDataset(1000, 0.05, seed=42).get_data(), \"Circle\"),\n    (CheckerboardDataset(1000, 4.0, 0.01, seed=42).get_data(), \"Checkerboard\"),\n    (PinwheelDataset(1000, 5, 0.05, seed=42).get_data(), \"Pinwheel\")\n]\n\n# Plot each dataset\nfor i, (data, title) in enumerate(datasets):\n    plt.subplot(2, 3, i+1)\n    plt.scatter(data[:, 0], data[:, 1], s=3, alpha=0.6)\n    plt.title(title)\n    plt.grid(True, alpha=0.3)\n    plt.axis('equal')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>    -    -    -    -    -    -  <p></p>"},{"location":"examples/datasets/#device-support","title":"Device Support","text":"<p>All dataset classes support placing tensors directly on specific devices:</p> <pre><code># Generate data on GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ngpu_dataset = GaussianMixtureDataset(1000, 4, 0.1, device=device, seed=42)\ngpu_data = gpu_dataset.get_data()\nprint(f\"Data is on: {gpu_data.device}\")\n</code></pre>"},{"location":"examples/datasets/#training-example","title":"Training Example","text":"<p>Here's a simplified example of using these datasets for training an energy-based model, similar to what's shown in the mlp_cd_training.py example:</p> Complete ExampleKey Components <pre><code># Imports\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.losses import ContrastiveDivergence\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define an energy function\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Setup training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create dataset directly with device specification\ndataset = TwoMoonsDataset(n_samples=3000, noise=0.05, seed=42, device=device)\ndataloader = DataLoader(dataset, batch_size=256, shuffle=True, drop_last=True)\n\n# Model components\nenergy_model = MLPEnergy(input_dim=2, hidden_dim=16).to(device)\nsampler = LangevinDynamics(\n    energy_function=energy_model,\n    step_size=0.1, \n    noise_scale=0.1,\n    device=device\n)\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_model,\n    sampler=sampler,\n    n_steps=10\n).to(device)\n\n# Optimizer\noptimizer = optim.Adam(energy_model.parameters(), lr=1e-3)\n\n# Training loop (simplified)\nfor epoch in range(5):  # Just a few epochs for demonstration\n    for data_batch in dataloader:\n        optimizer.zero_grad()\n        loss, _ = loss_fn(data_batch)\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n</code></pre> <ul> <li>Dataset: <code>TwoMoonsDataset</code> placed directly on device</li> <li>Energy Function: Simple MLP implementing <code>BaseEnergyFunction</code></li> <li>Sampler: <code>LangevinDynamics</code> for generating samples</li> <li>Loss: <code>ContrastiveDivergence</code> for EBM training</li> <li>Training Loop: Standard PyTorch pattern with DataLoader</li> </ul> <p>For more detailed examples, see Training Energy Models.</p>"},{"location":"examples/datasets/#summary","title":"Summary","text":"<p>Key Features</p> <ul> <li>Dataset Variety: 8 distinct 2D distributions for different testing scenarios</li> <li>PyTorch Integration: Built as <code>torch.utils.data.Dataset</code> subclasses</li> <li>Device Support: Create datasets directly on CPU or GPU</li> <li>Configurability: Extensive parameterization for all distributions</li> <li>Reproducibility: Seed support for deterministic generation</li> </ul> <p>These dataset classes provide diverse 2D distributions for testing energy-based models. Each distribution has different characteristics that can challenge different aspects of model learning:</p> Dataset Testing Focus Gaussian Mixtures Mode-seeking behavior Two Moons Non-linear decision boundaries Swiss Roll &amp; Circle Manifold learning capabilities Checkerboard Multiple modes in regular patterns Pinwheel Complex spiral structure with varying density <p>The class-based implementation provides seamless integration with PyTorch's DataLoader system, making it easy to incorporate these datasets into your training pipeline.</p>"},{"location":"examples/datasets/#see-also","title":"See Also","text":"<ul> <li>Energy Function Implementations</li> <li>Sampler Options</li> <li>Training Guide</li> <li>EBM Applications </li> </ul>"},{"location":"examples/energy_functions/","title":"Energy Functions","text":""},{"location":"examples/energy_functions/#energy-function-examples","title":"Energy Function Examples","text":"<p>This section demonstrates the various energy functions available in TorchEBM and how to visualize them.</p>"},{"location":"examples/energy_functions/#basic-energy-landscapes","title":"Basic Energy Landscapes","text":"<p>The <code>landscape_2d.py</code> example shows how to create and visualize basic energy functions:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create the energy function\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Create a grid for visualization\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Create 3D surface plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\nsurf = ax.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.8)\n</code></pre>"},{"location":"examples/energy_functions/#multimodal-energy-functions","title":"Multimodal Energy Functions","text":"<p>The <code>multimodal.py</code> example demonstrates more complex energy functions with multiple local minima:</p> <pre><code>class MultimodalEnergy:\n    \"\"\"\n    A 2D energy function with multiple local minima to demonstrate sampling behavior.\n    \"\"\"\n    def __init__(self, device=None, dtype=torch.float32):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.dtype = dtype\n\n        # Define centers and weights for multiple Gaussian components\n        self.centers = torch.tensor(\n            [[-1.0, -1.0], [1.0, 1.0], [-0.5, 1.0], [1.0, -0.5]],\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n        self.weights = torch.tensor(\n            [1.0, 0.8, 0.6, 0.7], device=self.device, dtype=self.dtype\n        )\n\n    def __call__(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Calculate energy as negative log of mixture of Gaussians\n        dists = torch.cdist(x, self.centers)\n        energy = -torch.log(\n            torch.sum(self.weights * torch.exp(-0.5 * dists.pow(2)), dim=-1)\n        )\n        return energy\n</code></pre>"},{"location":"examples/energy_functions/#parametric-energy-functions","title":"Parametric Energy Functions","text":"<p>The <code>parametric.py</code> example shows how to create energy functions with adjustable parameters:</p> <pre><code># Create a figure with multiple subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\n# Calculate energy landscapes for different barrier heights\nbarrier_heights = [0.5, 1.0, 2.0, 4.0]\n\nfor i, barrier_height in enumerate(barrier_heights):\n    # Create energy function with the specified barrier height\n    energy_fn = DoubleWellEnergy(barrier_height=barrier_height)\n\n    # Compute energy values\n    # ...\n\n    # Create contour plot\n    contour = axes[i].contourf(X, Y, Z, 50, cmap=\"viridis\")\n    fig.colorbar(contour, ax=axes[i], label=\"Energy\")\n    axes[i].set_title(f\"Double Well Energy (Barrier Height = {barrier_height})\")\n</code></pre>"},{"location":"examples/energy_functions/#running-the-examples","title":"Running the Examples","text":"<p>To run these examples:</p> <pre><code># List available energy function examples\npython examples/main.py --list\n\n# Run a specific example\npython examples/main.py energy_functions/energy_functions/landscape_2d\npython examples/main.py energy_functions/energy_functions/multimodal\npython examples/main.py energy_functions/energy_functions/parametric\n</code></pre>"},{"location":"examples/energy_functions/#additional-resources","title":"Additional Resources","text":"<p>For more information on energy functions, see:</p> <ul> <li>API Reference: Energy Functions</li> <li>Guide: Energy Functions </li> <li>Visualization Guide</li> </ul>"},{"location":"examples/samplers/hmc/","title":"Hamiltonian Monte Carlo","text":""},{"location":"examples/samplers/hmc/#hamiltonian-monte-carlo-sampling","title":"Hamiltonian Monte Carlo Sampling","text":"<p>This example demonstrates how to use the Hamiltonian Monte Carlo (HMC) sampler in TorchEBM to efficiently sample from energy landscapes.</p> <p>Key Concepts Covered</p> <ul> <li>Basic usage of Hamiltonian Monte Carlo</li> <li>High-dimensional sampling</li> <li>Working with diagnostics</li> <li>GPU acceleration</li> <li>Custom mass matrix configuration</li> </ul>"},{"location":"examples/samplers/hmc/#overview","title":"Overview","text":"<p>Hamiltonian Monte Carlo (HMC) is an advanced Markov Chain Monte Carlo (MCMC) method that uses the geometry of the energy landscape to make more efficient sampling proposals. By incorporating gradient information and simulating Hamiltonian dynamics, HMC can explore distributions more efficiently than random-walk methods, particularly in high dimensions.</p>"},{"location":"examples/samplers/hmc/#basic-example","title":"Basic Example","text":"<p>The following example shows how to sample from a 2D Gaussian distribution using HMC:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.hmc import HamiltonianMonteCarlo\n\n# Create energy function for a 2D Gaussian\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndim = 2  # dimension of the state space\nn_steps = 100  # steps between samples\nn_samples = 1000  # num of samples\nmean = torch.tensor([1.0, -1.0], device=device)\ncov = torch.tensor([[1.0, 0.5], [0.5, 2.0]], device=device)\nenergy_fn = GaussianEnergy(mean, cov)\n\n# Initialize HMC sampler\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=5,\n    device=device\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = hmc_sampler.sample(\n    x=initial_state,\n    n_steps=n_steps\n)\n\n# Plot results\nsamples = samples.cpu().numpy()\nplt.figure(figsize=(10, 5))\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\nplt.title(\"Samples from 2D Gaussian using HMC\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.show()\n</code></pre>"},{"location":"examples/samplers/hmc/#sample-distribution-visualization","title":"Sample Distribution Visualization","text":"HMC Sampling from 2D Gaussian <p>This plot shows 1000 samples from a 2D Gaussian distribution generated using Hamiltonian Monte Carlo. Note how the samples efficiently cover the target distribution's high-probability regions. The samples reflect the covariance structure with the characteristic elliptical shape around the mean [1.0, -1.0], represented by the red point and dashed ellipse.</p>"},{"location":"examples/samplers/hmc/#how-hmc-works","title":"How HMC Works","text":"<p>Hamiltonian Monte Carlo uses principles from physics to improve sampling efficiency:</p> <ol> <li>Hamiltonian System: Introduces momentum variables alongside position variables</li> <li>Leapfrog Integration: Simulates the Hamiltonian dynamics using a symplectic integrator</li> <li>Metropolis Acceptance: Ensures detailed balance by accepting/rejecting proposals</li> <li>Momentum Resampling: Periodically resamples momentum to explore different directions</li> </ol> <p>The HMC update consists of these steps:</p> <ol> <li>Sample momentum variables from a Gaussian distribution</li> <li>Simulate Hamiltonian dynamics using leapfrog integration</li> <li>Compute the acceptance probability based on the change in total energy</li> <li>Accept or reject the proposal based on the acceptance probability</li> <li>Repeat from step 1</li> </ol>"},{"location":"examples/samplers/hmc/#key-parameters","title":"Key Parameters","text":"<p>The HMC sampler has several important parameters:</p> Parameter Description <code>step_size</code> Size of each leapfrog step - controls the discretization granularity <code>n_leapfrog_steps</code> Number of leapfrog steps per proposal - controls trajectory length <code>mass</code> Optional parameter to adjust the momentum distribution (defaults to identity) <code>device</code> Device to run computations on (\"cpu\" or \"cuda\")"},{"location":"examples/samplers/hmc/#working-with-diagnostics","title":"Working with Diagnostics","text":"<p>HMC provides several diagnostic metrics to monitor sampling quality. The diagnostics tensor includes information about the sampling process:</p> <pre><code>final_samples, diagnostics = hmc_sampler.sample(\n    n_samples=n_samples,\n    n_steps=n_steps,\n    dim=dim,\n    return_trajectory=True,\n    return_diagnostics=True,\n)\n\n# The diagnostics tensor has batch_shape (k_steps, 4, n_samples, dim) and contains:\nmean_values = diagnostics[:, 0, :, :]  # Mean of samples at each step\nvariance_values = diagnostics[:, 1, :, :]  # Variance of samples at each step \nenergy_values = diagnostics[:, 2, :, :]  # Energy values of samples\nacceptance_rates = diagnostics[:, 3, :, :]  # Acceptance rates for each sample\n\n# To get the overall acceptance rate at the last step:\noverall_acceptance_rate = acceptance_rates[-1].mean()\n</code></pre> <p>Good Acceptance Rates</p> <p>For HMC, an acceptance rate between 60-90% typically indicates good performance. If the rate is too low, the step size should be decreased. If it's too high, the step size might be inefficiently small.</p>"},{"location":"examples/samplers/hmc/#performance-considerations","title":"Performance Considerations","text":"<ul> <li> <p> GPU Acceleration</p> <p>HMC can benefit significantly from GPU acceleration, especially for large sample sizes or high-dimensional problems.</p> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn, \n    step_size=0.1, \n    n_leapfrog_steps=10, \n    device=device\n)\n</code></pre> </li> <li> <p> Parameter Tuning</p> <p>The performance of HMC is sensitive to the choice of step size and number of leapfrog steps.</p> <pre><code># For higher dimensions, use more leapfrog steps\nn_leapfrog_steps = max(5, int(np.sqrt(dim)))\n\n# Step size should decrease with dimension\nstep_size = min(0.1, 0.5 * dim**(-0.25))\n</code></pre> </li> <li> <p> Warm-up Period</p> <p>Consider discarding initial samples to allow the chain to reach the target distribution.</p> <pre><code># Run 100 warm-up steps before collecting samples\nwarm_up_samples = hmc_sampler.sample(\n  x=initial_state, n_steps=100\n)\n# Use the final state as the starting point\nsamples = hmc_sampler.sample(\n  x=warm_up_samples, n_steps=1000\n)\n</code></pre> </li> <li> <p> Parallel Chains</p> <p>Run multiple chains in parallel to improve exploration and assess convergence.</p> <pre><code># Run 10 chains in parallel\nn_chains = 10\nsamples = hmc_sampler.sample(\n  dim=dim, n_steps=1000, n_samples=n_chains\n)\n</code></pre> </li> </ul>"},{"location":"examples/samplers/hmc/#comparison-with-langevin-dynamics","title":"Comparison with Langevin Dynamics","text":"<p>HMC differs from Langevin dynamics in several important ways:</p> Feature HMC Langevin Dynamics Computational Cost Higher (multiple gradient evaluations per step) Lower (one gradient evaluation per step) Exploration Efficiency More efficient, especially in high dimensions Less efficient, more random walk behavior Parameters to Tune Step size and number of leapfrog steps Step size and noise scale Acceptance Step Uses Metropolis acceptance No explicit acceptance step Autocorrelation Typically lower Typically higher"},{"location":"examples/samplers/hmc/#using-custom-mass-matrix","title":"Using Custom Mass Matrix","text":"<p>The mass parameter in HMC affects how momentum is sampled and can improve sampling efficiency for certain distributions:</p> <pre><code># Custom mass parameter (diagonal values)\nmass = torch.tensor([0.1, 1.0], device=device)\n\n# Initialize HMC sampler with custom mass\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10,\n    mass=mass,\n    device=device\n)\n\n# Generate samples\ninitial_state = torch.zeros(n_samples, dim, device=device)\nsamples = hmc_sampler.sample(\n    x=initial_state,\n    n_steps=n_steps\n)\n</code></pre> <p>The mass parameter can be provided as either:</p> <ul> <li>A scalar value (float) that's applied to all dimensions</li> <li>A tensor of values for a diagonal mass matrix</li> </ul> <p>When using a diagonal mass matrix, each dimension can have different momentum scaling. This can be useful when dimensions have different scales or variances.</p>"},{"location":"examples/samplers/hmc/#custom-mass-matrix-results","title":"Custom Mass Matrix Results","text":"HMC Sampling with Custom Mass Matrix <p>This plot shows samples from the same 2D Gaussian distribution using HMC with a custom diagonal mass parameter [0.1, 1.0]. The mass parameter affects the sampling dynamics, allowing more efficient exploration of the distribution. The red point indicates the mean, and the dashed ellipse represents the 2\u03c3 confidence region.</p>"},{"location":"examples/samplers/hmc/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"<p>The following visualization compares standard HMC with HMC using a custom mass parameter:</p> <p> </p> HMC Implementation Comparison <p>This side-by-side comparison shows standard HMC (left) and HMC with a custom mass parameter (right) sampling from the same Gaussian distribution. Both methods effectively sample the distribution, but with slightly different dynamics due to the mass parameter configuration.</p>"},{"location":"examples/samplers/hmc/#conclusion","title":"Conclusion","text":"<p>Hamiltonian Monte Carlo provides efficient sampling for complex, high-dimensional distributions. It leverages gradient information to make informed proposals, resulting in faster mixing and lower autocorrelation compared to simpler methods. While it requires more computation per step than methods like Langevin dynamics, it often requires fewer steps overall to achieve the same sampling quality.</p> <p>By adjusting parameters like the mass value, step size, and number of leapfrog steps, you can optimize HMC for specific sampling tasks and distribution characteristics. </p>"},{"location":"examples/samplers/langevin_dynamics/","title":"Langevin Dynamics Sampling","text":""},{"location":"examples/samplers/langevin_dynamics/#langevin-dynamics-sampling","title":"Langevin Dynamics Sampling","text":"<p>This example demonstrates how to use the Langevin Dynamics sampler in TorchEBM to generate samples from various energy functions.</p> <p>Key Concepts Covered</p> <ul> <li>Basic usage of Langevin Dynamics</li> <li>Parallel sampling with multiple chains</li> <li>Performance considerations</li> <li>Working with diagnostics</li> </ul>"},{"location":"examples/samplers/langevin_dynamics/#overview","title":"Overview","text":"<p>Langevin dynamics is a powerful sampling method that uses gradients of the energy function to guide the exploration of the state space, combined with random noise to ensure proper exploration. It's particularly useful for sampling from complex, high-dimensional distributions.</p>"},{"location":"examples/samplers/langevin_dynamics/#basic-example","title":"Basic Example","text":"<p>The following example shows how to sample from a 2D Gaussian distribution using Langevin dynamics:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n\ndef basic_example():\n    \"\"\"\n    Simple Langevin dynamics sampling from a 2D Gaussian distribution.\n    \"\"\"\n    # Create energy function for a 2D Gaussian\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dim = 2  # dimension of the state space\n    n_steps = 100  # steps between samples\n    n_samples = 1000  # num of samples\n    mean = torch.tensor([1.0, -1.0])\n    cov = torch.tensor([[1.0, 0.5], [0.5, 2.0]])\n    energy_fn = GaussianEnergy(mean, cov, device=device)\n\n    # Initialize sampler\n    sampler = LangevinDynamics(\n        energy_function=energy_fn,\n        step_size=0.01,\n        noise_scale=0.1,\n        device=device,\n    )\n\n    # Generate samples\n    initial_state = torch.zeros(n_samples, dim, device=device)\n    samples = sampler.sample(\n        x=initial_state,\n        n_steps=n_steps,\n        n_samples=n_samples,\n    )\n\n    # Plot results\n    samples = samples.cpu().numpy()\n    plt.figure(figsize=(10, 5))\n    plt.scatter(samples[:, 0], samples[:, 1], alpha=0.1)\n    plt.title(\"Samples from 2D Gaussian using Langevin Dynamics\")\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.show()\n</code></pre>"},{"location":"examples/samplers/langevin_dynamics/#high-dimensional-sampling","title":"High-Dimensional Sampling","text":"<p>Langevin dynamics scales well to high-dimensional spaces. Here's an example sampling from a 10D Gaussian:</p> <pre><code>import torch\nimport time\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n\ndef langevin_gaussain_sampling():\n    energy_fn = GaussianEnergy(mean=torch.zeros(10), cov=torch.eye(10))\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Initialize Langevin dynamics model\n    langevin_sampler = LangevinDynamics(\n        energy_function=energy_fn, step_size=5e-3, device=device\n    ).to(device)\n\n    # Initial state: batch of 100 samples, 10-dimensional space\n    ts = time.time()\n    # Run Langevin sampling for 500 steps\n    final_x = langevin_sampler.sample(\n        dim=10, n_steps=500, n_samples=10000, return_trajectory=False\n    )\n\n    print(final_x.shape)  # Output: (10000, 10)  (final state)\n    print(\"Time taken: \", time.time() - ts)\n\n    # Sample with diagnostics and trajectory\n    n_samples = 250\n    n_steps = 500\n    dim = 10\n    final_samples, diagnostics = langevin_sampler.sample(\n        n_samples=n_samples,\n        n_steps=n_steps,\n        dim=dim,\n        return_trajectory=True,\n        return_diagnostics=True,\n    )\n    print(final_samples.shape)  # Output: (250, 500, 10)\n    print(diagnostics.shape)  # (500, 3, 250, 10)\n</code></pre>"},{"location":"examples/samplers/langevin_dynamics/#working-with-diagnostics","title":"Working with Diagnostics","text":"<p>TorchEBM can return diagnostic information during sampling to monitor the sampling process:</p> <pre><code>from typing import Tuple\nimport torch\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\nfrom torchebm.core import HarmonicEnergy\n\n\ndef sampling_utilities_example():\n    \"\"\"\n    Example demonstrating various utility features:\n    1. Chain thinning (future updates)\n    2. Device management\n    3. Custom diagnostics\n    4. Convergence checking\n    \"\"\"\n\n    # Initialize sampler with GPU support if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    sampler = LangevinDynamics(\n        energy_function=HarmonicEnergy(), step_size=0.01, noise_scale=0.1\n    ).to(device)\n\n    # Generate samples with diagnostics\n    initial_state = torch.tensor([2.0], device=device)\n    samples, diagnostics = sampler.sample(\n        x=initial_state,\n        n_steps=50,\n        n_samples=1000,\n        return_diagnostics=True,\n    )\n\n    # Custom analysis of results\n    def analyze_convergence(\n            samples: torch.Tensor, diagnostics: list\n    ) -&gt; Tuple[float, float]:\n        \"\"\"Example utility function to analyze convergence.\"\"\"\n        mean = samples.mean().item()\n        std = samples.std().item()\n        return mean, std\n\n    mean, std = analyze_convergence(samples, diagnostics)\n    print(f\"Sample Statistics - Mean: {mean:.3f}, Std: {std:.3f}\")\n</code></pre>"},{"location":"examples/samplers/langevin_dynamics/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/samplers/langevin_dynamics/#gpu-acceleration","title":"GPU Acceleration","text":"<p>TorchEBM's Langevin dynamics sampler works efficiently on both CPU and GPU. When available, using a GPU can significantly accelerate sampling, especially for high-dimensional distributions or large sample sizes.</p> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsampler = LangevinDynamics(\n    energy_function=energy_fn, \n    step_size=0.01,\n    device=device\n).to(device)\n</code></pre>"},{"location":"examples/samplers/langevin_dynamics/#parallel-sampling","title":"Parallel Sampling","text":"<p>The sampler automatically handles parallel sampling when you specify <code>n_samples &gt; 1</code>. This parallelism is particularly efficient on GPUs.</p> <pre><code># Generate 1000 samples in parallel\nsamples = sampler.sample(\n    dim=10,\n    n_steps=100,\n    n_samples=1000\n)\n</code></pre>"},{"location":"examples/samplers/langevin_dynamics/#advanced-parameters","title":"Advanced Parameters","text":"<p>The Langevin dynamics sampler supports several parameters for fine-tuning:</p> Parameter Description <code>step_size</code> Controls the size of update steps <code>noise_scale</code> Controls the amount of random exploration <code>decay</code> Optional decay factor for step size during sampling <code>thinning</code> How many steps to skip between saved samples <code>return_trajectory</code> Whether to return the entire sampling trajectory <code>return_diagnostics</code> Whether to collect and return diagnostic information"},{"location":"examples/samplers/langevin_dynamics/#key-considerations","title":"Key Considerations","text":"<p>When using Langevin dynamics, keep in mind:</p> <ol> <li>Step Size: Too large can cause instability, too small can make sampling inefficient</li> <li>Burn-in Period: Initial samples may be far from the target distribution</li> <li>Energy Gradient: Ensure your energy function has a well-defined gradient</li> <li>Tuning: Optimal parameters depend on the specific energy landscape</li> </ol>"},{"location":"examples/samplers/langevin_dynamics/#conclusion","title":"Conclusion","text":"<p>Langevin dynamics is a versatile sampling approach suitable for many energy-based models. It combines the efficiency of gradient-based methods with the exploration capability of stochastic methods, making it an excellent choice for complex distributions.</p> <p>For a more visual exploration of Langevin dynamics, see the Langevin Sampler Trajectory example that visualizes sampling trajectories overlaid on energy landscapes.</p>"},{"location":"examples/samplers/langevin_dynamics/#visualization-result","title":"Visualization Result","text":"Langevin Dynamics Sampling from 2D Gaussian <p>This plot shows 1000 samples from a 2D Gaussian distribution generated using Langevin dynamics. The samples are concentrated around the mean [1.0, -1.0] and reflect the covariance structure with the characteristic elliptical shape.</p>"},{"location":"examples/samplers/langevin_dynamics/#working-with-double-well-energy-and-diagnostics","title":"Working with Double Well Energy and Diagnostics","text":"<p>Here's an example showing a trajectory from a Double Well energy function along with energy diagnostics:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create Double Well energy function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Initialize sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.1,\n    noise_scale=0.3,\n    device=device\n)\n\n# Generate samples with trajectory and diagnostics\ninitial_state = torch.tensor([-1.5], device=device).view(1, 1)\ntrajectory, diagnostics = sampler.sample(\n    x=initial_state,\n    n_steps=5000,\n    return_trajectory=True,\n    return_diagnostics=True\n)\n\n# Plot trajectory and energy\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nax1.plot(trajectory[0, :, 0].cpu().numpy())\nax1.set_title(\"Single Chain Trajectory\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Position\")\nax2.plot(diagnostics[:, 2, 0, 0].cpu().numpy())\nax2.set_title(\"Energy Evolution\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Energy\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/samplers/langevin_dynamics/#diagnostics-visualization","title":"Diagnostics Visualization","text":"Double Well Trajectory and Energy <p>The left plot shows a single sampling chain trajectory in a Double Well energy landscape. The trajectory moves between the two wells over time. The right plot shows the corresponding energy evolution during sampling, with drops indicating transitions between wells. </p>"},{"location":"examples/samplers/langevin_trajectory/","title":"Langevin Sampler Trajectory","text":""},{"location":"examples/samplers/langevin_trajectory/#langevin-sampler-trajectory","title":"Langevin Sampler Trajectory","text":"<p>This example demonstrates how to visualize the trajectories of Langevin dynamics samplers on multimodal energy landscapes.</p> <p>Key Concepts Covered</p> <ul> <li>Creating custom energy functions</li> <li>Visualizing energy landscapes</li> <li>Tracking and plotting sampling trajectories</li> <li>Working with multimodal distributions</li> </ul>"},{"location":"examples/samplers/langevin_trajectory/#overview","title":"Overview","text":"<p>Visualizing sampling trajectories helps understand how different sampling algorithms explore the energy landscape. This example creates a multimodal energy function and visualizes multiple sampling chains as they traverse the landscape.</p>"},{"location":"examples/samplers/langevin_trajectory/#multimodal-energy-function","title":"Multimodal Energy Function","text":"<p>First, we define a custom energy function with multiple local minima:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\nclass MultimodalEnergy:\n    \"\"\"\n    A 2D energy function with multiple local minima to demonstrate sampling behavior.\n    \"\"\"\n\n    def __init__(self, device=None, dtype=torch.float32):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.dtype = dtype\n\n        # Define centers and weights for multiple Gaussian components\n        self.centers = torch.tensor(\n            [[-1.0, -1.0], [1.0, 1.0], [-0.5, 1.0], [1.0, -0.5]],\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n        self.weights = torch.tensor(\n            [1.0, 0.8, 0.6, 0.7], device=self.device, dtype=self.dtype\n        )\n\n    def __call__(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Ensure input has correct dtype and batch_shape\n        x = x.to(dtype=self.dtype)\n        if x.dim() == 1:\n            x = x.view(1, -1)\n\n        # Calculate distance to each center\n        dists = torch.cdist(x, self.centers)\n\n        # Calculate energy as negative log of mixture of Gaussians\n        energy = -torch.log(\n            torch.sum(self.weights * torch.exp(-0.5 * dists.pow(2)), dim=-1)\n        )\n\n        return energy\n\n    def gradient(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Ensure input has correct dtype and batch_shape\n        x = x.to(dtype=self.dtype)\n        if x.dim() == 1:\n            x = x.view(1, -1)\n\n        # Calculate distances and Gaussian components\n        diff = x.unsqueeze(1) - self.centers\n        exp_terms = torch.exp(-0.5 * torch.sum(diff.pow(2), dim=-1))\n        weights_exp = self.weights * exp_terms\n\n        # Calculate gradient\n        normalizer = torch.sum(weights_exp, dim=-1, keepdim=True)\n        gradient = torch.sum(\n            weights_exp.unsqueeze(-1) * diff / normalizer.unsqueeze(-1), dim=1\n        )\n\n        return gradient\n\n    def to(self, device):\n        self.device = device\n        self.centers = self.centers.to(device)\n        self.weights = self.weights.to(device)\n        return self\n</code></pre>"},{"location":"examples/samplers/langevin_trajectory/#visualization-function","title":"Visualization Function","text":"<p>Next, we create a function to visualize the energy landscape and multiple Langevin dynamics sampling trajectories:</p> <pre><code>def visualize_energy_landscape_and_sampling():\n    # Set up device and dtype\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dtype = torch.float32\n\n    # Create energy function\n    energy_fn = MultimodalEnergy(device=device, dtype=dtype)\n\n    # Initialize the standard Langevin dynamics sampler from the library\n    sampler = LangevinDynamics(\n        energy_function=energy_fn,\n        step_size=0.01,\n        noise_scale=0.1,\n        device=device\n    )\n\n    # Create grid for energy landscape visualization\n    x = np.linspace(-3, 3, 100)\n    y = np.linspace(-3, 3, 100)\n    X, Y = np.meshgrid(x, y)\n\n    # Calculate energy values\n    grid_points = torch.tensor(\n        np.stack([X.flatten(), Y.flatten()], axis=1), device=device, dtype=dtype\n    )\n    energy_values = energy_fn(grid_points).cpu().numpy().reshape(X.shape)\n\n    # Set up sampling parameters\n    dim = 2  # 2D energy function\n    n_steps = 200\n\n    # Create figure\n    plt.figure(figsize=(10, 8))\n\n    # Plot energy landscape with clear contours\n    contour = plt.contour(X, Y, energy_values, levels=20, cmap=\"viridis\")\n    plt.colorbar(contour, label=\"Energy\")\n\n    # Run multiple independent chains from different starting points\n    n_chains = 5\n\n    # Define distinct colors for the chains\n    colors = plt.cm.tab10(np.linspace(0, 1, n_chains))\n\n    # Generate seeds for random starting positions to make chains start in different areas\n    seeds = [42, 123, 456, 789, 999]\n\n    for i, seed in enumerate(seeds):\n        # Set the seed for reproducibility\n        torch.manual_seed(seed)\n\n        # Run one chain using the standard API\n        trajectory = sampler.sample(\n            dim=dim,  # 2D space\n            n_samples=1,  # Single chain\n            n_steps=n_steps,  # Number of steps\n            return_trajectory=True  # Return full trajectory\n        )\n\n        # Extract trajectory data\n        traj_np = trajectory.cpu().numpy().squeeze(0)  # Remove n_samples dimension\n\n        # Plot the trajectory\n        plt.plot(\n            traj_np[:, 0],\n            traj_np[:, 1],\n            'o-',\n            color=colors[i],\n            alpha=0.6,\n            markersize=3,\n            label=f\"Chain {i + 1}\"\n        )\n\n        # Mark the start and end points\n        plt.plot(traj_np[0, 0], traj_np[0, 1], 'o', color=colors[i], markersize=8)\n        plt.plot(traj_np[-1, 0], traj_np[-1, 1], '*', color=colors[i], markersize=10)\n\n    # Add labels and title\n    plt.title(\"Energy Landscape and Langevin Dynamics Sampling Trajectories\")\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n</code></pre>"},{"location":"examples/samplers/langevin_trajectory/#running-the-example","title":"Running the Example","text":"<p>To run the example, simply execute:</p> <pre><code>if __name__ == \"__main__\":\n    print(\"Running energy landscape visualization...\")\n    visualize_energy_landscape_and_sampling()\n</code></pre>"},{"location":"examples/samplers/langevin_trajectory/#expected-results","title":"Expected Results","text":"<p>When you run this example, you'll see a contour plot of the energy landscape with multiple chains of Langevin dynamics samples overlaid. The visualization shows:</p> <ul> <li>Energy landscape: Contour lines representing the multimodal energy function</li> <li>Multiple sampling chains: Different colored trajectories starting from random initial points</li> <li>Trajectory progression: You can see how samples move from high-energy regions to low-energy regions</li> </ul> <p> </p> Langevin Dynamics Sampling Trajectories <p>The key insights from this visualization:</p> <ol> <li>Sampling chains are attracted to areas of low energy (high probability)</li> <li>Chains can get trapped in local minima and have difficulty crossing energy barriers</li> <li>The stochastic nature of Langevin dynamics helps chains occasionally escape local minima</li> <li>Sampling efficiency depends on starting points and energy landscape geometry</li> </ol>"},{"location":"examples/samplers/langevin_trajectory/#understanding-multimodal-sampling","title":"Understanding Multimodal Sampling","text":"<p>Multimodal distributions present special challenges for sampling algorithms:</p> <p>Challenges in Multimodal Sampling</p> <ol> <li>Energy barriers: Chains must overcome barriers between modes</li> <li>Mode-hopping: Chains may have difficulty transitioning between distant modes</li> <li>Mixing time: The time required to adequately explore all modes increases</li> <li>Mode coverage: Some modes may be missed entirely during finite sampling</li> </ol> <p>The visualization helps understand these challenges by showing:</p> <ul> <li>How chains explore the space around each mode</li> <li>Whether chains successfully transition between modes</li> <li>If certain modes are favored over others</li> <li>The impact of initialization on the final sampling distribution</li> </ul>"},{"location":"examples/samplers/langevin_trajectory/#api-usage-notes","title":"API Usage Notes","text":"<p>This example demonstrates several key aspects of using the TorchEBM library:</p> <ol> <li>Creating custom energy functions: How to implement a custom energy function with gradient support</li> <li>Using the Langevin dynamics sampler: Using the standard library API</li> <li>Parallel chain sampling: Running multiple chains to explore different areas of the space</li> <li>Trajectory tracking: Enabling <code>return_trajectory=True</code> to record the full sampling path</li> </ol> <p>The standard pattern for using <code>LangevinDynamics.sample</code> is:</p> <pre><code># Initialize the sampler\nsampler = LangevinDynamics(energy_function=my_energy_fn, step_size=0.01)\n\n# Run sampling with trajectory tracking\ntrajectory = sampler.sample(\n    dim=2,  # Dimension of the space\n    n_samples=10,  # Number of parallel chains\n    n_steps=100,  # Number of steps to run\n    return_trajectory=True  # Return the full trajectory rather than just final points\n)\n</code></pre>"},{"location":"examples/samplers/langevin_trajectory/#extensions-and-variations","title":"Extensions and Variations","text":"<p>This example can be extended in various ways:</p> <ol> <li>Compare different samplers: Add HMC or other samplers for comparison</li> <li>Vary step size and noise: Show the impact of different parameters</li> <li>Use more complex energy functions: Create energy functions with more challenging landscapes</li> <li>Add diagnostics visualization: Plot energy evolution and other metrics alongside trajectories</li> </ol>"},{"location":"examples/samplers/langevin_trajectory/#visualization-results","title":"Visualization Results","text":"<p>When running the example, you'll see a visualization of the energy landscape with multiple sampling chains:</p> <p> </p> This visualization shows a multimodal energy landscape (contour lines) with five independent      Langevin dynamics sampling chains (colored trajectories). Each chain starts from a random position      (marked by a circle) and evolves through 200 steps (ending at the stars). The trajectories show how the      chains are attracted to the energy function's local minima. Note how some chains follow the gradient to the      nearest minimum, while others may explore multiple regions of the space."},{"location":"examples/training/training_ebm_gaussian/","title":"Training an EBM on a Gaussian Mixture","text":""},{"location":"examples/training/training_ebm_gaussian/#training-an-ebm-on-a-gaussian-mixture","title":"Training an EBM on a Gaussian Mixture","text":"<p>This tutorial demonstrates how to train an energy-based model (EBM) on a 2D Gaussian mixture distribution using the TorchEBM library. We'll build a simple MLP-based energy function, train it with Contrastive Divergence, and visualize the results.</p> <p>Key Concepts Covered</p> <ul> <li>Building an MLP-based energy function</li> <li>Training with Contrastive Divergence</li> <li>Sampling with Langevin dynamics</li> <li>Visualizing the energy landscape and samples</li> </ul>"},{"location":"examples/training/training_ebm_gaussian/#overview","title":"Overview","text":"<p>Energy-based models provide a flexible framework for modeling complex probability distributions. This tutorial focuses on a simple but illustrative example: learning a 2D Gaussian mixture distribution. This is a good starting point because:</p> <ol> <li>It's easy to visualize in 2D</li> <li>It has multimodal structure that challenges simple models</li> <li>We can generate synthetic training data with known properties</li> </ol>"},{"location":"examples/training/training_ebm_gaussian/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have TorchEBM installed:</p> <pre><code>pip install torchebm\n</code></pre> <p>We'll also use the following libraries:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.losses import ContrastiveDivergence\nfrom torchebm.datasets import GaussianMixtureDataset\n</code></pre>"},{"location":"examples/training/training_ebm_gaussian/#step-1-define-the-energy-function","title":"Step 1: Define the Energy Function","text":"<p>We'll create a simple MLP (Multi-Layer Perceptron) energy function by subclassing <code>BaseEnergyFunction</code>:</p> <pre><code>class MLPEnergy(BaseEnergyFunction):\n    \"\"\"A simple MLP to act as the energy function.\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dim: int = 64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),  # Output a single scalar energy value\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.network(x).squeeze(-1)\n</code></pre> <p>This energy function maps input points to scalar energy values. Lower energy corresponds to higher probability density.</p>"},{"location":"examples/training/training_ebm_gaussian/#step-2-create-the-dataset","title":"Step 2: Create the Dataset","text":"<p>TorchEBM provides built-in datasets for testing and development. Let's use the <code>GaussianMixtureDataset</code>:</p> <pre><code># Hyperparameters\nN_SAMPLES = 500\nINPUT_DIM = 2\nHIDDEN_DIM = 64\nSEED = 42\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create the dataset\ndataset = GaussianMixtureDataset(\n    n_samples=N_SAMPLES,\n    n_components=4,  # Four Gaussian components\n    std=0.1,\n    radius=1.5,\n    device=device,\n    seed=SEED,\n)\n\n# Get the full tensor for visualization purposes\nreal_data_for_plotting = dataset.get_data()\nprint(f\"Data batch_shape: {real_data_for_plotting.shape}\")\n\n# Create DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=256,\n    shuffle=True,\n    drop_last=True,\n)\n</code></pre> <p>We can visualize the generated data to see what our target distribution looks like:</p> <pre><code>plt.figure(figsize=(6, 6))\nplt.scatter(real_data_for_plotting[:, 0].cpu().numpy(), \n            real_data_for_plotting[:, 1].cpu().numpy(), \n            s=10, alpha=0.5)\nplt.title(\"Target Distribution: 2D Gaussian Mixture\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p></p> <p>The target distribution consists of 4 Gaussian components arranged in a circle.</p>"},{"location":"examples/training/training_ebm_gaussian/#step-3-define-the-visualization-function","title":"Step 3: Define the Visualization Function","text":"<p>We'll create a function to visualize the energy landscape and generated samples during training:</p> <pre><code>@torch.no_grad()\ndef plot_energy_and_samples(\n        energy_fn: BaseEnergyFunction,\n        real_samples: torch.Tensor,\n        sampler: LangevinDynamics,\n        epoch: int,\n        device: torch.device,\n        grid_size: int = 100,\n        plot_range: float = 3.0,\n        k_sampling: int = 100,\n):\n    \"\"\"Plots the energy surface, real data, and model samples.\"\"\"\n    plt.figure(figsize=(8, 8))\n\n    # Create grid for energy surface plot\n    x_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)\n    y_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)\n    xv, yv = torch.meshgrid(x_coords, y_coords, indexing=\"xy\")\n    grid = torch.stack([xv.flatten(), yv.flatten()], dim=1)\n\n    # Calculate energy on the grid\n    energy_fn.eval()\n    energy_values = energy_fn(grid).cpu().numpy().reshape(grid_size, grid_size)\n\n    # Plot energy surface (using probability density for better visualization)\n    log_prob_values = -energy_values\n    log_prob_values = log_prob_values - np.max(log_prob_values)\n    prob_density = np.exp(log_prob_values)\n\n    plt.contourf(\n        xv.cpu().numpy(),\n        yv.cpu().numpy(),\n        prob_density,\n        levels=50,\n        cmap=\"viridis\",\n    )\n    plt.colorbar(label=\"exp(-Energy) (unnormalized density)\")\n\n    # Generate samples from the current model\n    vis_start_noise = torch.randn(\n        500, real_samples.shape[1], device=device\n    )\n    model_samples_tensor = sampler.sample(x=vis_start_noise, n_steps=k_sampling)\n    model_samples = model_samples_tensor.cpu().numpy()\n\n    # Plot real and model samples\n    real_samples_np = real_samples.cpu().numpy()\n    plt.scatter(\n        real_samples_np[:, 0],\n        real_samples_np[:, 1],\n        s=10,\n        alpha=0.5,\n        label=\"Real Data\",\n        c=\"white\",\n        edgecolors=\"k\",\n        linewidths=0.5,\n    )\n    plt.scatter(\n        model_samples[:, 0],\n        model_samples[:, 1],\n        s=10,\n        alpha=0.5,\n        label=\"Model Samples\",\n        c=\"red\",\n        edgecolors=\"darkred\",\n        linewidths=0.5,\n    )\n\n    plt.xlim(-plot_range, plot_range)\n    plt.ylim(-plot_range, plot_range)\n    plt.title(f\"Epoch {epoch}\")\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n</code></pre> <p>Visualizing Analytical Energy Functions</p> <p>For more detailed information on analytical energy-function visualizations and techniques, see our Energy Visualization Guide. You can find visualized 2D toy datasets in Datasets examples.</p>"},{"location":"examples/training/training_ebm_gaussian/#step-4-set-up-the-training-components","title":"Step 4: Set Up the Training Components","text":"<p>Now we'll set up the model, sampler, loss function, and optimizer:</p> <pre><code># Hyperparameters for training\nBATCH_SIZE = 256\nEPOCHS = 200\nLEARNING_RATE = 1e-3\nSAMPLER_STEP_SIZE = 0.1\nSAMPLER_NOISE_SCALE = 0.1\nCD_K = 10  # MCMC steps for CD\nUSE_PCD = False  # Whether to use Persistent CD\nVISUALIZE_EVERY = 20\n\n# Create the energy model\nenergy_model = MLPEnergy(INPUT_DIM, HIDDEN_DIM).to(device)\n\n# Set up the Langevin dynamics sampler\nsampler = LangevinDynamics(\n    energy_function=energy_model,\n    step_size=SAMPLER_STEP_SIZE,\n    noise_scale=SAMPLER_NOISE_SCALE,\n    device=device,\n)\n\n# Set up the Contrastive Divergence loss\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_model, \n    sampler=sampler, \n    n_steps=CD_K, \n    persistent=USE_PCD\n).to(device)\n\n# Optimizer\noptimizer = optim.Adam(energy_model.parameters(), lr=LEARNING_RATE)\n</code></pre> <p>Langevin Dynamics</p> <p>Langevin dynamics is a sampling method that uses gradient information to explore the energy landscape. It adds noise to the gradient updates, allowing the sampler to overcome energy barriers and explore multimodal distributions.</p> <p>Persistent Contrastive Divergence</p> <p>Setting <code>persistent=True</code> enables Persistent Contrastive Divergence (PCD), which maintains a set of persistent chains between parameter updates. This can lead to better exploration of the energy landscape and improved training stability, especially for complex distributions.</p>"},{"location":"examples/training/training_ebm_gaussian/#step-5-the-training-loop","title":"Step 5: The Training Loop","text":"<p>Now we're ready to train our energy-based model:</p> <pre><code>print(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    energy_model.train()  # Ensure model is in training mode\n    epoch_loss = 0.0\n    for i, data_batch in enumerate(dataloader):\n        # Zero gradients before calculation\n        optimizer.zero_grad()\n\n        # Calculate Contrastive Divergence loss\n        loss, negative_samples = loss_fn(data_batch)\n\n        # Backpropagate the loss\n        loss.backward()\n\n        # Optional: Gradient clipping can help stabilize training\n        torch.nn.utils.clip_grad_norm_(energy_model.parameters(), max_norm=1.0)\n\n        # Update the energy function parameters\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_epoch_loss = epoch_loss / len(dataloader)\n    print(f\"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_epoch_loss:.4f}\")\n\n    # Visualize progress periodically\n    if (epoch + 1) % VISUALIZE_EVERY == 0 or epoch == 0:\n        print(\"Generating visualization...\")\n        energy_model.eval()  # Set model to evaluation mode for visualization\n        plot_energy_and_samples(\n            energy_fn=energy_model,\n            real_samples=real_data_for_plotting,\n            sampler=sampler,\n            epoch=epoch + 1,\n            device=device,\n            plot_range=2.5,\n            k_sampling=200,  # More steps for better visualization\n        )\n</code></pre>"},{"location":"examples/training/training_ebm_gaussian/#training-progress-visualization","title":"Training Progress Visualization","text":"<ul> <li> <p> Epoch 10</p> <p></p> <p>Early training: The model has begun to identify the four modes.</p> </li> <li> <p> Epoch 20</p> <p></p> <p>Continued progress: The energy landscape is taking shape.</p> </li> <li> <p> Epoch 30</p> <p></p> <p>Mid-training: The modes are becoming well-defined.</p> </li> <li> <p> Epoch 100</p> <p></p> <p>Final model: The energy function has learned the four-mode structure.</p> </li> </ul> <p>As training progresses, we can see how the energy landscape evolves to capture the four-mode structure of our target distribution. The brighter regions in the contour plot represent areas of higher probability density (lower energy), and the red points show samples drawn from the model.</p>"},{"location":"examples/training/training_ebm_gaussian/#step-6-final-evaluation","title":"Step 6: Final Evaluation","text":"<p>After training, we generate a final set of samples from our model for evaluation:</p> <pre><code># Final visualization\nprint(\"Generating final visualization...\")\nenergy_model.eval()\nplot_energy_and_samples(\n    energy_fn=energy_model,\n    real_samples=real_data_for_plotting,\n    sampler=sampler,\n    epoch=EPOCHS,\n    device=device,\n    plot_range=2.5,\n    k_sampling=500,  # More steps for better mixing\n)\n</code></pre>"},{"location":"examples/training/training_ebm_gaussian/#understanding-the-results","title":"Understanding the Results","text":"<p>Our model has successfully learned the four-mode structure of the target distribution. The contour plot shows four distinct regions of low energy (high probability) corresponding to the four Gaussian components.</p> <p>The red points (samples from our model) closely match the distribution of the white points (real data), indicating that our energy-based model has effectively captured the target distribution.</p> <p>This example demonstrates the core workflow for training energy-based models with TorchEBM:</p> <ol> <li>Define an energy function</li> <li>Set up a sampler for generating negative samples</li> <li>Use Contrastive Divergence for training</li> <li>Monitor progress through visualization</li> </ol> <p></p> <p>The final energy landscape shows that the model has successfully learned the four-mode structure of the target distribution.</p>"},{"location":"examples/training/training_ebm_gaussian/#tips-for-training-ebms","title":"Tips for Training EBMs","text":"<p>When training your own energy-based models, consider these tips:</p> <ul> <li> <p> Sampling Parameters</p> <p>The step size and noise scale of the Langevin dynamics sampler are critical. Too large values can lead to unstable sampling, while too small values may result in poor mixing.</p> <pre><code># Try different step sizes and noise scales\nsampler = LangevinDynamics(\n    energy_function=energy_model,\n    step_size=0.01,  # Smaller for stability\n    noise_scale=0.005  # Smaller for more accurate sampling\n)\n</code></pre> </li> <li> <p> CD Steps</p> <p>The number of MCMC steps in Contrastive Divergence affects the quality of negative samples. More steps generally lead to better results but increase computation time.</p> <pre><code># For complex distributions, use more steps\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_model,\n    sampler=sampler,\n    n_steps=20,  # Increase for better samples\n    persistent=True\n)\n</code></pre> </li> <li> <p> Learning Rate</p> <p>Energy-based models can be sensitive to the learning rate. Start with a smaller learning rate and gradually increase if needed.</p> <pre><code># Use a smaller learning rate for stability\noptimizer = optim.Adam(\n    energy_model.parameters(),\n    lr=0.0005,  # Start small\n    weight_decay=1e-5  # Regularization helps\n)\n</code></pre> </li> <li> <p> Neural Network Architecture</p> <p>The choice of architecture and activation function can affect the smoothness of the energy landscape.</p> <pre><code># Try different architectures and activations\nself.network = nn.Sequential(\n    nn.Linear(input_dim, hidden_dim),\n    nn.SiLU(),  # Smoother than ReLU\n    nn.Linear(hidden_dim, hidden_dim * 2),  # Wider middle layer\n    nn.SiLU(),\n    nn.Linear(hidden_dim * 2, hidden_dim),\n    nn.SiLU(),\n    nn.Linear(hidden_dim, 1)\n)\n</code></pre> </li> </ul>"},{"location":"examples/training/training_ebm_gaussian/#complete-code","title":"Complete Code","text":"<p>Here's the complete code for this example:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.losses import ContrastiveDivergence\nfrom torchebm.datasets import GaussianMixtureDataset\n\n\nclass MLPEnergy(BaseEnergyFunction):\n    \"\"\"A simple MLP to act as the energy function.\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dim: int = 64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),  # Output a single scalar energy value\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.network(x).squeeze(-1)\n\n\n@torch.no_grad()\ndef plot_energy_and_samples(\n        energy_fn, real_samples, sampler, epoch, device, grid_size=100, plot_range=3.0, k_sampling=100\n):\n    \"\"\"Plots the energy surface, real data, and model samples.\"\"\"\n    plt.figure(figsize=(8, 8))\n\n    # Create grid for energy surface plot\n    x_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)\n    y_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)\n    xv, yv = torch.meshgrid(x_coords, y_coords, indexing=\"xy\")\n    grid = torch.stack([xv.flatten(), yv.flatten()], dim=1)\n\n    # Calculate energy on the grid\n    energy_fn.eval()\n    energy_values = energy_fn(grid).cpu().numpy().reshape(grid_size, grid_size)\n\n    # Plot energy surface (using probability density for better visualization)\n    log_prob_values = -energy_values\n    log_prob_values = log_prob_values - np.max(log_prob_values)\n    prob_density = np.exp(log_prob_values)\n\n    plt.contourf(\n        xv.cpu().numpy(),\n        yv.cpu().numpy(),\n        prob_density,\n        levels=50,\n        cmap=\"viridis\",\n    )\n    plt.colorbar(label=\"exp(-Energy) (unnormalized density)\")\n\n    # Generate samples from the current model\n    vis_start_noise = torch.randn(500, real_samples.shape[1], device=device)\n    model_samples_tensor = sampler.sample(x=vis_start_noise, n_steps=k_sampling)\n    model_samples = model_samples_tensor.cpu().numpy()\n\n    # Plot real and model samples\n    real_samples_np = real_samples.cpu().numpy()\n    plt.scatter(\n        real_samples_np[:, 0],\n        real_samples_np[:, 1],\n        s=10,\n        alpha=0.5,\n        label=\"Real Data\",\n        c=\"white\",\n        edgecolors=\"k\",\n        linewidths=0.5,\n    )\n    plt.scatter(\n        model_samples[:, 0],\n        model_samples[:, 1],\n        s=10,\n        alpha=0.5,\n        label=\"Model Samples\",\n        c=\"red\",\n        edgecolors=\"darkred\",\n        linewidths=0.5,\n    )\n\n    plt.xlim(-plot_range, plot_range)\n    plt.ylim(-plot_range, plot_range)\n    plt.title(f\"Epoch {epoch}\")\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(f\"energy_landscape_epoch_{epoch}.png\")\n    plt.show()\n\n\ndef main():\n    # Hyperparameters\n    N_SAMPLES = 500\n    INPUT_DIM = 2\n    HIDDEN_DIM = 64\n    BATCH_SIZE = 256\n    EPOCHS = 200\n    LEARNING_RATE = 1e-3\n    SAMPLER_STEP_SIZE = 0.1\n    SAMPLER_NOISE_SCALE = 0.1\n    CD_K = 10\n    USE_PCD = False\n    VISUALIZE_EVERY = 20\n    SEED = 42\n\n    # Device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Data Loading\n    dataset = GaussianMixtureDataset(\n        n_samples=N_SAMPLES,\n        n_components=4,\n        std=0.1,\n        radius=1.5,\n        device=device,\n        seed=SEED,\n    )\n\n    # Get the full tensor for visualization purposes\n    real_data_for_plotting = dataset.get_data()\n    print(f\"Data batch_shape: {real_data_for_plotting.shape}\")\n\n    # Create DataLoader\n    dataloader = DataLoader(\n        dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=True,\n    )\n\n    # Model Components\n    energy_model = MLPEnergy(INPUT_DIM, HIDDEN_DIM).to(device)\n    sampler = LangevinDynamics(\n        energy_function=energy_model,\n        step_size=SAMPLER_STEP_SIZE,\n        noise_scale=SAMPLER_NOISE_SCALE,\n        device=device,\n    )\n    loss_fn = ContrastiveDivergence(\n        energy_function=energy_model, sampler=sampler, k_steps=CD_K, persistent=USE_PCD\n    ).to(device)\n\n    # Optimizer\n    optimizer = optim.Adam(energy_model.parameters(), lr=LEARNING_RATE)\n\n    print(\"Starting training...\")\n    for epoch in range(EPOCHS):\n        energy_model.train()\n        epoch_loss = 0.0\n        for i, data_batch in enumerate(dataloader):\n            optimizer.zero_grad()\n            loss, negative_samples = loss_fn(data_batch)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(energy_model.parameters(), max_norm=1.0)\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_epoch_loss = epoch_loss / len(dataloader)\n        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Average Loss: {avg_epoch_loss:.4f}\")\n\n        if (epoch + 1) % VISUALIZE_EVERY == 0 or epoch == 0:\n            print(\"Generating visualization...\")\n            energy_model.eval()\n            plot_energy_and_samples(\n                energy_fn=energy_model,\n                real_samples=real_data_for_plotting,\n                sampler=sampler,\n                epoch=epoch + 1,\n                device=device,\n                plot_range=2.5,\n                k_sampling=200,\n            )\n\n    print(\"Training finished.\")\n\n    # Final visualization\n    print(\"Generating final visualization...\")\n    energy_model.eval()\n    plot_energy_and_samples(\n        energy_fn=energy_model,\n        real_samples=real_data_for_plotting,\n        sampler=sampler,\n        epoch=EPOCHS,\n        device=device,\n        plot_range=2.5,\n        k_sampling=500,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/training/training_ebm_gaussian/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we've learned how to:</p> <ol> <li>Define a simple energy-based model using an MLP</li> <li>Generate synthetic data from a 2D Gaussian mixture using TorchEBM's dataset utilities</li> <li>Train the model using Contrastive Divergence and Langevin dynamics</li> <li>Visualize the energy landscape and generated samples throughout training</li> </ol> <p>Energy-based models provide a powerful and flexible framework for modeling complex probability distributions. While we've focused on a simple 2D example, the same principles apply to more complex, high-dimensional distributions.</p>"},{"location":"examples/visualization/","title":"Visualizatio Examples","text":""},{"location":"examples/visualization/#visualizatio-examples","title":"Visualizatio Examples","text":"<p>This section demonstrates various Visualizatio Examples and techniques available for visualizing energy functions and sampling processes.</p>"},{"location":"examples/visualization/#basic-visualizations","title":"Basic Visualizations","text":""},{"location":"examples/visualization/#contour-plots","title":"Contour Plots","text":"<p>The <code>contour_plots.py</code> example demonstrates basic contour plots for energy functions:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create the energy function\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Create a grid for visualization\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Create contour plot\nplt.figure(figsize=(10, 8))\ncontour = plt.contourf(X, Y, Z, 50, cmap=\"viridis\")\nplt.colorbar(label=\"Energy\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Double Well Energy Landscape\")\n</code></pre>"},{"location":"examples/visualization/#distribution-comparison","title":"Distribution Comparison","text":"<p>The <code>distribution_comparison.py</code> example compares sampled distributions to their ground truth:</p> <pre><code># Create figure with multiple plots\nfig = plt.figure(figsize=(15, 5))\n\n# Ground truth contour\nax1 = fig.add_subplot(131)\ncontour = ax1.contourf(X, Y, Z, 50, cmap=\"Blues\")\nfig.colorbar(contour, ax=ax1, label=\"Density\")\nax1.set_title(\"Ground Truth Density\")\n\n# Sample density (using kernel density estimation)\nax2 = fig.add_subplot(132)\nh = ax2.hist2d(samples_np[:, 0], samples_np[:, 1], bins=50, cmap=\"Reds\", density=True)\nfig.colorbar(h[3], ax=ax2, label=\"Density\")\nax2.set_title(\"Sampled Distribution\")\n\n# Scatter plot of samples\nax3 = fig.add_subplot(133)\nax3.scatter(samples_np[:, 0], samples_np[:, 1], alpha=0.5, s=3)\nax3.set_title(\"Sample Points\")\n</code></pre>"},{"location":"examples/visualization/#advanced-visualizations","title":"Advanced Visualizations","text":""},{"location":"examples/visualization/#trajectory-animation","title":"Trajectory Animation","text":"<p>The <code>trajectory_animation.py</code> example visualizes sampling trajectories on energy landscapes:</p> <pre><code># Extract trajectory coordinates\ntraj_x = trajectory[0, :, 0].numpy()\ntraj_y = trajectory[0, :, 1].numpy()\n\n# Plot trajectory with colormap based on step number\nplt.figure(figsize=(10, 8))\ncontour = plt.contourf(X, Y, Z, 50, cmap=\"viridis\", alpha=0.7)  # Energy landscape\npoints = plt.scatter(\n    traj_x, traj_y, c=np.arange(len(traj_x)), cmap=\"plasma\", s=5, alpha=0.7\n)\nplt.colorbar(points, label=\"Sampling Step\")\n\n# Plot arrows to show direction of trajectory\nstep = 50  # Plot an arrow every 50 steps\nplt.quiver(\n    traj_x[:-1:step],\n    traj_y[:-1:step],\n    traj_x[1::step] - traj_x[:-1:step],\n    traj_y[1::step] - traj_y[:-1:step],\n    scale_units=\"xy\",\n    angles=\"xy\",\n    scale=1,\n    color=\"red\",\n    alpha=0.7,\n)\n</code></pre>"},{"location":"examples/visualization/#parallel-chains","title":"Parallel Chains","text":"<p>The <code>parallel_chains.py</code> example visualizes multiple sampling chains:</p> <pre><code># Plot contour\nplt.figure(figsize=(12, 10))\ncontour = plt.contourf(X, Y, Z, 50, cmap=\"viridis\", alpha=0.7)\nplt.colorbar(label=\"Energy\")\n\n# Plot each trajectory with a different color\ncolors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\"]\nfor i in range(num_chains):\n    traj_x = trajectories[i, :, 0].numpy()\n    traj_y = trajectories[i, :, 1].numpy()\n\n    plt.plot(traj_x, traj_y, alpha=0.7, linewidth=1, c=colors[i], label=f\"Chain {i+1}\")\n\n    # Mark start and end points\n    plt.scatter(traj_x[0], traj_y[0], c=\"black\", s=50, marker=\"o\")\n    plt.scatter(traj_x[-1], traj_y[-1], c=colors[i], s=100, marker=\"*\")\n</code></pre>"},{"location":"examples/visualization/#energy-over-time","title":"Energy Over Time","text":"<p>The <code>energy_over_time.py</code> example tracks energy values during sampling:</p> <pre><code># Track the trajectory and energy manually\ntrajectory = torch.zeros((1, n_steps, dim))\nenergy_values = torch.zeros(n_steps)\ncurrent_sample = initial_point.clone()\n\n# Run the sampling steps and store each position and energy\nfor i in range(n_steps):\n    current_sample = sampler.langevin_step(\n        current_sample, torch.randn_like(current_sample)\n    )\n    trajectory[:, i, :] = current_sample.clone().detach()\n    energy_values[i] = energy_fn(current_sample).item()\n\n# Plot energy evolution\nplt.figure(figsize=(10, 6))\nplt.plot(energy_values.numpy())\nplt.xlabel(\"Step\")\nplt.ylabel(\"Energy\")\nplt.title(\"Energy Evolution During Sampling\")\nplt.grid(True, alpha=0.3)\n</code></pre>"},{"location":"examples/visualization/#running-the-examples","title":"Running the Examples","text":"<p>To run these examples:</p> <pre><code># List available visualization examples\npython examples/main.py --list\n\n# Run basic visualization examples\npython examples/main.py visualization/basic/contour_plots\npython examples/main.py visualization/basic/distribution_comparison\n\n# Run advanced visualization examples\npython examples/main.py visualization/advanced/trajectory_animation\npython examples/main.py visualization/advanced/parallel_chains\npython examples/main.py visualization/advanced/energy_over_time\n</code></pre>"},{"location":"examples/visualization/#additional-resources","title":"Additional Resources","text":"<p>For more information on visualization tools, see:</p> <ul> <li>Matplotlib Documentation</li> </ul>"},{"location":"examples/visualization/energy_visualization/","title":"Energy Landscape Visualization","text":""},{"location":"examples/visualization/energy_visualization/#energy-landscape-visualization","title":"Energy Landscape Visualization","text":"<p>This example demonstrates how to visualize various energy function landscapes in TorchEBM.</p> <p>Key Concepts Covered</p> <ul> <li>Visualizing energy functions in 2D</li> <li>Comparing different energy landscapes</li> <li>Using matplotlib for 3D visualization</li> </ul>"},{"location":"examples/visualization/energy_visualization/#overview","title":"Overview","text":"<p>Energy-based models rely on energy functions that define landscapes over the sample space. Visualizing these landscapes helps us understand the behavior of different energy functions and the challenges in sampling from them.</p> <p>This example shows how to plot various built-in energy functions from TorchEBM: - Rosenbrock - Ackley - Rastrigin - Double Well - Gaussian - Harmonic</p>"},{"location":"examples/visualization/energy_visualization/#code-example","title":"Code Example","text":"<pre><code>import numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom torchebm.core.base_energy_function import (\n    RosenbrockEnergy, AckleyEnergy, RastriginEnergy,\n    DoubleWellEnergy, GaussianEnergy, HarmonicEnergy\n)\n\n\ndef plot_energy_function(energy_fn, x_range, y_range, title):\n    x = np.linspace(x_range[0], x_range[1], 100)\n    y = np.linspace(y_range[0], y_range[1], 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.zeros_like(X)\n\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n            Z[i, j] = energy_fn(point).item()\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X, Y, Z, cmap='viridis')\n    ax.set_title(title)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('Energy')\n    plt.show()\n\n\nenergy_functions = [\n    (RosenbrockEnergy(), [-2, 2], [-1, 3], 'Rosenbrock Energy Function'),\n    (AckleyEnergy(), [-5, 5], [-5, 5], 'Ackley Energy Function'),\n    (RastriginEnergy(), [-5, 5], [-5, 5], 'Rastrigin Energy Function'),\n    (DoubleWellEnergy(), [-2, 2], [-2, 2], 'Double Well Energy Function'),\n    (GaussianEnergy(torch.tensor([0.0, 0.0]),\n                    torch.tensor([[1.0, 0.0], [0.0, 1.0]])),\n     [-3, 3], [-3, 3], 'Gaussian Energy Function'),\n    (HarmonicEnergy(), [-3, 3], [-3, 3], 'Harmonic Energy Function')\n]\n\n# Plot each energy function\nfor energy_fn, x_range, y_range, title in energy_functions:\n    plot_energy_function(energy_fn, x_range, y_range, title)\n</code></pre>"},{"location":"examples/visualization/energy_visualization/#energy-function-characteristics","title":"Energy Function Characteristics","text":""},{"location":"examples/visualization/energy_visualization/#rosenbrock-energy","title":"Rosenbrock Energy","text":"<p>The Rosenbrock function creates a long, narrow, banana-shaped valley. The global minimum is inside the valley at (1,1), but finding it is challenging because the valley is very flat and the gradient provides little guidance.</p> <p>Mathematical Definition: \\(E(x) = \\sum_{i=1}^{n-1} \\left[ a(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \\right]\\)</p>"},{"location":"examples/visualization/energy_visualization/#ackley-energy","title":"Ackley Energy","text":"<p>The Ackley function is characterized by a nearly flat outer region and a large hole at the center. It has many local minima but only one global minimum at (0,0).</p> <p>Mathematical Definition: \\(E(x) = -a \\exp\\left(-b\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_i^2}\\right)\\) \\(- \\exp\\left(\\frac{1}{d}\\sum_{i=1}^{d}\\cos(c x_i)\\right) + a + \\exp(1)\\)</p>"},{"location":"examples/visualization/energy_visualization/#rastrigin-energy","title":"Rastrigin Energy","text":"<p>The Rastrigin function has many regularly distributed local minima, making it highly multimodal. Its surface looks like an \"egg carton.\" The global minimum is at (0,0).</p> <p>Mathematical Definition: \\(E(x) = An + \\sum_{i=1}^n \\left[ x_i^2 - A\\cos(2\\pi x_i) \\right]\\)</p>"},{"location":"examples/visualization/energy_visualization/#double-well-energy","title":"Double Well Energy","text":"<p>The Double Well function features two distinct minima (wells) separated by an energy barrier. It's a classic example of a bimodal distribution and is often used to test sampling algorithms' ability to traverse energy barriers.</p> <p>Mathematical Definition: \\(E(x) = a(x^2 - b)^2\\)</p>"},{"location":"examples/visualization/energy_visualization/#gaussian-and-harmonic-energy","title":"Gaussian and Harmonic Energy","text":"<p>The Gaussian energy function represents a simple quadratic energy landscape with a single minimum. It corresponds to a multivariate Gaussian distribution in the probability space.</p> <p>Mathematical Definition:</p> <p>\\(E(x) = \\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\)</p>"},{"location":"examples/visualization/energy_visualization/#harmonic-energy","title":"Harmonic Energy","text":"<p>The Harmonic energy function represents a simple quadratic potential (like a spring). It has a single global minimum at the origin and is convex everywhere. </p> <p>Mathematical Definition:</p> <p>\\(E(x) = \\frac{1}{2}\\sum_{i=1}^{d}x_i^2\\)</p>"},{"location":"examples/visualization/energy_visualization/#visualization-results","title":"Visualization Results","text":"<p>Below are the visualizations of the energy functions described above. Each visualization shows both a 3D surface plot and a 2D contour plot to help understand the landscape structure.</p>"},{"location":"examples/visualization/energy_visualization/#rosenbrock-energy_1","title":"Rosenbrock Energy","text":"<p> The Rosenbrock function creates a long, narrow, banana-shaped valley. Finding the global minimum at (1,1) is challenging because the valley is very flat and provides little gradient information.</p>"},{"location":"examples/visualization/energy_visualization/#ackley-energy_1","title":"Ackley Energy","text":"<p>The Ackley function has a nearly flat outer region and a large hole at the center. It contains many local minima but only one global minimum at (0,0).</p>"},{"location":"examples/visualization/energy_visualization/#rastrigin-energy_1","title":"Rastrigin Energy","text":"<p>The Rastrigin function's \"egg carton\" surface has many regularly distributed local minima, making it highly multimodal. The global minimum is at (0,0).</p>"},{"location":"examples/visualization/energy_visualization/#double-well-energy_1","title":"Double Well Energy","text":"<p>The Double Well energy features two distinct minima (wells) separated by an energy barrier, making it ideal for testing sampling algorithms' ability to traverse energy barriers.</p>"},{"location":"examples/visualization/energy_visualization/#gaussian-energy","title":"Gaussian Energy","text":"<p>The Gaussian energy function has a simple quadratic landscape with a single minimum, corresponding to a multivariate Gaussian distribution.</p> <p>Note</p> <p>Visualizing these energy functions helps understand why some distributions are more challenging to sample from than others. For instance, the narrow valleys in the Rosenbrock function or the many local minima in the Rastrigin function make it difficult for sampling algorithms to efficiently explore the full distribution.</p>"},{"location":"examples/visualization/energy_visualization/#extensions","title":"Extensions","text":"<p>You can extend this example to:</p> <ol> <li>Create custom energy functions by implementing the <code>BaseEnergyFunction</code> interface</li> <li>Visualize energy functions in higher dimensions using projection techniques</li> <li>Animate sampling trajectories on top of these energy landscapes</li> </ol> <p>For more advanced energy landscape visualizations, including contour plots and comparing sampling algorithms, see the Langevin Sampler Trajectory example. </p>"},{"location":"guides/","title":"Guides","text":""},{"location":"guides/#getting-started-with-torchebm","title":"Getting Started with TorchEBM","text":"<p>Welcome to the TorchEBM guides section! These comprehensive guides will help you understand how to use TorchEBM effectively for your energy-based modeling tasks.</p>"},{"location":"guides/#core-concepts","title":"Core Concepts","text":"<ul> <li> <p> Energy Functions</p> <p>Learn about the foundation of energy-based models and how to work with energy functions in TorchEBM.</p> <p> Energy Functions Guide</p> </li> <li> <p> Samplers</p> <p>Discover how to generate samples from energy landscapes using various sampling algorithms.</p> <p> Samplers Guide</p> </li> <li> <p> Loss Functions</p> <p>Explore different loss functions for training energy-based models.</p> <p> BaseLoss Functions Guide</p> </li> <li> <p> Custom Neural Networks</p> <p>Learn how to create and use neural networks as energy functions.</p> <p> Custom Neural Networks Guide</p> </li> <li> <p> Training EBMs</p> <p>Master the techniques for effectively training energy-based models.</p> <p> Training Guide</p> </li> <li> <p> Visualization</p> <p>Visualize energy landscapes and sampling results to gain insights.</p> <p> Visualization Guide</p> </li> </ul>"},{"location":"guides/#quick-start","title":"Quick Start","text":"<p>If you're new to energy-based models, we recommend the following learning path:</p> <ol> <li>Start with the Introduction to understand basic concepts</li> <li>Follow the Installation guide to set up TorchEBM</li> <li>Read the Energy Functions guide to understand the fundamentals</li> <li>Explore the Samplers guide to learn how to generate samples</li> <li>Study the Training guide to learn how to train your models</li> </ol>"},{"location":"guides/#basic-example","title":"Basic Example","text":"<p>Here's a simple example to get you started with TorchEBM:</p> <pre><code>import torch\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Create an energy function (2D Gaussian)\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n\n# Create a sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Generate samples\nsamples = sampler.sample(\n    dim=2, n_steps=100, n_samples=1000\n)\n\n# Print sample statistics\nprint(f\"Sample mean: {samples.mean(0)}\")\nprint(f\"Sample std: {samples.std(0)}\")\n</code></pre>"},{"location":"guides/#common-patterns","title":"Common Patterns","text":"<p>Here are some common patterns you'll encounter throughout the guides:</p>"},{"location":"guides/#energy-function-definition","title":"Energy Function Definition","text":"<pre><code>from torchebm.core import BaseEnergyFunction\nimport torch\n\n\nclass MyEnergyFunction(BaseEnergyFunction):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return torch.sum(x ** 2, dim=-1)\n</code></pre>"},{"location":"guides/#sampler-usage","title":"Sampler Usage","text":"<pre><code>from torchebm.samplers.langevin_dynamics import LangevinDynamics\n\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\nsamples = sampler.sample(\n    dim=2, n_steps=100, n_samples=1000\n)\n</code></pre>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<p>Once you're familiar with the basics, you can:</p> <ul> <li>Explore detailed Examples that demonstrate TorchEBM in action</li> <li>Check the API Reference for comprehensive documentation</li> <li>Learn how to contribute to TorchEBM in the Developer Guide</li> </ul> <p>Remember that all examples in these guides are tested with the latest version of TorchEBM, and you can run them in your own environment to gain hands-on experience. </p>"},{"location":"guides/custom_neural_networks/","title":"Custom Neural Networks","text":""},{"location":"guides/custom_neural_networks/#custom-neural-network-energy-functions","title":"Custom Neural Network Energy Functions","text":"<p>Energy-based models (EBMs) are highly flexible, and one of their key advantages is that the energy function can be parameterized using neural networks. This guide explains how to create and use neural network-based energy functions in TorchEBM.</p>"},{"location":"guides/custom_neural_networks/#overview","title":"Overview","text":"<p>Neural networks provide a powerful way to represent complex energy landscapes that can't be easily defined analytically. By using neural networks as energy functions:</p> <ul> <li>You can capture complex, high-dimensional distributions</li> <li>The energy function can be learned from data</li> <li>You gain the expressivity of modern deep learning architectures</li> </ul>"},{"location":"guides/custom_neural_networks/#basic-neural-network-energy-function","title":"Basic Neural Network Energy Function","text":"<p>To create a neural network-based energy function in TorchEBM, you need to subclass the <code>BaseEnergyFunction</code> base class and implement the <code>forward</code> method:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import BaseEnergyFunction\n\n\nclass NeuralNetEnergyFunction(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=128):\n        super().__init__()\n\n        # Define neural network architecture\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x has batch_shape (batch_size, input_dim)\n        # Output should have batch_shape (batch_size,)\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"guides/custom_neural_networks/#design-considerations","title":"Design Considerations","text":"<p>When designing neural network energy functions, consider the following:</p>"},{"location":"guides/custom_neural_networks/#network-architecture","title":"Network Architecture","text":"<p>The choice of architecture depends on the data type and complexity:</p> <ul> <li>MLPs: Good for generic, low-dimensional data</li> <li>CNNs: Effective for images and data with spatial structure</li> <li>Transformers: Useful for sequential data or when attention mechanisms are beneficial</li> <li>Graph Neural Networks: For data with graph structure</li> </ul>"},{"location":"guides/custom_neural_networks/#output-requirements","title":"Output Requirements","text":"<p>Remember the following key points:</p> <ol> <li>The energy function should output a scalar value for each sample in the batch</li> <li>Lower energy values should correspond to higher probability density</li> <li>The neural network must be differentiable for gradient-based sampling methods to work</li> </ol>"},{"location":"guides/custom_neural_networks/#scale-and-normalization","title":"Scale and Normalization","text":"<p>Energy values should be properly scaled to avoid numerical issues:</p> <ul> <li>Very large energy values can cause instability in sampling</li> <li>Energy functions that grow too quickly may cause sampling algorithms to fail</li> </ul>"},{"location":"guides/custom_neural_networks/#example-mlp-energy-function-for-2d-data","title":"Example: MLP Energy Function for 2D Data","text":"<p>Here's a complete example with a simple MLP energy function:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import (\n    BaseEnergyFunction,\n    CosineScheduler,\n)\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.losses import ContrastiveDivergence\nfrom torchebm.datasets import GaussianMixtureDataset\nfrom torch.utils.data import DataLoader\n\n# Set random seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\n\n\n# Create a simple MLP energy function\nclass MLPEnergyFunction(BaseEnergyFunction):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1),\n        )\n\n    def forward(self, x):\n        return self.model(x).squeeze(-1)\n\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create the dataset\ndataset = GaussianMixtureDataset(\n    n_samples=1000,\n    n_components=5,  # 5 Gaussian components\n    std=0.1,  # Standard deviation\n    radius=1.5,  # Radius of the mixture\n    device=device,\n    seed=SEED,\n)\n\n# Create dataloader\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n\n# Create model\nmodel = MLPEnergyFunction(input_dim=2, hidden_dim=64).to(device)\nSAMPLER_NOISE_SCALE = CosineScheduler(\n    initial_value=2e-1, final_value=1e-2, total_steps=50\n)\n\n# Create sampler\nsampler = LangevinDynamics(\n    energy_function=model,\n    step_size=0.01,\n    device=device,\n    noise_scale=SAMPLER_NOISE_SCALE,\n)\n\n# Create loss function\nloss_fn = ContrastiveDivergence(\n    energy_function=model,\n    sampler=sampler,\n    k_steps=10,  # Number of MCMC steps\n    persistent=False,  # Set to True for Persistent Contrastive Divergence\n    device=device,\n)\n\n# Create optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Training loop\nn_epochs = 200\nfor epoch in range(n_epochs):\n    epoch_loss = 0.0\n\n    for batch in dataloader:\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Compute loss (automatically handles positive and negative samples)\n        loss, neg_samples = loss_fn(batch)\n\n        # Backpropagation\n        loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    # Print progress every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")\n\n\n# Generate samples from the trained model\ndef generate_samples(model, n_samples=500):\n    # Create sampler\n    sampler = LangevinDynamics(energy_function=model, step_size=0.005, device=device)\n\n    # Initialize from random noise\n    initial_samples = torch.randn(n_samples, 2).to(device)\n\n    # Sample using MCMC\n    with torch.no_grad():\n        samples = sampler.sample(\n            initial_state=initial_samples,\n            dim=initial_samples.shape[-1],\n            n_samples=n_samples,\n            n_steps=1000,\n        )\n\n    return samples.cpu()\n\n\n# Generate samples\nsamples = generate_samples(model)\nprint(f\"Generated {len(samples)} samples from the energy-based model\")\n</code></pre>"},{"location":"guides/custom_neural_networks/#example-convolutional-energy-function-for-images","title":"Example: Convolutional Energy Function for Images","text":"<p>For image data, convolutional architectures are more appropriate:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import BaseEnergyFunction\n\n\nclass ConvolutionalEnergyFunction(BaseEnergyFunction):\n    def __init__(self, channels=1, width=28, height=28):\n        super().__init__()\n\n        # Convolutional part\n        self.conv_net = nn.Sequential(\n            nn.Conv2d(channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.SELU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 14x14\n            nn.SELU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.SELU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),  # 7x7\n            nn.SELU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.SELU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),  # 4x4\n            nn.SELU(),\n        )\n\n        # Calculate the size of the flattened features\n        feature_size = 128 * (width // 8) * (height // 8)\n\n        # Final energy output\n        self.energy_head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(feature_size, 128),\n            nn.SELU(),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x):\n        # Ensure x is batched and has correct channel dimension\n        if x.ndim == 3:  # Single image with channels\n            x = x.unsqueeze(0)\n        elif x.ndim == 2:  # Single grayscale image\n            x = x.unsqueeze(0).unsqueeze(0)\n\n        # Extract features and compute energy\n        features = self.conv_net(x)\n        energy = self.energy_head(features).squeeze(-1)\n\n        return energy\n</code></pre>"},{"location":"guides/custom_neural_networks/#advanced-pattern-composed-energy-functions","title":"Advanced Pattern: Composed Energy Functions","text":"<p>You can combine multiple analytical energy functions with multiple neural networks for best of both worlds:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import BaseEnergyFunction, GaussianEnergy\n\n\nclass CompositionalEnergyFunction(BaseEnergyFunction):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super().__init__()\n\n        # Analytical component: Gaussian energy\n        self.analytical_component = GaussianEnergy(\n            mean=torch.zeros(input_dim),\n            cov=torch.eye(input_dim)\n        )\n\n        # Neural network component\n        self.neural_component = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Weight for combining components\n        self.alpha = nn.Parameter(torch.tensor(0.5))\n\n    def forward(self, x):\n        # Analytical energy\n        analytical_energy = self.analytical_component(x)\n\n        # Neural network energy\n        neural_energy = self.neural_component(x).squeeze(-1)\n\n        # Combine using learned weight\n        # Use sigmoid to keep alpha between 0 and 1\n        alpha = torch.sigmoid(self.alpha)\n        combined_energy = alpha * analytical_energy + (1 - alpha) * neural_energy\n\n        return combined_energy\n</code></pre>"},{"location":"guides/custom_neural_networks/#training-strategies","title":"Training Strategies","text":"<p>Training neural network energy functions requires special techniques:</p>"},{"location":"guides/custom_neural_networks/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>A common approach is contrastive divergence, which minimizes the energy of data samples while maximizing the energy of samples from the model:</p> <pre><code>loss_fn = ContrastiveDivergence(\n    energy_function=model,\n    sampler=sampler,\n    k_steps=10,  # Number of MCMC steps\n    persistent=False,  # Set to True for Persistent Contrastive Divergence\n    device=device,\n)\n\ndef train_step_contrastive_divergence(data_batch):\n    # Zero gradients\n    optimizer.zero_grad()\n\n    # Compute loss (automatically handles positive and negative samples)\n    loss, neg_samples = loss_fn(data_batch)\n\n    # Backpropagation\n    loss.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"guides/custom_neural_networks/#score-matching","title":"Score Matching","text":"<p>Score matching is another approach that avoids the need for MCMC sampling:</p> <pre><code># Use score matching for training\nsm_loss_fn = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",  # More efficient for higher dimensions\n    hutchinson_samples=5,\n    device=device,\n)\n\nbatch_loss = train_step_contrastive_divergence(data_batch)\n</code></pre>"},{"location":"guides/custom_neural_networks/#tips-for-neural-network-energy-functions","title":"Tips for Neural Network Energy Functions","text":"<ol> <li>Start Simple: Begin with a simple architecture and gradually increase complexity</li> <li>Regularization: Use weight decay or spectral normalization to prevent extreme energy values</li> <li>Gradient Clipping: Apply gradient clipping during training to prevent instability</li> <li>Initialization: Careful initialization of weights can help convergence</li> <li>Monitoring: Track energy values during training to ensure they stay in a reasonable range</li> <li>Batch Normalization: Use with caution as it can affect the shape of the energy landscape</li> <li>Residual Connections: Can help with gradient flow in deeper networks</li> </ol>"},{"location":"guides/custom_neural_networks/#conclusion","title":"Conclusion","text":"<p>Neural network energy functions provide a powerful way to model complex distributions in energy-based models. By leveraging the flexibility of deep learning architectures, you can create expressive energy functions that capture intricate patterns in your data.</p> <p>Remember to carefully design your architecture, choose appropriate training methods, and monitor the behavior of your energy function during training and sampling. </p>"},{"location":"guides/energy_functions/","title":"Energy Functions","text":""},{"location":"guides/energy_functions/#energy-functions","title":"Energy Functions","text":"<p>Energy functions are the core component of Energy-Based Models. In TorchEBM, energy functions define the probability distribution from which we sample and learn.</p>"},{"location":"guides/energy_functions/#built-in-energy-functions","title":"Built-in Energy Functions","text":"<p>TorchEBM provides several built-in energy functions for common use cases:</p>"},{"location":"guides/energy_functions/#gaussian-energy","title":"Gaussian Energy","text":"<p>The multivariate Gaussian energy function defines a normal distribution:</p> <pre><code>from torchebm.core import GaussianEnergy\nimport torch\n\n# Standard Gaussian\ngaussian = GaussianEnergy(\n    mean=torch.zeros(2),\n    cov=torch.eye(2)\n)\n\n# Custom mean and covariance\ncustom_mean = torch.tensor([1.0, -1.0])\ncustom_cov = torch.tensor([[1.0, 0.5], [0.5, 2.0]])\ncustom_gaussian = GaussianEnergy(\n    mean=custom_mean,\n    cov=custom_cov\n)\n</code></pre>"},{"location":"guides/energy_functions/#double-well-energy","title":"Double Well Energy","text":"<p>The double well potential has two local minima separated by a barrier:</p> <pre><code>from torchebm.core import DoubleWellEnergy\n\n# Default barrier height = 2.0\ndouble_well = DoubleWellEnergy()\n\n# Custom barrier height\ncustom_double_well = DoubleWellEnergy(barrier_height=5.0)\n</code></pre>"},{"location":"guides/energy_functions/#rosenbrock-energy","title":"Rosenbrock Energy","text":"<p>The Rosenbrock function has a narrow, curved valley with a global minimum:</p> <pre><code>from torchebm.core import RosenbrockEnergy\n\n# Default parameters a=1.0, b=100.0\nrosenbrock = RosenbrockEnergy()\n\n# Custom parameters\ncustom_rosenbrock = RosenbrockEnergy(a=2.0, b=50.0)\n</code></pre>"},{"location":"guides/energy_functions/#rastrigin-energy","title":"Rastrigin Energy","text":"<p>The Rastrigin function has many local minima arranged in a regular pattern:</p> <pre><code>from torchebm.core import RastriginEnergy\n\nrastrigin = RastriginEnergy()\n</code></pre>"},{"location":"guides/energy_functions/#ackley-energy","title":"Ackley Energy","text":"<p>The Ackley function has many local minima with a single global minimum:</p> <pre><code>from torchebm.core import AckleyEnergy\n\nackley = AckleyEnergy()\n</code></pre>"},{"location":"guides/energy_functions/#using-energy-functions","title":"Using Energy Functions","text":"<p>Energy functions in TorchEBM implement these key methods:</p>"},{"location":"guides/energy_functions/#energy-calculation","title":"Energy Calculation","text":"<p>Calculate the energy of a batch of samples:</p> <pre><code># x batch_shape: [batch_size, dimension]\nenergy_values = energy_function(x)  # returns [batch_size]\n</code></pre>"},{"location":"guides/energy_functions/#gradient-calculation","title":"Gradient Calculation","text":"<p>Calculate the gradient of the energy with respect to the input:</p> <pre><code># Requires grad enabled\nx.requires_grad_(True)\nenergy_values = energy_function(x)\n\n\n# Calculate gradients\ngradients = torch.autograd.grad(\n    energy_values.sum(), x, create_graph=True\n)[0]  # batch_shape: [batch_size, dimension]\n</code></pre>"},{"location":"guides/energy_functions/#device-management","title":"Device Management","text":"<p>Energy functions can be moved between devices:</p> <pre><code># Move to GPU\nenergy_function = energy_function.to(\"cuda\")\n\n# Move to CPU\nenergy_function = energy_function.to(\"cpu\")\n</code></pre>"},{"location":"guides/energy_functions/#creating-custom-energy-functions","title":"Creating Custom Energy Functions","text":"<p>You can create custom energy functions by subclassing the <code>BaseEnergyFunction</code> base class:</p> <pre><code>from torchebm.core import BaseEnergyFunction\nimport torch\n\n\nclass MyCustomEnergy(BaseEnergyFunction):\n    def __init__(self, param1, param2):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def forward(self, x):\n        # Implement your energy function here\n        # x batch_shape: [batch_size, dimension]\n        # Return batch_shape: [batch_size]\n        return torch.sum(self.param1 * x ** 2 + self.param2 * torch.sin(x), dim=-1)\n</code></pre>"},{"location":"guides/energy_functions/#neural-network-energy-functions","title":"Neural Network Energy Functions","text":"<p>For more complex energy functions, you can use neural networks:</p> <pre><code>import torch.nn as nn\nfrom torchebm.core import BaseEnergyFunction\n\n\nclass NeuralNetworkEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x batch_shape: [batch_size, input_dim]\n        return self.network(x).squeeze(-1)  # Return batch_shape: [batch_size]\n</code></pre>"},{"location":"guides/energy_functions/#best-practices","title":"Best Practices","text":"<ol> <li>Numerical Stability: Avoid energy functions that can produce NaN or Inf values</li> <li>Scaling: Keep energy values within a reasonable range to avoid numerical issues</li> <li>Conditioning: Well-conditioned energy functions are easier to sample from</li> <li>Gradients: Ensure your energy function has well-behaved gradients</li> <li>Batching: Implement energy functions to efficiently handle batched inputs </li> </ol>"},{"location":"guides/getting_started/","title":"Getting Started","text":""},{"location":"guides/getting_started/#getting-started","title":"Getting Started","text":"<p>This guide will help you get started with TorchEBM by walking you through the installation process and demonstrating some basic usage examples.</p>"},{"location":"guides/getting_started/#installation","title":"Installation","text":"<p>TorchEBM can be installed directly from PyPI:</p> <pre><code>pip install torchebm\n</code></pre>"},{"location":"guides/getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or newer</li> <li>PyTorch 1.10.0 or newer</li> <li>CUDA (optional, but recommended for performance)</li> </ul>"},{"location":"guides/getting_started/#installation-from-source","title":"Installation from Source","text":"<p>If you wish to install the development version:</p> <pre><code>git clone https://github.com/soran-ghaderi/torchebm.git\ncd torchebm\npip install -e .\n</code></pre>"},{"location":"guides/getting_started/#quick-start","title":"Quick Start","text":"<p>Here's a simple example to get you started with TorchEBM:</p> <pre><code>import torch\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers.langevin_dynamics import LangevinDynamics\n\n# Set device for computation\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Define a 2D Gaussian energy function for visualization\nenergy_fn = GaussianEnergy(\n    mean=torch.zeros(2, device=device),\n    cov=torch.eye(2, device=device)\n)\n\n# Initialize Langevin dynamics sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    device=device\n).to(device)\n\n# Generate 1000 samples\nsamples = sampler.sample(\n    dim=2,\n    n_steps=100,\n    n_samples=1000,\n    return_trajectory=False\n)\n\nprint(f\"Generated {samples.shape[0]} samples of dimension {samples.shape[1]}\")\n</code></pre>"},{"location":"guides/getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Energy Functions available in TorchEBM</li> <li>Explore different Sampling Algorithms</li> <li>Try out the Examples for visualizations and advanced usage</li> <li>Check the API Reference for detailed documentation</li> </ul>"},{"location":"guides/getting_started/#common-issues","title":"Common Issues","text":""},{"location":"guides/getting_started/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>If you encounter CUDA out of memory errors, try: - Reducing the number of samples - Reducing the dimension of the problem - Switching to CPU if needed</p>"},{"location":"guides/getting_started/#support","title":"Support","text":"<p>If you encounter any issues or have questions: - Check the FAQ - Open an issue on GitHub</p>"},{"location":"guides/introduction/","title":"Introduction","text":""},{"location":"guides/introduction/#introduction-to-torchebm","title":"Introduction to TorchEBM","text":"<p>TorchEBM is a CUDA-accelerated library for Energy-Based Models (EBMs) built on PyTorch. It provides efficient implementations of sampling, inference, and learning algorithms for EBMs, with a focus on scalability and performance.</p>"},{"location":"guides/introduction/#what-are-energy-based-models","title":"What are Energy-Based Models?","text":"<p>Energy-Based Models (EBMs) are a class of machine learning models that define a probability distribution through an energy function. The energy function assigns a scalar energy value to each configuration of the variables of interest, with lower energy values indicating more probable configurations.</p> <p>The probability density of a configuration x is proportional to the negative exponential of its energy:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>where:</p> <ul> <li>\\(E(x)\\) is the energy function</li> <li>\\(Z = \\int e^{-E(x)} dx\\) is the normalizing constant (partition function)</li> </ul> <p>EBMs are powerful because they can model complex dependencies between variables and capture multimodal distributions. They are applicable to a wide range of tasks including generative modeling, density estimation, and representation learning.</p>"},{"location":"guides/introduction/#why-torchebm","title":"Why TorchEBM?","text":"<p>While Energy-Based Models are powerful, they present several challenges:</p> <ul> <li>The partition function Z is often intractable to compute directly</li> <li>Sampling from EBMs requires advanced Markov Chain Monte Carlo (MCMC) methods</li> <li>Training can be computationally intensive</li> </ul> <p>TorchEBM addresses these challenges by providing:</p> <ul> <li>Efficient samplers: CUDA-accelerated implementations of MCMC samplers like Langevin Dynamics and Hamiltonian Monte Carlo</li> <li>Training methods: Implementations of contrastive divergence and other specialized loss functions</li> <li>Integration with PyTorch: Seamless compatibility with the PyTorch ecosystem</li> </ul>"},{"location":"guides/introduction/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/introduction/#energy-functions","title":"Energy Functions","text":"<p>Energy functions are the core component of EBMs. TorchEBM provides implementations of common energy functions like Gaussian, Double Well, and Rosenbrock, as well as a base class for creating custom energy functions.</p>"},{"location":"guides/introduction/#samplers","title":"Samplers","text":"<p>Sampling from EBMs typically involves MCMC methods. TorchEBM implements several sampling algorithms:</p> <ul> <li>Langevin Dynamics: Updates samples using gradient information plus noise</li> <li>Hamiltonian Monte Carlo: Uses Hamiltonian dynamics for efficient exploration</li> <li>Other samplers: Various specialized samplers for different applications</li> </ul>"},{"location":"guides/introduction/#baseloss-functions","title":"BaseLoss Functions","text":"<p>Training EBMs requires specialized methods to estimate and minimize the difference between the model distribution and the data distribution. TorchEBM implements several loss functions including contrastive divergence and score matching techniques.</p>"},{"location":"guides/introduction/#applications","title":"Applications","text":"<p>Energy-Based Models and TorchEBM can be applied to various tasks:</p> <ul> <li>Generative modeling</li> <li>Density estimation</li> <li>Unsupervised representation learning</li> <li>Out-of-distribution detection</li> <li>Structured prediction</li> </ul>"},{"location":"guides/introduction/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Getting Started guide to install TorchEBM and run your first examples</li> <li>Check the Guides for more detailed information on specific components</li> <li>Explore the Examples for practical applications </li> </ul>"},{"location":"guides/loss_functions/","title":"Loss Functions","text":""},{"location":"guides/loss_functions/#loss-functions","title":"Loss Functions","text":"<p>Training energy-based models involves estimating and minimizing the difference between the model distribution and the data distribution. TorchEBM provides various loss functions to accomplish this.</p>"},{"location":"guides/loss_functions/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>Contrastive Divergence (CD) is one of the most popular methods for training energy-based models. It uses MCMC sampling to generate negative examples from the current model.</p>"},{"location":"guides/loss_functions/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses import ContrastiveDivergence\nfrom torchebm.samplers import LangevinDynamics\n\n# Define a custom energy function\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Create energy model, sampler, and loss function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = MLPEnergy(input_dim=2, hidden_dim=64).to(device)\n\n# Set up sampler for negative samples\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.1,\n    device=device\n)\n\n# Create Contrastive Divergence loss\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_fn,\n    sampler=sampler,\n    k_steps=10,  # Number of MCMC steps\n    persistent=False,  # Standard CD (non-persistent)\n)\n\n# Define optimizer\noptimizer = optim.Adam(energy_fn.parameters(), lr=0.001)\n\n# During training:\ndata_batch = torch.randn(128, 2).to(device)  # Your real data batch\noptimizer.zero_grad()\nloss, negative_samples = loss_fn(data_batch)\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"guides/loss_functions/#advanced-options","title":"Advanced Options","text":"<p>The <code>ContrastiveDivergence</code> loss function in TorchEBM supports several advanced options:</p>"},{"location":"guides/loss_functions/#persistent-contrastive-divergence-pcd","title":"Persistent Contrastive Divergence (PCD)","text":"<p>PCD maintains a buffer of negative samples across training iterations, which can lead to better mixing:</p> <pre><code># Create Persistent Contrastive Divergence loss\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_fn,\n    sampler=sampler,\n    k_steps=10,\n    persistent=True,  # Enable PCD\n    buffer_size=1024,  # Size of the persistent buffer\n    buffer_init='rand'  # How to initialize the buffer ('rand' or 'data')\n)\n</code></pre>"},{"location":"guides/loss_functions/#using-schedulers-for-sampling-parameters","title":"Using Schedulers for Sampling Parameters","text":"<p>You can use schedulers to dynamically adjust the sampler's step size or noise scale during training:</p> <pre><code>from torchebm.core import CosineScheduler, ExponentialDecayScheduler, LinearScheduler\n\n# Define schedulers for step size and noise scale\nstep_size_scheduler = CosineScheduler(\n    start_value=3e-2,\n    end_value=5e-3,\n    n_steps=100\n)\n\nnoise_scheduler = CosineScheduler(\n    start_value=3e-1,\n    end_value=1e-2,\n    n_steps=100\n)\n\n# Create sampler with schedulers\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=step_size_scheduler,\n    noise_scale=noise_scheduler,\n    device=device\n)\n\n# Create CD loss with this sampler\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_fn,\n    sampler=sampler,\n    k_steps=10,\n    persistent=True\n)\n</code></pre>"},{"location":"guides/loss_functions/#score-matching","title":"Score Matching","text":"<p>Score Matching is another approach for training EBMs that avoids the need for MCMC sampling. It directly optimizes the score function (gradient of log-density).</p>"},{"location":"guides/loss_functions/#basic-usage_1","title":"Basic Usage","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses import ScoreMatching\nfrom torchebm.datasets import GaussianMixtureDataset\n\n# Define a custom energy function\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n\n    def forward(self, x):\n        return self.net(x).squeeze(-1)  # a scalar value\n\n# Setup model and device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = MLPEnergy(input_dim=2).to(device)\n\n# Create score matching loss\nsm_loss_fn = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",  # More efficient for higher dimensions\n    hutchinson_samples=5,\n    device=device\n)\n\n# Setup optimizer\noptimizer = optim.Adam(energy_fn.parameters(), lr=0.001)\n\n# Setup data\ndataset = GaussianMixtureDataset(\n    n_samples=500, n_components=4, std=0.1, seed=123\n).get_data()\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training loop\nfor epoch in range(10):\n    epoch_loss = 0.0\n    for batch_data in dataloader:\n        batch_data = batch_data.to(device)\n\n        optimizer.zero_grad()\n        loss = sm_loss_fn(batch_data)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_loss = epoch_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/10, Loss: {avg_loss:.6f}\")\n</code></pre>"},{"location":"guides/loss_functions/#variants-of-score-matching","title":"Variants of Score Matching","text":"<p>TorchEBM supports different variants of score matching:</p>"},{"location":"guides/loss_functions/#explicit-score-matching","title":"Explicit Score Matching","text":"<p>This is the standard form of score matching, which requires computing the Hessian of the energy function:</p> <pre><code>sm_loss_fn = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"exact\",  # Explicitly compute Hessian (slow for high dimensions)\n    device=device\n)\n</code></pre>"},{"location":"guides/loss_functions/#hutchinsons-trick","title":"Hutchinson's Trick","text":"<p>To make score matching more efficient, we can use Hutchinson's trick to estimate the trace of the Hessian:</p> <pre><code>sm_loss_fn = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",  # Use Hutchinson's trick\n    hutchinson_samples=5,  # Number of noise samples to use\n    device=device\n)\n</code></pre>"},{"location":"guides/loss_functions/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Denoising score matching adds noise to data points and tries to learn the score of the noised distribution:</p> <pre><code>from torchebm.losses import DenoisingScoreMatching\n\ndsm_loss_fn = DenoisingScoreMatching(\n    energy_function=energy_fn,\n    sigma=0.1,  # Noise level\n    device=device\n)\n\n# During training:\noptimizer.zero_grad()\nloss = dsm_loss_fn(data_batch)\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"guides/loss_functions/#noise-contrastive-estimation-nce","title":"Noise Contrastive Estimation (NCE)","text":"<p>NCE is another alternative for training EBMs that uses a noise distribution to avoid computing the partition function:</p> <pre><code>from torchebm.losses import NoiseContrastiveEstimation\nimport torch.distributions as D\n\n# Define a noise distribution\nnoise_dist = D.Normal(0, 1)\n\n# Create NCE loss\nnce_loss_fn = NoiseContrastiveEstimation(\n    energy_function=energy_fn,\n    noise_distribution=noise_dist,\n    noise_samples_per_data=10,\n    device=device\n)\n\n# During training:\noptimizer.zero_grad()\nloss = nce_loss_fn(data_batch)\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"guides/loss_functions/#complete-training-example-with-loss-function","title":"Complete Training Example with Loss Function","text":"<p>Here's a complete example showing how to train an EBM using Contrastive Divergence loss:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.losses import ContrastiveDivergence\nfrom torchebm.datasets import TwoMoonsDataset\n\n# Define energy function\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nINPUT_DIM = 2\nHIDDEN_DIM = 16\nBATCH_SIZE = 256\nEPOCHS = 100\nLEARNING_RATE = 1e-3\nCD_K = 10  # MCMC steps for Contrastive Divergence\nUSE_PCD = True  # Use Persistent Contrastive Divergence\n\n# Setup data\ndataset = TwoMoonsDataset(n_samples=3000, noise=0.05, seed=42, device=device)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n\n# Create model, sampler, and loss function\nenergy_model = MLPEnergy(INPUT_DIM, HIDDEN_DIM).to(device)\nsampler = LangevinDynamics(\n    energy_function=energy_model,\n    step_size=0.1,\n    device=device,\n)\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_model,\n    sampler=sampler,\n    k_steps=CD_K,\n    persistent=USE_PCD,\n    buffer_size=BATCH_SIZE,\n).to(device)\n\n# Optimizer\noptimizer = optim.Adam(energy_model.parameters(), lr=LEARNING_RATE)\n\n# Training loop\nlosses = []\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    energy_model.train()\n    epoch_loss = 0.0\n\n    for i, data_batch in enumerate(dataloader):\n        optimizer.zero_grad()\n\n        # Calculate Contrastive Divergence loss\n        loss, negative_samples = loss_fn(data_batch)\n\n        # Backpropagate and optimize\n        loss.backward()\n\n        # Optional: Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(energy_model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_epoch_loss = epoch_loss / len(dataloader)\n    losses.append(avg_epoch_loss)\n    print(f\"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_epoch_loss:.4f}\")\n\n# Plot the training loss\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('docs/assets/images/loss_functions/cd_training_loss.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/loss_functions/#choosing-the-right-loss-function","title":"Choosing the Right Loss Function","text":"<p>Different loss functions are suitable for different scenarios:</p> <ul> <li>Contrastive Divergence: Good general-purpose method, especially with complex energy landscapes</li> <li>Persistent CD: Better mixing properties than standard CD, but requires more memory</li> <li>Score Matching: Avoids sampling but can be numerically unstable in high dimensions</li> <li>Denoising Score Matching: More stable than standard score matching, good for high dimensions</li> <li>NCE: Works well with complex distributions where sampling is difficult</li> </ul>"},{"location":"guides/loss_functions/#tips-for-stable-training","title":"Tips for Stable Training","text":"<ol> <li>Regularization: Add L2 regularization to prevent the energy from collapsing</li> <li>Gradient Clipping: Use <code>torch.nn.utils.clip_grad_norm_</code> to prevent unstable updates</li> <li>Learning Rate: Use a small learning rate, especially at the beginning</li> <li>Sampling Steps: Increase the number of sampling steps k for better negative samples</li> <li>Batch Size: Use larger batch sizes for more stable gradient estimates </li> <li>Parameter Schedulers: Use schedulers for sampler parameters to improve mixing</li> <li>Monitor Energy Values: Ensure the energy values don't collapse to very large negative values </li> </ol>"},{"location":"guides/parallel_sampling/","title":"Parallel Sampling","text":""},{"location":"guides/parallel_sampling/#parallel-sampling","title":"Parallel Sampling","text":"<p>This guide explains how to efficiently sample from energy functions in parallel using TorchEBM.</p>"},{"location":"guides/parallel_sampling/#overview","title":"Overview","text":"<p>Parallel sampling allows you to generate multiple samples simultaneously, leveraging modern hardware like GPUs for significant speedups. TorchEBM is designed for efficient parallel sampling, making it easy to generate thousands or even millions of samples with minimal code.</p>"},{"location":"guides/parallel_sampling/#basic-parallel-sampling","title":"Basic Parallel Sampling","text":"<p>The simplest way to perform parallel sampling is to initialize multiple chains and let TorchEBM handle the parallelization:</p> <pre><code>import torch\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\nimport torch.nn as nn\n\n# Define a simple energy function\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Create the energy function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = MLPEnergy(input_dim=2, hidden_dim=32).to(device)\n\n# Create the sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.1,\n    noise_scale=0.01,\n    device=device\n)\n\n# Generate 10,000 samples in parallel (each with dimension 2)\nn_samples = 10000\ndim = 2\ninitial_points = torch.randn(n_samples, dim, device=device)\n\n# All samples are processed in parallel\nsamples = sampler.sample(\n    x=initial_points,\n    n_steps=1000,\n    return_trajectory=False\n)\n\nprint(f\"Generated {samples.shape[0]} samples of dimension {samples.shape[1]}\")\n</code></pre>"},{"location":"guides/parallel_sampling/#gpu-acceleration","title":"GPU Acceleration","text":"<p>For maximum performance, TorchEBM leverages GPU acceleration when available. This provides dramatic speedups for parallel sampling:</p> <pre><code>import time\nimport torch\nfrom torchebm.core import DoubleWellEnergy\nfrom torchebm.samplers import LangevinDynamics\n\n# Create energy function\nenergy_fn = DoubleWellEnergy()\n\n# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = energy_fn.to(device)\n\n# Create sampler\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    device=device\n)\n\n# Number of samples to generate\nn_samples = 50000\ndim = 2\n\n# Initialize starting points\ninitial_points = torch.randn(n_samples, dim, device=device)\n\n# Time the sampling process\nstart_time = time.time()\nsamples = sampler.sample(\n    x=initial_points,\n    n_steps=1000,\n    return_trajectory=False\n)\nend_time = time.time()\n\nprint(f\"Generated {n_samples} samples in {end_time - start_time:.2f} seconds\")\nprint(f\"Average time per sample: {(end_time - start_time) / n_samples * 1000:.4f} ms\")\n</code></pre>"},{"location":"guides/parallel_sampling/#batch-processing-for-large-sample-sets","title":"Batch Processing for Large Sample Sets","text":"<p>When generating a very large number of samples, you might need to process them in batches to avoid memory issues:</p> <pre><code>import torch\nimport numpy as np\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\n\n# Create energy function and sampler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenergy_fn = MLPEnergy(input_dim=2, hidden_dim=64).to(device)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    device=device\n)\n\n# Total number of samples to generate\ntotal_samples = 1000000\ndim = 2\nbatch_size = 10000\nnum_batches = total_samples // batch_size\n\n# Initialize array to store all samples\nall_samples = np.zeros((total_samples, dim))\n\n# Generate samples in batches\nfor i in range(num_batches):\n    print(f\"Generating batch {i+1}/{num_batches}\")\n\n    # Initialize random starting points for this batch\n    initial_points = torch.randn(batch_size, dim, device=device)\n\n    # Sample\n    batch_samples = sampler.sample(\n        x=initial_points,\n        n_steps=1000,\n        return_trajectory=False\n    )\n\n    # Store samples\n    start_idx = i * batch_size\n    end_idx = (i + 1) * batch_size\n    all_samples[start_idx:end_idx] = batch_samples.cpu().numpy()\n\nprint(f\"Generated {total_samples} samples in total\")\n</code></pre>"},{"location":"guides/parallel_sampling/#multi-gpu-sampling","title":"Multi-GPU Sampling","text":"<p>For even larger-scale sampling, you can distribute the workload across multiple GPUs:</p> <pre><code>import torch\nimport torch.multiprocessing as mp\nfrom torchebm.core import DoubleWellEnergy\nfrom torchebm.samplers import LangevinDynamics\n\ndef sample_on_device(rank, n_samples, n_steps, result_queue):\n    # Set device based on process rank\n    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n\n    # Create energy function and sampler for this device\n    energy_fn = DoubleWellEnergy().to(device)\n    sampler = LangevinDynamics(\n        energy_function=energy_fn,\n        step_size=0.01,\n        device=device\n    )\n\n    # Generate samples\n    initial_points = torch.randn(n_samples, 2, device=device)\n    samples = sampler.sample(\n        x=initial_points,\n        n_steps=n_steps,\n        return_trajectory=False\n    )\n\n    # Move samples to CPU and put in queue\n    result_queue.put(samples.cpu())\n\ndef main():\n    # Number of GPUs to use\n    n_gpus = torch.cuda.device_count()\n    if n_gpus == 0:\n        print(\"No GPUs available, using CPU\")\n        n_gpus = 1\n\n    print(f\"Using {n_gpus} device(s) for sampling\")\n\n    # Total samples and steps\n    total_samples = 100000\n    samples_per_device = total_samples // n_gpus\n    n_steps = 1000\n\n    # Create a queue to get results\n    result_queue = mp.Queue()\n\n    # Start processes\n    processes = []\n    for rank in range(n_gpus):\n        p = mp.Process(\n            target=sample_on_device,\n            args=(rank, samples_per_device, n_steps, result_queue)\n        )\n        p.start()\n        processes.append(p)\n\n    # Collect results\n    all_samples = []\n    for _ in range(n_gpus):\n        all_samples.append(result_queue.get())\n\n    # Wait for all processes to finish\n    for p in processes:\n        p.join()\n\n    # Combine all samples\n    all_samples = torch.cat(all_samples, dim=0)\n    print(f\"Generated {all_samples.shape[0]} samples\")\n\nif __name__ == \"__main__\":\n    mp.set_start_method('spawn')\n    main()\n</code></pre>"},{"location":"guides/parallel_sampling/#performance-tips-for-parallel-sampling","title":"Performance Tips for Parallel Sampling","text":"<ol> <li> <p>Use the correct device: Always specify the device when creating samplers to ensure proper hardware acceleration.</p> </li> <li> <p>Batch size tuning: Find the optimal batch size for your hardware. Too small wastes parallelism, too large may cause memory issues.</p> </li> <li> <p>Data type optimization: Consider using <code>torch.float16</code> (half precision) for even faster sampling on compatible GPUs:</p> </li> </ol> <pre><code># Use half precision for faster sampling\ninitial_points = torch.randn(10000, 2, device=device, dtype=torch.float16)\nenergy_fn = energy_fn.half()  # Convert model to half precision\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01,\n    device=device\n)\nsamples = sampler.sample(x=initial_points, n_steps=1000)\n</code></pre> <ol> <li> <p>Minimize data transfers: Keep data on the GPU as much as possible. CPU-GPU transfers are slow.</p> </li> <li> <p>Pre-allocate memory: For repetitive sampling, reuse the same tensor to avoid repeated allocations.</p> </li> </ol>"},{"location":"guides/parallel_sampling/#conclusion","title":"Conclusion","text":"<p>Parallel sampling in TorchEBM allows you to efficiently generate large numbers of samples from your energy-based models. By leveraging GPU acceleration and batch processing, you can significantly speed up sampling, enabling more efficient model evaluation and complex applications.</p> <p>Whether you're generating samples for visualization, evaluation, or downstream tasks, TorchEBM's parallel sampling capabilities provide the performance and scalability you need. </p>"},{"location":"guides/samplers/","title":"Samplers","text":""},{"location":"guides/samplers/#sampling-algorithms","title":"Sampling Algorithms","text":"<p>Sampling from energy-based models is a core task in TorchEBM. This guide explains the different sampling algorithms available and how to use them effectively.</p>"},{"location":"guides/samplers/#overview-of-sampling","title":"Overview of Sampling","text":"<p>In energy-based models, we need to sample from the probability distribution defined by the energy function:</p> \\[p(x) = \\frac{e^{-E(x)}}{Z}\\] <p>Since the normalizing constant Z is typically intractable, we use Markov Chain Monte Carlo (MCMC) methods to generate samples without needing to compute Z.</p>"},{"location":"guides/samplers/#langevin-dynamics","title":"Langevin Dynamics","text":"<p>Langevin Dynamics is a gradient-based MCMC method that updates samples using the energy gradient plus Gaussian noise. It's one of the most commonly used samplers in energy-based models due to its simplicity and effectiveness.</p>"},{"location":"guides/samplers/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\nimport torch.nn as nn\n\n# Define a custom energy function\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Create an energy function\nenergy_fn = MLPEnergy(input_dim=2, hidden_dim=32)\n\n# Create a Langevin dynamics sampler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlangevin_sampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.1,\n    noise_scale=0.01,\n    device=device\n)\n\n# Generate samples\ninitial_points = torch.randn(100, 2, device=device)  # 100 samples of dimension 2\nsamples = langevin_sampler.sample(\n    x=initial_points,\n    n_steps=1000,\n    return_trajectory=False\n)\n\nprint(samples.shape)  # Shape: [100, 2]\n</code></pre>"},{"location":"guides/samplers/#parameters","title":"Parameters","text":"<ul> <li><code>energy_function</code>: The energy function to sample from</li> <li><code>step_size</code>: Step size for gradient updates (controls exploration vs. stability)</li> <li><code>noise_scale</code>: Scale of the noise (default is sqrt(2*step_size))</li> <li><code>device</code>: The device to perform sampling on (e.g., \"cuda\" or \"cpu\")</li> </ul>"},{"location":"guides/samplers/#advanced-features","title":"Advanced Features","text":"<p>The <code>LangevinDynamics</code> sampler in TorchEBM comes with several advanced features:</p>"},{"location":"guides/samplers/#returning-trajectories","title":"Returning Trajectories","text":"<p>For visualization or analysis, you can get the full trajectory of the sampling process:</p> <pre><code>trajectory = langevin_sampler.sample(\n    x=initial_points,\n    n_steps=1000,\n    return_trajectory=True\n)\n\nprint(trajectory.shape)  # Shape: [n_samples, n_steps, dim]\n</code></pre>"},{"location":"guides/samplers/#dynamic-parameter-scheduling","title":"Dynamic Parameter Scheduling","text":"<p>TorchEBM allows you to dynamically adjust the step size and noise scale during sampling using schedulers:</p> <pre><code>from torchebm.core import CosineScheduler, LinearScheduler, ExponentialDecayScheduler\n\n# Create schedulers\nstep_size_scheduler = CosineScheduler(\n    start_value=3e-2,\n    end_value=5e-3,\n    n_steps=100\n)\n\nnoise_scheduler = CosineScheduler(\n    start_value=3e-1,\n    end_value=1e-2,\n    n_steps=100\n)\n\n# Create sampler with schedulers\ndynamic_sampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=step_size_scheduler,\n    noise_scale=noise_scheduler,\n    device=device\n)\n</code></pre>"},{"location":"guides/samplers/#hamiltonian-monte-carlo-hmc","title":"Hamiltonian Monte Carlo (HMC)","text":"<p>HMC uses Hamiltonian dynamics to make more efficient proposals, leading to better exploration of the distribution:</p> <pre><code>from torchebm.samplers import HamiltonianMonteCarlo\nfrom torchebm.core import DoubleWellEnergy\n\n# Create an energy function\nenergy_fn = DoubleWellEnergy()\n\n# Create an HMC sampler\nhmc_sampler = HamiltonianMonteCarlo(\n    energy_function=energy_fn,\n    step_size=0.1,\n    n_leapfrog_steps=10,\n    device=device\n)\n\n# Generate samples\nsamples = hmc_sampler.sample(\n    x=torch.randn(100, 2, device=device),\n    n_steps=500,\n    return_trajectory=False\n)\n</code></pre>"},{"location":"guides/samplers/#integration-with-loss-functions","title":"Integration with Loss Functions","text":"<p>Samplers in TorchEBM are designed to work seamlessly with loss functions for training energy-based models:</p> <pre><code>from torchebm.losses import ContrastiveDivergence\n\n# Create a loss function that uses the sampler internally\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_fn,\n    sampler=langevin_sampler,\n    k_steps=10,\n    persistent=True,\n    buffer_size=1024\n)\n\n# During training, the loss function will use the sampler to generate negative samples\noptimizer.zero_grad()\nloss, negative_samples = loss_fn(data_batch)\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"guides/samplers/#parallel-sampling","title":"Parallel Sampling","text":"<p>TorchEBM supports parallel sampling to speed up the generation of multiple samples:</p> <pre><code># Generate multiple chains in parallel\nn_samples = 1000\ndim = 2\ninitial_points = torch.randn(n_samples, dim, device=device)\n\n# All chains are processed in parallel on the GPU\nsamples = langevin_sampler.sample(\n    x=initial_points,\n    n_steps=1000,\n    return_trajectory=False\n)\n</code></pre>"},{"location":"guides/samplers/#sampler-visualizations","title":"Sampler Visualizations","text":"<p>Visualizing the sampling process can help understand the behavior of your model. Here's an example showing how to visualize Langevin Dynamics trajectories:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchebm.core import DoubleWellEnergy, LinearScheduler, WarmupScheduler\nfrom torchebm.samplers import LangevinDynamics\n\n# Create energy function and sampler\nenergy_fn = DoubleWellEnergy(barrier_height=5.0)\n\n# Define a cosine scheduler for the Langevin dynamics\nscheduler_linear = LinearScheduler(\n    initial_value=0.05,\n    final_value=0.03,\n    total_steps=100\n)\n\nscheduler = WarmupScheduler(\n    main_scheduler=scheduler_linear,\n    warmup_steps=10,\n    warmup_init_factor=0.01\n)\n\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=scheduler\n\n)\n\n# Initial point\ninitial_point = torch.tensor([[-2.0, 0.0]], dtype=torch.float32)\n\n# Run sampling and get trajectory\ntrajectory = sampler.sample(\n    x=initial_point,\n    dim=2,\n    n_steps=1000,\n    return_trajectory=True\n)\n\n# Background energy landscape\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Visualize\nplt.figure(figsize=(10, 8))\nplt.contourf(X, Y, Z, 50, cmap='viridis', alpha=0.7)\nplt.colorbar(label='Energy')\n\n# Extract trajectory coordinates\ntraj_x = trajectory[0, :, 0].numpy()\ntraj_y = trajectory[0, :, 1].numpy()\n\n# Plot trajectory\nplt.plot(traj_x, traj_y, 'r-', linewidth=1, alpha=0.7)\nplt.scatter(traj_x[0], traj_y[0], c='black', s=50, marker='o', label='Start')\nplt.scatter(traj_x[-1], traj_y[-1], c='blue', s=50, marker='*', label='End')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Langevin Dynamics Trajectory')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.savefig('langevin_trajectory.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/samplers/#choosing-a-sampler","title":"Choosing a Sampler","text":"<ul> <li>Langevin Dynamics: Good for general-purpose sampling, especially with neural network energy functions</li> <li>Hamiltonian Monte Carlo: Better exploration of complex energy landscapes, but more computationally expensive</li> <li>Metropolis-Adjusted Langevin Algorithm (MALA): Similar to Langevin Dynamics but with an accept/reject step</li> </ul>"},{"location":"guides/samplers/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use GPU acceleration: Batch processing of samples on GPU can significantly speed up sampling</li> <li>Adjust step size: Too large \u2192 unstable sampling; too small \u2192 slow mixing</li> <li>Dynamic scheduling: Use parameter schedulers to automatically adjust step size and noise during sampling</li> <li>Monitor energy values: Track energy values to ensure proper mixing and convergence 5**Multiple chains**: Run multiple chains from different starting points to better explore the distribution</li> </ol>"},{"location":"guides/samplers/#custom-samplers","title":"Custom Samplers","text":"<p>TorchEBM provides flexible base classes for creating your own custom sampling algorithms. All samplers inherit from the <code>BaseSampler</code> abstract base class which defines the core interfaces and functionalities.</p>"},{"location":"guides/samplers/#creating-a-custom-sampler","title":"Creating a Custom Sampler","text":"<p>To implement a custom sampler, you need to subclass <code>BaseSampler</code> and implement at minimum the <code>sample()</code> method:</p> <pre><code>from torchebm.core import BaseSampler, BaseEnergyFunction\nimport torch\nfrom typing import Optional, Union, Tuple, List, Dict\n\nclass MyCustomSampler(BaseSampler):\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        my_parameter: float = 0.1,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n    ):\n        super().__init__(energy_function=energy_function, dtype=dtype, device=device)\n        self.my_parameter = my_parameter\n\n        # You can register schedulers for parameters that change during sampling\n        self.register_scheduler(\"my_parameter\", ConstantScheduler(my_parameter))\n\n    def custom_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Implement a single step of your sampling algorithm\"\"\"\n        # Get current parameter value (if using schedulers)\n        param_value = self.get_scheduled_value(\"my_parameter\")\n\n        # Compute gradient of the energy function\n        gradient = self.energy_function.gradient(x)\n\n        # Implement your sampling logic\n        noise = torch.randn_like(x)\n        new_x = x - param_value * gradient + noise * 0.01\n\n        return new_x\n\n    @torch.no_grad()\n    def sample(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        \"\"\"Implementation of the abstract sample method\"\"\"\n        # Reset any schedulers to their initial state\n        self.reset_schedulers()\n\n        # Initialize samples if not provided\n        if x is None:\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(self.device)\n\n        # Setup trajectory storage if requested\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        # Setup diagnostics if requested\n        if return_diagnostics:\n            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)\n\n        # Main sampling loop\n        for i in range(n_steps):\n            # Step all schedulers before each MCMC step\n            self.step_schedulers()\n\n            # Apply your custom sampling step\n            x = self.custom_step(x)\n\n            # Record trajectory if requested\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n            # Compute and store diagnostics if requested\n            if return_diagnostics:\n                # Your diagnostic computations here\n                pass\n\n        # Return results based on what was requested\n        if return_trajectory:\n            if return_diagnostics:\n                return trajectory, diagnostics\n            return trajectory\n        if return_diagnostics:\n            return x, diagnostics\n        return x\n\n    def _setup_diagnostics(self, dim: int, n_steps: int, n_samples: int = None) -&gt; torch.Tensor:\n        \"\"\"Optional method to setup diagnostic storage\"\"\"\n        if n_samples is not None:\n            return torch.empty(\n                (n_steps, 3, n_samples, dim), device=self.device, dtype=self.dtype\n            )\n        else:\n            return torch.empty((n_steps, 3, dim), device=self.device, dtype=self.dtype)\n</code></pre>"},{"location":"guides/samplers/#key-components","title":"Key Components","text":"<p>When implementing a custom sampler, consider these key aspects:</p> <ol> <li> <p>Energy Function: All samplers work with an energy function that defines the target distribution.</p> </li> <li> <p>Parameter Scheduling: Use the built-in scheduler system to manage parameters that change during sampling:    </p><pre><code># Register a scheduler in __init__\nself.register_scheduler(\"step_size\", ConstantScheduler(0.01))\n\n# Get current value during sampling\ncurrent_step_size = self.get_scheduled_value(\"step_size\")\n\n# Step all schedulers in each iteration\nself.step_schedulers()\n</code></pre><p></p> </li> <li> <p>Device and Precision Management: The base class handles device placement and precision settings:    </p><pre><code># Move sampler to a specific device\nmy_sampler = my_sampler.to(\"cuda:0\")\n</code></pre><p></p> </li> <li> <p>Diagnostics Collection: Implement <code>_setup_diagnostics()</code> to collect sampling statistics.</p> </li> </ol>"},{"location":"guides/samplers/#example-simplified-langevin-dynamics","title":"Example: Simplified Langevin Dynamics","text":"<p>Here's a simplified example of a Langevin dynamics sampler:</p> <pre><code>class SimpleLangevin(BaseSampler):\n    def __init__(\n        self,\n        energy_function: BaseEnergyFunction,\n        step_size: float = 0.01,\n        noise_scale: float = 1.0,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[Union[str, torch.device]] = None,\n    ):\n        super().__init__(energy_function=energy_function, dtype=dtype, device=device)\n        self.register_scheduler(\"step_size\", ConstantScheduler(step_size))\n        self.register_scheduler(\"noise_scale\", ConstantScheduler(noise_scale))\n\n    def langevin_step(self, x: torch.Tensor) -&gt; torch.Tensor:\n        step_size = self.get_scheduled_value(\"step_size\")\n        noise_scale = self.get_scheduled_value(\"noise_scale\")\n\n        gradient = self.energy_function.gradient(x)\n        noise = torch.randn_like(x)\n\n        new_x = (\n            x \n            - step_size * gradient \n            + torch.sqrt(torch.tensor(2.0 * step_size)) * noise_scale * noise\n        )\n        return new_x\n\n    @torch.no_grad()\n    def sample(\n        self,\n        x: Optional[torch.Tensor] = None,\n        dim: int = 10,\n        n_steps: int = 100,\n        n_samples: int = 1,\n        thin: int = 1,\n        return_trajectory: bool = False,\n        return_diagnostics: bool = False,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:\n        self.reset_schedulers()\n\n        if x is None:\n            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)\n        else:\n            x = x.to(self.device)\n\n        if return_trajectory:\n            trajectory = torch.empty(\n                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device\n            )\n\n        for i in range(n_steps):\n            self.step_schedulers()\n            x = self.langevin_step(x)\n\n            if return_trajectory:\n                trajectory[:, i, :] = x\n\n        if return_trajectory:\n            return trajectory\n        return x\n</code></pre>"},{"location":"guides/samplers/#tips-for-custom-samplers","title":"Tips for Custom Samplers","text":"<ol> <li> <p>Performance: Use <code>@torch.no_grad()</code> for the sampling loop to disable gradient computation.</p> </li> <li> <p>GPU Compatibility: Handle device placement correctly, especially when generating random noise.</p> </li> <li> <p>Validation: Ensure your sampler works with simple distributions before moving to complex ones.</p> </li> <li> <p>Diagnostics: Implement helpful diagnostics to monitor convergence and sampling quality.</p> </li> <li> <p>Mixed Precision: For better performance on modern GPUs, use the built-in mixed precision support.</p> </li> </ol>"},{"location":"guides/training/","title":"Training EBMs","text":""},{"location":"guides/training/#training-energy-based-models","title":"Training Energy-Based Models","text":"<p>This guide covers the fundamental techniques for training energy-based models (EBMs) using TorchEBM. We'll explore various training methods, loss functions, and optimization strategies to help you effectively train your models.</p>"},{"location":"guides/training/#overview","title":"Overview","text":"<p>Training energy-based models involves estimating the parameters of an energy function such that the corresponding probability distribution matches a target data distribution. Unlike in traditional supervised learning, this is often an unsupervised task where the goal is to learn the underlying structure of the data.</p> <p>The training process typically involves:</p> <ol> <li>Defining an energy function (parameterized by a neural network or analytical form)</li> <li>Choosing a training method and loss function</li> <li>Optimizing the energy function parameters</li> <li>Evaluating the model using sampling and visualization techniques</li> </ol>"},{"location":"guides/training/#defining-an-energy-function","title":"Defining an Energy Function","text":"<p>In TorchEBM, you can create custom energy functions by subclassing <code>BaseEnergyFunction</code>:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchebm.core import BaseEnergyFunction\n\nclass MLPEnergy(BaseEnergyFunction):\n    \"\"\"A simple MLP to act as the energy function.\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dim: int = 64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Tanh(),  # Optional: can help stabilize training\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"guides/training/#training-with-contrastive-divergence","title":"Training with Contrastive Divergence","text":"<p>Contrastive Divergence (CD) is one of the most common methods for training EBMs. Here's a complete example of training with CD using TorchEBM:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nimport os\n\nfrom torchebm.core import BaseEnergyFunction, CosineScheduler\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.losses import ContrastiveDivergence\nfrom torchebm.datasets import TwoMoonsDataset\n\n# Set seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\n# Create output directory for plots\nos.makedirs(\"training_plots\", exist_ok=True)\n\n# Hyperparameters\nINPUT_DIM = 2\nHIDDEN_DIM = 16\nBATCH_SIZE = 256\nEPOCHS = 200\nLEARNING_RATE = 1e-3\n\n# Use dynamic schedulers for sampler parameters\nSAMPLER_STEP_SIZE = CosineScheduler(start_value=3e-2, end_value=5e-3, n_steps=100)\nSAMPLER_NOISE_SCALE = CosineScheduler(start_value=3e-1, end_value=1e-2, n_steps=100)\n\nCD_K = 10  # Number of MCMC steps for Contrastive Divergence\nUSE_PCD = True  # Whether to use Persistent Contrastive Divergence\nVISUALIZE_EVERY = 20\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load dataset\ndataset = TwoMoonsDataset(n_samples=3000, noise=0.05, seed=42, device=device)\nreal_data_for_plotting = dataset.get_data()\ndataloader = DataLoader(\n    dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=True,\n)\n\n# Setup model components\nenergy_model = MLPEnergy(INPUT_DIM, HIDDEN_DIM).to(device)\nsampler = LangevinDynamics(\n    energy_function=energy_model,\n    step_size=SAMPLER_STEP_SIZE,\n    noise_scale=SAMPLER_NOISE_SCALE,\n    device=device,\n)\nloss_fn = ContrastiveDivergence(\n    energy_function=energy_model,\n    sampler=sampler,\n    k_steps=CD_K,\n    persistent=USE_PCD,\n    buffer_size=BATCH_SIZE,\n).to(device)\n\n# Optimizer\noptimizer = optim.Adam(energy_model.parameters(), lr=LEARNING_RATE)\n\n# Training loop\nlosses = []\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    energy_model.train()\n    epoch_loss = 0.0\n\n    for i, data_batch in enumerate(dataloader):\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Calculate Contrastive Divergence loss\n        loss, negative_samples = loss_fn(data_batch)\n\n        # Backpropagate and optimize\n        loss.backward()\n\n        # Optional: Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(energy_model.parameters(), max_norm=1.0)\n\n        # Update parameters\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    # Calculate average loss for this epoch\n    avg_epoch_loss = epoch_loss / len(dataloader)\n    losses.append(avg_epoch_loss)\n    print(f\"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_epoch_loss:.4f}\")\n\n    # Visualize progress\n    if (epoch + 1) % VISUALIZE_EVERY == 0 or epoch == 0:\n        print(\"Generating visualization...\")\n        plot_energy_and_samples(\n            energy_fn=energy_model,\n            real_samples=real_data_for_plotting,\n            sampler=sampler,\n            epoch=epoch + 1,\n            device=device,\n            plot_range=2.5,\n            k_sampling=200,\n        )\n\n# Plot the training loss\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.grid(True, alpha=0.3)\nplt.savefig('docs/assets/images/training/cd_training_loss.png')\nplt.show()\n</code></pre>"},{"location":"guides/training/#visualization-during-training","title":"Visualization During Training","text":"<p>It's important to visualize the model's progress during training. Here's a helper function to plot the energy landscape and samples:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.samplers import LangevinDynamics\n\n@torch.no_grad()\ndef plot_energy_and_samples(\n    energy_fn: BaseEnergyFunction,\n    real_samples: torch.Tensor,\n    sampler: LangevinDynamics,\n    epoch: int,\n    device: torch.device,\n    grid_size: int = 100,\n    plot_range: float = 3.0,\n    k_sampling: int = 100,\n):\n    \"\"\"Plots the energy surface, real data, and model samples.\"\"\"\n    plt.figure(figsize=(8, 8))\n\n    # Create grid for energy surface plot\n    x_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)\n    y_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)\n    xv, yv = torch.meshgrid(x_coords, y_coords, indexing=\"xy\")\n    grid = torch.stack([xv.flatten(), yv.flatten()], dim=1)\n\n    # Calculate energy on the grid\n    energy_fn.eval()\n    energy_values = energy_fn(grid).cpu().numpy().reshape(grid_size, grid_size)\n\n    # Plot energy surface (using probability density for better visualization)\n    log_prob_values = -energy_values\n    log_prob_values = log_prob_values - np.max(log_prob_values)\n    prob_density = np.exp(log_prob_values)\n\n    plt.contourf(\n        xv.cpu().numpy(),\n        yv.cpu().numpy(),\n        prob_density,\n        levels=50,\n        cmap=\"viridis\",\n    )\n    plt.colorbar(label=\"exp(-Energy) (unnormalized density)\")\n\n    # Generate samples from the current model for visualization\n    vis_start_noise = torch.randn(\n        500, real_samples.shape[1], device=device\n    )\n    model_samples_tensor = sampler.sample(x=vis_start_noise, n_steps=k_sampling)\n    model_samples = model_samples_tensor.cpu().numpy()\n\n    # Plot real and model samples\n    real_samples_np = real_samples.cpu().numpy()\n    plt.scatter(\n        real_samples_np[:, 0],\n        real_samples_np[:, 1],\n        s=10,\n        alpha=0.5,\n        label=\"Real Data\",\n        c=\"white\",\n        edgecolors=\"k\",\n        linewidths=0.5,\n    )\n    plt.scatter(\n        model_samples[:, 0],\n        model_samples[:, 1],\n        s=10,\n        alpha=0.5,\n        label=\"Model Samples\",\n        c=\"red\",\n        edgecolors=\"darkred\",\n        linewidths=0.5,\n    )\n\n    plt.xlim(-plot_range, plot_range)\n    plt.ylim(-plot_range, plot_range)\n    plt.title(f\"Epoch {epoch}\")\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(f\"docs/assets/images/training/ebm_training_epoch_{epoch}.png\")\n    plt.close()\n</code></pre>"},{"location":"guides/training/#training-with-score-matching","title":"Training with Score Matching","text":"<p>An alternative to Contrastive Divergence is Score Matching, which doesn't require MCMC sampling:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses import ScoreMatching\nfrom torchebm.datasets import GaussianMixtureDataset\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Setup model, loss, and optimizer\nenergy_fn = MLPEnergy(input_dim=2).to(device)\nsm_loss_fn = ScoreMatching(\n    energy_function=energy_fn,\n    hessian_method=\"hutchinson\",  # More efficient for higher dimensions\n    hutchinson_samples=5,\n    device=device,\n)\noptimizer = optim.Adam(energy_fn.parameters(), lr=0.001)\n\n# Setup data\ndataset = GaussianMixtureDataset(\n    n_samples=500, n_components=4, std=0.1, seed=123\n).get_data()\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training Loop\nlosses = []\nfor epoch in range(50):\n    epoch_loss = 0.0\n    for batch_data in dataloader:\n        batch_data = batch_data.to(device)\n\n        optimizer.zero_grad()\n        loss = sm_loss_fn(batch_data)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_loss = epoch_loss / len(dataloader)\n    losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/50, Loss: {avg_loss:.6f}\")\n\n# Plot the training loss\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Score Matching Training Loss')\nplt.grid(True, alpha=0.3)\nplt.savefig('docs/assets/images/training/sm_training_loss.png')\nplt.show()\n</code></pre>"},{"location":"guides/training/#comparing-training-methods","title":"Comparing Training Methods","text":"<p>Here's how the major training methods for EBMs compare:</p> Method Pros Cons Best For Contrastive Divergence (CD) - Simple to implement- Computationally efficient- Works well for simple distributions - May not converge to true gradient- Limited mode exploration with short MCMC runs- Can lead to poor samples Restricted Boltzmann Machines, simpler energy-based models Persistent CD (PCD) - Better mode exploration than CD- More accurate gradient estimation- Improved sample quality - Requires maintaining persistent chains- Can be unstable with high learning rates- Chains can get stuck in metastable states Deep Boltzmann Machines, models with complex energy landscapes Score Matching - Avoids MCMC sampling- Consistent estimator- Stable optimization - Requires computing Hessian diagonals- High computational cost in high dimensions- Need for second derivatives Continuous data, models with tractable derivatives Denoising Score Matching - Avoids explicit Hessian computation- More efficient than standard score matching- Works well for high-dimensional data - Performance depends on noise distribution- Trade-off between noise level and estimation accuracy- May smooth out important details Image modeling, high-dimensional continuous distributions Sliced Score Matching - Linear computational complexity- No Hessian computation needed- Scales well to high dimensions - Approximation depends on number of projections- Less accurate with too few random projections- Still requires gradient computation High-dimensional problems where other score matching variants are too expensive"},{"location":"guides/training/#advanced-training-techniques","title":"Advanced Training Techniques","text":""},{"location":"guides/training/#gradient-clipping","title":"Gradient Clipping","text":"<p>Gradient clipping is essential for stable EBM training:</p> <pre><code># After loss.backward()\ntorch.nn.utils.clip_grad_norm_(energy_model.parameters(), max_norm=1.0)\noptimizer.step()\n</code></pre>"},{"location":"guides/training/#regularization-techniques","title":"Regularization Techniques","text":"<p>Adding regularization can help stabilize training:</p> <pre><code># L2 regularization\nweight_decay = 1e-4\noptimizer = optim.Adam(energy_model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\n\n# Spectral normalization for stability\nfrom torch.nn.utils import spectral_norm\n\nclass RegularizedMLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            spectral_norm(nn.Linear(input_dim, hidden_dim)),\n            nn.ReLU(),\n            spectral_norm(nn.Linear(hidden_dim, hidden_dim)),\n            nn.ReLU(),\n            spectral_norm(nn.Linear(hidden_dim, 1))\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n</code></pre>"},{"location":"guides/training/#tips-for-successful-training","title":"Tips for Successful Training","text":"<ol> <li>Start Simple: Begin with a simple energy function and dataset, then increase complexity</li> <li>Monitor Energy Values: Watch for energy collapse (very negative values) which indicates instability</li> <li>Adjust Sampling Parameters: Tune MCMC step size and noise scale for effective exploration</li> <li>Use Persistent CD: For complex distributions, persistent CD often yields better results</li> <li>Visualize Frequently: Regularly check the energy landscape and samples to track progress</li> <li>Gradient Clipping: Always use gradient clipping to prevent explosive gradients</li> <li>Parameter Scheduling: Use schedulers for learning rate, step size, and noise scale</li> <li>Batch Normalization: Consider adding batch normalization in your energy network</li> <li>Ensemble Methods: Train multiple models and ensemble their predictions for better results</li> <li>Patience: EBM training can be challenging - be prepared to experiment with hyperparameters </li> </ol>"},{"location":"guides/visualization/","title":"Visualization","text":""},{"location":"guides/visualization/#visualization-in-torchebm","title":"Visualization in TorchEBM","text":"<p>Data visualization is an essential tool for understanding, analyzing, and communicating the behavior of energy-based models. This guide covers various visualization techniques available in TorchEBM to help you gain insights into energy landscapes, sampling processes, and model performance.</p>"},{"location":"guides/visualization/#energy-landscape-visualization","title":"Energy Landscape Visualization","text":"<p>Visualizing energy landscapes is crucial for understanding the structure of the probability distribution you're working with. TorchEBM provides utilities to create both 2D and 3D visualizations of energy functions.</p>"},{"location":"guides/visualization/#basic-energy-landscape-visualization","title":"Basic Energy Landscape Visualization","text":"<p>Here's a simple example to visualize a 2D energy function:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create the energy function\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Create a grid for visualization\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Create 3D surface plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nsurf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('Energy')\nax.set_title('Double Well Energy Landscape')\nplt.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#visualizing-energy-as-probability-density","title":"Visualizing Energy as Probability Density","text":"<p>Often, it's more intuitive to visualize the probability density (exp(-Energy)) rather than the energy itself:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy\n\n# Create the energy function\nenergy_fn = DoubleWellEnergy(barrier_height=2.0)\n\n# Create a grid for visualization\ngrid_size = 100\nplot_range = 3.0\nx_coords = np.linspace(-plot_range, plot_range, grid_size)\ny_coords = np.linspace(-plot_range, plot_range, grid_size)\nX, Y = np.meshgrid(x_coords, y_coords)\nZ = np.zeros_like(X)\n\n# Compute energy values\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Convert energy to probability density (unnormalized)\n# Subtract max for numerical stability before exponentiating\nlog_prob_values = -Z\nlog_prob_values = log_prob_values - np.max(log_prob_values)\nprob_density = np.exp(log_prob_values)\n\n# Create contour plot\nplt.figure(figsize=(10, 8))\ncontour = plt.contourf(X, Y, prob_density, levels=50, cmap='viridis')\nplt.colorbar(label='exp(-Energy) (unnormalized density)')\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Double Well Probability Density')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#sampling-trajectory-visualization","title":"Sampling Trajectory Visualization","text":"<p>Visualizing the trajectory of sampling algorithms can provide insights into their behavior and convergence properties.</p>"},{"location":"guides/visualization/#visualizing-langevin-dynamics-trajectories","title":"Visualizing Langevin Dynamics Trajectories","text":"<pre><code>from torchebm.core import DoubleWellEnergy, LinearScheduler, WarmupScheduler\nfrom torchebm.samplers import LangevinDynamics\n\n# Create energy function and sampler\nenergy_fn = DoubleWellEnergy(barrier_height=5.0)\n\n# Define a cosine scheduler for the Langevin dynamics\nscheduler_linear = LinearScheduler(\n    initial_value=0.05,\n    final_value=0.03,\n    total_steps=100\n)\n\nscheduler = WarmupScheduler(\n    main_scheduler=scheduler_linear,\n    warmup_steps=10,\n    warmup_init_factor=0.01\n)\n\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=scheduler\n\n)\n\n# Initial point\ninitial_point = torch.tensor([[-2.0, 0.0]], dtype=torch.float32)\n\n# Run sampling and get trajectory\ntrajectory = sampler.sample(\n    x=initial_point,\n    dim=2,\n    n_steps=1000,\n    return_trajectory=True\n)\n\n# Background energy landscape\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Visualize\nplt.figure(figsize=(10, 8))\nplt.contourf(X, Y, Z, 50, cmap='viridis', alpha=0.7)\nplt.colorbar(label='Energy')\n\n# Extract trajectory coordinates\ntraj_x = trajectory[0, :, 0].numpy()\ntraj_y = trajectory[0, :, 1].numpy()\n\n# Plot trajectory\nplt.plot(traj_x, traj_y, 'r-', linewidth=1, alpha=0.7)\nplt.scatter(traj_x[0], traj_y[0], c='black', s=50, marker='o', label='Start')\nplt.scatter(traj_x[-1], traj_y[-1], c='blue', s=50, marker='*', label='End')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Langevin Dynamics Trajectory')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.savefig('langevin_trajectory.png')\nplt.show()\n</code></pre>"},{"location":"guides/visualization/#visualizing-multiple-chains","title":"Visualizing Multiple Chains","text":"<pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import RastriginEnergy\nfrom torchebm.samplers import LangevinDynamics\n\n# Set random seed for reproducibility\ntorch.manual_seed(44)\nnp.random.seed(43)\n\n# Create energy function and sampler\nenergy_fn = RastriginEnergy(a=10.0)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.008\n)\n\n# Parameters for sampling\ndim = 2\nn_steps = 1000\nnum_chains = 5\n\n# Generate random starting points\ninitial_points = torch.randn(num_chains, dim) * 3\n\n# Run sampling and get trajectory\ntrajectories = sampler.sample(\n    x=initial_points,\n    dim=dim,\n    n_samples=num_chains,\n    n_steps=n_steps,\n    return_trajectory=True\n)\n\n# Create background contour\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\nprint(trajectories.shape)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)\n        Z[i, j] = energy_fn(point).item()\n\n# Plot contour\nplt.figure(figsize=(12, 10))\ncontour = plt.contourf(X, Y, Z, 50, cmap='viridis', alpha=0.7)\nplt.colorbar(label='Energy')\n\n# Plot each trajectory with a different color\ncolors = ['red', 'blue', 'green', 'orange', 'purple']\nfor i in range(num_chains):\n    traj_x = trajectories[i, :, 0].numpy()\n    traj_y = trajectories[i, :, 1].numpy()\n\n    plt.plot(traj_x, traj_y, alpha=0.7, linewidth=1, c=colors[i],\n             label=f'Chain {i + 1}')\n\n    # Mark start and end points\n    plt.scatter(traj_x[0], traj_y[0], c='black', s=50, marker='o')\n    plt.scatter(traj_x[-1], traj_y[-1], c=colors[i], s=100, marker='*')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Multiple Langevin Dynamics Sampling Chains on Rastrigin Potential')\nplt.legend()\nplt.tight_layout()\nplt.savefig('multiple_chains.png')\nplt.show()\n</code></pre>"},{"location":"guides/visualization/#distribution-visualization","title":"Distribution Visualization","text":"<p>Visualizing the distribution of samples can help assess the quality of your sampling algorithm.</p>"},{"location":"guides/visualization/#comparing-generated-samples-with-ground-truth","title":"Comparing Generated Samples with Ground Truth","text":"<pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom torchebm.core import GaussianEnergy\nfrom torchebm.samplers import LangevinDynamics\n\n# Create a Gaussian energy function\nmean = torch.tensor([1.0, -1.0])\ncov = torch.tensor([[1.0, 0.5], [0.5, 1.0]])\nenergy_fn = GaussianEnergy(mean=mean, cov=cov)\n\n# Sample using Langevin dynamics\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=0.01\n)\n\n# Generate samples\nn_samples = 5000\nburn_in = 200\n\n# Initialize random samples\nx = torch.randn(n_samples, 2)\n\n# Perform sampling\nsamples = sampler.sample(\n    x=x,\n    n_steps=1000,\n    burn_in=burn_in,\n    return_trajectory=False\n)\n\n# Convert to numpy for visualization\nsamples_np = samples.numpy()\nmean_np = mean.numpy()\ncov_np = cov.numpy()\n\n# Create a grid for the ground truth density\nx = np.linspace(-3, 5, 100)\ny = np.linspace(-5, 3, 100)\nX, Y = np.meshgrid(x, y)\npos = np.dstack((X, Y))\n\n# Calculate multivariate normal PDF\nrv = stats.multivariate_normal(mean_np, cov_np)\nZ = rv.pdf(pos)\n\n# Create figure with multiple plots\nfig = plt.figure(figsize=(15, 5))\n\n# Ground truth contour\nax1 = fig.add_subplot(131)\nax1.contourf(X, Y, Z, 50, cmap='Blues')\nax1.set_title('Ground Truth Density')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\n# Sample density (using kernel density estimation)\nax2 = fig.add_subplot(132)\nh = ax2.hist2d(samples_np[:, 0], samples_np[:, 1], bins=50, cmap='Reds', density=True)\nplt.colorbar(h[3], ax=ax2, label='Density')\nax2.set_title('Sampled Distribution')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\n\n# Scatter plot of samples\nax3 = fig.add_subplot(133)\nax3.scatter(samples_np[:, 0], samples_np[:, 1], alpha=0.5, s=3)\nax3.set_title('Sample Points')\nax3.set_xlabel('x')\nax3.set_ylabel('y')\nax3.set_xlim(ax2.get_xlim())\nax3.set_ylim(ax2.get_ylim())\n\nplt.tight_layout()\nplt.savefig('distribution_comparison_updated.png')\nplt.show()\n</code></pre>"},{"location":"guides/visualization/#energy-evolution-visualization","title":"Energy Evolution Visualization","text":"<p>Tracking how energy values evolve during sampling can help assess convergence.</p> <pre><code>import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom torchebm.core import DoubleWellEnergy, GaussianEnergy, CosineScheduler\nfrom torchebm.samplers import LangevinDynamics\n\n\nSAMPLER_STEP_SIZE = CosineScheduler(\n    initial_value=1e-2, final_value=1e-3, total_steps=50\n)\n\nSAMPLER_NOISE_SCALE = CosineScheduler(\n    initial_value=2e-1, final_value=1e-2, total_steps=50\n)\n\n# Create energy function and sampler\nenergy_fn = GaussianEnergy(mean=torch.tensor([0.0, 0.0]), cov=torch.eye(2) * 0.5)\nsampler = LangevinDynamics(\n    energy_function=energy_fn,\n    step_size=SAMPLER_STEP_SIZE,\n    noise_scale=SAMPLER_NOISE_SCALE\n)\n\n# Parameters for sampling\ndim = 2\nn_steps = 200\ninitial_point = torch.tensor([[-2.0, 0.0]], dtype=torch.float32)\n\n# Track the energy during sampling\nenergy_values = []\ncurrent_sample = initial_point.clone()\n\n# Run the sampling steps and store each energy\nfor i in range(n_steps):\n    noise = torch.randn_like(current_sample)\n    current_sample = sampler.langevin_step(current_sample, noise)\n    energy_values.append(energy_fn(current_sample).item())\n\n# Convert to numpy for plotting\nenergy_values_np = np.array(energy_values)\n\n# Plot energy evolution\nplt.figure(figsize=(10, 6))\nplt.plot(energy_values_np)\nplt.xlabel('Step')\nplt.ylabel('Energy')\nplt.title('Energy Evolution During Langevin Dynamics Sampling')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('energy_evolution_updated.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#visualizing-training-progress-with-different-loss-functions","title":"Visualizing Training Progress with Different Loss Functions","text":"<p>You can also visualize how different loss functions affect the training dynamics:</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchebm.core import BaseEnergyFunction\nfrom torchebm.losses import ContrastiveDivergence, ScoreMatching\nfrom torchebm.samplers import LangevinDynamics\nfrom torchebm.datasets import TwoMoonsDataset\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define a simple MLP energy function\nclass MLPEnergy(BaseEnergyFunction):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SELU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Training function\ndef train_and_record_loss(loss_type, n_epochs=100):\n    # Reset model\n    energy_model = MLPEnergy(input_dim=2, hidden_dim=32).to(device)\n\n    # Setup sampler and loss\n    sampler = LangevinDynamics(\n        energy_function=energy_model,\n        step_size=0.1,\n        device=device\n    )\n\n    if loss_type == 'CD':\n        loss_fn = ContrastiveDivergence(\n            energy_function=energy_model,\n            sampler=sampler,\n            k_steps=10,\n            persistent=True\n        )\n    elif loss_type == 'SM':\n        loss_fn = ScoreMatching(\n            energy_function=energy_model,\n            hutchinson_samples=5\n        )\n\n    optimizer = optim.Adam(energy_model.parameters(), lr=0.001)\n\n    # Record loss\n    losses = []\n\n    # Train\n    for epoch in range(n_epochs):\n        epoch_loss = 0.0\n        for batch in dataloader:\n            optimizer.zero_grad()\n            loss = loss_fn(batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        losses.append(avg_loss)\n        if (epoch + 1) % 10 == 0:\n            print(f\"{loss_type} - Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n\n    return losses\n\n# Setup data and device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndataset = TwoMoonsDataset(n_samples=1000, noise=0.1, device=device)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n\n# Train with different loss functions\ncd_losses = train_and_record_loss('CD')\nsm_losses = train_and_record_loss('SM')\n\n# Plot losses\nplt.figure(figsize=(10, 6))\nplt.plot(cd_losses, label='Contrastive Divergence')\nplt.plot(sm_losses, label='Score Matching')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('loss_comparison.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"guides/visualization/#conclusion","title":"Conclusion","text":"<p>Effective visualization is key to understanding and debugging energy-based models. TorchEBM provides tools for visualizing energy landscapes, sampling trajectories, and model performance. These visualizations can help you gain insights into your models and improve their design and performance.</p> <p>Remember to adapt these examples to your specific needs - you might want to visualize higher-dimensional spaces using dimensionality reduction techniques, or create specialized plots for your particular application. </p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2024/#2024","title":"2024","text":""},{"location":"blog/category/research/","title":"Research","text":""},{"location":"blog/category/research/#research","title":"Research","text":""},{"location":"blog/category/tutorials/","title":"Tutorials","text":""},{"location":"blog/category/tutorials/#tutorials","title":"Tutorials","text":""},{"location":"blog/category/examples/","title":"Examples","text":""},{"location":"blog/category/examples/#examples","title":"Examples","text":""}]}