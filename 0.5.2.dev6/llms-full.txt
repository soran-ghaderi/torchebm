# TorchEBM

> Energy-Based Modeling library for PyTorch

TorchEBM is an Energy-Based Modeling library for PyTorch that provides tools for 
sampling, inference, and learning in complex distributions. It supports classical 
EBMs via MCMC sampling and modern flow/diffusion-based generative models.

Key components:
- Samplers: LangevinDynamics, HamiltonianMonteCarlo, FlowSampler, GradientDescentSampler
- Losses: ContrastiveDivergence, ScoreMatching, EquilibriumMatchingLoss
- Interpolants: LinearInterpolant, CosineInterpolant, VariancePreservingInterpolant
- Integrators: EulerMaruyama, Heun, Leapfrog
- Models: ConditionalTransformer2D, neural network components
- Datasets: TwoMoons, EightGaussians, SwissRoll, and more synthetic datasets


# Getting Started

ðŸ“

# **PyTorch Toolkit for Generative Modeling**

A high-performance PyTorch library that makes Energy-Based Models **accessible** and **efficient** for researchers and practitioners alike.

[â­ Star on GitHub](https://github.com/soran-ghaderi/torchebm)

**TorchEBM** provides components for ðŸ”¬ sampling, ðŸ§  inference, and ðŸ“Š model training.

[Getting Started](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/tutorials/index.md) [Examples](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/examples/index.md) [API Reference](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/index.md) [Development](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/developer_guide/index.md)

______________________________________________________________________

## What is ðŸ“ TorchEBM?

**TorchEBM** is a PyTorch library for Energy-Based Models (EBMs), a powerful class of generative models. It provides a flexible framework to define, train, and generate samples using energy-based models.

______________________________________________________________________

## Equilibrium Matching in Action

**Equilibrium Matching:** Comparing Linear, VP, and Cosine interpolants transforming noise into structured data distributions.

- **Equilibrium Matching**

  ______________________________________________________________________

  Train generative models by learning velocity fields that transform noise into data.

  [Flow Sampler](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/samplers/flow.md)

- **Multiple Interpolants**

  ______________________________________________________________________

  Choose from Linear, Variance-Preserving, and Cosine interpolation schemes.

  [Interpolants](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/interpolants/index.md)

______________________________________________________________________

## Core Components

TorchEBM is structured around several key components:

- **Models**

  ______________________________________________________________________

  Define energy functions using `BaseModel`, from analytical forms to custom neural networks.

  [Details](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/examples/training/index.md)

- **Samplers**

  ______________________________________________________________________

  Generate samples with MCMC samplers like Langevin Dynamics and Hamiltonian Monte Carlo.

  [Details](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/examples/samplers/index.md)

- **Loss Functions**

  ______________________________________________________________________

  Train models with loss functions like Contrastive Divergence and Score Matching.

  [Details](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/losses/index.md)

- **Datasets**

  ______________________________________________________________________

  Use synthetic dataset generators for testing and visualization.

  [Details](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/examples/datasets/index.md)

- **Visualization**

  ______________________________________________________________________

  Visualize energy landscapes, sampling, and training dynamics.

  [Details](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/examples/visualization/index.md)

- **Accelerated Computing**

  ______________________________________________________________________

  Accelerate sampling and training with CUDA implementations.

  [Details](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/cuda/index.md)

______________________________________________________________________

## Quick Start

Install the library using pip:

```bash
pip install torchebm
```

Here's a minimal example of defining an energy function and a sampler:

```python
import torch
from torchebm.core import GaussianModel
from torchebm.samplers import LangevinDynamics

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GaussianModel(mean=torch.zeros(2), cov=torch.eye(2), device=device)

sampler = LangevinDynamics(model=model, step_size=0.01, device=device)

initial_points = torch.randn(500, 2, device=device)
samples = sampler.sample(x=initial_points, n_steps=100)

print(f"Output batch_shape: {samples.shape}") # (B, len) -> torch.Size([500, 2])
```

______________________________________________________________________

Latest Release

TorchEBM is currently in early development. Check our [GitHub repository](https://github.com/soran-ghaderi/torchebm) for the latest updates and features.

## Community & Contribution

TorchEBM is an open-source project developed with the research community in mind.

- **Bug Reports & Feature Requests:** Please use the [GitHub Issues](https://github.com/soran-ghaderi/torchebm/issues).
- **Contributing Code:** We welcome contributions! Please see the [Contributing Guidelines](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/developer_guide/index.md). Consider following the [Commit Conventions](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/developer_guide/code_guidelines/index.md).
- **Show Support:** If you find TorchEBM helpful for your work, please consider starring the repository on [GitHub](https://github.com/soran-ghaderi/torchebm)!

______________________________________________________________________

## Citation

Please consider citing the TorchEBM repository if it contributes to your research:

```bibtex
@misc{torchebm_library_2025,
  author       = {Ghaderi, Soran and Contributors},
  title        = {TorchEBM: A PyTorch Library for Training Energy-Based Models},
  year         = {2025},
  url          = {https://github.com/soran-ghaderi/torchebm},
}
```

______________________________________________________________________

## License

TorchEBM is available under the **MIT License**. See the [LICENSE file](https://github.com/soran-ghaderi/torchebm/blob/master/LICENSE) for details.

# Getting Started with TorchEBM

This guide provides a hands-on introduction to TorchEBM. You'll learn how to install the library, understand its core components, and train your first Energy-Based Model (EBM) on a synthetic dataset.

## 1. Installation

TorchEBM can be installed from PyPI. Ensure you have PyTorch installed first.

```bash
pip install torchebm
```

Prerequisites

- Python 3.8+
- PyTorch 1.10.0+
- CUDA is optional but highly recommended for performance.

## 2. The Core Concepts

An Energy-Based Model defines a probability distribution over data (x) through an **energy function** (E(x)). The probability is defined as (p(x) = \\frac{e^{-E(x)}}{Z}), where lower energy corresponds to higher probability.

TorchEBM is built around two key components:

1. **Energy Functions**: These are learnable functions (often neural networks) that map input data to a scalar energy value.
1. **Samplers**: These are algorithms, typically based on Markov Chain Monte Carlo (MCMC), used to draw samples from the probability distribution defined by the energy function.

Let's explore these concepts with code.

### Concept 1: The Energy Function

An energy function is a `torch.nn.Module` that takes a tensor `x` of shape `(batch_size, *dims)` and returns a tensor of energy values of shape `(batch_size,)`.

TorchEBM provides several pre-built energy functions for testing and experimentation. Here's how to use the `GaussianEnergy` function, which models a multivariate normal distribution.

```python
import torch
from torchebm.core import GaussianEnergy

device = "cuda" if torch.cuda.is_available() else "cpu"

energy_fn = GaussianEnergy(
    mean=torch.zeros(2, device=device),
    cov=torch.eye(2, device=device)
).to(device)

x = torch.tensor([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]], device=device)

energy = energy_fn(x)

print(f"Input shape: {x.shape}")
print(f"Energy shape: {energy.shape}")
print(f"Energies:\n{energy}")
```

The point `[0.0, 0.0]` is the mean of the distribution and thus has the lowest energy. As points move away from the mean, their energy increases.

### Concept 2: The Sampler

Samplers generate data points from the distribution defined by an energy function. They typically work by starting from random initial points and iteratively refining them to have lower energy (higher probability).

Let's use the `LangevinDynamics` sampler to draw samples from our `GaussianEnergy` distribution.

```python
from torchebm.samplers import LangevinDynamics

sampler = LangevinDynamics(
    energy_function=energy_fn,
    step_size=0.1,
    noise_scale=1.0
).to(device)

samples = sampler.sample(
    dim=2,
    n_samples=1000,
    n_steps=100
)

print(f"Generated samples shape: {samples.shape}")
```

You have now sampled from your first energy-based model! These samples approximate a 2D Gaussian distribution.

## 3. Training Your First EBM

Now let's put everything together and train an EBM with a neural network as the energy function. The goal is to train the model to represent a synthetic "two moons" dataset.

### Step 1: Create a Dataset

First, we'll generate a `TwoMoonsDataset` and create a `DataLoader` to iterate through it in batches.

```python
import torch
from torch.utils.data import DataLoader
from torchebm.datasets import TwoMoonsDataset

dataset = TwoMoonsDataset(n_samples=5000)

dataloader = DataLoader(dataset, batch_size=128, shuffle=True)
```

### Step 2: Define a Neural Energy Function

Next, we'll create a simple Multi-Layer Perceptron (MLP) to serve as our energy function. This network will take 2D points as input and output a single energy value for each.

```python
import torch.nn as nn
from torchebm.core import BaseEnergyFunction

class NeuralEnergy(BaseEnergyFunction):
    def __init__(self, input_dim=2, hidden_dim=64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x):
        return self.network(x).squeeze(-1)

neural_energy_fn = NeuralEnergy().to(device)
```

### Step 3: Set up the Training Components

To train the EBM, we need three things:

1. **A Loss Function**: We'll use `ContrastiveDivergence`, a standard loss function for EBMs. It works by pushing down the energy of real data ("positive" samples) and pushing up the energy of generated data ("negative" samples).
1. **A Sampler**: The loss function needs a sampler to generate the negative samples. We'll use `LangevinDynamics` again.
1. **An Optimizer**: A standard PyTorch optimizer like `Adam`.

```python
from torchebm.losses import ContrastiveDivergence
from torchebm.samplers import LangevinDynamics
from torch.optim import Adam

sampler = LangevinDynamics(
    energy_function=neural_energy_fn,
    step_size=10.0,
    noise_scale=0.1,
    n_steps=60
)

cd_loss = ContrastiveDivergence(sampler=sampler)

optimizer = Adam(neural_energy_fn.parameters(), lr=1e-4)
```

### Step 4: The Training Loop

Now we'll write a standard PyTorch training loop. For each batch of real data, we calculate the contrastive divergence loss and update the model's weights.

```python
for epoch in range(100):
    for batch_data in dataloader:
        real_samples = batch_data.to(device)

        optimizer.zero_grad()

        loss = cd_loss(real_samples, neural_energy_fn)

        loss.backward()
        optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

print("Training finished!")
```

This loop adjusts the weights of our neural network so that its energy landscape matches the "two moons" data distribution.

## Next Steps

Congratulations on training your first Energy-Based Model with TorchEBM!

- Learn more about the different [Samplers](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/tutorials/samplers/index.md) available.
- Explore other [Loss Functions](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/tutorials/loss_functions/index.md) for training EBMs.
- See how to create [Custom Neural Networks](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/tutorials/custom_neural_networks/index.md) for more complex energy functions.
- Check out the [Visualization](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/tutorials/visualization/index.md) guide to see how you can plot your energy landscapes and samples.
# Core Concepts

# Sampling Algorithms

Sampling from energy-based models is a core task in TorchEBM. This guide explains the different sampling algorithms available and how to use them effectively.

## Overview of Sampling

In energy-based models, we need to sample from the probability distribution defined by the model:

[p(x) = \\frac{e^{-E(x)}}{Z}]

Since the normalizing constant Z is typically intractable, we use Markov Chain Monte Carlo (MCMC) methods to generate samples without needing to compute Z.

## Langevin Dynamics

Langevin Dynamics is a gradient-based MCMC method that updates samples using the energy gradient plus Gaussian noise. It's one of the most commonly used samplers in energy-based models due to its simplicity and effectiveness.

### Basic Usage

```python
import torch
from torchebm.core import BaseModel
from torchebm.samplers import LangevinDynamics
import torch.nn as nn

class MLPModel(BaseModel):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x):
        return self.network(x).squeeze(-1)

model = MLPModel(input_dim=2, hidden_dim=32)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
langevin_sampler = LangevinDynamics(
    model=model,
    step_size=0.1,
    noise_scale=0.01,
    device=device
)

initial_points = torch.randn(100, 2, device=device)
samples = langevin_sampler.sample(
    x=initial_points,
    n_steps=1000,
    return_trajectory=False
)

print(samples.shape)
```

### Parameters

- `model`: The model to sample from
- `step_size`: Step size for gradient updates (controls exploration vs. stability)
- `noise_scale`: Scale of the noise (default is sqrt(2\*step_size))
- `device`: The device to perform sampling on (e.g., "cuda" or "cpu")

### Advanced Features

The `LangevinDynamics` sampler in TorchEBM comes with several advanced features:

#### Returning Trajectories

For visualization or analysis, you can get the full trajectory of the sampling process:

```python
trajectory = langevin_sampler.sample(
    x=initial_points,
    n_steps=1000,
    return_trajectory=True
)

print(trajectory.shape)  # Shape: [n_samples, n_steps, dim]
```

#### Dynamic Parameter Scheduling

TorchEBM allows you to dynamically adjust the step size and noise scale during sampling using schedulers:

```python
from torchebm.core import CosineScheduler, LinearScheduler, ExponentialDecayScheduler

step_size_scheduler = CosineScheduler(
    start_value=3e-2,
    end_value=5e-3,
    n_steps=100
)

noise_scheduler = CosineScheduler(
    start_value=3e-1,
    end_value=1e-2,
    n_steps=100
)

dynamic_sampler = LangevinDynamics(
    model=model,
    step_size=step_size_scheduler,
    noise_scale=noise_scheduler,
    device=device
)
```

## Hamiltonian Monte Carlo (HMC)

HMC uses Hamiltonian dynamics to make more efficient proposals, leading to better exploration of the distribution:

```python
from torchebm.samplers import HamiltonianMonteCarlo
from torchebm.core import DoubleWellModel

model = DoubleWellModel()

hmc_sampler = HamiltonianMonteCarlo(
    model=model,
    step_size=0.1,
    n_leapfrog_steps=10,
    device=device
)

samples = hmc_sampler.sample(
    x=torch.randn(100, 2, device=device),
    n_steps=500,
    return_trajectory=False
)
```

## Integration with Loss Functions

Samplers in TorchEBM are designed to work seamlessly with loss functions for training energy-based models:

```python
from torchebm.losses import ContrastiveDivergence

loss_fn = ContrastiveDivergence(
    model=model,
    sampler=langevin_sampler,
    k_steps=10,
    persistent=True,
    buffer_size=1024
)

optimizer.zero_grad()
loss, negative_samples = loss_fn(data_batch)
loss.backward()
optimizer.step()
```

## Parallel Sampling

TorchEBM supports parallel sampling to speed up the generation of multiple samples:

```python
n_samples = 1000
dim = 2
initial_points = torch.randn(n_samples, dim, device=device)

samples = langevin_sampler.sample(
    x=initial_points,
    n_steps=1000,
    return_trajectory=False
)
```

## Sampler Visualizations

Visualizing the sampling process can help understand the behavior of your model. Here's an example showing how to visualize Langevin Dynamics trajectories:

```python
import numpy as np
import matplotlib.pyplot as plt
import torch
from torchebm.core import DoubleWellModel, LinearScheduler, WarmupScheduler
from torchebm.samplers import LangevinDynamics

model = DoubleWellModel(barrier_height=5.0)

scheduler_linear = LinearScheduler(
    initial_value=0.05,
    final_value=0.03,
    total_steps=100
)

scheduler = WarmupScheduler(
    main_scheduler=scheduler_linear,
    warmup_steps=10,
    warmup_init_factor=0.01
)

sampler = LangevinDynamics(
    model=model,
    step_size=scheduler

)

initial_point = torch.tensor([[-2.0, 0.0]], dtype=torch.float32)

trajectory = sampler.sample(
    x=initial_point,
    dim=2,
    n_steps=1000,
    return_trajectory=True
)

x = np.linspace(-3, 3, 100)
y = np.linspace(-3, 3, 100)
X, Y = np.meshgrid(x, y)
Z = np.zeros_like(X)

for i in range(X.shape[0]):
    for j in range(X.shape[1]):
        point = torch.tensor([X[i, j], Y[i, j]], dtype=torch.float32).unsqueeze(0)
        Z[i, j] = model(point).item()

plt.figure(figsize=(10, 8))
plt.contourf(X, Y, Z, 50, cmap='viridis', alpha=0.7)
plt.colorbar(label='Energy')

traj_x = trajectory[0, :, 0].numpy()
traj_y = trajectory[0, :, 1].numpy()

plt.plot(traj_x, traj_y, 'r-', linewidth=1, alpha=0.7)
plt.scatter(traj_x[0], traj_y[0], c='black', s=50, marker='o', label='Start')
plt.scatter(traj_x[-1], traj_y[-1], c='blue', s=50, marker='*', label='End')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Langevin Dynamics Trajectory')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('langevin_trajectory.png')
plt.show()
```

## Choosing a Sampler

- **Langevin Dynamics**: Good for general-purpose sampling, especially with neural network models
- **Hamiltonian Monte Carlo**: Better exploration of complex energy landscapes, but more computationally expensive
- **Metropolis-Adjusted Langevin Algorithm (MALA)**: Similar to Langevin Dynamics but with an accept/reject step

## Performance Tips

1. **Use GPU acceleration**: Batch processing of samples on GPU can significantly speed up sampling
1. **Adjust step size**: Too large â†’ unstable sampling; too small â†’ slow mixing
1. **Dynamic scheduling**: Use parameter schedulers to automatically adjust step size and noise during sampling
1. **Monitor energy values**: Track energy values to ensure proper mixing and convergence 5\*\*Multiple chains\*\*: Run multiple chains from different starting points to better explore the distribution

## Custom Samplers

TorchEBM provides flexible base classes for creating your own custom sampling algorithms. All samplers inherit from the `BaseSampler` abstract base class which defines the core interfaces and functionalities.

### Creating a Custom Sampler

To implement a custom sampler, you need to subclass `BaseSampler` and implement at minimum the `sample()` method:

```python
from torchebm.core import BaseSampler, BaseModel
import torch
from typing import Optional, Union, Tuple, List, Dict

class MyCustomSampler(BaseSampler):
    def __init__(
        self,
        model: BaseModel,
        my_parameter: float = 0.1,
        dtype: torch.dtype = torch.float32,
        device: Optional[Union[str, torch.device]] = None,
    ):
        super().__init__(model=model, dtype=dtype, device=device)
        self.my_parameter = my_parameter

        self.register_scheduler("my_parameter", ConstantScheduler(my_parameter))

    def custom_step(self, x: torch.Tensor) -> torch.Tensor:
        param_value = self.get_scheduled_value("my_parameter")

        gradient = self.model.gradient(x)

        noise = torch.randn_like(x)
        new_x = x - param_value * gradient + noise * 0.01

        return new_x

    @torch.no_grad()
    def sample(
        self,
        x: Optional[torch.Tensor] = None,
        dim: int = 10,
        n_steps: int = 100,
        n_samples: int = 1,
        thin: int = 1,
        return_trajectory: bool = False,
        return_diagnostics: bool = False,
        *args,
        **kwargs,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:
        self.reset_schedulers()

        if x is None:
            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)
        else:
            x = x.to(self.device)

        if return_trajectory:
            trajectory = torch.empty(
                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device
            )

        if return_diagnostics:
            diagnostics = self._setup_diagnostics(dim, n_steps, n_samples=n_samples)

        for i in range(n_steps):
            self.step_schedulers()

            x = self.custom_step(x)

            if return_trajectory:
                trajectory[:, i, :] = x

            if return_diagnostics:
                pass

        if return_trajectory:
            if return_diagnostics:
                return trajectory, diagnostics
            return trajectory
        if return_diagnostics:
            return x, diagnostics
        return x

    def _setup_diagnostics(self, dim: int, n_steps: int, n_samples: int = None) -> torch.Tensor:
        """Optional method to setup diagnostic storage"""
        if n_samples is not None:
            return torch.empty(
                (n_steps, 3, n_samples, dim), device=self.device, dtype=self.dtype
            )
        else:
            return torch.empty((n_steps, 3, dim), device=self.device, dtype=self.dtype)
```

### Key Components

When implementing a custom sampler, consider these key aspects:

1. **Model**: All samplers work with a model that defines the target distribution.

1. **Parameter Scheduling**: Use the built-in scheduler system to manage parameters that change during sampling:

   ```python
   self.register_scheduler("step_size", ConstantScheduler(0.01))

   current_step_size = self.get_scheduled_value("step_size")

   self.step_schedulers()
   ```

1. **Device and Precision Management**: The base class handles device placement and precision settings:

   ```python
   my_sampler = my_sampler.to("cuda:0")
   ```

1. **Diagnostics Collection**: Implement `_setup_diagnostics()` to collect sampling statistics.

### Example: Simplified Langevin Dynamics

Here's a simplified example of a Langevin dynamics sampler:

```python
class SimpleLangevin(BaseSampler):
    def __init__(
        self,
        model: BaseModel,
        step_size: float = 0.01,
        noise_scale: float = 1.0,
        dtype: torch.dtype = torch.float32,
        device: Optional[Union[str, torch.device]] = None,
    ):
        super().__init__(model=model, dtype=dtype, device=device)
        self.register_scheduler("step_size", ConstantScheduler(step_size))
        self.register_scheduler("noise_scale", ConstantScheduler(noise_scale))

    def langevin_step(self, x: torch.Tensor) -> torch.Tensor:
        step_size = self.get_scheduled_value("step_size")
        noise_scale = self.get_scheduled_value("noise_scale")

        gradient = self.model.gradient(x)
        noise = torch.randn_like(x)

        new_x = (
            x 
            - step_size * gradient 
            + torch.sqrt(torch.tensor(2.0 * step_size)) * noise_scale * noise
        )
        return new_x

    @torch.no_grad()
    def sample(
        self,
        x: Optional[torch.Tensor] = None,
        dim: int = 10,
        n_steps: int = 100,
        n_samples: int = 1,
        thin: int = 1,
        return_trajectory: bool = False,
        return_diagnostics: bool = False,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[dict]]]:
        self.reset_schedulers()

        if x is None:
            x = torch.randn(n_samples, dim, dtype=self.dtype, device=self.device)
        else:
            x = x.to(self.device)

        if return_trajectory:
            trajectory = torch.empty(
                (n_samples, n_steps, dim), dtype=self.dtype, device=self.device
            )

        for i in range(n_steps):
            self.step_schedulers()
            x = self.langevin_step(x)

            if return_trajectory:
                trajectory[:, i, :] = x

        if return_trajectory:
            return trajectory
        return x
```

### Tips for Custom Samplers

1. **Performance**: Use `@torch.no_grad()` for the sampling loop to disable gradient computation.
1. **GPU Compatibility**: Handle device placement correctly, especially when generating random noise.
1. **Validation**: Ensure your sampler works with simple distributions before moving to complex ones.
1. **Diagnostics**: Implement helpful diagnostics to monitor convergence and sampling quality.
1. **Mixed Precision**: For better performance on modern GPUs, use the built-in mixed precision support.

# Loss Functions

Training energy-based models involves estimating and minimizing the difference between the model distribution and the data distribution. TorchEBM provides various loss functions to accomplish this.

## Contrastive Divergence

Contrastive Divergence (CD) is one of the most popular methods for training energy-based models. It uses MCMC sampling to generate negative examples from the current model.

### Basic Usage

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchebm.core import BaseModel
from torchebm.losses import ContrastiveDivergence
from torchebm.samplers import LangevinDynamics

class MLPModel(BaseModel):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, 1),
            nn.Tanh(),
        )

    def forward(self, x):
        return self.network(x).squeeze(-1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MLPModel(input_dim=2, hidden_dim=64).to(device)

sampler = LangevinDynamics(
    model=model,
    step_size=0.1,
    device=device
)

loss_fn = ContrastiveDivergence(
    model=model,
    sampler=sampler,
    k_steps=10
)

optimizer = optim.Adam(model.parameters(), lr=0.001)

data_batch = torch.randn(128, 2).to(device)
optimizer.zero_grad()
loss, negative_samples = loss_fn(data_batch)
loss.backward()
optimizer.step()
```

### Advanced Options

The `ContrastiveDivergence` loss function in TorchEBM supports several advanced options:

#### Persistent Contrastive Divergence (PCD)

PCD maintains a buffer of negative samples across training iterations, which can lead to better mixing. You can enable it by setting `persistent=True`.

```python
loss_fn = ContrastiveDivergence(
    model=model,
    sampler=sampler,
    k_steps=10,
    persistent=True,
    buffer_size=1024
)
```

#### Using Schedulers for Sampling Parameters

You can use schedulers to dynamically adjust the sampler's step size or noise scale during training:

```python
from torchebm.core import CosineScheduler, ExponentialDecayScheduler, LinearScheduler

step_size_scheduler = CosineScheduler(
    start_value=3e-2,
    end_value=5e-3,
    n_steps=100
)

noise_scheduler = CosineScheduler(
    start_value=3e-1,
    end_value=1e-2,
    n_steps=100
)

sampler = LangevinDynamics(
    model=model,
    step_size=step_size_scheduler,
    noise_scale=noise_scheduler,
    device=device
)

loss_fn = ContrastiveDivergence(
    model=model,
    sampler=sampler,
    k_steps=10,
    persistent=True
)
```

## Score Matching Methods

Score Matching is another approach for training EBMs that avoids the need for MCMC sampling. It directly optimizes the score function (gradient of log-density).

### Score Matching

This is the standard form of score matching, which requires computing the Hessian of the model's energy function. This can be computationally expensive.

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchebm.core import BaseModel
from torchebm.losses import ScoreMatching
from torchebm.datasets import GaussianMixtureDataset

class MLPModel(BaseModel):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, 1),
        )

    def forward(self, x):
        return self.net(x).squeeze(-1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MLPModel(input_dim=2).to(device)

sm_loss_fn = ScoreMatching(
    model=model,
    device=device
)

optimizer = optim.Adam(model.parameters(), lr=0.001)
dataset = GaussianMixtureDataset(n_samples=500, n_components=4, std=0.1, seed=123).get_data()
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for epoch in range(10):
    epoch_loss = 0.0
    for batch_data in dataloader:
        batch_data = batch_data.to(device)

        optimizer.zero_grad()
        loss = sm_loss_fn(batch_data)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(dataloader)
    print(f"Epoch {epoch+1}/10, Loss: {avg_loss:.6f}")
```

### Sliced Score Matching

To make score matching more efficient and scalable, Sliced Score Matching (SSM) approximates the trace of the Hessian using Hutchinson's trick with random projections.

```python
from torchebm.losses import SlicedScoreMatching

ssm_loss_fn = SlicedScoreMatching(
    model=model,
    n_projections=5
)
optimizer.zero_grad()
loss = ssm_loss_fn(data_batch)
loss.backward()
optimizer.step()
```

### Denoising Score Matching

Denoising score matching (DSM) is another efficient alternative. It adds noise to the data points and learns the score of the noised data distribution, avoiding the need to compute the Hessian.

```python
from torchebm.losses import DenoisingScoreMatching

dsm_loss_fn = DenoisingScoreMatching(
    model=model,
    sigma=0.1
)

optimizer.zero_grad()
loss = dsm_loss_fn(data_batch)
loss.backward()
optimizer.step()
```

## Complete Training Example with Loss Function

Here's a complete example showing how to train an EBM using Contrastive Divergence loss:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

from torchebm.core import BaseModel
from torchebm.samplers import LangevinDynamics
from torchebm.losses import ContrastiveDivergence
from torchebm.datasets import TwoMoonsDataset

class MLPModel(BaseModel):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, 1),
            nn.Tanh(),
        )

    def forward(self, x):
        return self.network(x).squeeze(-1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

INPUT_DIM = 2
HIDDEN_DIM = 16
BATCH_SIZE = 256
EPOCHS = 100
LEARNING_RATE = 1e-3
CD_K = 10
USE_PCD = True

dataset = TwoMoonsDataset(n_samples=3000, noise=0.05, seed=42, device=device)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)

model = MLPModel(INPUT_DIM, HIDDEN_DIM).to(device)
sampler = LangevinDynamics(
    model=model,
    step_size=0.1,
    device=device,
)
loss_fn = ContrastiveDivergence(
    model=model,
    sampler=sampler,
    k_steps=CD_K,
    persistent=USE_PCD,
    buffer_size=BATCH_SIZE
).to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

losses = []
print("Starting training...")
for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0.0

    for i, data_batch in enumerate(dataloader):
        optimizer.zero_grad()

        loss, negative_samples = loss_fn(data_batch)

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        epoch_loss += loss.item()

    avg_epoch_loss = epoch_loss / len(dataloader)
    losses.append(avg_epoch_loss)
    print(f"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_epoch_loss:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('docs/assets/images/loss_functions/cd_training_loss.png')
plt.show()
```

## Choosing the Right Loss Function

Different loss functions are suitable for different scenarios:

- **Contrastive Divergence**: A good general-purpose method, especially for complex energy landscapes.
- **Persistent CD**: Can provide better mixing properties than standard CD, but requires more memory for the replay buffer.
- **Score Matching**: Avoids MCMC sampling but can be numerically unstable and computationally expensive in high dimensions.
- **Sliced Score Matching**: A scalable and more stable version of Score Matching, suitable for high-dimensional data.
- **Denoising Score Matching**: More stable than standard score matching and computationally efficient, making it a good choice for many problems.

## Tips for Stable Training

1. **Regularization**: Add L2 regularization to prevent the energy from collapsing.
1. **Gradient Clipping**: Use `torch.nn.utils.clip_grad_norm_` to prevent unstable updates.
1. **Learning Rate**: Use a small learning rate, especially at the beginning of training.
1. **Sampling Steps**: Increase the number of sampling steps (`k_steps` in CD) for better quality negative samples.
1. **Batch Size**: Use larger batch sizes for more stable gradient estimates.
1. **Parameter Schedulers**: Use schedulers for sampler parameters to improve mixing during MCMC.
1. **Monitor Energy Values**: Ensure the energy values for positive and negative samples do not collapse or diverge.

# Training Energy-Based Models

This guide covers the fundamental techniques for training energy-based models (EBMs) using TorchEBM. We'll explore various training methods, loss functions, and optimization strategies to help you effectively train your models.

## Overview

Training energy-based models involves estimating the parameters of a model such that the corresponding probability distribution matches a target data distribution. Unlike in traditional supervised learning, this is often an unsupervised task where the goal is to learn the underlying structure of the data.

The training process typically involves:

1. Defining a model (parameterized by a neural network or analytical form)
1. Choosing a training method and loss function
1. Optimizing the model parameters
1. Evaluating the model using sampling and visualization techniques

## Defining a Model

In TorchEBM, you can create custom models by subclassing `BaseModel`:

```python
import torch
import torch.nn as nn
from torchebm.core import BaseModel

class MLPModel(BaseModel):
    def __init__(self, input_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SELU(),
            nn.Linear(hidden_dim, 1),
            nn.Tanh(),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x).squeeze(-1)
```

## Training with Contrastive Divergence

Contrastive Divergence (CD) is one of the most common methods for training EBMs. Here's a complete example of training with CD using TorchEBM:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import os

from torchebm.core import BaseModel, CosineScheduler
from torchebm.samplers import LangevinDynamics
from torchebm.losses import ContrastiveDivergence
from torchebm.datasets import TwoMoonsDataset

torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

os.makedirs("training_plots", exist_ok=True)

INPUT_DIM = 2
HIDDEN_DIM = 16
BATCH_SIZE = 256
EPOCHS = 200
LEARNING_RATE = 1e-3

SAMPLER_STEP_SIZE = CosineScheduler(start_value=3e-2, end_value=5e-3, n_steps=100)
SAMPLER_NOISE_SCALE = CosineScheduler(start_value=3e-1, end_value=1e-2, n_steps=100)

CD_K = 10
USE_PCD = True
VISUALIZE_EVERY = 20

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

dataset = TwoMoonsDataset(n_samples=3000, noise=0.05, seed=42, device=device)
real_data_for_plotting = dataset.get_data()
dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
)

model = MLPModel(INPUT_DIM, HIDDEN_DIM).to(device)
sampler = LangevinDynamics(
    model=model,
    step_size=SAMPLER_STEP_SIZE,
    noise_scale=SAMPLER_NOISE_SCALE,
    device=device,
)
loss_fn = ContrastiveDivergence(
    model=model,
    sampler=sampler,
    k_steps=CD_K,
    persistent=USE_PCD,
    buffer_size=BATCH_SIZE,
).to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

losses = []
print("Starting training...")
for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0.0

    for i, data_batch in enumerate(dataloader):
        optimizer.zero_grad()

        loss, negative_samples = loss_fn(data_batch)

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        epoch_loss += loss.item()

    avg_epoch_loss = epoch_loss / len(dataloader)
    losses.append(avg_epoch_loss)
    print(f"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_epoch_loss:.4f}")

    if (epoch + 1) % VISUALIZE_EVERY == 0 or epoch == 0:
        print("Generating visualization...")
        plot_energy_and_samples(
            model=model,
            real_samples=real_data_for_plotting,
            sampler=sampler,
            epoch=epoch + 1,
            device=device,
            plot_range=2.5,
            k_sampling=200,
        )

# Plot the training loss
plt.figure(figsize=(10, 6))
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True, alpha=0.3)
plt.savefig('docs/assets/images/training/cd_training_loss.png')
plt.show()
```

## Visualization During Training

It's important to visualize the model's progress during training. Here's a helper function to plot the energy landscape and samples:

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
from torchebm.core import BaseModel
from torchebm.samplers import LangevinDynamics

@torch.no_grad()
def plot_energy_and_samples(
    model: BaseModel,
    real_samples: torch.Tensor,
    sampler: LangevinDynamics,
    epoch: int,
    device: torch.device,
    grid_size: int = 100,
    plot_range: float = 3.0,
    k_sampling: int = 100,
):
    plt.figure(figsize=(8, 8))

    x_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)
    y_coords = torch.linspace(-plot_range, plot_range, grid_size, device=device)
    xv, yv = torch.meshgrid(x_coords, y_coords, indexing="xy")
    grid = torch.stack([xv.flatten(), yv.flatten()], dim=1)

    model.eval()
    energy_values = model(grid).cpu().numpy().reshape(grid_size, grid_size)

    log_prob_values = -energy_values
    log_prob_values = log_prob_values - np.max(log_prob_values)
    prob_density = np.exp(log_prob_values)

    plt.contourf(
        xv.cpu().numpy(),
        yv.cpu().numpy(),
        prob_density,
        levels=50,
        cmap="viridis",
    )
    plt.colorbar(label="exp(-Energy) (unnormalized density)")

    vis_start_noise = torch.randn(
        500, real_samples.shape[1], device=device
    )
    model_samples_tensor = sampler.sample(x=vis_start_noise, n_steps=k_sampling)
    model_samples = model_samples_tensor.cpu().numpy()

    real_samples_np = real_samples.cpu().numpy()
    plt.scatter(
        real_samples_np[:, 0],
        real_samples_np[:, 1],
        s=10,
        alpha=0.5,
        label="Real Data",
        c="white",
        edgecolors="k",
        linewidths=0.5,
    )
    plt.scatter(
        model_samples[:, 0],
        model_samples[:, 1],
        s=10,
        alpha=0.5,
        label="Model Samples",
        c="red",
        edgecolors="darkred",
        linewidths=0.5,
    )

    plt.xlim(-plot_range, plot_range)
    plt.ylim(-plot_range, plot_range)
    plt.title(f"Epoch {epoch}")
    plt.xlabel("X1")
    plt.ylabel("X2")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f"docs/assets/images/training/ebm_training_epoch_{epoch}.png")
    plt.close()
```

## Training with Score Matching

An alternative to Contrastive Divergence is Score Matching, which doesn't require MCMC sampling:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from torchebm.core import BaseModel
from torchebm.losses import ScoreMatching
from torchebm.datasets import GaussianMixtureDataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = MLPModel(input_dim=2).to(device)
sm_loss_fn = ScoreMatching(
    model=model,
    hessian_method="hutchinson",
    hutchinson_samples=5,
    device=device,
)
optimizer = optim.Adam(model.parameters(), lr=0.001)

dataset = GaussianMixtureDataset(
    n_samples=500, n_components=4, std=0.1, seed=123
).get_data()
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

losses = []
for epoch in range(50):
    epoch_loss = 0.0
    for batch_data in dataloader:
        batch_data = batch_data.to(device)

        optimizer.zero_grad()
        loss = sm_loss_fn(batch_data)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(dataloader)
    losses.append(avg_loss)
    print(f"Epoch {epoch+1}/50, Loss: {avg_loss:.6f}")

# Plot the training loss
plt.figure(figsize=(10, 6))
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Score Matching Training Loss')
plt.grid(True, alpha=0.3)
plt.savefig('docs/assets/images/training/sm_training_loss.png')
plt.show()
```

## Comparing Training Methods

Here's how the major training methods for EBMs compare:

| Method                          | Pros                                                                                                                       | Cons                                                                                                                                   | Best For                                                                        |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |
| **Contrastive Divergence (CD)** | - Simple to implement - Computationally efficient - Works well for simple distributions                                    | - May not converge to true gradient - Limited mode exploration with short MCMC runs - Can lead to poor samples                         | Restricted Boltzmann Machines, simpler energy-based models                      |
| **Persistent CD (PCD)**         | - Better mode exploration than CD - More accurate gradient estimation - Improved sample quality                            | - Requires maintaining persistent chains - Can be unstable with high learning rates - Chains can get stuck in metastable states        | Deep Boltzmann Machines, models with complex energy landscapes                  |
| **Score Matching**              | - Avoids MCMC sampling - Consistent estimator - Stable optimization                                                        | - Requires computing Hessian diagonals - High computational cost in high dimensions - Need for second derivatives                      | Continuous data, models with tractable derivatives                              |
| **Denoising Score Matching**    | - Avoids explicit Hessian computation - More efficient than standard score matching - Works well for high-dimensional data | - Performance depends on noise distribution - Trade-off between noise level and estimation accuracy - May smooth out important details | Image modeling, high-dimensional continuous distributions                       |
| **Sliced Score Matching**       | - Linear computational complexity - No Hessian computation needed - Scales well to high dimensions                         | - Approximation depends on number of projections - Less accurate with too few random projections - Still requires gradient computation | High-dimensional problems where other score matching variants are too expensive |

## Advanced Training Techniques

### Gradient Clipping

Gradient clipping is essential for stable EBM training:

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

### Regularization Techniques

Adding regularization can help stabilize training:

```python
weight_decay = 1e-4
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)

from torch.nn.utils import spectral_norm

class RegularizedMLPModel(BaseModel):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.network = nn.Sequential(
            spectral_norm(nn.Linear(input_dim, hidden_dim)),
            nn.ReLU(),
            spectral_norm(nn.Linear(hidden_dim, hidden_dim)),
            nn.ReLU(),
            spectral_norm(nn.Linear(hidden_dim, 1))
        )

    def forward(self, x):
        return self.network(x).squeeze(-1)
```

## Tips for Successful Training

1. **Start Simple**: Begin with a simple model and dataset, then increase complexity
1. **Monitor Energy Values**: Watch for energy collapse (very negative values) which indicates instability
1. **Adjust Sampling Parameters**: Tune MCMC step size and noise scale for effective exploration
1. **Use Persistent CD**: For complex distributions, persistent CD often yields better results
1. **Visualize Frequently**: Regularly check the energy landscape and samples to track progress
1. **Gradient Clipping**: Always use gradient clipping to prevent explosive gradients
1. **Parameter Scheduling**: Use schedulers for learning rate, step size, and noise scale
1. **Batch Normalization**: Consider adding batch normalization in your energy network
1. **Ensemble Methods**: Train multiple models and ensemble their predictions for better results
1. **Patience**: EBM training can be challenging - be prepared to experiment with hyperparameters
# API Reference

# Torchebm > Core

## Contents

### Modules

- [Base_integrator](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/base_integrator/index.md)
- [Base_interpolant](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/base_interpolant/index.md)
- [Base_loss](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/base_loss/index.md)
- [Base_model](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/base_model/index.md)
- [Base_sampler](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/base_sampler/index.md)
- [Base_scheduler](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/base_scheduler/index.md)
- [Base_trainer](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/base_trainer/index.md)
- [Device_mixin](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/core/device_mixin/index.md)

## API Reference

### torchebm.core

Core functionality for energy-based models, including energy functions, base sampler class, and training utilities.

# Torchebm > Samplers

## Contents

### Modules

- [Flow](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/samplers/flow/index.md)
- [Gradient_descent](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/samplers/gradient_descent/index.md)
- [Hmc](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/samplers/hmc/index.md)
- [Langevin_dynamics](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/samplers/langevin_dynamics/index.md)

## API Reference

### torchebm.samplers

Sampling algorithms for energy-based models and generative models.

Includes:

- MCMC samplers (Langevin dynamics, HMC) for energy-based models
- Gradient-based optimization samplers for energy minimization
- Flow/diffusion samplers for trained generative models

# Torchebm > Losses

## Contents

### Modules

- [Contrastive_divergence](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/losses/contrastive_divergence/index.md)
- [Equilibrium_matching](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/losses/equilibrium_matching/index.md)
- [Loss_utils](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/losses/loss_utils/index.md)
- [Score_matching](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/losses/score_matching/index.md)

## API Reference

### torchebm.losses

Loss functions for training energy-based models and generative models.

# Torchebm > Interpolants

## Contents

### Modules

- [Cosine](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/interpolants/cosine/index.md)
- [Linear](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/interpolants/linear/index.md)
- [Variance_preserving](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/interpolants/variance_preserving/index.md)

## API Reference

### torchebm.interpolants

Stochastic interpolants for generative modeling.

Interpolants define conditional probability paths between source (noise) and target (data) distributions, parameterized by schedules Î±(t) and Ïƒ(t).

# Torchebm > Integrators

## Contents

### Modules

- [Euler_maruyama](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/integrators/euler_maruyama/index.md)
- [Heun](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/integrators/heun/index.md)
- [Integrator_utils](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/integrators/integrator_utils/index.md)
- [Leapfrog](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/integrators/leapfrog/index.md)

## API Reference

### torchebm.integrators

Integrators for solving differential equations in energy-based models.

# Torchebm > Models

## Contents

### Subpackages

- [Components](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/models/components/index.md)

### Modules

- [Conditional_transformer_2d](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/models/conditional_transformer_2d/index.md)
- [Wrappers](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/models/wrappers/index.md)

## API Reference

### torchebm.models

Model namespace.

TorchEBM is designed for plug-and-play experimentation:

- try different losses with the same backbone
- try different backbones with the same loss
- use samplers as long as the model signature matches

This package therefore exposes *reusable building blocks* under `torchebm.models.components` and a small set of generic backbones/wrappers.

# Torchebm > Datasets

## Contents

### Modules

- [Generators](https://soran-ghaderi.github.io/torchebm/0.5.2.dev6/api/torchebm/datasets/generators/index.md)

## API Reference

### torchebm.datasets
